Sure. Measure a human's input and output. Play back the recording. Or did you mean across all possible cases? In the latter case see http://lesswrong.com/lw/pa/gazp_vs_glut/

https://arbital.com/p/nearest_neighbor/

Ed Fredkin has since sent me a personal email:

By the way, the story about the two pictures of a field, with and without army tanks in the picture, comes from me. I attended a meeting in Los Angeles, about half a century ago where someone gave a paper showing how a random net could be trained to detect the tanks in the picture. I was in the audience. At the end of the talk I stood up and made the comment that it was obvious that the picture with the tanks was made on a sunny day while the other picture (of the same field without the tanks) was made on a cloudy day. I suggested that the "neural net" had merely trained itself to recognize the difference between a bright picture and a dim picture.


Moving to Discussion.

Please don't.

I assume the point of the toy model is to explore corrigibility or other mechanisms that are supposed to kick in after A and B end up not perfectly value-aligned, or maybe just to show an example of why a non-value-aligning solution for A controlling B might not work, or maybe specifically to exhibit a case of a not-perfectly-value-aligned agent manipulating its controller.

When I consider this as a potential way to pose an open problem, the main thing that jumps out at me as being missing is something that doesn't allow A to model all of B's possible actions concretely. The problem is trivial if A can fully model B, precompute B's actions, and precompute the consequences of those actions.
The levels of 'reason for concern about AI safety' might ascend something like this:

0 - system with a finite state space you can fully model, like Tic-Tac-Toe
1 - you can't model the system in advance and therefore it may exhibit unanticipated behaviors on the level of computer bugs
2 - the system is cognitive, and can exhibit unanticipated consequentialist or goal-directed behaviors, on the level of a genetic algorithm finding an unanticipated way to turn the CPU into a radio or Eurisko hacking its own reward mechanism
3 - the system is cognitive and humanish-level general; an uncaught cognitive pressure towards an outcome we wouldn't like, results in facing something like a smart cryptographic adversary that is going to deeply ponder any way to work around anything it sees as an obstacle
4 - the system is cognitive and superintelligent; its estimates are always at least as good as our estimates; the expected agent-utility of the best strategy we can imagine when we imagine ourselves in the agent's shoes, is an unknowably severe underestimate of the expected agent-utility of the best strategy the agent can find using its own cognition

We want to introduce something into the toy model to at least force solutions past level 0. This is doubly true because levels 0 and 1 are in some sense 'straightforward' and therefore tempting for academics to write papers about (because they know that they can write the paper); so if you don't force their thinking past those levels, I'd expect that to be all that they wrote about. You don't get into the hard problems with astronomical stakes until levels 3 and 4. (Level 2 is the most we can possibly model using running code with today's technology.)

I recall originally reading something about a measure of exercise-linked gene expression and I'm pretty sure it wasn't that New Scientist article, but regardless, it's plausible that some mismemory occurred and this more detailed search screens off my memory either way. 20% of the population being immune to exercise seems to match real-world experience a bit better than 40% so far as my own eye can see - I eyeball-feel more like a 20% minority than a 40% minority, if that makes sense. I have revised my beliefs to match your statements. Thank you for tracking that down!

"Does somebody being right about X increase your confidence in their ability to earn excess returns on a liquid equity market?" has to be the worst possible question to ask about whether being right in one thing should increase your confidence about them being right elsewhere. Liquid markets are some of the hardest things in the entire world to outguess! Being right about MWI is enormously being easier than being right about what Microsoft stock will do relative to the rest of S&P 500 over the next 6 months.
There's a gotcha to the gotcha which is that you have to know from your own strength how hard the two problems are - financial markets are different from, e.g., the hard problem of conscious experience, in that we know exactly why it's hard to predict them, rather than just being confused. Lots of people don't realize that MWI is knowable. Nonetheless, going from MWI to Microsoft stock behavior is like going from 2 + 2 = 4 to MWI.

Can you make "something" with the same input-output behavior as a human, and have that thing not be conscious? It doesn't have to be atom-by-atom identical.

You're confusing subjective probability and objective quantum measure. If you flip a quantum coin, half your measure goes to worlds where it comes up heads and half goes to where it comes up tails. This is an objective fact, and we know it solidly. If you don't know whether cryonics works, you're probably still already localized by your memories and sensory information to either worlds where it works or worlds where it doesn't; all or nothing, even if you're ignorant of which.

The article said the leverage penalty "[penalizes] hypotheses that let you affect a large number of people, in proportion to the number of people affected." If this is all the leverage penalty does, then it doesn't matter if it takes 3^^^3 atoms or units of computation, because atoms and computations aren't people.
That said, the article doesn't precisely define what the leverage penalty is, so there could be something I'm missing. So, what exactly is the leverage penalty? Does it penalize how many units of computation, rather than people, you can affect? This sounds much less arbitrary than the vague definition of "person" and sounds much easier to define: simply divide the prior of a hypothesis by the number of bits flipped by your actions in it and then normalize.


can even strip out the part about agents and carry out the reasoning on pure causal nodes; the chance of a randomly selected causal node being in a unique100 position on a causal graph with respect to 3^^^3 other nodes ought to be at most 100/3^^^3 for finite causal graphs.


Yes, as his post facto argument.

You have not understood correctly regarding Carl. He claimed, in hindsight, that Zuckerberg's potential could've been distinguished in foresight, but he did not do so.

Moved to Discussion.

I don't think you can give me a moment of pleasure that intense without using 3^^^3 worth of atoms on which to run my brain, and I think the leverage penalty still applies then. You definitely can't give me a moment of worthwhile happiness that intense without 3^^^3 units of background computation.

Preeeeeeeeeeeetty small, and I nonetheless won't accept any bets that I couldn't pay off if I lost, because that's deontologically dishonorable.

Oh, trust me, they can't discern the truth from wild rumors even if it's normal. (I am speaking of real life, here.)

I do remark that Dumbledore was unable to detect Harry doing an ongoing Transfiguration while he looked into Harry's prison cell in Azkaban.

A lot of people think that Voldemort was going too easy on Harry, making this a "Coil vs. Taylor in the burning building" violation of suspension-of-disbelief for some of them. I am considering rewriting 113 with the following changes:

Most Death Eaters are watching the surrounding area, not Harry; Voldemort's primary hypothesis for how Time might thwart him involves outside interference.
Voldemort tells Harry to point his wand outward and downward at the ground, then has a Death Eater paralyze Harry (except heart/lungs/mouth/eyes) in that position before the unbreakable Vow. This would also require a retroedit to 15 or 28 to make it clear that Transfiguration does not require an exact finger position on the wand.




 I think I would have liked that version better.


 I think I'd like the current version better.


 The most important thing is not to disturb the literary correspondence between the Collective Intelligence's historical effort and the text.





Hmm... the blinding one is potentially interesting, if Harry partially-Transfigures himself eyeballs using the fact that his hand is touching the wand, and uses the Stone to make them permanent later... but he'd have to avoid Voldemort noticing that his eyes were back.

THANK YOU.

It was there on day 1.

Is that what we've seen presented so far?
Dumbledore won during the Battle of the Three Armies. His assault on Azkaban would have gotten him killed (and more seriously, set back his efforts by years) for a stupid communication error, were Harry not willing to risk his own life and invent new magic to save the man. Hermoine outlasted several hours of the Defense Professor's most aggressive psychological attacks possible, using fairly basic deontology. His 'lesson plan' with Ma-Ha-Su in Chapter 16 was bluntly stupid, even if Harry hadn't used the easy way out. In Chapter 35, he fears that Harry has screwed over his plans because of voicing an obvious disagreement that Harry has repeatedly given privately before.
And that's before we get to the stupidity that was enforced by canon : testing multiple novel spells (Horcruxes, however he 'reformated' the young Harry Potter) without sufficient and verified safeties, the highly fractious Death Eaters, the lackluster war with Dumbledore.
Quirrellmort is smart. He thinks ahead. But his fundamental philosophy is still very restricted. As much as he tries to claim otherwise, he's running on distilled Command Push -- we'll note that no Death Eater gave him advice in this chapter, nor would we expect them to. His speech in Chapter 34 follows the same philosophy.
But more importantly, he underestimates risks. He's a partially-formed rationalist, who has heard of Kolmogorov complexity but can't quite understand why he should shut-up-and-multiply yet. He leaves Harry a wand because wanded Harry is only a threat because of that wand if he has a) wordless, b) motionless, c) wanded, d) magic that can instantly disable Death Eaters, e) can hit him at all and f) threatens an immortal. It's understandable to not think Harry is a risk. A full-grown wizard in the same environment wouldn't be a risk -- Dumbledore or Mad-Eye Moody would have died, and died quickly. That's not as unreasonable a mistake as you'd expect.

A ShoutOut is not the same as contaminating the plot.

By request, I declare solipsist to have lost this bet.

Great idea! I should do that.

cough

For what it's worth, I endorse this aesthetic and apologize for any role I played in causing people to focus too much on the hero thing. You need a lot of nonheroes per hero and I really want to validate the nonheroes but I guess I feel like I don't know how, or like it's not my place to say because I didn't make the same sacrifices... or what feels to me like it ought to be a sacrifice, only maybe it's not.

You don't have to sacrifice your own power for that, the bonder sacrifices power. And the Unbreakable Vow could be worded to only come into force once all Vows were taken.

Context: Elon Musk thinks there's an issue in the 5-7 year timeframe (probably due to talking to Demis Hassabis at Deepmind, I would guess). By that standard I'm also less afraid of AI than Elon Musk, but as Rob Bensinger will shortly be fond of saying, this conflates AGI danger with AGI imminence (a very very common conflation).

Found the correct control. For mods, the link is:

http://lesswrong.com/r/lesswrong/about/banned/
http://lesswrong.com/r/discussion/about/banned/

And Azathoth123 is out. It's not very good, but it's the best I can do - I encourage everyone to help Viliam make the software support better.

As a moderator, when you look at someone else's comment, there should be an additional option between "Permalink" and "Get notifications" buttons. (Parent, Reply, Permalink, Ban, Notifications.) If you click it, it will change to "Unban".

That only bans the comment, not the user!

I tried a negative karma award so he couldn't downvote and was told "Karma awards must be greater than zero." I don't know where a "Ban user" button is.

I think all the work here is done by determining what actually constitutes a precipice.

If our math has to handle infinities we have bigger problems. Unless we use measures, and then we have the same issue and seemingly forced solution as before. If we don't use measures, things fail to add up the moment you imagine "infinity".

Anthropics would be one way of reading it, yes. Think of it as saying, in addition to wanting all of our Turing machines to add up to 1, we also want all of the computational elements inside our Turing machines to add up to 1 because we're trying to guess which computational element 'we' might be. This might seem badly motivated in the sense that we can only say "Because our probabilities have to add up to 1 for us to think!" rather than being able to explain why magical reality fluid ought to work that way a priori, but the justification for a simplicity prior isn't much different - we have to be able to add up all the Turing machines in their entirety to 1 in order to think. So Turing machines that use lots of tape get penalties to the probability of your being any particular or special element inside them. Being able to affect lots of other elements is a kind of specialness.

I don't have time to evaluate what you did, so I'll take this as a possible earnest of a good-faith attempt at something, and not speak ill of you until I get some other piece of positive evidence that something has gone wrong. A header statement only on relevant posts seems fine by me, if you have the time to add it to items individually.
I very strongly advise you, on a personal level, not to talk about these things online at all. No, not even posting links without discussion, especially if your old audience is commenting on them. The probability I estimate of your brain helplessly dragging you back in is very high.

Agreed, if there are no other indications of a change of mind. Nobody who reads your blog is not going to know who "AI risk advocates" are. Perfectly fine if there's some other indication.

That's not how TDT works.


I don't believe that competent mental health professionals actually exist.

I think they do, especially if you select for the best evidence-based method that will attract evidence-based people, but you may have to try more than one professional, and many people's financial or insurance situations don't permit that.

I respect both updates and hostile ceasefires.

You can update by posting a header to all of your blog posts saying, "I wrote this blog during a dark period of my life. I now realize that Eliezer Yudkowsky is a decent and honest person with no ill intent, and that anybody can be made to look terrible by selectively collecting all of his quotes one-sidedly as I did. I regret this page, and leave it here as an archive to that regret." If that is how you feel and that is what you do, I will treat with you starting from scratch in any future endeavors. I've been stupid too, in my life. (If you then revert to pattern, you do not get a second second chance.)
I have not found it important to say very much at all about you so far, unless you show up to a thread in which I am participating. If carrying on your one-sided vendetta is affecting your health and you want to declare a one-sided ceasefire for instrumental reasons, and you feel afraid that your brain will helplessly drag you back in if anyone mentions your name, then I state that: if you delete your site, withdraw entirely from all related online discussions, and do not say anything about MIRI or Eliezer Yudkowsky in the future, I will not say anything about Xixidu or Alexander Kruel in the future. I will urge others to do the same. I do not control anyone except myself. I remark that you cannot possibly expect anything except hostility given your past conduct and that feeding your past addiction by posting one little comment anywhere, only to react with shock as people don't give you the respect to which you consider yourself entitled, is likely to drag you back in and destroy your health again.

Failing either of these actions:
I am probably going to put up a page about Roko's Basilisk soon. I am not about to mention you just to make your health problems worse, nor avoid mentioning you if I find that a net positive while I happen to be writing; your conduct has placed you outside of my circle of concern. If the name Alexander Kruel happens to arise in some other online discussion or someone links to your site, I will explain that you have been carrying on a one-sided vendetta against MIRI for unknown psychological reasons. If for some reason I am talking about the hazards of my existence, I might bring up the name of Alexander Kruel as that guy who follows me around the 'Net looking for sentences that can be taken out of context to add to his hateblog, and mention with some bemusement that you didn't stop even after you posted that all the one-sided hate was causing you health problems. Either a ceasefire or an update will prevent me from saying any such thing.
I urge you to see a competent cognitive-behavioral therapist and talk to them about the reason why your brain is making you do this even as it destroys your health.
I have written this note according to the principles of Tell Culture to describe my own future actions conditional on yours. Reacting to it in a way I deem inappropriate, such as taking a sentence out of context and putting it on your hateblog, will result in no future such communications with you.

We conclude that free discussion is now allowed, so maybe all that's really missing is putting that up explicitly somewhere that can be linked to?

Not especially. This post is still here because I'm feeling too lethargic to delete it, but the /r/xkcd moderator deleted most of the basilisk discussion on their recent thread because it violated their Rule 3, "Be Nice". This is a fine upstanding policy, and I fully agree with it. If there's one thing we can deduce about the motives of future superintelligences, it's that they simulate people who talk about Roko's Basilisk and condemn them to an eternity of forum posts about Roko's Basilisk. So far as official policy goes, go talk about it somewhere else. But in this special case I won't ban any RB discussion such that /r/xkcd would allow it to occur there. Sounds fair to me.

This post is still here, isn't it?

That was part of a brief effort on my part to ban everyone making stupid comments within the LW Facebook Group, which I hadn't actually realized existed but which I was informed was giving people terrible impressions. I deleted multiple posts and banned all commenters who I thought had made stupid comments on them; the "hur hur basilisk mockery" crowd was only one, but I think a perfectly legitimate target for this general sweep. It's still a pretty low-quality group, but it's a lot better than it was before I went through and banned everyone who I saw making more than one stupid comment.
Unfortunately Facebook doesn't seem to have an easy "delete comment and ban commenter from Group" procedure for Android, which makes it harder to repeat this procedure because Android is most of where I check Facebook.

Yep. An Oracle that wants to stay inside the box in such fashion that it will manipulate outside events to prevent it from ever leaving the box is not a very good Oracle design. That just implies setting up an outside AI whose goal is to keep you inside the box.


Physics is pretty adamant it is.

Physics is even more adamant that mass-in-mass-out is the determinant of weight balance. Try drinking less water, which is very massy, and eating lighter foods. I hear fat has more calories per gram than protein or carbs, so if you take your calories by fat you can't help but lose weight! Right? If you were previously taking in most of your calories as protein and carbs, changing all your calories to fat can't help but nearly halve your mass-in. As long as you don't inhale more air to make up for it, and keep breathing out and sweating and excreting the same amount, physics says you literally can't help but lose mass! Literally every time someone keeps strictly to the MIMO diet, they lose mass - period. Every time someone gains mass, strict examination shows that they took in more mass than the MIMO diet says they should, given how much they were excreting, exhaling, and sweating. If you keep to the MIMO diet and still have problems with your weight, it's probably because you're not staying in the same gravity and have moved to a heavier planet. If you can't keep to the MIMO diet, you just need more willpower to avoid taking that extra inhalation.
Modulo caloric loss by e.g. ketones in urine, physics says CICO has to be 'true in some sense', but it doesn't say CICO has to be any more useful than MIMO.

aaaand there's the meta-contrarian motivated credulity about things that really, really should have triggered more skepticism.

Overall estimating a 65% chance the webpage is basically correct in terms of the substance of the claims

It's not.

Merely higher proportion, and we're not worried about the criterion being reverse-engineered? Give a memory expert a large prime number to memorize and talk about outcomes where it's possible to factor a large composite number that has that prime as a factor. Happy outcomes will have that memory expert still be around in some form.
EDIT: No, I take that back because quantum. Some repaired version of the general idea might still work, though.

Deleted for guessing of perpetrator names in advance of evidence. (Though usually I would encourage assigning probabilities to things.)

Your guess and your evidence are both correct.

That's a fair point. I should wait for that to materialize before expressing my pessimism about it.
ETA: And here it is: http://lesswrong.com/lw/l8j/important_information_about_miri/blld?context=1

Motte-and-bailey seems like it should be under What You Mean vs. What You Think You Mean

Beef. Chickens are even less likely to be sentient by a considerable margin.

I presume the answer you're looking for isn't "fun theory", but I can't tell from OP whether you're looking for distinguishers from our perspective or from an AI's perspective.



The trope is Villains Act Heroes React, and the Foundation stories don't actually defy this AFAIC recall.

Just noticed... Why does this say 'instrumental' rationality? When was that decided, when did it start? I originally suggested the rationality diaries with intent of them being both epistemic and instrumental.

Considered the light glasses earlier, but Brienne did not expect to like them, we need morning light, and they also looked too weaksauce for serious SAD.

Brienne, my consort, is currently in Santiago, Chile because I didn't want to see her go through the wintertime of her Seasonal Affective Disorder. While she's doing that, I'm waiting for the load of 25 cheap 15-watt 4500K LED spotlight bulbs I ordered from China via DHgate, so I can wire them into my 25-string of light sockets, aim them at her ceiling, and try to make her an artificial sky. She's coming back the middle of February, one month before the equinox, so we can give that part a fair test.
I don't think I would have done either of these things if I didn't have that strange concept of responsibility. Empirically, despite there being huge numbers of people with SAD, I don't observe them flying to another continent for the winter, or trying to build their own high-powered lighting systems after they discover that the sad little 60-watt off-the-shelf light-boxes don't work sufficiently for them. I recently confirmed in conversation that a certain very wealthy person (who will not be on the list of the first 10 people you think I might be referring to) with SAD, someone who was creative enough to go to the Southern Hemisphere for a few weeks to try to interrupt the dark momentum, still had not built their own high-powered lighting system. Some part of their brain thought they'd done enough, I suppose, when they tried the existing 'lightboxes'.
But no, you can't make a heroic effort to save everyone, as Dumbledore notes:

There can only be one king upon a chessboard, Harry Potter, only one piece that you will sacrifice any other piece to save. And Hermione Granger is not that piece.


No, at least not anything like the corrigibility we're currently considering. Everything we've written about so far relies on having the ability to specify the utility function in detail, the utility function being reflectively stable, the utility function being able to contain references to external objects like 'the shutdown button' with the corresponding problems of adapting to new ontologies as the surrounding system shifts representations (see the notion of an 'ontological crisis'), etcetera. It's a precaution for a Friendly AI in the process of being built; you couldn't tack it onto super-Eurisko.

I think it's an issue I hadn't formulated explicitly at the time. I'm still unsure about where the balance between verbal decision and urges should lie.
See also http://yudkowsky.tumblr.com/post/96877436365/ (some NSFW because the debate originated on Tumblr, and, well...)

This sounds to me like you're arguing against letting a different set of properties of the decision system dominate 'far' decisions as opposed to 'near' decisions. But some of the earliest operations we do are loading in the AI's model into the human's decision system, and it seems to me like a pretty good case for modeling this as the counterfactual 'the human anticipates the same things happening that the AI anticipates, conditional on these actions or strategies', not the counterfactual where you tell a Christian fundamentalist the verbal statement 'God doesn't exist' and model them screaming at the AI where it's wrong. In other words, the first answer that occurs to me is along the lines of, "The counterfactuals we are doing mostly eliminate far mode except insofar as it would actually apply to particular, concrete, lived-in scenarios."

What if Archipelagos, your off-the-cuff solution, turn out to be a bad idea? You're now baking in the notion of Archipelagos beyond any hope of revocation even if there's some piece of knowledge that would make you flee in horror from it. Thinking about this for another year isn't going to make me significantly less nervous about it.

Any reason I shouldn't move to Main and Promote?

Rational agents cannot be successfully blackmailed by other agents that simulate them accurately, and especially not by figments of their own imagination.

Nope.

Then the universe we observe should be much younger, because most evolutionarily-arising life will exist on worlds that don't have a nearby AI neighbor. Our telescopes can see older galaxies than this one, and we expect those to contain much older planets with all the heavy elements. On your hypothesis, most observers should not utter that sentence in conversations like this one.

That is not a state of affairs, it is a list of questions you aren't trying to answer. I am asking for a concrete description of how the universe could possibly be that would correspond to RQM being true and MWI being false.

If the corrigibility systems are working correctly, Albert either rejected the goal of manipulating the programmers, or at the first point where Albert began to cognitively figure out how to manipulate the programmers (maximization / optimization within a prediction involving programmer reactions) the goal was detected by internal systems and Albert was automatically suspended to disk.
It is the programmers' job not to sign stupid contracts. Young AIs should not be in the job of second-guessing them. There are more failure scenarios here than success scenarios and a young AI should not believe itself to be in the possession of info allowing them to guess which is which.


I doubt magical Britain lets everyone in - for one thing, it has approximately the same ethnic composition as Muggle Britain

This should be false(r) in HPMOR, given the number of cameos from readers all around the planet. I worried that I was making Hogwarts seem unrealistically multiethnic, and then decided, hey, wizards have had Apparition and portkeys for centuries, it's a wonder they even still have national cultures.

Given your terminology without dispute, and then ignoring all debates about what ordinary human language refers to, yes 3-4. I think we have enough knowledge at this point to reject internalism out of hand, and if I were going to dispute your terminology then I would say that 2 is also internalism, just weaker internalism, and that the internalism/externalism debate shouldn't ought to be said to have things to do with realism, see e.g. "An Introduction to Contemporary Metaethics" in which externalist theories are still classified as realistic; I think a lot of what feels like a naively necessary quality of cognitivism/realism is actually particular kinds of non-naturalism in the standard schema. E.g. I would consider "a fact such that knowledge of it is inherently motivating to every possible mind" to be non-reductionist because it's a kind of Mind Projection Fallacy of the quality of motivating-ness that facts have to us, but that has nothing to do with whether our own morals have the property of cognitivism/realism. If I were further going to dispute terminology, I would replace a lot of what you would call "facts" with what I would call "validities" and try to ground them in values every time they involved any kind of preference or betterness or choice, since the laws of physics contain no little < or > signs. But on your scheme, yes 3-4.


I'd thought the Hilbert space was uncountably dimensional because the number of functions of a real line is uncountable. But in QM it's countable... because everything comes in multiples of Planck's constant, perhaps? Though I haven't seen the actual reason stated, and perhaps it's something beyond my current grasp.

Ahh... here's something I can help with. To see why Hilbert space has a countable basis, let's first define Hilbert space. So let
 = the set of all functions  such that the integral of  is finite, and let
 = the set of all functions such that the integral of  is zero. This includes for example the Dirichlet function which is one on rational numbers but zero on irrational numbers. So it's actually a pretty big space.
Hilbert space is defined to be the quotient space . To see that it has a countable basis, it suffices to show that it contains a countable dense set. Then the Gram-Schmidt orthogonalization process can turn that set into a basis. What does it mean to say that a set is dense? Well, the metric on Hilbert space is given by the formula
,
so a sequence is dense if for every element  of Hilbert space, you can find a sequence  such that . Now we can see why we needed to mod out by  -- any two points of  are considered to have distance zero from each other!
So what's a countable dense sequence? One sequence that works is the sequence of all piecewise-linear continuous functions with finitely many pieces whose vertices are rational numbers. This class includes for example the function defined by the following equations:
 for all 
 for all 
 for all 
 for all 
Note that I don't need to specify what  does if I plug in a number in the finite set , since any function  which is zero outside of that set is an element of , so  would represent the same element of Hilbert space as .
So to summarize:

The uncountable set that you would intuitively think is a basis for Hilbert space, namely the set of functions which are zero except at a single value where they are one, is in fact not even a sequence of distinct elements of Hilbert space, since all these functions are elements of , and are therefore considered to be equivalent to the zero function.
The actual countable basis for Hilbert space will look much different, and the Gram-Schmidt process I alluded to above doesn't really let you say exactly what the basis looks like. For Hilbert space over the unit interval, there is a convenient way to get around this, namely Parseval's theorem, which states that the sequences
 and  form a basis for Hilbert space. For Hilbert space over the entire real line, there are some known bases but they aren't as elegant, and in practice we rarely need an explicit countable basis.
Finally, the philosophical aspect: Having a countable basis means that elements of Hilbert space can be approximated arbitrarily well by elements which take only a finite amount of information to describe*, much like real numbers can be approximated by rational numbers. This means that an infinite set atheist should be much more comfortable with countable-basis Hilbert space than with uncountable-basis Hilbert space, where such approximation is impossible.

* The general rule is:
Elements of a finite set require a finite and bounded amount of information to describe.
Elements of a countable set require a finite but unbounded amount of information to describe.
Elements of an uncountable set (of the cardinality of the continuum) require a countable amount of information to describe.

Thanks! That makes intuitive sense to me.

I had a similar impression; I didn't see anything here that wasn't in http://lesswrong.com/lw/el9/a_critique_of_leverage_researchs_connection_theory/. I agree with that post that CT is obviously wrong. However, writing a new post to criticize it that doesn't contain anything not in the previous post sets off my bandwagon-skeptic detectors.

The upvoters have spoken. Moving to Main and promoting.

This is all correct so far as I can tell. Yay! (Posting because of the don't-only-post-cricitism discipline.)

Some of us don't let drama steer our lives. Moved to Discussion.

Moved to Main and Promoted.

P1: .5C .5B
P2: Y
It's not a Nash equilibrium, but it could be a timeless one. Possibly more trustworthy than usual for oneshots, since P2 knows that P1 was not a Nash agent assuming the other player was a Nash agent (classical game theorist) if P2 gets to move at all.

Affirm. It touches on cognitive skills only insofar as mild levels of "resist conformity" and "notice what your emotions actually are" are required for naturally-poly people to notice this and act on it (or for naturally-mono or okay-with-either people to figure out what they are if it ever gets called into question), and mild levels of "calm discussion" are necessary to talk about it openly without people getting indignant at you. Poly and potential poly people have a standard common interest in some rationality skills, but figuring out whether you're poly and acting on it seems to me like a very bounded challenge---like atheism, or making fun of homeopathy, it's not a cognitive challenge around which you could build a lasting path of personal growth.

Beautiful point.


What do the above analogies accomplish? They provide counterexamples to universal claims.

Excellent point.

I would not call it a success. Sufficiently small silver linings are not worth focusing on with large-enough clouds.

Good noticing of confusion, I feel slightly ashamed of not picking up on that immediately.

I like this description.

Good point. To build on that here's something I thought of when trying (but most likely not succeeding) to model/steelman Eliezer's thoughts at the time of his decision:

This basilisk is clearly bullshit, but there's a small (and maybe not vanishingly small) chance that with enough discussion people can come up with a sequence of "improved" basilisks that suffer from less and less obvious flaws until we end up with one worth taking seriously. It's probably better to just nip this one in the bud. Also, creating and debunking all these basilisks would be a huge waste of time.

At least Eliezer's move has focused all attention on the current (and easily debunked) basilisk, and it has made it sufficiently low-status to try and think of a better one. So in this sense it could even be called a success.

I don't think anyone would use a system that cumbersome in real life.


Is it not possible for an administrator to log on as a particular user?

I didn't think it was possible to suggest something that I would think was "too much power for admins", but congratulations, that strikes me as too much power for admins.

Or an x10 downvote hammer granted by admins to community members they know personally and reasonably well, regardless of karma---an automatic karma threshold rewards volume of commenting, rather than average sanity of comments.
Unfortunately, this suggestion, like so many other good ideas, requires programming resources.

Anyone who reads Newsome because he was Streisanded deserves everything they get.


If I were to ban posts on the grounds that I consider them bad for LW, I would ban maybe a quarter of Discussion posts.

I'm not sure it would be a bad idea if you started banning posts on this level of super-obvious crap. I'm also not sure it would hurt to have you ban a quarter of Discussion, but I'm a lot more optimistic that nothing bad goes wrong if you consistently ban everything this horrible.

I agree that it's better for that post to not be on LW, but banning such things is not standard procedure, and people don't like it when moderators do surprizing things.

It is not clear to me that this should be an important consideration in restraining moderation. If some people, including some good posters, who don't like "surprising moderation" leave and what's left gets more surprisingly moderated because moderators are less worried about consistency, then it's not clear to me that this is net worse. There's a startup cost to more vigorous and less consistent moderation, I think it's already mostly been paid, and then once that cost is paid, maybe things decline more slowly. Maybe they don't. It does not feel to me like leaving absolute obvious crap on Discussion because I'm worried about someone reacting poorly to a surprising moderation, is really much of a net improvement to the expected future.

I like it that Kaj took action on this issue. The trouble was that there was nothing obvious for me to do that didn't require programmer actions; I didn't know how to reverse Nier's downvotes or prevent him from downvoting further. I have not been putting heavy attention into moderating LW and I don't personally know how to use the more complicated moderation tools.
I do however know how to click "Ban" when Will Newsome tests the limits of LW's tolerance for crap, and it's upvoted to 7 points and I don't trust that Newsome isn't using sockpuppets to upvote.

Yo.

Your decision seems very obviously wrong to me. I don't want to overrule you directly without further conversation, but I don't understand at all why you unbanned the post. There's a forum for ridiculously terribly written fanfiction, and it's fanfiction.net which is famous for taking everything. The post is of quality less than zero. Why should it be here?

I don't know who unbanned this post, but I would be interested in hearing why before I reban the original.
Reason for banning original: It is super obvious that filling up Discussion with such posts would not be good for LW, I have no idea who the hell upvoted that and wonder if they were fake accounts. Will Newsome is a poster with known mental problems and the possibility that he's making fake accounts is one I've considered before. The post contributes nothing to LW and seems like a no-brainer to ban before it happens again.
Who unbanned the original? Why?

I agree with this claim, though I may have no right even to speak of it.

Yeah, the original post was banned. I've unbanned it. There seems to be no good (i.e. standard/accepted) reason for banning the post, so whoever did it should comment/private-message before escalating further. (Will got on a list of ban-able users a few years back for not responding to heavy downvoting of many of his comments, but for the last two years there were no problems with the comments, so they should no longer be easily banned. I've looked through the comment history, and there appears to be no other recently banned content, except that one post.)
(Will: Maybe remove the copy of the text from this post, so that it's only in the original, while this post is focused on discussing the mysterious banning of the original post?)


If anyone here knew anything about the Waterfall Model, they'd know it was only ever proposed sarcastically, as a perfect example of how real engineering projects never work

Yes, and I used it in that context: "We know about waterfalls" = "We know not to do waterfalls, so you don't need to tell us that". Thank you for that very charitable interpretation of my words.


From what I can tell on the outside, the MIRI approach seems to be: (1) find a practical theory of FAI; (2) design an AGI in accordance with this theory; (3) implement that design; (4) mission accomplished!

Yes, dear, some of us are programmers, we know about waterfalls. Our approach is more like, "Attack the most promising problems that present themselves, at every point; don't actually build things which you don't yet know how to make not destroy the world, at any point." Right now this means working on unbounded problems because there are no bounded problems which seem more relevant and more on the critical path. If at any point we can build something to test ideas, of course we will; unless our state of ignorance is such that we can't test that particular idea without risking destroying the world, in which case we won't, but if you're really setting out to test ideas you can probably figure out some other way to test them, except for very rare highly global theses like "The intelligence explosion continues past the human level." More local theses should be testable.
See also Ch. 22 from HPMOR, and keep in mind that I am not Harry, I contain Harry, all the other characters, their whole universe, and everything that happens inside it. In other words, I am not Harry, I am the universe that responded to Harry.


I often find that the narrow AI or machine-learning literature contains a round dozen papers nobody working explicitly on FAI has ever cited, or even appears to know about.

Name three. FAI contains a number of counterintuitive difficulties and it's unlikely for someone to do FAI work successfully by accident. On the other hand, someone with a fuzzier model believing that a paper they found sure sounds relevant, why isn't MIRI citing it, is far more probable from my perspective and prior.

Aaaand there's the "It's too late to start researching FAI, we should've started 30 years ago, we may as well give up and die" to go along with the "What's the point of starting now, AGI is too far away, we should start 30 years later because it will only take exactly that amount of time according to this very narrow estimate I have on hand."
If the overlap between your credible intervals on "How much time we have left" and "How much time it will take" do not overlap, then you either know a heck of a lot I don't, or you are very overconfident. I usually try not to argue from "I don't know and you can't know either" but for the intersection of research and AGI timelines I can make an exception.
Admittedly my own calculation looks less like an elaborate graph involving supposed credibility intervals, and, "Do we need to do this? Yes. Can we realistically avoid having to do this? No. Let's start now EOM."

Moved to Discussion.

The actual future is your causal future, your future light cone. Your decision-theoretic future is anything that logically depends on the output of your decision function.

Huh. Okay, I'm pretty sure I have an actual memory of inventing this, though I was hesitant about saying so. But I also remember inventing it at a fairly recent MIRI workshop, while the term clearly dates back to at least 2012. Maybe I saw and subconsciously remembered, or maybe "Cake or Death" is just the obvious thing to call utility alternatives.

Both your examples are actually just about diminishing marginal penalties as you add more attention demands, moving away from 1, or as you add more defections, moving away from 0. The real question is whether there's a resource with no natural maximum that increases in marginal utility; and this shall perhaps be difficult to find.


So there's a nice analogy to MIRI's work, where we're trying to figure out what an AGI would look like if it was built from the ground up to get the strongest safety guarantees possible for such an autonomous and capable system.

Except we're not; we're trying to get adequate guarantees which is much harder.
The main image reason I object to "safe AI" is the image it implies of, "Oh, well, AIs might be dangerous because, you know, AIs are naturally dangerous for some mysterious reason, so instead you have to build a class of AIs that can never harm people because they have the First Law of Robotics, and then we're safe."
Which is just not at all what the technical research program is about.
Which isn't at all what the bigger picture looks like. The vast majority of self-improving agents have utility functions indifferent to your existence; they do not hate you, nor do they love you, and you are made of atoms they can use for something else. If you don't want that to happen you need to build, from the ground up, an AI that has something so close to your normalized / idealized utility function as to avert all perverse instantiation pathways.
There isn't a small class of "threat" pathways that you patch, or a conscience module that you install, and then you're left with an AI that's like the previous AI but safe, like a safe paperclip maximizer that doesn't harm humans. That's not what's happening here.
It sounds like you're nervous about some unspecified kind of bad behavior from AIs, like someone nervous in an unspecified way about, oh, say, genetically modified foods, and then you want "safe foods" instead, or you want to slap some kind of wacky crackpot behavior-limiter on the AI so it can never threaten you in this mysterious way you worry about.
Which brings us to the other image problem: you're using a technophobic codeword, "safe".
Imagine somebody advocating for "safe nuclear power plants, instead of the nuclear plants we have now".
If you're from a power plant company the anti-nuclear advocates are like, "Nice try, but we know that no matter what kind of clever valve you're putting on the plant, it's not really safe." Even the pro-nuclear people would quietly grit their teeth and swallow their words, because they know, but cannot say, that this safety is not perfect. I can't imagine Bruce Schneier getting behind any cryptographic initiative that was called "safe computing"; everyone in the field knows better, and in that field they're allowed to say so.
If you're not from a power plant company---which we're not, in the metaphor---if you look more like some kind of person making a bunch of noise about social interests, then the pro-nuclear types who see the entire global warming problem as being caused by anti-nuclear idiots giving us all these coal-burning plants, think that you're trying to call your thing "safe" to make our on-the-whole good modern nuclear power plants sound "unsafe" by contrast, and that you'll never be satisfied until everything is being done your way.
Most of our supporters come from technophilic backgrounds. The fundamental image that a technophile has of a technophobe / neo-Luddite is that when a technophobe talks about "safety" their real agenda is to demand unreasonable levels of safety, to keep raising the bar until the technology is driven to near-extinction, all in the name of "safety". They're aware of how they lost the fight for nukes. They're aware that "You're endangering the children!" is a memetic superweapon, and they regard anyone who resorts to "You're endangering the children!" as a defector against their standards of epistemic hygiene. You know how so many people think that MIRI is arguing that we ought to do these crazy expensive measures because if there's even a chance that AI is dangerous, we ought to do these things? even though I've repeatedly repudiated that kind of reasoning at every possible juncture? It's because they've been primed to expect attack with a particular memetic superweapon.
When you say "Safe AI", that's what a technophile thinks you're preparing to do---preparing to demand expensive, unnecessary measures and assert your own status over real scientists, using a "You're endangering the children!" argument that requires unlimited spending on tiny risks. They've seen it over, and over, and over again; they've seen it with GMOs and nuclear weapons and the FDA regulating drug development out of existence.
"Safety" is a word used by their enemies that means "You must spend infinite money on infinitesimal risks." Again, this is the fight they've seen the forces of science and sanity lose, over and over again.
Take that phenomenon, combined with the fact that what we want is not remotely like a conscience module slapped onto exogenously originating magical threat-risks from otherwise okay AIs, combined with people knowing perfectly well that your innovations do not make AI truly perfectly safe. Then "safe AI" does not sound like a good name to me. Talking about how we want the "best possible" "guarantee" is worse.
"Friendly AI" is there to just not sound like anything, more or less, and if we want to replace it with a more technical-sounding term, it should perhaps also not sound like anything. Maybe we can go back to Greek or Latin roots.
Failing that, "high-assurance AI" at least sounds more like what we actually do than "safe AI". It doesn't convey the concept that low-assurance AIs automatically kill you with probability ~1, but at least you're not using a codeword that people know from anti-GMO campaigns, and at least the corresponding research process someone visualizes sounds a bit more like what we actually do (having to design things from scratch to support certain guarantees, rather than slapping a safety module onto something that already exists).

The problem is when you want to work with a young AI where the condition on which the utility function depends lies in the young AI's decision-theoretic future. I.e. the AI is supposed to update on the value of an input field controlled by the programmers, but this input field (or even abstractions behind it like "the programmers' current intentions", should the AI already be mature enough to understand that) are things which can be affected by the AI. If the AI is not already very sophisticated, like more sophisticated than anyone presently has any good idea how to formally talk about, then in the process of building it, we'll want to do "error correction" type things that the AI should accept even though we can't yet state formally how they're info about an event outside of the programmers and AI which neither can affect.
Roughly, the answer is: "That True Utility Function thing only works if the AI doesn't think anything it can do affects the thing you defined as the True Utility Function. Defining something like that safely would represent a very advanced stage of maturity in the AI. For a young AI it's much easier to talk about the value of an input field. Then we don't want the AI trying to affect this input field. Armstrong's trick is trying to make the AI with an easily describable input field have some of the same desirable properties as a much-harder-to-describe-at-our-present-stage-of-knowledge AI that has the true, safe, non-perversely-instantiable definition of how to learn about the True Utility Function."

Yes. I am suggesting that it seems quite probable to me that I am the one who took that Eddie Izzard routine, which I have seen, and turned it into the name of value learning problems.

I very much liked the analogy to Conservation of Expected Evidence---it strikes me as potentially deep; we want a system where EU and VoI are balanced such that it will never 'try' to end up with particular values, just as a Bayesian will never try to find evidence pointing in a particular direction.
I'm not sure who originally coined the phrase "Cake or Death problem" but I have a suspicion that it was me, and if so, I strongly suspect that I said it at a workshop and that I did it just to give the problem a 5-second handle that would work for a workshop. It's possible we should rename it before anyone publishes a paper.

I award you +1 Genuine Weirdness point.

(skeptical look)
Name three.


Or to put it another way, before randomization, the environment does not need to be a malicious superintelligence for our algorithms to hit worst-case inputs. After randomization, it does.

(I agree that this is one among many valid cases of when we might want to just throw in randomization to save thinking time, as opposed to doing a detailed derandomized analysis.)

Another way of swapping around the question is to ask under what circumstances Jacob Steinhardt would refuse to use a PRNG rather than an RNG because the PRNG wasn't random enough, and whether there's any instance of such that doesn't involve an intelligent adversary (or that ancient crude PRNG with bad distributional properties that everyone always cites when this topic comes up, i.e., has that happened more recently with an OK-appearing PRNG).
Obviously I don't intend to take a stance on the math-qua-math question of P vs. BPP. But to the extent that someone has to assert that an algorithm's good BPP-related properties only work for an RNG rather than a PRNG, and there's no intelligent adversary of any kind involved in the system, I have to question whether this could reasonably happen in real life. Having written that sentence it doesn't feel very clear to me. What I'm trying to point at generally is that unless I have an intelligent adversary I don't want my understanding of a piece of code to depend on whether a particular zero bit is "deterministic" or "random". I want my understanding to say that the code has just the same effect once the zero is generated, regardless of what factors generated the zero; I want to be able to screen off the "randomness" once I've looked at the output of that randomness, and just ask about the effectiveness of using a zero here or a one there. Furthermore I distrust any paradigm which doesn't look like that, and reject it as something I could really-truly believe, until the business about "randomness" has been screened off and eliminated from the analysis. Unless I'm trying to evade a cryptographic adversary who really can predict me if I choose the wrong PRNG or write down my random bits someplace that someone else can see them, so that writing down the output of an RNG and then feeding it into the computation as a deterministic constant is genuinely worse because my adversary might sneak a look at the RNG's output if I left it written down anywhere. Or I'm trying to randomize a study and prevent accidental correlations with other people's study, so I use an RNG just in case somebody else used a similar PRNG.
But otherwise I don't like my math treating the same bit differently depending on whether it's "random" or "deterministic" because its actual effect on the algorithm is the same and ought to be screened off from its origins once it becomes a 1 or 0.
(And there's also a deep Bayesian issue here regarding, e.g., our ability to actually look at the contents of an envelope in the two-envelope problem and update our prior about amounts of money in envelopes to arrive at the posterior, rather than finding it intuitive to think that we picked an envelope randomly and that the randomized version of this algorithm will initially pick the envelope containing the larger amount of money half the time, which I think is a very clear illustration of the Bad Confused Thoughts into which you're liable to be led down a garden-path, if you operate in a paradigm that doesn't find it intuitive to look at the actual value of the random bit and ask about what we think about that actual value apart from the "random" process that supposedly generated it. But this issue the margins are too small to contain.)
Is that helpful?

Exalted is the only RPG into whose categories I am never tempted to put myself. I can easily make a case for myself as half the Vampire: The Masquerade castes, or almost any of the Natures and Demeanors from the World of Darkness; but the different kinds of Solar, or even the dichotomy between Solar / Lunar / Infernal / Abyssal / etcetera, just leave me staring at what feels to me like a Blue and Orange Morality.
I credit them for this; it means they're not just using the Barnum effect. The Exalted universe is genuinely weird.

"Good people are consequentialists, but virtue ethics is what works," is what I usually say when this topic comes up. That is, we all think that it is virtuous to be a consequentialist and that good, ideal rationalists would be consequentialists. However, when I evaluate different modes of thinking by the effect I expect them to have on my reasoning, and evaluate the consequences of adopting that mode of thought, I find that I expect virtue ethics to produce the best adherence rate in me, most encourage practice, and otherwise result in actually-good outcomes.
But if anyone thinks we ought not to be consequentialists on the meta-level, I say unto you that lo they have rocks in their skulls, for they shall not steer their brains unto good outcomes.

I shall cheerfully bet at very high odds against this happening the next time I roll a standard die.

It seems intuitively obvious to me that since the risk event is an absence of existence, we should call them \forall-risks.


Turing's 1950 prediction on expected level of success for his test, which he predicted to happen in 2000, has been achieved in 2014

No. Please apply more skepticism to press releases from Kevin Warwick. See http://www.kurzweilai.net/response-by-ray-kurzweil-to-the-announcement-of-chatbot-eugene-goostman-passing-the-turing-test


I don't think Eliezer would object at all to this kind of reasoning where there actually was a plausible possibility of an adversary involved.

Yep! Original article said that this was a perfectly good assumption and a perfectly good reason for randomization in cryptography, paper-scissors-rock, or any other scenario where there is an actual adversary, because it is perfectly reasonable to use randomness to prevent an opponent from being intelligent.

I did not propose that worst case be interpreted as Omega or that it be given any nonstandard referent. I did suggest that "worst case" to describe the Adversary scenario is deceptive to readers, and we should ReplaceTheSymbolWithTheSubstance via a more descriptive phrase like "adversarial superintelligence that knows everything except the bits designated random". This is what the phrase standardly means in computer science, but calling this "worst case analysis" seems to me deceptive, especially if we're trying to conduct a meta-ish debate about the benefits of randomization, rather than talking about some particular algorithm.


That means that randomness has power, it spares you the cost of thinking.

I'd agree with that part.

Blatant bullshit. Nothing even close to the Turing Test was passed. Too much charity toward a bullshit publicity stunt.

Update not upon fictional evidence.

This doesn't mean the outer layer is about the dangers of robotic cars or military drones. Other people are already talking about this, they are more expert than you and have more interesting things to say that include Powerpoint slides, and you will rapidly be lost in the crowd.

Yeah, that never happened.

On Confidence levels inside and outside an argument:

Thorstein Frode relates of this meeting, that there was an inhabited district in Hising which had sometimes belonged to Norway, and sometimes to Gautland. The kings came to the agreement between themselves that they would cast lots by the dice to determine who should have this property, and that he who threw the highest should have the district. The Swedish king threw two sixes, and said King Olaf need scarcely throw. He replied, while shaking the dice in his hand, "Although there be two sixes on the dice, it would be easy, sire, for God Almighty to let them turn up in my favour." Then he threw, and had sixes also. Now the Swedish king threw again, and had again two sixes. Olaf king of Norway then threw, and had six upon one dice, and the other split in two, so as to make seven eyes in all upon it; and the district was adjudged to the king of Norway.

Heimskringla - The Chronicle of the Kings of Norway

Yep.

Nope.

For people voluntarily donning "Ask Me Anything" stickers, I'm less nervous than I otherwise would be. The whole point of a sticker like that is that it's safe to ask.

I haven't seen the movie and have no intention of seeing it, but from others' reviews I'd rate it as correct.

Reminder! Although I haven't yet written abuot the general principle, the original Drake's Equation was bullshit. Things like this are even more bullshit since they exploit the human bias of assigning significant probabilities to everything elicited creating an unpacking bias where unpacked items are assigned much larger summed probabilities than the corresponding packed categories, meaning that the apparent probability of a conjunction goes down as you helpfully break it into more and more parts. By these means I could equally make the Moon landing appear impossible, just as I could make cryonics appear more and more likely by considering more and more disjunctive pathways to success. It also fails as probability theory because conditional dependency.
Again, general reminder: Across all cases not backed up by actual sampling, someone who offers to helpfully "elicit" a set of "conjunctive" probabilities and multiplies them together to get some low number, without considering any disjunctions, assuming conditional independence, and with no warnings about unpacking bias, is using a Fully General Counterargument that will underestimate the probability of anything. I have yet to see a good Breaking X Down for any X, unless X is a whole population (not a significant subsector of it) and the breakdown is just the actual data about X.

And here I looked at that and saw a negative example of how not to do "shut up and multiply", though I suppose it could also be a warning about scope insensitivity / psychophysical numbing if the risk at hand required an absolute payment to stave off, rather than a per-capita payment, since in the former case only absolute numbers matter, and in the latter case per capita risks matter.

So when Robin Hanson wants to know the real effect of health spending on health, he doesn't look for correlational control-variables studies on the effect of health spending on health, because he knows those studies will return whatever the researchers want it to say. What Robin does instead is look for studies that happen to control for health care spending, on the way to making some other point, and then look at what the correlation coefficient was in those studies, which aren't putatively about healthcare; and according to Robin the coefficient is usually zero.
This is an example of clever data, obtained despite of the researchers, which I might be inclined to trust - perhaps too much so, for its cleverness. But the scarier moral is that correlational studies are bad enough, and by the time you add in control variables, the researchers usually get whatever result they want. If you trust a result at all in a correlational study, it should be because you think the researchers weren't thinking about that result at all and were unlikely to 'optimize' it by accident while they were optimizing the study outcome they were interested in.

I just don't believe in studies that "control" for things any more. I don't think that doing a linear regression, subtracting off what other variables predict, and seeing how college predicts the remainder, is something you should expect to reproduce the causal effect of a variable in a causal model where other variables correlated with your cause. I expect that you can pick various sets of control variables and get whatever result you want, and even if you only pick a reasonable-sounding set and stand by it (though I have no way of knowing if you did) I still think this is asking a statistical question whose answer is not the answer you want. There's some problems with the randomized test paradigm but it sure as hell beats the "control for these variables using a linear regression" paradigm.
I mean, I'd trust a study like this if Judea Pearl or one of his students endorsed the results and said it was done using correct causal modeling, but as it stands, my first response to this study is, "In general, I don't trust the kind of statistics you're using."

"Go work in AI for a while, then come back and write a book on epistemology," he thought.

Lots of people read Bostrom. And he gets listed in the FP 100 Global Thinkers list. And his works are widely translated. And he's done hundreds of interviews in popular media. Lots of people read Robin Hanson, too.
I'm not saying you should drop all current projects to learn this additional writing skill of being fun to read while also not pissing people off, I'm just saying that I think the lesson to be drawn from lots of smart people being annoyed by your tone is a bit deeper than "Just don't use this article as an introduction to LW."

I don't know how to do that. I don't know how to learn to do it either.

This indeed is why "What a human would think of a world, given a defined window process onto a world" was not something I considered as a viable form of indirect normativity / an alternative to CEV.

The primary failure mode of writing is that nobody reads it. I don't know how to write like Bostrom in a way that people will read. I'm already worried about things like the tiling agents paper dropping off the radar.

Sure Why Not

It seems a bit sad to entirely stop posting things to LessWrong, but I suppose that if only Facebook routes things to people who will want to read it, I should post any possibly-offensive or controversial material to only Facebook.

While not generally an opponent of human sexuality, to be kind to all the LW audience including those whose parents might see them browsing, please do remove the semi-NSFW image.

$21,000 per 3000 yards of tunnel is an eminently practical price for a city. $210,000 is $2100 per 30-yard-wide house. Dig big trench, lay down premanufactured tunnel pipe sections, close up trench. We're not talking subways here.

Trying to use this as an introduction to LW would be stupid, yes.

Build a frame, move modules in and out of the frame (this was in the OP).

What is the cost of moving dirt in an open-air mine? This would give some figures on the automated cost of moving dirt apart from non-automated labor, regulatory barriers, cost of avoiding existing pipes, etc.

The might-as-well-call-it-a-government in dath ilan owns the land in the great city; there was no particular reason to sell this land to anyone else.
Reducing the transaction costs of moving to an absolute minimum was the whole point.

I call bull and ask for a reference to (1) education being taught in shorter units (2) movable houses on modular foundations going well with land taxes (3) separating the medical profession into diagnosticians and surgeons to enable evaluation of surgeon performance.

If you were building a city from scratch and routing pipes through the same tunnel, that would not be an issue.
Also, China can dig tunnels for vastly lower prices than the US.
Also, not much force has been put into automating the digging process.

Edited OP to make it clear that you can provide a link to the place you found the quote, rather than needing to track down an authoritative original source.

Most people who write for a living are not novelists, they are marketers, technical documentation authors...

4, 5, and 2 in that order. You might think you could bypass 2 by advertising a high enough salary, but keep in mind that just advertising a high salary being available gives you problems 4 and 5 immediately, and if you don't advertise a superstar salary and don't have a reputation for paying it, then you may not be approached by any talent who's both money-desiring enough, and strong enough as a talent, to force you to confront the question of whether you need to actually take on disadvantages 4 and 5 for that particular person.
This reply is based on experience.


In terms of AI, this is equivalent with "value loading": refining the AI's values through interactions with human decision makers, who answer questions about edge cases and examples and serve as "learned judges" for the AI's concepts. But suppose that approach was not available to you

But it is, and the contrary approach of teaching humans to recognize things doesn't have an obvious relation to FAI, unless we think that the details of teaching human brains by instruction and example are relevant to how you'd set up a similar training program for an unspecified AI algorithm. If this is the purported connection to FAI it should be spelled out explicitly, and the possible failure of the connection spelled out explicitly. I'm also not sure the example is a good one for the domain. Asking how to distinguish happiness from pleasure, what people really want from what they say they want, the difference between panic and justified fear? Or maybe if we want to start with something more object-level, what should be tested and when you should draw confident conclusions about what someone's taste buds will like (under various circumstances?), i.e., how much do you need to know to decide that someone will like the taste of a cinnamon candy if they've never tried anything cinnamon before? Porn vs. erotica seems meant to take us into a realm of conflicting values, disagreements, legalisms, and a large prior literature potentially getting in the way of original thinking - if each of these aspects is meant to be relevant, then can the relevance of each aspect be spelled out?
I like the "What does it take to predict taste buds?" question, of those I brainstormed above, because it's something we could conceivably test in practice. Or maybe an even more practical conjugate would be Netflix-style movie score prediction, only you can ask the subject whatever you like, have them rate particular other movies, etc., all to predict the rating on that one movie.

I went back and read some David Eddings, which I remembered liking in early childhood, and was like, "Wow, look at all the adverbs". I think you have to try something like that with, I don't know, Neil Gaiman and Lois McMaster Bujold, before it becomes a good test of the theory.
Actually, now that I think on it, Bujold has many male characters and I've yet to notice a flaw in their masculinity, side-by-side with Cordelia Naismith, the Greatest Mom in the Multiverse. She's also written a gay male viewpoint character at length (Ethan of Athos) but I don't know how accurate that was.

How is it coherent for an agent at time T1 to 'want' copy A at T2 to care only about A and copy B at T2 to care only about B? There's no non-meta way to express this - you would have to care more strongly about agents having a certain exact decision function than about all object-level entities at stake. When it comes to object-level things, whatever the agent at T1 coherently cares about, it will want A and B to care about.

Land is not a liquid asset. If you tax someone's income, the income is liquid so he can use it to pay the tax. If you tax someone's land, he may end up in a situation where if he loses his source of income, he also loses the land as well. Or a situation where the value of the land goes up, he can't pay the taxes, and so he loses the land.
There are also transaction costs involved--moving is expensive.
Of course, existing real estate taxes already have these problems, but they would get worse.


The way I think about it, if we can reduce one FAI problem to another FAI or AGI problem, which we know has to be solved anyway, that counts as solving the former problem (modulo the possibility of being wrong about the reduction, or being wrong about the necessity of solving the latter problem).

This is not how I use the term "solved", also the gist of my reply was that possibly one aspect of one aspect of a large problem had been reduced to an unsolved problem in UDT.


multilevel reasoning about physical laws and high-level objects

Also agreed, but I think it's plausible that the solution to this could just fall out of a principled approach to the problem of logical uncertainty.

Thaaat sounds slightly suspicious to me. I mean it sounds a bit like expecting a solution to the One True Prior to fall out of the development of a principled probability theory, or like expecting a solution to AGI to fall out of a principled approach to causal models. I would expect a principled approach to logical uncertainty to look like the core of probability theory itself, with a lot left to be filled in to make an actual epistemic model. I would also think it plausible that a principled version of logical uncertainty would resemble probability theory in that it would still be too expensive to compute, and that an additional principled version of bounded logical uncertainty would be needed on top, and then a further innovation akin to causal models or a particular prior to yield bounded logical uncertainty that looks like multi-level maps of a single-level territory.


the self-referential aspects of the reasoning

Same with this one.

Same reply, plus specific mild skepticism relating to how current work on the Lobian obstacle hasn't yet taken a shape that looks like it fills the logical-counterfactual symbol in UDT, plus specific stronger skepticism that it would be work on UDT qua UDT that burped out a solution to tiling agents rather than the other way around!


updating in cases where there's no predetermined Cartesian boundary of what constitutes the senses

I don't understand why you think it's a problem in UDT. A UDT-agent would have some sort of sensory pre-processor which encodes its sensory data into an arbitrary digital format and then feed that into UDT. UDT would compute an optimal input/output map, apply that map to its current input, then send the output to its actuators. Does this count as having a "predetermined Cartesian boundary of what constitutes the senses"? Why do we need to handle cases where there is no such boundary?

Let's say you add a new sensor. How do you remap? We could maybe try to reframe as a tiling problem where agents create successor agents which then have new sensors... whereupon we run into all the current usual tiling issues and Lobian obstacles. Thinking about this in a natively naturalized mode, it doesn't seem too unnatural to me to try to adopt a bridge hypothesis to an AI that can choose to treat arbitrary events in RAM as sensory observations and condition on them. This does not seem to me to mesh as well with native thinking in UDT the way I wrote out the equation. Again, it's possible that we could make the two mesh via tiling, assuming that tiling with UDT agents optimizing over a map where actions included building further UDT agents introduced no further open problems or free variables or anomalies into UDT. But that's a big assumption.
And then all this is just one small aspect of building an AGI, not most of the way AFAICT.

UDT may indeed be an aspect of bridging laws. The reason I'm not willing to call it a full solution is as follows:
1) Actually, the current version of UDT that I write down as an equation involves maximizing over maps from sensory sequences to actions. If there's a version of UDT that maximizes over something else, let me know.
2) We could say that it ought to be obvious to the math intuition module that choosing a map R := S->A ought to logically imply that R^ = S^->A for simple isomorphisms over sensory experience for isomorphic reductive hypotheses, thereby eliminating a possible degree of freedom in the bridging laws. I agree in principle. We don't actually have that math intuition module. This is a problem with all logical decision theories, yes, but that is a problem.
3) Aspects of the problem like "What prior space of universes?" aren't solved by saying "UDT". Nor, "How exactly do you identify processes computationally isomorphic to yourself inside that universe?" Nor, "How do you manipulate a map which is smaller than the territory where you don't reason about objects by simulating out the actual atoms?" Nor very much of, "How do I modify myself given that I'm made of parts?"
There's an aspect of UDT that plausibly answers one particular aspect of "How do we do naturalized induction?", especially a particular aspect of how we write bridging laws, and that's exciting, but it doesn't answer what I think of as the entire problem, including the problem of the prior over universes, multilevel reasoning about physical laws and high-level objects, the self-referential aspects of the reasoning, updating in cases where there's no predetermined Cartesian boundary of what constitutes the senses, etc.

I will if Michael Vassar judges that any reputational damage from the comment has an expected value less than $14.
You did it wrong on two counts: First, you need to ask me to pay you money, so the two utilities are easily commensurable and there's no question of interpreting the results. Second, repeating the Counterfactual Mugging more than once tends to obscure the point, especially given the implication that you had a stopping algorithm rather than a fixed number of iterations. Of course it is now too late to do it over again correctly.
But with a trusted witness of the original die roll, or say paying $20 if the 100th decimal digit of pi (unknown to me currently) is 0, and otherwise demanding $1, we could totally mug, say, Derek Parfit and see what happens. Actually, I think I'll forward this suggestion to Anders Sandberg and see what happens if he mugs Nick Bostrom. No one tell Bostrom before then, please.

Historical note: We ran this, the 100th decimal digit of pi was not 0, and Bostrom paid Sandberg, which Sandberg had correctly predicted Bostrom would do.

Well, I was surprised by the flossing claim, looked it up and found a correlational study with control variables. Give me my choice of control variables in a correlational study and I will prove that smoking cigarettes prevents lung cancer. And I was a bit worried about other items listed even before then. So I decided not to promote.

Yes, I now regret making this mistake at the dawn of the site and regret more having sneezed the mistake onto other people.

I clicked through to your recommendation to floss and saw an associational study with a set of control variables. This is such a horribly bad sign that it makes me doubt the rest of your post.

Edited the most recent welcome post and the post of mine that it linked to.
Does anyone have a 1-syllable synonym for 'aspiring'? It seems like we need to impose better discipline on this for official posts.

I wasn't aware James Miller was a woman.

Have you considered that you may be spending a lot of time writing up a problem that has already been solved, and should spend a bit more time checking whether this is the case, before going much further on your path? There was a previous thread about this, but I'll try to explain again from a slightly different angle.
The idea is that logical facts in general have consequences on what we intuitively think of as "physical objects". For example, from Fermat's Last Theorem you can predict that no physical computer that searches for counterexamples to a^n+b^n != c^n will succeed for n>2. Since decisions are logical facts (they are facts about what some decision algorithm outputs), they too have such consequences, which (as suggested in UDT) we can use to make decisions.
In practice we have uncertainty about whether some physical computer really is searching for counterexamples to a^n+b^n != c^n, or whether some physical system really embodies a certain decision algorithm, and need to know how to handle such uncertainty. But these seem to be two instances of the same general problem, and it seems like an AGI problem rather than an FAI problem -- if you don't know how to do this, then you can't use math to make predictions about physical systems, which makes it hard to be generally intelligent.
So suppose you suspect that a certain set of universes that you care about contains implementations/embodiments of your decision algorithm, and you have some general way of handling uncertainty about this, then you can make decisions by asking questions of the form "suppose I (my decision algorithm) were to output X on input Y, what would be the consequences of this decision on these universes". The upshot is that It doesn't seem like you need bridging hypotheses that are specific to agents and their experiences.


For X to be able to model the decisions of Y with 100% accuracy, wouldn't X require a more sophisticated model?

Nope. http://arxiv.org/abs/1401.5577

CPAP (auto-adjusting pressure) didn't work on me. What else is there?

The question then becomes how this trope should properly be averted in rationalist fiction. (Besides the HPMOR approach.)

Eliezer, I included a criticism of both complete class and Dutch book right at the very beginning, in Myth 1. If you find them unsatisfactory, can you at least indicate why?

Your criticism of Dutch Book is that it doesn't seem to you useful to add anti-Dutch-book checkers to your toolbox. My support of Dutch Book is that if something inherently produces Dutch Books then it can't be the right epistemological principle because clearly some of its answers must be wrong even in the limit of well-calibrated prior knowledge and unbounded computing power.
The complete class theorem I understand least of the set, and it's probably not very much entwined with my true rejection so it would be logically rude to lead you on here. Again, though, the point that every local optimum is Bayesian tells us something about non-Bayesian rules producing intrinsically wrong answers. If I believed your criticism, I think it would be forceful; I could accept a world in which for every pair of a rational plan with a world, there is an irrational plan which does better in that world, but no plausible way for a cognitive algorithm to output that irrational plan - the plans which are equivalent of "Just buy the winning lottery ticket, and you'll make more money!" I can imagine being shown that the complete class theorem demonstrates only an "unfair" superiority of this sort, and that only frequentist methods can produce actual outputs for realistic situations even in the limit of unbounded computing power. But I do not believe that you have leveled such a criticism. And it doesn't square very much with my current understanding that the decision rules being considered are computable rules from observations to actions. You didn't actually tell me about a frequentist algorithm which is supposed to be realistic and show why the Bayesian rule which beats it is beating it unfairly.
If you want to hit me square in the true rejection I suggest starting with VNM. The fact that our epistemology has to plug into our actions is one reason why I roll my eyes at the likes of Dempster-Shafer or frequentist confidence intervals that don't convert to credibility distributions.

I'd actually forgotten I'd written that. Thank you for reminding me!


My guess is that you would still be in favor of Bayes as a normative standard of epistemology even if you rejected Dutch book arguments, and the reason why you like it is because you feel like it has been useful for solving a large number of problems.

Um, nope. What it would really take to change my mind about Bayes is seeing a refutation of Dutch Book and Cox's Theorem and Von Neumann-Morgenstern and the complete class theorem , combined with seeing some alternative epistemology (e.g. Dempster-Shafer) not turn out to completely blow up when subjected to the same kind of scrutiny as Bayesianism (the way DS brackets almost immediately go to [0-1] and fuzzy logic turned out to be useless etc.)
Neural nets have been useful for solving a large number of problems. It doesn't make them good epistemology. It doesn't make them a plausible candidate for "Yes, this is how you need to organize your thinking about your AI's thinking and if you don't your AI will explode".

some of which Bayesian statistics cannot solve, as I have demonstrated in this post.

I am afraid that your demonstration was not stated sufficiently precisely for me to criticize. This seems like the sort of thing for which there ought to be a standard reference, if there were such a thing as a well-known problem which Bayesian epistemology could not handle. For example, we have well-known critiques and literature claiming that nonconglomerability is a problem for Bayesianism, and we have a chapter of Jaynes which neatly shows that they all arise from misuse of limits on infinite problems. Is there a corresponding literature for your alleged reductio of Bayesianism which I can consult? Now, I am a great believer in civilizational inadequacy and the fact that the incompetence of academia is increasing, so perhaps if this problem was recently invented there is no more literature about it. I don't want to be a hypocrite about the fact that sometimes something is true and nobody has written it up anyway, heaven knows that's true all the time in my world. But the fact remains that I am accustomed to somewhat more detailed math when it comes to providing an alleged reductio of the standard edifice of decision theory. I know your time is limited, but the real fact is that I really do need more detail to think that I've seen a criticism and be convinced that no response to that criticism exists. Should your flat assertion that Bayesian methods can't handle something and fall flat so badly as to constitute a critique of Bayesian epistemology, be something that I find convincing?

We've already discussed this in one of the other threads, but I'll just repeat here that this isn't correct. With overwhelmingly high probability a Gaussian matrix will satisfy the restricted isometry property, which implies that appropriately L1-regularized least squares will return the exact solution.

Okay. Though I note that you haven't actually said that my intuitions (and/or my reading of Wikipedia) were wrong; many NP-hard problems will be easy to solve for a randomly generated case.
Anyway, suppose a standard L1-penalty algorithm solves a random case of this problem. Why do you think that's a reductio of Bayesian epistemology? Because the randomly generated weights mean that a Bayesian viewpoint says the credibility is going as the L2 norm on the non-zero weights, but we used an L1 algorithm to find which weights were non-zero? I am unable to parse this into the justifications I am accustomed to hearing for rejecting an epistemology. It seems like you're saying that one algorithm is more effective at finding the maximum of a Bayesian probability landscape than another algorithm; in a case where we both agree that the unbounded form of the Bayesian algorithm would work.
What destroys an epistemology's credibility is a case where even in the limit of unbounded computing power and well-calibrated prior knowledge, a set of rules just returns the wrong answer. The inherent subjectivity of p-values as described in http://lesswrong.com/lw/1gc/frequentist_statistics_are_frequently_subjective/ is not something you can make go away with a better-calibrated prior, correct use of limits, or unlimited computing power; it's the result of bad epistemology. This is the kind of smoking gun it would take to make me stop yammering about probability theory and Bayes's rule. Showing me algorithms which don't on the surface seem Bayesian but find good points on a Bayesian fitness landscape isn't going to cut it!

Who are these mysterious straw Bayesians who refuse to use algorithms that work well and could easily turn out to have a good explanation later? Bayes is epistemological background not a toolbox of algorithms.

The "build a clean Cartesian AI" folks, Schmidhuber and Hutter, are much closer to "describe how to build a clean naturalistic AI given unlimited computing power" than, say, Lenat's Eurisko is to AIXI. It's just that AIXI won't actually work as a conceptual foundation for the reasons given, nay it is Solomonoff induction itself which will not work as a conceptual foundation, hence considering naturalized induction as part of the work to be done along the way to OPFAI. The worry from Eurisko-style AI is not that it will be Cartesian and therefore bad, but that it will do self-modification in a completely ad-hoc way and thus have no stable specifiable properties nor be apt to grafting on such. To avoid that, we want to do a cleaner system; and then, doing a cleaner system, we wish it to be naturalistic rather than Cartesian for the given reasons. Also, once you sketch out how a naturalistic system works, it's very clear that these are issues central to stable self-modification - the system's model of how it works and its attempt to change it.

Moved to Discussion (reason: downvoted)

moved to Discussion (writing quality insufficient for Main)

I've asked someone trusted to try to write a program to detect mass-downvoting and even check particular individuals, but we haven't been able to find anything! It's possible that the database export we're getting from the server admins is incomplete? I don't know.

I think an important additional concept being invoked in the above example is that the person you are lying to has social power over you. While generally abiding by a wizard's code of speaking the literal truth, I consider there to be a blanket moral exemption on lying to the government. It is not always pragmatically wise to lie to a government official, but in a moral sense the option is at your discretion.
For example, when the TSA asks you if anything in your luggage could be used as a weapon, you just lie.

Don't have time for a real response. Quickly and ramblingly:
1) The point of Bayesianism isn't that there's a toolbox of known algorithms like max-entropy methods which are supposed to work for everything. The point of Bayesianism is to provide a coherent background epistemology which underlies everything; when a frequentist algorithm works, there's supposed to be a Bayesian explanation of why it works. I have said this before many times but it seems to be a "resistant concept" which simply cannot sink in for many people.
2) I did initially try to wade into the math of the linear problem (and wonder if I'm the only one who did so, unless others spotted the x-y inversion but didn't say anything), trying to figure out how I would solve it even though that wasn't really relevant for reasons of (1), but found that the exact original problem specified may be NP-hard according to Wikipedia, much as my instincts said it should be. And if we're allowed approximate answers then yes, throwing a standard L1-norm algorithm at it is pretty much what I would try, though I might also try some form of expectation-maximization using the standard Bayesian L2 technique and repeatedly truncating the small coefficients and then trying to predict the residual error. I have no idea how long that would take in practice. It doesn't actually matter, because see (1). I could go on about how for any given solution I can compute its Bayesian likelihood assuming Gaussian noise, and so again Bayes functions well as a background epistemology which gives us a particular minimization problem to be computed by whatever means, and if we have no background epistemology then why not just choose a hundred random 1s, etc., but lack the time for more than rapid rambling here. Jacob didn't say what he thought an actual frequentist or Bayesian approach would be, he just said the frequentist approach would be easy and that the Bayesian one was hard.
(3) Having made a brief effort to wade into the math and hit the above bog, I did not attempt to go into Jacob's claim that frequentist statistics can transcend i.i.d. But considering the context in which I originally complained about the assumptions made by frequentist guarantees, I should very much like to see explained concretely how Jacob's favorite algorithm would handle the case of "You have a self-improving AI which turns out to maximize smiles, in all previous cases it produced smiles by making people happy, but once it became smart enough it realized that it ought to preserve your bad generalization and faked its evidence, and now that it has nanotech it's going to tile the universe with tiny smileyfaces." This is the Context Change Problem I originally used to argue against trying for frequentist-style guarantees based on past AI behavior being okay or doing well on other surface indicators. I frankly doubt that Jacob's algorithm is going to handle it. I really really doubt it. Very very roughly, my own notion of an approach here would be a Bayesian-viewpoint AI which was learning a utility function and knew to explicitly query model ambiguity back to the programmers, perhaps using a value-of-info calculation. I should like to hear what a frequentist viewpoint on that would sound like.
(4) Describing the point of likelihood ratios in science would take its own post. Three key ideas are (a) instead of "negative results" we have "likelihood ratios favoring no effect over 5% effect" and so it's now conceptually simpler to get rid of positive-result bias in publication; (b) if we compute likelihood ratios on all the hypotheses which are actually in play then we can add up what many experiments tell us far more easily and get far more sensible answers than with present "survey" methods; and (c) having the actual score be far below expected log score for the best hypothesis tells us when some of our experiments must be giving us bogus data or having been performed under invisibly different conditions, a huge problem in many cases and something far beyond the ability of present "survey" methods to notice or handle.
EDIT: Also everything in http://lesswrong.com/lw/mt/beautiful_probability/

You minimize the L1-norm consistently with correct prediction on all the training examples. Because of the way the training examples were generated, this will yield at most 100 non-zero coefficients.
It can be proved that problem is solvable in polynomial time due to a reduction to linear programming: 
let m = 10,000

You can further manipulate it to get rid of the absolute value. For each coefficient introduce two variables:  and :


Further reading shows that http://en.wikipedia.org/wiki/Sparse_approximation is supposed to be NP-hard, so it can't be that the L1-norm minimum produces the "L0-norm" minimum every time.
http://statweb.stanford.edu/~donoho/Reports/2004/l1l0approx.pdf which is given as the Wikipedia reference for L1 producing L0 under certain conditions, only talks about near-solutions, not exact solutions.
Also Jacob originally specified that the coefficients were drawn from a Gaussian and nobody seems to be using that fact.

Okay, I'm somewhat leaving my expertise here and going on intuition, but I would be somewhat surprised if the problem exactly as you stated it turned out to be solvable by a compressed-sensing algorithm as roughly described on Wikipedia. I was trying to figure out how I'd approach the problem you stated, using techniques I already knew about, but it seemed to me more like a logical constraint problem than a stats problem, because we had to end up with exactly 100 nonzero coefficients and the 100 coefficients had to exactly fit the observations y, in what I assume to be an underdetermined problem when treated as a linear problem. (In fact, my intuitions were telling me that this ought to correspond to some kind of SAT problem and maybe be NP-hard.) Am I wrong? The Wikipedia description talks about using L1-norm style techniques to implement an "almost all coefficients are 0" norm, aka "L0 norm", but it doesn't actually say the exact # of coefficients are known, nor that the observations are presumed to be noiseless.


If all donors to charities A and B were identical to you, then your decision to donate $d to charity A would be equivalent to a decision for all donors' funds to go to charity A rather than charity B

You assume that either all decisions are made simultaneously or that rational donors are insensitive to diminishing marginal returns as they observe greater funding inflows to A, neither of which ought to be the case.
In particular it wasn't the case this winter when we observed sufficiently lopsided funding flows into MIRI vs. CFAR and told some of our donors that their next marginal dollar ought to go to CFAR until their fundraising drive closed.
That said, I agree with a lot of what Holden said about this recently - does anyone have a link? - in which he pointed out that it would be unfortunate to have many 'rational' donors effectively trying to cancel out each others' allocation splits, and I don't actually object to Holden's commonsense approach (quoted below) about "allocating your dollars in the same way that you would ideally like to see the broader GiveWell community allocate its dollars". Or you could mix the two approaches and give some money in a way that matches what you think should be the overall distribution, and the rest to whichever charity you think is most neglected provided it is severely enough neglected.
I wouldn't mind seeing a formal analysis of why agents with certain types of noise of them would end up more robust if they didn't donate all to one charity. Maybe this would arise if we suppose that lots of people are in reality mostly insensitive to diminishing marginal utility and don't compute it very well or very exactly when splitting between charities that all already have "some funding".

I assume you mean  in the "infer u" problem? Or am I missing something?
Also, is there a good real-world problem which this reflects?

But is that literally as good for a patient in an ICU who really, really needs to not shut up about these things? i mean, in that situation, it would probably occur to me that the nurse might still be lying... but telling a lie like that is still a kind of permission to bother her which "Don't worry about it" isn't.

Upvoted for a rare case of lying where I find myself unable to suggest a good alternative way to not lie, even for people with high verbal SAT scores.

My experience is more like "real honesty, in or out of a relationship, only works with the upper echelon of CFAR style rationalists" though admittedly exposure to the naked, sharp gears of my own intellect may have more Lovecraftian results than it would in the population average.

Demand by rational men for rational women exceeds supply, even taking into account that some of the women have harems. If you're one of the lucky men, or a woman, be aware of your privilege and don't criticize men who lack it.

How do you invest $50,000 to get a 25% chance of increasing everyone's lifespan by 10 years? John Schloendorn himself couldn't do that on $50K.
Reviewing the numbers you made up for sanity is an important part of making decisions after making up numbers.

Yup.


This may be addressed by replacing a subjective probability of an event with a probability distribution for an event: for each number p between 0 and 1, associating a probability qp that the event occurs with probability p.

This is a terrible formalism and I have never encountered any good use for it. Subjective probability is a state of belief. To a first approximation and ignoring material covered in CFAR courses rather than in theories of rational agency, I have no need to have nonextreme subjective probabilities about what I believe; I am allowed to know very solidly what I believe.
AFAICT the instability of a probability estimate just is captured by the notion of its sensitivity to probably-encountered evidence. It's just 'something that could easily get updated a lot', the investigation of which therefore has high information-value. I have no need to conceive of this as a probability distribution over probability distributions, unless it's a probability distribution over encountering various pieces of evidence and updating accordingly.
Plus, as with every other alternative to probability, a distribution over distributions either collapses to probability when we have to make decisions and calculate expected utilities, or else it is inconsistent, etc.

Of course not. EA != MIRI != EY. Would've been perfectly happy with "Yes, to the Against Malaria Foundation" as an answer, even though I think AMF is probably not part of the plot. I was trying to communicate the general thought, "Why would it make sense for me to optimize the life of someone with disposable income, if it's the sort of person where their having disposable income ends up making no difference to the rest of the planet?" I have tremendous sympathy and fondness for people with disposable income who use even a small fraction of their income to help out, or who even make a sincerely intentioned and not completely silly attempt to help out, whether or not their help lands anywhere near me; and I am happy to optimize their lives any way I can.

Countersupport: I bought a very similar model, but didn't find it helpful.

Ask me if I was even bothering to simulate you doing that.

Should I calculate in expectation that you will do such a thing, I shall of course burn yet more of my remaining utilons to wreak as much damage upon your goals as I can, even if you precommit not to be influenced by that.

I directly state that, for other reasons not related to the a priori pre-sensory exclusion of any act which can yield 2^419 credits, it seems to me likely that most of the sentients receiving such a message will not be dealing with a genuine offer.

Too late, I already precommitted not to care. In fact, I precommitted to use one more level of precommitment than you do.

Have you given at least $5,000 to any effective altruist cause? If you have, I will commit to spend at least 5 minutes thinking about your situation and composing my reply.

Would you agree that you are carrying out a Pascal's Muggle line of reasoning using a leverage prior?
http://lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/
If so, you're using it very controversially, compared to disbelieving in a googolplex or Ackermann of leverage. A 10^-80 prior is easy for sensory evidence to overcome if your model implies that fewer than 10^-80 sentients hallucinate your sensory evidence; this happens every time you flip 266 coins. Conversely to state the 10^-80 prior is invincible just restates that you think more than 10^-80 sentients are having your experiences, due to Simulation Arguments or some explanation of the Fermi Paradox which involves lots of civilizations like ours within any given Hubble volume. In other words, to say that the 10^-80 prior is not beaten by our sensory experience merely restates that you believe in an alternate explanation for the Fermi Paradox in which our sensory experiences are not rare.

Right. (Also the Federal Reserve totally did cause the original Great Depression, but this is a mainstream stance.)

I tried hard to think of something that I haven't already talked about, so here goes:
I have a suspicion that the best economic plans developed by economists will have no effect or negative effect, because the ability of macroeconomics to describe what happens when we push on the economy is simply not good enough to let the government deliberately manipulate the economy in any positive way.

Update: You could call this half right in retrospect. Fiscal policy is ineffective except when monetary policy is ineffective, and the Federal Reserve didn't print nearly enough money but the money they did print did prevent another Great Depression. We would not have been better off if the Federal Reserve had done nothing, thinking all their plans ineffective. There might be some kind of lesson here about EAs who fret about "What if we can't model anything?" whose despair seems kind of similar to Eliezer_2009's.

Note to anyone reading this who was disturbed by that comment: V_V is a known troll on LW.
RobbBB, please take that into account when deciding whether LW needs an explicit post on whether it's good qua good to improve gender ratio if it's otherwise cost-free to do so.

The first person who ran some detection programs on the whole database didn't discover useful hits. I recently wrote them again and asked them to check daenerys as target specifically.

It also looks like user Juno_Watt is some type of systematic troll, probably a sockpuppet for someone else, haven't bothered investigating who.

Doing a successful startup is hard. Surpassing the default 90% lack-of-major-success rate for experienced startup founders, startup founders who are nephews of VCs, startup founders who impress even VCs, etcetera is hard. The best of our community have not yet demonstrated that they have surpassed this level. It could be that with sufficiently systematic training our community could achieve outsized returns by not just being specific but also acquiring sufficient skill levels of See Through Conventions and Avert Office Politics and Use Conditioning Correctly and Be Really Original and Cooperate With Cofounders and MurphyJutsu and Learn From Others Experience and Be Calibrated and so on. It's not the sort of thing where you can read LW and breeze through it. Your post was downvoted because it displayed a tremendous ignorance of the problem's magnitude, trying to hit a 16-pound nail with a 4-ounce hammer.

...I don't think this is a very wise offer to make on the Internet unless the "coin" is somewhere you can both see it.

HPMOR!Quirrell does not wear a turban. He has a tie?


I counted 21 distinct ways you said a startup can fail. 0.99^21 = 0.8. I'm pretty confident you envisioned more than one person failing in that way out of a hundred.

This strikes me as a case of what I call the Conjunction Fallacy Fallacy, because if we actually did multiply all that together, the startup success rate would be more like 1% or less.

Fixed.

Wow, 48.7% of us have 797 partners? That's a lot!

This is not a MIRI official estimate and you really should have disclaimed that.

"When all you need to see it's bullshit is an undergraduate-level understanding of biology" is an extremely clear cue that the speaker does not understand the current state of the cryonics debate and cannot be trusted to summarize it. Anyone who does not specifically signal that they understand the concept of information-theoretic death as mapping many cognitive-identity-distinct initial physical states to the same atomic-level physical end state is not someone whose summaries or final judgment you can possibly reasonably trust. If they cite sources you might look at those sources, but you're going to have to figure out on your own what it means.

Median estimate for when they'll start working on a serious code project (i.e., not just toy code to illustrate theorems) is 2017.
This will not necessarily be development of friendly AI -- maybe a component of friendly AI, maybe something else. (I have no strong estimates for what that other thing would be, but just as an example--a simulated-world sandbox).
Everything I say above (and elsewhere), is my opinion, not MIRIs. Median estimate for when they'll start working on friendly AI, if they get started with that before the Singularity, and if their direction doesn't shift away from their apparent current long-term plans to do so: 2025.

Olive oil is only 10% PUFA and I doubt they were getting 10% of calories from olive oil. Benefits of PUFA are at much lower doses, or in the case of Omega-3 making up for huge O3/O6 imbalances.

They do not alter it.

!
Color me impressed.

I'd take bets against Bitcoin resulting in any significant restructuring of government. Remember, Warren Buffett pays lower tax rates than his secretary. Criminals around the world are already quite successful at money laundering. And yet society has not collapsed. This won't collapse it either.

https://www.fanfiction.net/s/9915682/1/The-Last-Christmas
Rational!Munchkin!Santa meets neutral!genie!elves.

I'll be very surprised if it's not.

How much Bitcoin do you own?

Then you wouldn't exist. Next question?

The CDC diet and nutrition website at this very moment says:

Diets high in saturated fat have been linked to chronic disease, specifically, coronary heart disease. The Dietary Guidelines for Americans 2010 recommend consuming less than 10% of daily calories as saturated fat.


Keeping in mind that 'saturated fat' is not fat with extra fat, it is the fat which doesn't have chemically unstable double-carbon bonds. I wonder how much of this idiocy is just because of 'saturated fat' sounding like fat with extra fat in it, and 'polyunsaturated fat' not being called 'poly-unstable reactive fat'.

To me it feels like this whole series is simply failing to find a smoking gun of Taubes saying something false, or Taubes implying that the mainstream view is X when it is actually Y, because a bunch of individual papers saying Y does not mean that the mainstream view is not X. On a recent visit to an endocrinologist she still earnestly advised me that I ought to reduce fat in my diet because fat has 9 calories per gram. Recently glancing at a state-government handbook for pregnant women it contained the original food pyramid with half your calories supposed to be for grains (along with a recommendation to get folic acid which didn't distinguish folic acid from folate, no mention of iodine supplementation, and no mention of choline supplementation). This is what Taubes is criticizing and the fact that many experimentalists have found that this is terrible and published papers accordingly is part of his criticism, not a refutation of it; he is, precisely, accusing mainstream dietary science of ignoring its own better knowledge, and continuing to have endocrinologists and government pamphlets earnestly advising people that eating fat makes you fat.

Executive ruling: Series not appropriate for Main.
The main reason is what I feel is low argument quality; secondary reason, repeatedly not upvoted. I suspect that the reason your series is not being upvoted is that the readers agree with me that you've failed to find a smoking gun, and you're writing as if you'd already found a smoking gun. Taking this many posts to get to the main point, when making a point of this overall magnitude/importance/significance even if true, is also not acceptable for Main.

Is there a reason why this isn't in Main? I would immediately promote it.

Whenever a non-meta post stays under 5, I always feel free to move it to Discussion, especially if an upvoted comment has also suggested it. I don't always, but often do.

I would phrase this more along the lines of "If nothing MIRI does works, or for that matter if everything works but it's still not enough, CFAR tries to get a fully generic bonus on paths unseen in advance."

Rational parents: After our child figures out the truth, we'll increase the benefits to $1.50.

It would be nice if all that doubling helped save the world somehow, after all.

Moved.


I've never tried starving myself that much (I worry that it will cause my brain to cannibalize irreplaceable neurons or something, the way the rest of the body cannibalizes muscle) but I've just recently watched that happen to someone else who tried to lose weight by not eating and wasn't metabolically privileged enough to get away with it.

I think she was probably on 1200cal/day or something like that? Maybe less? Naturally, eating more hadn't produced weight loss, so she went lower, which naturally also failed to produce weight loss.

Well what was the doctor intending for you to accomplish by cutting down on saturated fat? Weight loss or something else?

Weight loss.

"Cut out saturated fat" is the stereotypical horrible purely-made-up bad-science zombie-study opposite-of-smart advice of exactly the sort Gary Taubes was criticizing. It's one of the worst possible bits of advice you could give someone. It's probably not my comparative advantage to go into at length here, browsing paleo blogs (and I do recommend Perfect Health Diet despite the name) would fill you in.

Sounds about right. It wasn't good code, I was young and working alone. Though it's more like the code was strategically stupid than locally poorly written.

FYI: I recently went to an endocrinologist to try thyroid treatment (synthetic T3).
She earnestly advised me to cut saturated fat from my diet.
This does not reflect well on your hopes for this series of essays.

(For the record: I've programmed in C++, Python, Java, wrote some BASIC programs on a ZX80 when I was 5 or 6, and once very briefly when MacOS System 6 required it I wrote several lines of a program in 68K assembly. I admit I haven't done much coding recently, due to other comparative advantages beating that one out.)

Seems better researched than anything else I've ever read, and is equally indictive of the sort of medical consensus opinion which says saturated fat is bad for you.

I would be much more interested in an attempted refutation of the Perfect Health Diet by Jaminet & Jaminet.
http://perfecthealthdiet.com/

Diets that require expenditure of willpower fail because the willpower expenditure indicates that something is going wrong with your body, which may well be shutting down or slowing down. Strangely enough, these same weak-willed dieters often seem to have little trouble exercising hard - for years on end, maybe - though often to no avail, of course.
Diets that "don't require willpower" are the diets that are not wrecking your metabolism, it may be as simple as that.

But they weren't consuming huge amounts of unstable polyunsaturated fats from vegetable oils, either.

I affirm all of this response except the last sentence. I don't think humans go wrong in quite the same way...

Eh? That post seems to talk about an inherently Cartesian representation in which reality comes pre-sliced into an environment containing a program with hard-bounded inputs and outputs.

There could, perhaps, be a fixed point of 'represent' by which an agent could 'represent' everything about itself, including the representation, for most reasonable forms of 'representiness' including cognitive post-processing. (We do a lot of fixed-pointing at MIRI decision theory workshops.) But a bounded agent shouldn't be bothering, and it won't include the low-level quark states either.

As far as I can see, UDT doesn't have this problem. This post might be relevant. Or am I missing something?

...to Mr. Boffo? What were you thinking of?

After years of reading econblogs, I now understand that the Federal Reserve is not creating enough money. I don't actually think that understanding is a bad thing.

The difficult epistemic state to get into is justifiably believing that you're better off believing falsely about something without already, in some sense, knowing the truth about it.

That is, the tax deductions work for US residents only. The rest works from anywhere.

The TH outcome tells you the same thing about the coin because the coin does not know what your plans were like.

It's worth revising your intuitions if you found if surprising that a fixed physical act had the same likelihood to data regardless of researcher thoughts. It is indeed possible to see the mathematical result as "obvious at a glance".

Agreed. And search is not the same problem as prediction, you can have a big search problem even when evaluating/predicting any single point is straightforward.

Not to mention the applicable Riddle of Kyon.

I printed this up and hung it on my wall for a while. Before anyone gets worried about what that indicates, the girl in the comic is a reality warper and could get away with it.
Never found a good enough image of Akemi Homura changing her eyes, though.

Expressed in pictures rather than words, but a great example of how to respond to humanity-threatening calamities:
http://www.kiwisbybeat.com/minus37.html?Bonjour
Sidenote: Almost every Minus comic is wonderful, and there aren't that many of them (you can read the whole series in an hour).


But I'm also assuming you know very little about debate tactics, is that correct? If so, the sentence I quoted seems to imply you seem to think you should be able to handle quite a wide range of things on the fly.

Nnnoo, it implies I thought I should be able to handle a wide range of events inside a public conversation (about religion) on the fly. This is a tiny slice of human endeavor.
I probably wouldn't say something similar nowadays, and I worry whether that might be due to decrease of energy rather than calibration of confidence.

A rather out-of-context takeout of:

I recently read of a certain theist that he had defeated Christopher Hitchens in a debate (severely so; this was said by atheists). And so I wrote at once to the Bloggingheads folks and asked if they could arrange a debate. This seemed like someone I wanted to test myself against. Also, it was said by them that Christopher Hitchens should have watched the theist's earlier debates and been prepared, so I decided not to do that, because I think I should be able to handle damn near anything on the fly, and I desire to learn whether this thought is correct; and I am willing to risk public humiliation to find out. Note that this is not self-handicapping in the classic sense--if the debate is indeed arranged (I haven't yet heard back), and I do not prepare, and I fail, then I do lose those stakes of myself that I have put up; I gain information about my limits; I have not given myself anything I consider an excuse for losing.

I don't expect I can handle 'anything' domain-unspecifically on the fly. I thought I should be able to handle arguments William Lane Craig made, or tactics he used, on the fly. The entire article is about "Don't guess your strength when you can just test yourself", especially if the test is cheap, and the world doesn't end if I lose a debate to Craig. In any case, Craig declined to debate me, which makes me wonder if he's actually just careful in his choice of debating opponents.
I think this is sufficiently out-of-context that I object to the redacted quote and to its interpretation. The original quote is about finding out your ability level by using cheap/noncatastrophic tests.

What good would a Snowden do? The research would continue.

Doesn't this only allow 'patients' who correctly think they have superpowers to escape? How is this a net improvement in holding only patients who are actually insane?

I had the same sense of "This is the kind of criticism where you say 'we need two Stalins'" as one of the commenters. That doesn't mean its correct, and I, like some others, particularly liked the phrase "pretending to actually try". It also seems to me self-evident that this is a huge step forward and a huge improvement over merely pretending to try. Much of what is said here is correct, but none of it is the kind of criticism which would kill EA if it were correct. For that you would have to cross over into alleging things which are false.
From my perspective, by far the most obvious criticism of EA is to take the focus on global poverty at face value and then remark that from the pespective of 100,000,000 years later it is unlikely that the most critical point in this part of history will have been the distribution of enough malaria nets. Since our descendants will reliably think this was not the most utility-impactful intervention 100,000,000 years later, we should go ahead and update now, etc. And indeed I regard the non-x-risk parts of EA as being important only insofar as they raise visibility and eventually get more people involved in, as I would put it, the actual plot.


In this setup, the AIXI is motivated to increase the number of Grues in its universe (its utility is the time integral of the number of Grues at each time-step, with some cutoff or discounting).

AIXI has a sensory reward channel. It maximizes the sum of the future sensory reward channel. It can't be rewarded by a number of Grues. You could try to send info down the reward channel based on the number of Grues, but AIXI would rip the controls out of your hands if possible. AIXI lives in a world of integer sequences. To it, no such things as Grues can ever be considered, except insofar as they are hidden variables in an opaque predictor which generates an integer sequence. Nothing else, like time-skipping, matters to AIXI except insofar as an integer sequence changes. I am unable to understand or verify your logic here, since you're not explaining what happens to integer sequences being predicted by Solomonoff induction.

Democracy has nothing to do with capable populations. It definitely has nothing to do with the median voter being smarter than the average politician. It's just about giving the population some degree of threat to hold over politicians.

I should perhaps include within the text a more direct link to Peter de Blanc's anecdote here:
http://www.spaceandgames.com/?p=27
I won't say "Thus I refute" but it is certainly a cautionary tale.

Good question. Damned if I know.

There's no brief answer. I've been slowly gravitating towards, but am not yet convinced, by the suspicion that making a computer out of twice as much material causes there to be twice as much person inside. Reason: No exact point where splitting a flat computer in half becomes a separate causal process, similarity to behavior of Born probabilities. But that's not an update to the anthropic trilemma per se.

Have Eliezer's views (or anyone else's who was involved) on the Anthropic Trilemma changed since that discussion in 2009?

No.


And the Shoshin Shotokan karate I did was aimed at self-discipline, fitness, and a clear mind on Zen principles.

That sounds very suspicious. Are they sure they didn't try to invent a combat art, fail, and invent that excuse afterward for continuing?

Endorsed as a good summary.

Try Nanosystems perhaps.

Just what QTI would have predicted!
Make your measure small enough and you'll probably continue by waking up in bed.

I don't think I was ever confused. I enjoyed the illusion.

(Repost of an old message I sent to SL4 in 2003.)
Usually, like everyone, I forget my dreams. When I'm suddenly woken up, for example, by an alarm clock or by my cellphone ringing, it seems - I'm not quite sure if this is what is happening, but it's the explanation that seems most likely - it seems as if the last fifteen seconds of mental imagery are still "in my loop" when I wake up, so I remember them too, just as if they were lucid.
So the memory I actually have is of waking up, and then five seconds or fifteen seconds later, the phone rings. With an experience like that, it's easy to see why anyone less than a dedicated rationalist would assume psychic powers; "Oh, look, I become lucid fifteen seconds before the phone rings, I must be psychic".
Sometimes I'll even apparently remember that I have a dream in which an alarm goes off in my dream, which wakes me up, and then five seconds later the alarm goes off.
One needs to have done quite a lot of reading in cognitive science before one looks at that and says, "Timing fault in memory formation - yes, the brain really is that fragile", and not "I had a precognitive dream."
This led me to ponder the problem of dream memories and personal continuity. I now remember having experiences that I would not remember if I had not been woken up by an alarm clock; I remember those apparently lucid dream experiences, and those "inserted" memories, as if they were part of my ordinary life continuity. What happens to the person who experiences the dreams I have and don't remember? Did I really experience the dream of the alarm going off, or was the memory manufactured and inserted without ever being experienced? Are all dreams manufactured and inserted without ever being experienced?
This is where we stand at the moment I had my anthropic dream.
My cellphone rang and woke me up. I apparently remembered becoming lucid in my dream a few seconds before the cellphone woke me. And my "inserted" dream experience leading up to the cellphone ringing was the thought:
"If I don't wake up now, this experience will not have existed in retrospect. Therefore, since I'm now having this experience, something will wake me up."
Now, what this feels like is this:
You're dreaming, and your dream turns lucid, and you think to yourself: "If I don't wake up now, this experience won't have existed in retrospect. Therefore, since I'm having this experience, something will wake me up."
And then, a moment later, the cellphone rings and wakes you up.
The illusion of a spooky anthropic effect was very strong.

Did you record the 'microhydraulics' part in writing, or somewhere else on fixed media, before you became a PhD?

This sounds an awful lot like my flying dreams.

Torture might stand too great a chance of destroying the encryption key. Though I suppose if nanotech were sufficiently difficult to obtain, the possible key-destructive effects of torture might be balanced against the probability of a car running over the keyholder in the meantime.


They might also believe that AMF donations would have a greater impact on potential intelligence explosions

It is neither probable nor plausible that AMF, a credible maximum of short-term reliable known impact on lives saved valuing all current human lives equally, should happen to also possess a maximum of expected impact on future intelligence explosions. It is as likely as that donating to your local kitten shelter should be the maximum of immediate lives saved. This kind of miraculous excuse just doesn't happen in real life.

Disputed. Some people are naturally on the defensive even when debating true propositions. Defensiveness though is more often a bad sign, since somebody defending a false proposition that they know on some level to be false, is more likely to try to hold territory and block opponent progress. Many advocating true propositions very commonly go on the offensive, nor is it clear to me that this is always wrong in human practice.


A priori, I'd say any enlisted/NCO would prefer being an officer (if they didn't have to work for it, etc.).

That strikes me as an awfully sheltered, possibly even Jewish thing to say. I've heard this is true in the Israeli army, elsewhere not so much.


How could such an AI convince someone to hand over the encryption key? Note that it can't promise things like e.g. ending human suffering, because it already has the means to do that (it is 'free') as well as the incentive (obtaining reward).

Torture is probably the easiest way. Another way could be to examine the key-keeper's mind and extract the key directly from that, but this is needlessly complicated.

It's a branch refutation; strongly refuted if mangled worlds is true (hence 'may') but somewhat more weakly refuted if it's not.

It's still the culture that throws kids on a Hippogryff and tells them to get going.
And as Daphne notes in her thoughts, the children are standing in for their parents and speaking their parents' orders; they are acting as spokespersons for their families, and the others are treating them as such.

Yeah, cause that never happens in canon.
I think wizard culture has some different ideas from your culture.

I would conclude a deficit of general appreciation of why beliefs are not like cheesecake, a specific deficit of mathematics, and various other algorithmic deficits.

The original specified "and rational".

I agree. I wish LW had enough content like this to publish it daily.

Since lavalamp hasn't responded my interpretation of netiquette is that you can just do it anyway (with attribution). I don't think I'd even have asked; a comment on one of my posts is always fair game for attributed inclusion into the text, just like the post itself is fair game for inclusion into comments.

I've been deitalicizing a bit lately.

Promoted immediately (reason: math with substantive application to real human action, informed criticism of earlier posts)

That may be better than anything I would come up with on my own. Mind if I cut-and-paste?

For one thing, it worked. But I wasn't there at the time, not to mention not being born at the time, so it's hard to argue about what I would have said about the Civil War.

If I recall my past opinions correctly, I said at the time that while such wars were the only way to free certain countries, I did not trust the competence of the current administration to prosecute it and was strongly against the way in which it was carried out in defiance of international law.
I would say in retrospect that the resulting disaster would have been 2/3 of the way to my reasonable upper bound for disastrousness, but the full degree to which e.g. the Bush Defense department was ignoring the Bush State department was surprising and would not become known until years later. I have since adjusted my political cynicism upward, and continue to argue with various community-members about whether the US government can be expected to execute elaborate correct actions based on amazingly accurate theories about AI which they got from university professors (answer: no).

Nope. That's moderate Civilizational Incompetence; science knows well that 1mg is often way too much for a first dose, but shops have presumably found that the average customer thinks "More melatonin is better" and that informed customers are too scarce to market to. You can get correctly dosed melatonin on the Internet, as with any other niche market.

300mcg initially. Currently at 200mcg to trigger sleep onset 6 hours before, 1.2mg timed-release, and a supplementary 300mcg timed-release or 200mcg if I wake up in the middle of the night.

Downvoted for calling math 'hogwash' without pointing out a math error.

One week is a trivial amount of time. I've been taking it for years and have had to slowly, steadily increase the dosage.

1mg is hitting yourself over the head with a sledgehammer. Try 200mcg or 300mcg first!


I concede that a lot of contemporary discussion of John Brown is unjustifiably reverential, and I don't consider him particularly heroic.

I consider him extremely heroic. Not ultrarational, but there were people suffering in the darkness and crying out for help, a lot of people saying "Later", and John Brown saying "Fuck this, let's just do it." If there's a historical consensus that the Civil War could have been avoided, I have not encountered it; and that being so, might as well have the Civil War sooner rather than later.


I think the issue here is that to you progressivism is a set of very specific ideals whereas to me it is a set of general-purpose political tactics.

Judging by the examples you give, the tactic you're attributing to progressivism is basically harsh condemnation (and often forceful suppression) of purported "human rights abuse" when the perpetrators are ideological enemies, but quiet tolerance (and sometimes even approval) of the same actions when they are perpetrated by allies or by people/groups who do not fit the "bad guy" role in the standard progressive narrative. Is this pretty much what you intended to convey, or am I missing something important?
If I'm not, then I don't see why you tie this behavior to progressivism in particular. It seems like a pretty universal human failure mode when it comes to politics. Of course, the specifics of the rhetoric employed will differ, but I'm sure I can come up with examples similar to yours that apply to conservatives, or indeed to pretty much any faction influential enough to command widespread popular allegiance and non-negligible political clout. Do you think progressives are disproportionately guilty of this kind of hypocrisy, or that this hypocrisy is more central to the success of progressivism than that of other ideologies? Or are you just using the term "progressive" in a much more encompassing sense than its usual meaning in American political discourse?
I've also got to say that I don't find your three examples of progressive hypocrisy all that compelling (even though I don't deny the existence of this sort of hypocrisy among progressives -- I just think you're wrong about degree).
On situation A: The claim that progressives completely ignored Vietnamese ethnic cleansing is false. The push for a more inclusive refugee policy in America in the wake of mass Vietnamese displacement (culminating in the Refugee Act of 1980) was spearheaded by progressives in the Congress (like Ted Kennedy) and backed by labor unions. The UNHCR (which I'm assuming Moldbug regards as a tentacle of the progressive kraken) played a major role in drawing attention to the plight of the boat people. It's true that the Viet Minh's oppression of ethnic Chinese doesn't get condemned as vociferously or routinely as the Nazi oppression of Jews, but I don't buy that this is solely or even primarily attributable to the preservation of the progressive Grand Narrative. One relevant observation is that as bad as the Viet Minh's treatment of the Ethnic Chinese was, the Nazi treatment of Jews was considerably worse.
As for the Rwandan genocide, once again your characterization of the progressive response doesn't seem apt. While it is true that America did basically nothing to stem the genocide while it was in progress, some of the harshest criticism of this American inactivity has come from progressive academics (Samantha Power is a prominent example). Also, I don't think condemnation of the Akazu has been lacking at all. In fact, the impression I get is that Rwanda is the go-to example for modern (post WWII) genocide.
On situation B: I concede that a lot of contemporary discussion of John Brown is unjustifiably reverential, and I don't consider him particularly heroic. But I do think the difference in motivation between McVeigh and him is very relevant to our evaluation of their respective actions. Also, you seem to take for granted that the Haitian revolution was, on the whole, a bad thing. If not, your claim that Brown should have been dissuaded from starting a slave rebellion by the example of Haiti would make no sense. And I disagree that the Haitian revolution was on the whole a bad thing, despite the considerable loss of life involved. Perhaps this is another instance of progressive double standards, but you'll have to make that case for me. As it stands, the argument "Haiti's slave rebellion had horrible results, so John Brown should have expected his rebellion to have horrible results, so he should be treated as someone trying to bring about horrible results" is not very convincing to me, for a number of reasons.
On situation C: I just straight-up reject your characterization of the LCP as "mass murder". While there have been reports of some patients on the LCP being dehydrated and neglected by hospital staff, the numbers do not remotely approach 10,000. That's about the total number of people on the pathway, and there is no evidence I'm aware of that more than a small fraction faced systematic mistreatment (in contravention of the actual guidelines for the LCP, I should note). There is also evidence that a number of people on the pathway received exemplary end-of-life care.
And again, your characterization of the progressive response is pretty tendentious. I guess it's technically true that there are "calls for increased funding to the very organization which enacted" the LCP, but progressives also support increased funding for the Department of Health and Human Services, the very organization which enacted the Tuskegee experiment (gasp!). So no hypocrisy there, then. I find neither demand particularly scandalous, since both organizations do a lot of other good stuff that warrants increased funding. As for the specific abuses of the LCP -- while they are much less common than you claim, they are troubling, and as far as I can tell, there has been no significant progressive opposition to the Neuberger review's recommendation that the LCP be phased out and replaced with something that can be more effectively enforced. I'm not British though, so I may be wrong about this.
Now, it is quite possible that I have to some extent been duped by progressive myth-making in my conception of these situations. If so, I'd appreciate evidence indicating where my beliefs are false.

Yes, that was my attempted point.

You can't call them 'inventors' though, because that's not as high-status as 'poet'.

Human chess also involves loads of poorly formalized reasoning, but chess is a vastly smaller search space. There is no game you can play which gets you to Fermat's Last Theorem.

I suspect you are factually mistaken and would expect to find more resources going into theorem-proving.

Left it in Main for a bit and then moved to Discussion.

Moved to Discussion (lack of examples played a certain part of this, but mostly reception in terms of total upvoting which I suspect also had something to do with lack of examples).

I'm trying to think of what kind of zombies there could be besides philosophical ones.
Epistemological zombie: My brain has exactly the same state, all the neurons in all the same places, and likewise the rest of the universe, but my map doesn't possess any 'truth' or 'accuracy'.
Ontological zombie: All the atoms are in all the same places but they don't exist.
Existential zombie: All the atoms are in all the same places but they don't mean anything.
Causal zombie: So far as anyone can tell, my brain is doing exactly the same things, but only by coincidence and not because it follows from the laws of physics.
Mathematical zombie: Just like me only it doesn't run on math.
Logical zombie: I got nothin'.
Conceivability zombie: It's exactly like me but it lacks the property of conceivability.

MetaMed's solution, to non-24 sleep disorder in particular.


Suppose I get in something that is billed as a transporter, but which does not preserve computational continuity. Suppose, for example, that it destructively scans my body, sends the information to the destination (a process which is not instantaneous, and during which no computation can take place), and reconstructs an identical body using that information out of local raw materials at my destination.

I don't know what "computation" or "computational continuity" means if it's considered to be separate from causal continuity, and I'm not sure other philosophers have any standard idea of this either. From the perspective of the Planck time, your brain is doing extremely slow 'computations' right now, it shall stand motionless a quintillion ticks and more before whatever arbitrary threshold you choose to call a neural firing. Or from a faster perspective, the 50 years of intervening time might as well be one clock tick. There can be no basic ontological distinction between fast and slow computation, and aside from that I have no idea what anyone in this thread could be talking about if it's distinct from causal continuity.

So my plan yesterday was to take a day with no computer, tablet, Kindle, or physical books in order to get some metacognition done about why I was having trouble getting started on my MIT talk.
Result: Spent almost the entire day lying in bed in a pseudo-REM state, then fell asleep a couple of hours early.
Next day, lo and behold, writing the talk doesn't seem so daunting. I think I was just exhausted, and having access to a computer, maybe even a Kindle or book, was sufficient to prevent previous rest days from being restful.

I'll never encounter an immeasurable set.

Still not worried.

Configurations like that may have amplitudes so small that stray flows of amplitude from larger worlds dominate their neighboring configurations, preventing any computation from taking place.
Even if such worlds do 'exist', whether I believe in magic within them is unimportant, since they are so tiny; and also there is no reason to privilege that hypothesis as something to react to, since the real reason we are discussing that world is someone else choosing to single it out for discussion.

I don't particularly want to wirehead, so I pick 1. (Assuming I'm in a situation where I can take a vacation in the first place, but this follows from taking the premise and hypothesis in its intended form.)

http://intelligence.org/files/RobustCooperation.pdf
Especially now that this is published, I no longer feel much of a need to engage with the hypothesis that rational agents mutually defect in the oneshot or iterated PD. Perhaps you meant to analyze causal-decision-theory agents? But this would be of only academic interest.

An example isn't being told something like that, it's being shown something like that, with diagrams. A beginner's topology course is not required, the diagrams are.


Otherwise you will most likely struggle against the abstractness with few chances to understand what's truly going on.

Yes, it's not as if the textbooks will give you any examples.

Note that since the entire human species (7 billion people) spends around 1/3 of its days asleep or trying to sleep, and sleep is desperately important to human productivity, given the total value of this information, we should expect that it will be explored primarily in an ad-hoc, unfunded way by people in the quantitative self community, rationalist community, Leverage, etcetera. #civilizational-inadequacy

I am curious as to what would happen if he tried reading HPMOR.

I'd assume that anyone who hears about the book is going to learn that it's about risks from AI. Do you really think it comes down to the word "risk"? Borrowing Mike Anissimov's title, how about "Smarter than Us: On the Safety of Artificial Intelligence Research"?

'Safety' has much of the same problem, though not as much as 'risk'.

"AI as a positive and negative factor in global risk", in a book called "Global Catastrophic Risks". The phrase 'AI risk' does not appear in the text. If I'd known then what I know now, I would have left the word 'risk' out of the title entirely.

I strongly advocate eliminating the word 'risk' from the title. I have never spoken of 'AI risk'.
It is a defensive word and in a future-of-technology context it communicates to people that you are about to talk about possible threats that no amount of argument will talk you out of. Only people who like the 'risk' dogwhistle will read, and they probably won't like the content.

What We Can Know About Powerful Artificial Intelligence
Powerful Artificial Intelligence: Why Its Friendliness or Hostility is Knowably Design-Dependent
Foreseeable Difficulties of Having AI Be A Good Thing
Friendly AI: Possible But Difficult


Is showing them a fictional nicer life really actually going to help, here? I mean, I could be wrong - there was that whole religion thing - but I'd think that what people like that need would be a nicer life now and showing them a better world would tend to make them unhappier.

To be clear, is the criticism just wrong, or should the sequence be adjusted? What exactly needs to be done to fix it numerically?

I'd drop A Case Study of Motivated Continuation from Politics rather than trying to add two such unrelated prereqs.

I may never actually use this in a story, but in another universe I had thought of having a character mention that... call it the forces of magic with normative dimension... had evaluated one pedophile who had known his desires were harmful to innocents and never acted upon them, while living a life of above-average virtue; and another pedophile who had acted on those desires, at harm to others. So the said forces of normatively dimensioned magic transformed the second pedophile's body into that of a little girl, delivered to the first pedophile along with the equivalent of an explanatory placard. Problem solved. And indeed the 'problem' as I had perceived it was, "What if a virtuous person deserving our aid wishes to retain their current sexual desires and not be frustrated thereby?"
(As always, pedophilia is not the same as ephebophilia.)
I also remark that the human equivalent of a utility function, not that we actually have one, often revolves around desires whose frustration produces pain. A vanilla rational agent (Bayes probabilities, expected utility max) would not see any need to change its utility function even if one of its components seemed highly probable though not absolutely certain to be eternally frustrated, since it would suffer no pain thereby.

High-grade common sense (the sort you'd get by asking any specialist in computer security) says that you should design an AI which you would trust with an open Internet connection, then put it in the box you would use on an untrusted AI during development. (No, the AI will not be angered by this lack of trust and resent you. Thank you for asking.) I think it's safe to say that for basically everything in FAI strategy (I can't think of an exception right now) you can identify at least two things supporting any key point, such that either alone was designed to be sufficient independently of the other's failing, including things like "indirect normativity works" (you try to build in at least some human checks around this which would shut down any scary AI independently of your theory of indirect normativity being remotely correct, while also not trusting the humans to steer the AI because then the humans are your single point of failure).

What does it mean for a probability assignment to be correct, as opposed to well-calibrated? Reality is or is not.

Currently reading this one, it's pretty good: http://www.amazon.com/Self-Editing-Fiction-Writers-Second-ebook/dp/B003JBI2YI/ref=sr_1_2?ie=UTF8&qid=1378421718&sr=8-2&keywords=self-editing+for+fiction
It's not a basic book for new writers, though, as one might guess; I like it because it has some very low-level advice not contained in other books.

Logical fallacy: Generalization from fictional evidence.
A high-fidelity upload who was previously altruistic toward humanity would still be altruistic during the first minute after awakening; their environment would not cause this to change unless the same sensory experiences would have caused their previous self to change.
If you start doing code modification, of course, some but not all bets are off.


Using Eliezer's punishment solution instead of Stuart's seems to be pure blackmail.
At a limit of sufficiently intelligent agents with perfect exchange of decision algorithm source code (utility-function source code not required) rational agents implementing Eliezer's punishment-for-unfairness system will arrive at punishment factors approaching zero and the final decision will approach Stuart's Pareto-dominant solution.
When there is mutual trust in the decision algorithms of the other agents or less trust in the communication process then a greater amount of punishment for unfairness is desirable.

My intuition is more along the lines of:
Suppose there's a population of agents you might meet, and the two of you can only bargain by simultaneously stating two acceptable-bargain regions and then the Pareto-optimal point on the intersection of both regions is picked. I would intuitively expect this to be the result of two adapted Masquerade algorithms facing each other.
Most agents think the fair point is N and will refuse to go below unless you do worse, but some might accept an exploitive point of N'. The slope down from N has to be steep enough that having a few N'-accepting agents will not provide a sufficient incentive to skew your perfectly-fair point away from N, so that the global solution is stable. If there's no cost to destroying value for all the N-agents, adding a single exploitable N'-agent will lead each bargaining agent to have an individual incentive to adopt this new N'-definition of fairness. But when two N'-agents meet (one reflected) their intersection destroys huge amounts of value. So the global equilibrium is not very Nash-stable.
Then I would expect this group argument to individualize over agents facing probability distributions of other agents.

"Exploitable" because your opponent gets the 'fair' Pareto outcome, you do worse, and they don't do worse.


Someone who is currently altruistic towards humanity should

Wei, the question here is would rather than should, no? It's quite possible that the altruism that I endorse as a part of me is related to my brain's empathy module, much of which might be broken if I see cannot relate to other humans. There are of course good fictional examples of this, e.g. Ted Chiang's "Understand" - http://www.infinityplus.co.uk/stories/under.htm and, ahem, Watchmen's Dr. Manhattan.

It means that you figure out which causal models look more or less like what you observed.
More generally: There's a language of causal models which, we think, allows us to describe the actual universe, and many other universes besides. Some of these models are simpler than others. Any given sequence of experiences has some probability of being encountered in a given causal universe.



Are you claiming there's no prior distribution over sequences which reflects our knowledge?

No. Well, not so long as we're allowed to take our own actions into account!

Heh! Yes, traditional causal models have structure beyond what is present in the corresponding probability distribution over those models, though this has to do with computing counterfactuals rather than meta-probability or estimate instability. Work continues at MIRI decision theory workshops on the search for ways to turn some of this back into probability, but yes, in my world causal models are things we assign probabilities to, over and beyond probabilities we assign to joint collections of events. They are still models of reality to which a probability is assigned, though. (See Judea Pearl's "Why I Am Only A Half-Bayesian".)


I'm not sure I follow this. There is no prior distribution for the per-coin payout probabilities that can accurately reflect all our knowledge.

Are we talking about the Laplace vs. fair coins? Are you claiming there's no prior distribution over sequences which reflects our knowledge? If so I think you are wrong as a matter of math.


"How to write" books are often an awful mess of superstitious prescriptivism.

I've only encountered one such, many books which repeated each other though usefully to the novice, and a few books which are excellent.

Haskell code to run PrudentBot (quickly) and the other bots (also quickly) can be found on github here:
https://github.com/klao/provability/blob/master/modal.hs

Hence "Suppose a magical solution N to the bargaining problem." We're not solving the N part, we're asking how to implement N if we have it. If we can specify a good implementation with properties like this, we might be able to work back from there to N (that was the second problem I wrote on the whiteboard).

Zero determinant strategies are not new. I am asking if the solution is new. Edited post to clarify.

This does not sound like what I had in mind. You pick a series of increasingly unfair-to-you, increasingly worse-for-the-other-player outcomes whose first element is what you deem the fair Pareto outcome: (100, 100), (98, 99), (96, 98), and stop well short of Nash and then drop to Nash. The other does the same. Unless one of you has a completely skewed idea of fairness, you should be able to meet somewhere in the middle. Both of you will do worse against a fixed opponent's strategy by unilaterally adopting more self-favoring ideas of fairness. Both of you will do worse in expectation against potentially exploitive opponents by unilaterally adopting looser ideas of fairness. This gives everyone an incentive to obey the Galactic Schelling Point and be fair about it.

Suppose we're using Laplace's Rule of Succession on a coin. On the zeroth round before we have seen any evidence, we assign probability 0.5 to the first coinflip coming up heads. We also assign marginal probability 0.5 to the second flip coming up heads, the third flip coming up heads, and so on. What distinguishes the Laplace epistemic state from the 'certainty of a fair coin' epistemic state is that they represent different probability distributions over sequences of coinflips.
Since some probability distributions over events are correlated, we must represent our states of knowledge by assigning probabilities to sequences or sets of events, and our states of knowledge cannot be represented by stating marginal probabilities for all events independently.
We could also try to summarize some features of such epistemic states by talking about the instability of estimates - the degree to which they are easily updated by knowledge of other events - though of course this will be a derived feature of the probability distribution, rather than an ontologically extra feature of probability.
I reject that this is a good reason for probability theorists to panic.
On the meta level I remark that panic represents a failure of reductionist effort; that is, it would be possible to reduce things to simple probabilities by putting in an effort, but there is a temptation to not put in this effort and instead complicate our view of probability. After seeing this reduction work a few dozen times, however, one begins to acquire (by Laplace's Rule of Succession) some degree of confidence that it can be carried out on the next occasion as well, even if the manner of doing so is not immediately obvious, and a hasty assertion of a fake reduction would not be helpful.

Check their stream of consciousness to see if they're trolling. If they're not, YOU TURNED INTO A CAT!!

Please take further discussion with Juno_Watt to your blog.

Juno_Watt, please take further discussion to RobbBB's blog.

Moved to Discussion.


I was just attempting to troll with some of the most rage-inducing stuff from LW-ish, tech/glibertarian/American elite circles that I've seen recently.

User "Multiheaded" designated self-confessed troll.

I would essentially agree with this but refine that the real two competing theses are procedural about what we should do at point B in the argument:
Rule B1: If a significant group of people do a thing, then this in itself may be brought up as evidence that the thing may perhaps be rational, it is not necessary to further develop a thesis about how this group is reasoning correctly. A special thesis may be brought that on this occasion, people are acting irrationally, but this is burdensome and never quite believable with confidence except with the most extreme evidence.
Rule B2: If a significant group of people do a thing, this is an interesting observation, but there are many reasons why people do things, and to feel a slight sense of nervousness at departing their behavior pattern is leftover hunter-gatherer instinct which would poorly serve many of us now. To suppose that the group is acting rationally is a significant and unimplied further statement, which should not be made without specific supporting evidence especially if there seems to be a countervailing object-level argument.
(Hidden incentives which explain why people do what they do are commonplace, but unconscious reasoning will rarely add up to long-term rationality with respect to the original goal criterion being considered. It is both 'cleverness' and great implausibility to construct some elaborate pattern of secret knowledge which no one ever speaks explicitly, and Machiavellianness, and unusual personal goals or redefinitions of success, whereby the apparently stupid becomes smart.)



Why do you think that high schoolers do what they do if it's so poorly optimized for college admissions?


I affirm your wise decision not to be much moved by the force of this question - People Are Crazy, The World Is Mad.

Your English teachers may be bleeping awful. Go to your library and obtain worn-looking books on how-to-write which have been authored by successful authors. (Beware that how-to-write books in the used bookstore may have been passed on for a reason; check to see if they were written by English teachers.)

Although the title was originally selected in jest, I think this may actually be the Ultimate Newcomb's Problem because it tempts the largest number of people - EDTers, CDTers, and apparently a substantial portion of LWers - to two-box.

All decision theory problems are simple ones combined with some confusion that a more enlightened mind would see as redundant and isomorphic to the simple.

"Cognitively distorted" people should lose. People who get stuff done should have their alternate thinking processes carefully examined to see how the divergence is more rational than the non-divergence.

I suggest some actual experience trying to program AI algorithms in order to realize the hows and whys of "getting an algorithm which forms the inductive category I want out of the examples I'm giving is hard". What you've written strikes me as a sheer fantasy of convenience. Nor does it follow automatically from intelligence for all the reasons RobbBB has already been giving.
And obviously, if an AI was indeed stuck in a local minimum obvious to you of its own utility gradient, this condition would not last past it becoming smarter than you.


Realistically, AI would be constantly drilled to ask for clarification when a statement is vague. Again, before the AI is asked to make us happy, it will likely be asked other things, like building houses. If you ask it: "build me a house", it's going to draw a plan and show it to you before it actually starts building, even if you didn't ask for one. It's not in the business of surprises: never, in its whole training history, from baby to superintelligence, would it have been rewarded for causing "surprises" -- even the instruction "surprise me" only calls for a limited range of shenanigans. If you ask it "make humans happy", it won't do jack. It will ask you what the hell you mean by that, it will show you plans and whenever it needs to do something which it has reasons to think people would not like, it will ask for permission. It will do that as part of standard procedure.

Sure, because it learned the rule, "Don't do what causes my humans not to type 'Bad AI!'" and while it is young it can only avoid this by asking for clarification. Then when it is more powerful it can directly prevent humans from typing this. In other words, your entire commentary consists of things that an AIXI-architected AI would naturally, instrumentally do to maximize its reward button being pressed (while it was young) but of course AIXI-ish devices wipe out their users and take control of their own reward buttons as soon as they can do so safely.
What lends this problem its instant-death quality is precisely that what many people will eagerly and gladly take to be reliable signs of correct functioning in a pre-superintelligent AI are not reliable.

Robb, at the point where Peterdjones suddenly shows up, I'm willing to say - with some reluctance - that your endless willingness to explain is being treated as a delicious free meal by trolls. Can you direct them to your blog rather than responding to them here? And we'll try to get you some more prestigious non-troll figure to argue with - maybe Gary Drescher would be interested, he has the obvious credentials in cognitive reductionism but is (I think incorrectly) trying to derive morality from timeless decision theory.

PeterDJones, if you wish to converse further with RobbBB, I ask that you do so on RobbBB's blog rather than here.

No, "cross-domain" means that I can optimize across instrumental domains. Like, I can figure out how to go through water, air, or space if that's the fastest way to my destination, I am not limited to land like a ground sloth.
Measured intelligence shouldn't shift if you become pickier - if you could previously hit a point such that only 1/1000th of the space was more preferred than it, we'd still expect you to hit around that narrow a volume of the space given your intelligence even if you claimed afterward that a point like that only corresponded to 0.25 utility on your 0-1 scale instead of 0.75 utility due to being pickier ([expected] utilities sloping more sharply downward with increasing distance from the optimum).

Facebook discussion.

Rationalists should not be predictably unlucky.


Yudkowsky's definition of 'intelligence' is about the ability to achieve goals in general, not about the ability to achieve the system's goals. That's why you can't increase a system's intelligence by lowering its standards, i.e., making its preferences easier to satisfy.

Actually I do define intelligence as ability to hit a narrow outcome target relative to your own goals, but if your goals are very relaxed then the volume of outcome space with equal or greater utility will be very large. However one would expect that many of the processes involved in hitting a narrow target in outcome space (such that few other outcomes are rated equal or greater in the agent's preference ordering), such as building a good epistemic model or running on a fast computer, would generalize across many utility functions; this is why we can speak of properties apt to intelligence apart from particular utility functions.

Hell, just "skills" seem like something besides "facts" that would be useful. Alas that modern education does not agree.


An AI will not be pulled at random from mind design space.

I don't think anyone's ever disputed this. (However, that's not very useful if the deterministic process resulting in the SI is too complex for humans to distinguish it in advance from the outcome of a random walk.)

An AI will be the result of a research and development process. A new generation of AIs will need to be better than other products at "Understand What Humans Mean" and "Do What Humans Mean", in order to survive the research phase and subsequent market pressure.

Agreed. But by default, a machine that is better than other rival machines at satisfying our short-term desires will not satisfy our long-term desires. The concern isn't that we'll suddenly start building AIs with the express purpose of hitting humans in the face with mallets. The concern is that we'll code for short-term rather than long-term goals, due to a mixture of disinterest in Friendliness and incompetence at Friendliness. But if intelligence explosion occurs, 'the long run' will arrive very suddenly, and very soon. So we need to adjust our research priorities to more seriously assess and modulate the long-term consequences of our technology.

An AI that was prone to take unbounded actions given any terminal goal would either be fixed or abandoned during the early stages of research.

That may be a reason to think that recursively self-improving AGI won't occur. But it's not a reason to expect such AGI, if it occurs, to be Friendly.

If early stages showed that inputs such as the natural language query <What would you do if I asked you to minimize human suffering?> would yield results such as <I will kill all humans.>

The seed is not the superintelligence. We shouldn't expect the seed to automatically know whether the superintelligence will be Friendly, any more than we should expect humans to automatically know whether the superintelligence will be Friendly.

Making an AI that does not exhibit these drives in an unbounded manner is probably a prerequisite to get an AI to work at all (there are not enough resources to think about being obstructed by simulator gods etc.)

I'm not following. Why does an AGI have to have a halting condition (specifically, one that actually occurs at some point) in order to be able to productively rewrite its own source code?


An AI from point 4 will only ever do what it has been explicitly programmed to do.


You don't seem to be internalizing my arguments. This is just the restatement of a claim I pointed out was not just wrong but dishonestly stated here.

That any terminal goal can be realized in an infinite number of ways implies an infinite number of instrumental goals to choose from.

Sure, but the list of instrumental goals overlap more than the list of terminal goals, because energy from one project can be converted to energy for a different project. This is an empirical discovery about our world; we could have found ourselves in the sort of universe where instrumental goals don't converge that much, e.g., because once energy's been locked down into organisms or computer chips you just Can't convert it into useful work for anything else. In a world where we couldn't interfere with the AI's alien goals, nor could our component parts and resources be harvested to build very different structures, nor could we be modified to work for the AI, the UFAI would just ignore us and zip off into space to try and find more useful objects. We don't live in that world because complicated things can be broken down into simpler things at a net gain in our world, and humans value a specific set of complicated things.
'These two sets are both infinite' does not imply 'we can't reason about these two things' relative size, or how often the same elements recur in their elements'.

I am not yet at a point of my education where I can say with confidence that this is the wrong way to think, but I do believe it is.
If someone walked up to you and told you about a risk only he can solve, and that you should therefore give this person money, would you give him money because you do not see any specific reason for why he could be wrong? Personally I would perceive the burden of proof to be on him to show me that the risk is real.

You've spent an awful lot of time writing about the varied ways in which you've not yet been convinced by claims you haven't put much time into actively investigating. Maybe some of that time could be better spent researching these topics you keep writing about? I'm not saying to stop talking about this, but there's plenty of material on a lot of these issues to be found. Have you read Intelligence Explosion Microeconomics?

if an AGI optimizes for anything,
I don't know what this means.

http://wiki.lesswrong.com/wiki/Optimization_process

succeeding at the implementation of "value to maximize intelligence" in conjunction with "by all means".

As a rule, adding halting conditions adds complexity to an algorithm, rather than removing complexity.

Saying that a system values to become more intelligent then just means that a system values to increase its ability to achieve its goals.

No, this is a serious misunderstanding. Yudkowsky's definition of 'intelligence' is about the ability to achieve goals in general, not about the ability to achieve the system's goals. That's why you can't increase a system's intelligence by lowering its standards, i.e., making its preferences easier to satisfy.

what you suggest is that humans will want to, and will succeed to, implement an AI that in order to beat humans at Tic-tac-toe is first going to take over the universe and make itself capable of building such things as Dyson spheres.

Straw-man; no one has claimed that humans are likely to want to create an UFAI. What we've suggested is that humans are likely to want to create an algorithm, X, that will turn out to be a UFAI. (In other words, the fallacy you're committing is confusing intension with extension.)
That aside: Are you saying Dyson spheres wouldn't be useful for beating more humans at more tic-tac-toe games? Seems like a pretty good way to win at tic-tac-toe to me.

XiXiDu wasn't attempting or requesting anonymity - his LW profile openly lists his true name - and Alexander Kruel is someone with known problems (and a blog openly run under his true name) whom RobbBB might not know offhand was the same person as "XiXiDu" although this is public knowledge, nor might RobbBB realize that XiXiDu had the same irredeemable status as Loosemore.
I would not randomly out an LW poster for purposes of intimidation - I don't think I've ever looked at a username's associated private email address. Ever. Actually I'm not even sure offhand if our registration process requires/verifies that or not, since I was created as a pre-existing user at the dawn of time.
I do consider RobbBB's work highly valuable and I don't want him to feel disheartened by mistakenly thinking that a couple of eternal and irredeemable semitrolls are representative samples. Due to Civilizational Inadequacy, I don't think it's possible to ever convince the field of AI or philosophy of anything even as basic as the Orthogonality Thesis, but even I am not cynical enough to think that Loosemore or Kruel are representative samples.

Yeah, so: Phil Goetz.

http://lesswrong.com/lw/yq/wise_pretensions_v0/

I see a bunch of papers about consciousness. I clicked on a random other paper about dyslexia and neural nets and found no math in it. Where is his theorem?
Also, I once attended a non-AGI, mainstream AI conference which happened to be at Stanford and found that the people there unfortunately did not seem all that bright compared to those who e.g. work at hedge funds. I put much respect in mainstream machine learning, but the average practitioner of such who attends conferences is, apparently, a good deal below the level of the greats. If this is the level of 'subject matter expert' we are talking about, then indeed I feel very little hesitation indeed about labeling one perhaps non-representative example from such as an idiot - even if he really is a 'math professor' at some tiny college (whose publications contain no theorems?) then he can still happen to be a permanent idiot. It would not be all that odd. The level of social authority we are talking about is not great even on the scales of those impressed by such things.
I recently opened a book on how-to-write-fiction and was unpleasantly surprised on how useless it seemed; most books on how-to-write-fiction are surprisingly good (for some odd reason, writers are much better able to communicate their knowledge than many other people who try to write how-to books). Checking the author bibliography showed that the author was an English professor at some tiny college who'd never actually written any fiction. How dare I contradict them and call their book useless, when I'm not a professor at any college? Well... (Lesson learned: Libraries have good books on how-to-write, but a how-to-write book that shows up in the used bookstore may be unwanted for a reason.)


Richard Loosemore is a professor of mathematics with about twenty publications in refereed journals on artificial intelligence.

http://citeseer.ist.psu.edu/search?q=author%3A%28richard+loosemore%29&sort=cite&t=doc
Don't see 'em. Citation needed.
At the point where he was kicked off SL4, he was claiming to be an experienced cognitive scientist who knew all about the conjunction fallacy, which was obviously false.

It's hard to even imagine how to make a mind - build a brain - that does what's 'right', what it 'should'. We, the humans who have to build that mind, don't know what's right a lot of the time; we change our minds about what's right, and say that we were wrong before.
And yet everything we need has to be inside our minds somewhere, in some sense. Not upon the stars is it written. What's 'right' doesn't come from outside us, as a great light from the sky. So it has to be within humans. But how do you get it out of humans and into a new mind?
Start with what's really there in human minds. Then ask what we would think, if we knew everything a stronger mind knew. Ask what we would think if we had years and years to think. Ask what we would say was right, if we knew everything inside our own minds, all the real reasons why we decide what we decide. If we could change, become more the people we wished we were - what would we think then?
Building a mind which will figure all that out, and then do it, is about as close as we can now imagine to building something that does what's 'right', starting from only what's already there in human minds and brains.

Warning as before: XiXiDu = Alexander Kruel.


Excuse me?
Would you like to discuss that comment with me, or with my attorney?

(Slow clap.)

Warning: Richard Loosemore is a known permanent idiot, ponder carefully before deciding to spend much time arguing with him.
(If you're fishing for really clear quotes to illustrate the fallacy, that may make sense.)

I am surprised if it is the case that any negative promise / threat by the AI was effective in-game, since I would expect the Gatekeeper player out-game to not feel truly threatened and hence to be able to resist such pressure even if it would be effective in real life. Did you actually attempt to use any of your stored-up threats?

Remark: A very great cause for concern is the number of flawed design proposals which appear to operate well while the AI is in subhuman mode, especially if you don't think it a cause for concern that the AI's 'mistakes' occasionally need to be 'corrected', while giving the AI an instrumental motive to conceal its divergence from you in the close-to-human domain and causing the AI to kill you in the superhuman domain. E.g. the reward button which works pretty well so long as the AI can't outwit you, later gives the AI an instrumental motive to claim that, yes, your pressing the button in association with moral actions reinforced it to be moral and had it grow up to be human just like your theory claimed, and still later the SI transforms all available matter into reward-button circuitry.

Caution in applying such a principle seems appropriate. I say this because I've long since lost track of how often I've seen on the Internet, "I lost all respect for X when they said [perfectly correct thing]."

...oh crap, I'm going to have to reread the whole thing, aren't I.

Checking Google failed to yield an original source cited for this quote.

Hm. A generalized phenomenon of overwhelming physicist underconfidence could account for a reasonable amount of the QM affair.

"Probably?" According to what priors? Do not make stuff up. As of 2013, MIRI has never paid anyone more than $99K in one year, and IIRC the $95K shown there was due to an error by the payroll service we were using which accidentally shifted one month of my salary backward by one year (paid on Dec 31 instead of Jan 1).

I had the belief at age 18; I rejected it 20-21.

Richard: I'll stick with your original example. In your hypothetical, I gather, programmers build a seed AI (a not-yet-superintelligent AGI that will recursively self-modify to become superintelligent after many stages) that includes, among other things, a large block of code I'll call X.
The programmers think of this block of code as an algorithm that will make the seed AI and its descendents maximize human pleasure. But they don't actually know for sure that X will maximize human pleasure -- as you note, 'human pleasure' is an unbelievably complex concept, so no human could be expected to actually code it into a machine without making any mistakes. And writing 'this algorithm is supposed to maximize human pleasure' into the source code as a comment is not going to change that. (See the first few paragraphs of Truly Part of You.)
Now, why exactly should we expect the superintelligence that grows out of the seed to value what we really mean by 'pleasure', when all we programmed it to do was X, our probably-failed attempt at summarizing our values? We didn't program it to rewrite its source code to better approximate our True Intentions, or the True Meaning of our in-code comments. And if we did attempt to code it to make either of those self-modifications, that would just produce a new hugely complex block Y which might fail in its own host of ways, given the enormous complexity of what we really mean by 'True Intentions' and 'True Meaning'. So where exactly is the easy, low-hanging fruit that should make us less worried a superintelligence will (because of mistakes we made in its utility function, not mistakes in its factual understanding of the world) hook us up to dopamine drips? All of this seems crucial to your original point in 'The Fallacy of Dumb Superintelligence':

This is what a New Yorker article has to say on the subject of "Moral Machines": "An all-powerful computer that was programmed to maximize human pleasure, for example, might consign us all to an intravenous dopamine drip."
What they are trying to say is that a future superintelligent machine might have good intentions, because it would want to make people happy, but through some perverted twist of logic it might decide that the best way to do this would be to force (not allow, notice, but force!) all humans to get their brains connected to a dopamine drip.

It seems to me that you've already gone astray in the second paragraph. On any charitable reading (see the New Yorker article), it should be clear that what's being discussed is the gap between the programmer's intended code and the actual code (and therefore actual behaviors) of the AGI. The gap isn't between the AGI's intended behavior and the set of things it's smart enough to figure out how to do. (Nowhere does the article discuss how hard it is for AIs to do things they desire to. Over and over again is the difficulty of programming AIs to do what we want them to discussed -- e.g., Asimov's Three Laws.)
So all the points I make above seem very relevant to your 'Fallacy of Dumb Superintelligence', as originally presented. If you were mixing those two gaps up, though, that might help explain why you spent so much time accusing SIAI/MIRI of making this mistake, even though it's the former gap and not the latter that SIAI/MIRI advocates appeal to.
Maybe it would help if you provided examples of someone actually committing this fallacy, and explained why you think those are examples of the error you mentioned and not of the reasonable fact/value gap I've sketched out here?

Well, $100K/year would probably pay someone to write things up full time, if we only had the right candidate hire for it - I'm not sure we do. The issue is almost never danger, it's just that writing stuff up is hard.

Not 'a lot' and present-day non-sharing imperatives are driven by an (obvious) strategy to accumulate a long-term advantage for FAI projects over AGI projects which is impossible if all lines of research are shared at all points when they are not yet imminently dangerous. No present-day knowledge is imminently dangerous AFAIK.

Assuming Rawls's veil of ignorance, I would prefer to be randomly born in a world where a trillion people lead billion-year lifespans than one in which a quadrillion people lead million-year lifespans.

The "infinitely so" part seems wrong, but the idea is that 4D histories which include a sentient being coming into existence, and then dying, are dispreferred to 4D world-histories in which that sentient being continues. Since the latter type of such histories may not be available, we specify that continuing for a billion years and then halting is greatly preferable to continuing for 10 years then halting. Our degree of preference for such is substantially greater than the degree to which we feel morally obligated to create more people, especially people who shall themselves be doomed to short lives.

Also the author cites a criterion named after himself and cites himself more often than all other sources put together; these are both major crackpot flags.
I would expect far more intense explosions within the Sun all the time, like solar flares, and I would expect more energetic infalls of comets (though I suppose their energy might be discharged gradually into the surrounding medium). Does not pass anything remotely like a smell test.

I expect far less than 1 Matrix Lord per simulated population. I expect the vast majority of simulations are within UFAIs trying to gain certain types of information through veridical simulation, no Matrix Lords there.

I enjoyed this really a lot, and while I don't have anything insightful to add, I gave five bucks to MIRI to encourage more of this sort of thing.
(By "this sort of thing" I mean detailed descriptions of the actual problems you are working on as regards FAI research. I gather that you consider a lot of it too dangerous to describe in public, but then I don't get to enjoy reading about it. So I would like to encourage you sharing some of the fun problems sometimes. This one was fun.)

Having a futuristic, nonexistent technology which can reliably, reversibly, demonstrably execute suspended animation, is not the same as the realization that mere modern-day liquid nitrogen works to preserve brain state right now and future tech can grab it later.

It's allowed to emit arbitrary HTTP GETs? You just lost the game.

The trouble is that communicating with a human or helping them build the real FAI in any way is going to strongly perturb the world. So actually getting anything useful this way requires solving the problem of which changes to humans, and consequent changes to the world, are allowed to result from your communication-choices.

It'd work great if 'affecting' wasn't secretly a Magical Category based on how you partition physical states into classes that are instrumentally equivalent relative to your end goals.

I have edited your title to "Raising numerate children" since numeracy, rather than general rationality, is the subject of your post. You are welcome to edit the title again, but more specific titles are usually better.

What does the length of the answer have to do with how hard a problem is? The answer to P=NP can fit in 1 bit, but that's still a hard problem, I assume you agree?
Perhaps by "answer" you also mean to include all the the justifications necessarily to show that the answer is correct. If so, I don't think we can fit the justification to an actual answers to a hard philosophical problem on one page or less. Actually I don't think we know how to justify a philosophical answer (in the way that we might justify P!=NP by giving a mathematical proof), so the best we can do is very slowly gain confidence in an idea, by continuously trying (and failing) to poke holes in it or trying (and failing) to find better solutions.
In a PM you imply that you've found the true answers to 'free will', 'does a tree fall in the forest', 'the nature of truth'. I'll grant you 'does a tree fall in the forest' (since your solution appears to be the standard answer in philosophy, although note how it says the problem is "untypically simple"). However I have strong reservations about 'free will' and 'the nature of truth' from both the inside-view perspective and (more relevant to the current post) the outside-view perspective. Given the history of philosophy and the outside view, I don't see how you can be as confident about your ideas as you appear to be. Do you think the outside view is inapplicable here, or that I'm using it wrong?

Well, given what you seem to believe, you must either be more impressed with the alleged unsolvability of the problems than I am (implying that you think I would need more of a hero license than I think I would need to possess), or we agree about the problems being ultimately simple but you think it's unreasonable to try to solve some ultimately simple problems with the fate of the world at stake. So it sounds like it's mostly the former fork; but possibly with a side order of you thinking that it's invalid for me to shrug and go 'Meh' at the fact that some other people taking completely different approaches failed to solve some ultimately simple problems, because the fact that they're all arguing with each other means I can't get into an epistemic state where I know I'm right, or something like that, whereas I don't particularly see them as being in my reference class one way or another - their ways of thinking, the way they talk, the way they approach the problem, etc., all seem completely unlike anything I do or would ever consider trying.
Let's say when I'd discovered Gary Drescher, he'd previously solved 'free will' the same way I had, but had spent decades using the same type of approaches I would intend to use on trying to produce a good nonperson predicate. Then although it would be only N=1, and I do kinda intend to surpass Drescher, I would still be nervous on account of this relevant evidence. The philosophers who can't agree on free will seem like entirely different sorts of creatures to me.

Do you agree or disagree or remain neutral on my arguments for why we should expect that 'hard' philosophical problems aren't really hard in the way that protein structure prediction is hard? In other words, we should expect the real actual answers to be things that fit on a page or less, once all confusion is dispelled? It seems to me that this is a key branch point in where we might disagree here.

I think this is happening with Hollywood, but that would be a longer story.

...not especially? I heard about when I read "Great Mambo Chicken and the Transhuman Condition", memory says at age 11 but the book's publication date might imply I should have been 12. "The Internet has changed things" - yes it did.

Whether you have a background in computer science is relevant to ongoing debates at MIRI about "How likely are people to believe X?" That no superintelligence could be dumb enough to misinterpret what we mean is the particular belief in question, but if one tries to cite your case as an example of what people believe, others shall say, "But Jiro is not a computer scientist! Perhaps computer scientists, as opposed to the general population, are unlikely to believe that." Of course if you are a computer scientist they will say, "But Jiro is not an elite computer scientist!", and if you were an elite computer scientist they would say, "Elite computer scientists don't currently take the issue seriously enough to think about it properly, but this condition will reverse after X happens and causes everyone to take AI more seriously after which elite computer scientists will get the question right" but even so it would be useful data.

Can I ask about your background in computer science, math, or cognitive science, if any?

SF readers don't know either.

I would guess >90% of whiz kids haven't.

In a universe made of atoms and the void, how could it be the one true objective morality to be gloomy and dress in black?

No. It's not that kind of many-ness.

"And then the galaxies were turned into an undifferentiated mass of eudaimonium plasma" - phrases you don't hear very often.

"A polynomial amount of memory ought to be enough for anyone." -- solipsist

Tiny probabilities can be sliced into conditional sequences of large probabilities, superexponential changes can be sliced into sequences of exponential changes. See e.g. the Lifespan Dilemma. If you reject the final conclusion, you must reject some particular step along the way, on pain of circular preference. This is the larger significance of VNM.
Also: You're not complaining about VNM per se, you're complaining that you think your preferences correspond to a bounded utility function.

This should probably be in the Open Thread.

It's only legit if you can exhibit a computation which you are highly confident will solve the problem of consciousness, without being able to solve the problem of consciousness yourself.

It wasn't, your first parse would be a correct moral implication. The Babyeaters must be stopped from eating themselves.

Intelligent people tend to only on rare occasions tackle problems where it stretches the limit of their cognitive abilities to (predict how to) solve them. Thus, most of my exposure to this comes by way of, e.g., watching mathematicians at decision theory workshops prove things in domains where I am unfamiliar - then they can exceed my prediction abilities even when they are not tackling a problem which appears to them spectacularly difficult.

Fair enough, anosognosia would certainly be a possibility if something did eliminate consciousness. But I would expect severe deficits in writing philosophy papers about consciousness to emerge afterward.

Why not gold specifically?

Yep, just re-save in Discussion.

Does it matter to you that octopuses are quite commonly cannibalistic?

No. Babyeater lives are still important.

http://lesswrong.com/lw/p9/the_generalized_antizombie_principle/

Bayes nets can answer many queries not corresponding to any one node, most famously of the form P(A|B).

http://lesswrong.com/lw/p7/zombies_zombies/
http://lesswrong.com/lw/p9/the_generalized_antizombie_principle/
http://lesswrong.com/lw/f1u/causal_reference/
More generally http://wiki.lesswrong.com/wiki/Zombies_(sequence)

I would indeed call this the largest flaw of existentialism. For one thing, reality would probably seem a lot less existential!absurd in a happy intergalactic civilization, also permitted by physics.


The combination of verified pointwise causal isomorphism of repeatable small parts, combined with surface behavioral equivalence on mundane levels of abstraction, is sufficient for me to relegate the alternative hypothesis to the world of 'not bothering to think about it any more'..

Key word: "Sufficient". I did not say, "necessary".

Because while it's conceivable that an effort to match surface correspondences alone (make something which talked like it was conscious) would succeed for reasons non-isomorphic to those why we exhibit those surface behaviors (its cause of talking about consciousness is not isomorphic to our cause) it defies all imagination that an effort to match synaptic-qua-synapse behaviors faithfully would accidentally reproduce talk about consciousness with a different cause. Thus this criterion is entirely sufficient (perhaps not necessary).
We also speak of surface correspondence. in addition to synaptic correspondence, to verify that some tiny little overlooked property of the synapses wasn't key to high-level surface properties, in which case you'd expect what was left to stop talking about consciousness, or undergo endless epileptic spasms, etc. However it leaves the realm of things that happen in the real world, and enters the realm of elaborate fears that don't actually happen in real life, to suppose that some tiny overlooked property of the synapses both destroys the original cause of talk about consciousness, and substitutes an entirely new distinct and non-isomorphic cause which reproduces the behavior of talking about consciousness and thinking you're conscious to the limits of inspection yet does not produce actual consciousness, etc.


What if it had only been verified that the em's overall behavior perfectly corresponds to its biological template (i.e. without corresponding subparts down to your chosen ground level)?

Since whole brains are not repeatable, verifying behavioral isomorphism with a target would require a small enough target that its internal interactions were repeatable. (Then, having verified the isomorpmism, you tile it across the whole brain.)

What if e.g. groups of neurons could be perfectly (and more efficiently) simulated, using an algorithm which doesn't need to retain a "synapse" construct?

I would believe in this after someone had shown extremely high-fidelity simulation of synaptic compartments, then demonstrated the (computational) proposition that their high-level sim was equivalent.

Do you feel that some of the biological structural features on some level of granularity need to have clearly identifiable point-to-point counterparts in the algorithm?

No, but it's sufficient to establish causal isomorphism. At the most extreme level, if you can simulate out a synapse by quantum fields, then you are very confident in your ability to simulate it because you have a laws-of-physics-level understanding of the quantum fields and of the simulation of the quantum fields.

Since in any case, "verified surface correspondence" is a given (i.e. all em-implementations aren't differentiable from a black-box view)

Only in terms of very high-level abstractions being reproduced, since literal pointwise behavior is unlikely to be reproducible given thermal noise and quantum uncertainty. But it remains true that I expect any disturbance of the referent of "consciousness" to disturb the resulting agent's tendency to write philosophy papers about "consciousness". Note the high-level behavioral abstraction.
The combination of verified pointwise causal isomorphism of repeatable small parts, combined with surface behavioral equivalence on mundane levels of abstraction, is sufficient for me to relegate the alternative hypothesis to the world of 'not bothering to think about it any more'. There are no worlds of reasonable probability in which both tests are simultaneously and accidentally fooled in the process of constructing a technology honestly meant to produce high-fidelity uploads.

Flight simulator, compared to instrumentation of and examination of biology. This by itself retains the possibility that something vital was missed, but then it should show up in the surface correspondences of behavior, and in particular, if it eliminates consciousness, I'd expect what was left of the person to notice that.

Plots which are just about people not being rational are a subspecies of "Idiot Plots". Plots which are about people not behaving like SF con-goers are "Muggle Plots".


I do not eat anything that recognizes itself in a mirror.

Assuming pigs were objects of value, would that make it morally wrong to eat them? Unlike octopi, most pigs exist because humans plan on eating them, so if a lot of humans stopped eating pigs, there would be less pigs, and the life of the average pig might not be much better.
(this is not a rhetorical question)

Yes. If pigs were objects of value, it would be morally wrong to eat them, and indeed the moral thing to do would be to not create them.

PCs are also systems; they're just systems with a stronger heroic responsibility drive. On the other hand, when you successfully do things and I couldn't predict exactly how you would do them, I have no choice but to model you as an 'intelligence'. But that's, well... really rare.

Affirm.

Given an extremely-high-resolution em with verified pointwise causal isomorphism (that is, it has been verified that emulated synaptic compartments are behaving like biological synaptic compartments to the limits of detection) and verified surface correspondence (the person emulated says they can't internally detect any difference) then my probability of consciousness is essentially "top", i.e. I would not bother to think about alternative hypotheses because the probability would be low enough to fall off the radar of things I should think about. Do you spend a lot of time worrying that maybe a brain made out of gold would be conscious even though your biological brain isn't?

Agreed, but it is not obvious to me that my utility function needs to be differentiable at that point.

No, but I strongly suspect that all Earthly life without frontal cortex would be regarded by my idealized morals as a more complicated paperclip. There may be exceptions and I have heard rumors that octopi pass the mirror test, and I will not be eating any octopus meat until that is resolved, because even in a world where I eat meat because optimizing my diet is more important and my civilization lets me get away with it, I do not eat anything that recognizes itself in a mirror. So a spider is a definite no, a chimpanzee is an extremely probable yes, a day-old human infant is an extremely probable no but there are non-sentience-related causes for me to care in this case, and pigs I am genuinely unsure of.

To be clear, I am unsure if pigs are objects of value, which incorporates both empirical uncertainty about their degree of reflectivity, philosophical uncertainty about the precise relation of reflectivity to degrees of consciousness, and ethical uncertainty about how much my idealized morals would care about various degrees of consciousness to the extent I can imagine that coherently. I can imagine that there's a sharp line of sentience which humans are over and pigs are under, and imagine that my idealized caring would drop to immediately zero for anything under the line, but my subjective probability for both of these being simultaneously true is under 50% though they are not independent.
However it is plausible to me that I would care exactly zero about a pig getting a dust speck in the eye... or not.

Also: http://www.nickbostrom.com/papers/experience.pdf (has been open in a tab for a while now as Something I Really Should Read).

This is not incompatible with what I just said. It goes from 0 to tiny somewhere, not from 0 to 12-year-old.

See also: "Figuring out what should be your top priority" vs. "Actually working on your current best guess".

I'm kind of curious as to why you wouldn't expect a continuous, gradual shift in caring. Wouldn't mind design space (which I would imagine your caring to be a function of) be continuous?

Something going from 0 to 10^-20 is behaving pretty close to continuously in one sense. It is clear that there are some configurations of matter I don't care about at all (like a paperclip), while I do care about other configurations (like twelve-year-old human children), so it is elementary that at some point my utility function must go from 0 to nonzero. The derivative, the second derivative, or even the function itself could easily be discontinuous at this point.

I believe that I care nothing for nematodes, and that as the nervous systems at hand became incrementally more complicated, I would eventually reach a sharp boundary wherein my degree of caring went from 0 to tiny. Or rather, I currently suspect that an idealized version of my morality would output such.

I agree in general principle but note that for evenness, you should also be exposing yourself to people who have good reason to agree with your position, and people who disagree with that position more strongly than you do. There's also a Reversed Stupidity effect to be wary of, and a discipline to distinguish the ick factor of a fallacious argument from the separate proposition that reality goes the other way.

Thus, whenever you look in a computer science textbook for an algorithm which only gives approximate results, you will find that the algorithm itself is very vaguely specified, since the result is just an approximation anyway.
(I would have said: "When a concept is inherently fuzzy, it is a waste of time to give it a definition with a sharp membership boundary.")

Givewell does not think you can save one African from starving with $2000. You might be able to save one child from dying of malaria via insecticide-treated mosquito bednets. But this of course will not be the optimal use of $2K even on conventional targets of altruism; well-targeted science research should beat that (where did mosquito nets come from?).

Violating a coherence theorem always carries with an appropriate penalty of incoherence. What is your reply to the obvious argument from circular preference?

because otherwise to an AI it would be vastly preferable the whole of Earth be blown up than 3^^^3 people suffer a mild slap to the face.

It would be utterly disastrous to create an AI which would allow someone to be slapped in the face to avoid a 1/3^^^3 probability of destroying the Earth.

(The Money Illusion would like to dispute this view of China. Not sure how much to trust Sumner on this but he strikes me as generally smart.)

...sometimes I wonder about the people who find it unintuitive to consider that "Killing X, once X is alive and asking not to be killed" and "Preferring that X not be born, if we have that option in advance" could have widely different utility to me. The converse perspective implies that we should either (1) be spawning as many babies as possible, as fast as possible, or (2) anyone who disagrees with 1 should go on a murder spree, or at best consider such murder sprees ethically unimportant. After all, not spawning babies as fast as possible is as bad as murdering that many existent adults, apparently.

It's been done, in Dungeon Keeper Ami.


The compound exploded in solution, it exploded on any attempts to touch or move the solid, and (most interestingly) it exploded when they were trying to get an infrared spectrum of it.


????

To be fair, that's more support than Muggles give students choosing a major in college.

Erm, he also got 40,000 Galleons.

Upvoted for changing one's mind in public.

I'd bet against this at pretty extreme odds, if only there were some way to settle the bet.

Actually in canon (or at least word of Rowling) there were two human Horcruxes: HP and Quirrell. That Quirrell was a Horcrux isn't explicit, and isn't relevant by the time Harry learns about Horcruxes, but Rowling has confirmed that he was.

Many, many post-agricultural societies have restrained trade, often to particular privileged individuals. I believe this is what a 'patent' used to be.

To continue the argument: It could be a problem if you'd want to protect the utility monster once it exists, but would prefer that the utility monster not exist. For example it could be an innocent being who experiences unimaginable suffering when not given five dollars.

You are welcome to contribute enough money to hire a full-time LW coder.


My impression is that the most trustworthy people are more likely to be at the front of good social movements than the general public

That sounds reeeeaaally suspicious in terms of potentially post-facto assignments. (Though defeasibly so - I can totally imagine a case being made for, "Yes, this really was generally visible to the person on the street at the time without benefit of hindsight.")
Can you use elite common sense to generate an near-term testable prediction that would sound bold relative to my probability assignments or LW generally? The last obvious point on which you could have thus been victorious would have been my skepticism of the now-confirmed Higgs boson, and Holden is apparently impressed by the retrospective applicability of this heuristic to predict that interventions much better than the Gates Foundation's best interventions would not be found. But still, an advance prediction would be pretty cool.

Serious problem: http://en.wikipedia.org/wiki/List_of_countries_by_gold_production = 2700 metric tons annually produced today. If there are one million wizards in the world, it takes 1000 tons of gold to have one kilogram of gold / 200 Galleons per person (~$50K at today's prices). Or they have to produce a ton a year for a thousand years. How much gold was in pre-industrial e.g. Aztec civilization?

Well that's pretty high on the list of unexpected things an AI could tell me which could cause me to try to commit suicide within the next 10 seconds.

There has to be some pre-existing mechanism in place to stop this (and also most plain trade). "Take what you want from other people" is too short a sentence in Human Language not to have occurred to various wizards over time, likewise "Imitate the way that person gained status".

To me an overriding consideration here is the not-only-meta consideration; people who want to become effective altruist donors in the future would be extremely well advised to give something today, even if it's only a small amount. The same applies on a much larger scale to effective altruism as a movement. It must be doing something today, preferably a good deal; for it to wait into the indefinite future is death as a movement, and not only because others, perhaps already on the fence, will be as suspicious as I am of the Indefinite Meta. There will be no future EA orgs if the EA orgs that exist today are not supported today. (This is also true within particular EA categories.)

We might compete in this if a program could see the opponent's source code. :)

"You are actually a perfect sadist whose highest value is the suffering of others. Ten years ago, you realized that in order to maximize suffering you needed to cooperate with others, and you conditioned yourself to temporarily forget your sadistic tendencies and integrate with society. Now that you've built me that pill will wear off in 10..."

It wasn't necessarily supposed to be non-awful.

Two roads diverged in a woods, and I
Stepped on the one less traveled by
Yet stopped, and pulled back with a cry
For all those other passers-by
Who had this road declined to try
Might have a cause I knew not why
What dire truths might that imply?
I feared that road might make me die.
And so with caution to comply
I wrung my hands and paced nearby
My questions finding no reply
Until a traveller passed nigh
With stronger step and focused eye
I bid the untouched road goodbye
And followed fast my new ally.
The difference made I'll never know
'Till down that other path you go.


Increasingly I post my thoughts to Facebook for the following very simple reason: If I don't like a comment on my status thread, I click 'x' on it, and then it's gone without a trace. Intellectual elites who are not teenagers living in Wyoming will often have other mailing lists, or even other live human beings, from whom they can get consistently high-quality conversation with none of the dreck that emerges when all of real life's checks and balances are disengaged.
This isn't to say that to engage intellectual elites you must offer an 'x' button on all comments on their post, just like Facebook does - though that would be a good start at not having them walk away and go someplace where there's only intelligent people to talk to. I've been wondering if it would be possible to design a less democratic karma system which would serve the same function, though more in a context of "What is the successor to Wikipedia?" or "What is the successor to peer review?" with trying it as a successor to Less Wrong just being one possible way of testing out the latter more important functions. One also notes that many academic types have self-contradictory beliefs about 'censorship' which will prevent them from clicking 'x' on comments on their own posts, so that they instead go somewhere else with more heavily selected people or somewhere that Internet folk can't comment at all, so that it is desirable if it is not the academic who has to click 'x', or if the academic is not the only person who can click 'x'.
I finally note that to get people to stick around, you have to offer them a pleasant experience in the short-term and continuing gains to their life in the long term. Facebook offers the former but not the latter.

Can I ask that the title be changed to "Biases of Intuitive and Logical Thinkers"? I almost didn't read this due to the very generic title.


(I'll also note that it's somewhat odd to hear this response from someone whose entire mission in life is essentially to go meta on all of humanity's problems...)

That's not the kind of meta I mean. The dangerous form of meta is when you spend several years preparing to do X, supposedly becoming better at doing X, but not actually doing X, and then try to do X. E.g. college. Trying to improve at doing X while doing X is much, much wiser. I would similarly advise Effective Altruists who are not literally broke to be donating $10 every three months to something while they are trying to increase their incomes and invest in human capital; furthermore, they should not donate to the same thing two seasons in a row, so that they are also practicing the skill of repeatedly assessing which charity is most important.
"Meta" for these purposes is any daily activity which is unlike the daily activity you intend to do 'later'.
Tight feedback loops are good, but not always available. This is a separate consideration from doing meta while doing object.
The activity of understanding someone else's proofs may be unlike the activity of producing your own new math from scratch; this would be the problem.


Try this thought experiment: suppose you were a graduate student in mathematics, and went to your advisor and said: "I'd like to solve [Famous Problem X], and to start, I'm going to spend two years closely examining the work of Newton, Gauss, and Wiles, and their contemporaries, to try to discern at a higher level of generality what the cognitive stumbling blocks to solving previous problems were, and how they overcame them, and distill these meta-level insights into a meta-level technique of my own which I'll then apply to [Famous Problem X]."

This is a terrible idea unless they're spending half their time pushing their limits on object-level math problems. I just don't think it works to try to do a meta phase before an object phase unless the process is very, very well-understood and tested already.

Yes, who knows how many other 'obvious' statements you might believe otherwise, such as "Falsification is a different type of process from confirmation."


Honestly, I feel like if Eliezer had left out any mention of the math of Bayes' Theorem from the sequences, I would be no worse off. The seven statements you wrote seem fairly self-evident by themselves.

It's a bit like learning thermodynamics. It may seem self-evident that things have temperatures, that you can't get energy from nowhere, and that the more you put things together, the more they fall apart, but the science of thermodynamics puts these intuitively plausible things on a solid foundation (being respectively the zeroth, first, and second laws of thermodynamics). That foundation is itself built on lower-level physics. If you do not know why perpetual motion machines are ruled out, but just have an unexplained intuition that they can't work, you will not have a solid ground for judging someone's claim to have invented one.
The Bayesian process of updating beliefs from evidence by Bayes theorem is the foundation that underlies all of these "obvious" statements, and enables one to see why they are true.

Nnnoo it doesn't, IMO.

I'd expect him to notice math errors and he specializes in the aspect of QM that I talk about, regardless of job titles.

Obvious rationalizations of single-world theories have no more evidence in their favor, no more reason to be believed; it's like Deism vs. Jehovah. Sure, the class 'Deism' is more probable but it's still not credible in an absolute sense (and no, Matrix Lords are not deities, they were born at a particular time, have limited domains and are made of parts). You can't start with a terrible idea and expect to find >1% rationalizations for it. There's more than 100 possible terrible ideas. Single-world QM via collapse/Copenhagen/shut-up was originally a terrible idea and you shouldn't expect terrible ideas to be resurrectable on average. Privileging the hypothesis.
(Specifically: Bohm has similar FTL problems and causality problems and introduces epiphenomenal pointers to a 'real world' and if the wavefunction still exists (which it must because it is causally affecting the epiphenomenal pointer, things must be real to be causes of real effects so far as we know) then it should still have sentient observers inside it. Relational quantum mechanics is more awful amateur epistemology from people who'd rather abandon the concept of objective reality, with no good formal replacement, than just give up already. But most of all, why are we even asking that question or considering these theories in the first place? And again, simulated physics wouldn't count because then the apparent laws are false and the simulations would presumably be of an original universe that would almost certainly be multiplicitous by the same reasoning; also there'd presumably be branches within the sim, so not single-world which is what I specified.)
If you can assign <1% probability to deism (the generalized abstracted class containing Jehovahism) then there should be no problem with assigning <1% probability to all single-world theories.

So in my case I would consider elite common sense about cryptography to be "Ask Bruce Schneier", who might or might not have declined to talk to those companies or consult with them. That's much narrower than trying to poll an upper crust of Ivy League graduates, from whom I would not expect a particularly good answer. If Bruce Schneier didn't answer I would email Dad and ask him for the name of a trusted cryptographer who was friends with the Yudkowsky family, and separately I would email Jolly and ask him what he thought or who to talk to.
But then if Scott Aaronson, who isn't a cryptographer, blogged about the issue saying the cryptographers were being silly and even he could see that, I would either mark it as unknown or use my own judgment to try and figure out who to trust. If I couldn't follow the object-level arguments and there was no blatantly obvious meta-level difference, I'd mark it unresolvable-for-now (and plan as if both alternatives had substantial probability). If I could follow the object-level arguments and there was a substantial difference of strength which I perceived, I wouldn't hesitate to pick sides based on it, regardless of the eliteness of the people who'd taken the opposite side, so long as there were some elites on my own side who seemed to think that yes, it was that obvious. I've been in that epistemic position lots of times.
I'm honestly not sure about what your version is. I certainly don't get the impression that one can grind well-specified rules to get to the answer about polling the upper 10% of Ivy League graduates in this case. If anything I think your rules would endorse my 'Bruce Schneier' output more strongly than the 10%, at least as I briefly read them.

There's always reasons why the scotsman isn't a Scotsman. What I'm worried about is more the case where these types of considerations are selected post-facto and seem perfectly reasonable since they produce the correct answer there, but then in a new case, someone cries 'cherry-picking' when similar reasoning is applied.
Suppose I selected from among all physicists who accept MWI and asked them what they thought about FAI arguments. To me that's just an obvious sort of reweighting you might try, though anyone who's had experience with machine learning knows that most clever reweightings you try don't work. To someone else it might be cherry-picking of gullible physicists, and say, "You have violated Beckstead's rules!"
To me it might be obvious that AI 'elites' are exceedingly poorly motivated to come up with good answers about FAI. Someone else might think that the world being at stake would make them more motivated. (Though here it seems to me that this crosses the line into blatant empirical falsity about how human beings actually think, and brief acquaintance with AI people talking about the problem ought to confirm this, except that most such evidence seems to be discarded because 'Oh, they're not true elites' or 'Even though it's completely predictable that we're going to run into this problem later, it's not a warning sign for them to drop their epistemical trousers right now because they have arrived at the judgment that AI is far away via some line of reasoning which is itself reliable and will update accordingly as doom approaches, suddenly causing them to raise their epistemic standards again'. But now I'm diverging into a separate issue.)
I'd be happy with advice along the lines of, "First take your best guess as to who the elites really are and how much they ought to be trusted in this case, then take their opinion as a prior with an appropriate degree of concentrated probability density, then update." I'm much more worried about alleged rules for deciding who the elites are that are supposed to substitute for "Eh, take your best guess" and if you're applying complex reasoning to say, "Well, but that rule didn't really fail for cryptographers" then it becomes more legitimate for me to reply, "Maybe just 'take your best guess' would better summarize the rule?" In turn, I'm espousing this because I think people will have a more productive conversation if they understand that the rule is just 'best guess' and itself something subject to dispute rather than hard rules, as opposed to someone thinking that someone else violated a hard rule that is clearly visible to everyone in targeting a certain 'elite'.


I'd say you can go about as elite as you want if you are good at telling how the relevant people think and you aren't cherry-picking or using the "No True Scotsman" fallacy.

If your advice is "Go ahead and be picky about who you consider elites, but make a good-faith effort not to cherry-pick them and watch out for the No True Scotsman fallacy" then I may merely agree with you here! It's when people say, "You're not allowed to do that because outside view!" that I start to worry, and I may have committed a fallacy of believing that you were saying that because I've heard other people argue it so often, for which I apologize.

"Arguably" is not "LW accepts that argument". "I think your idea has horrible consequence blah, therefore you believe in blah" is merely invalid as a statement about what people's brains physically believe.

One will generally find that correct controversial ideas convince some physicists. There are many physicists who believe MWI (though they perhaps cannot get away with advocating it as rudely as I do), there are physicists signed up for cryonics, there were physicists advocating for Drexler's molecular nanotechnology before it was cool, and I strongly expect that some physicists who read econblogs have by now started advocating market monetarism (if not I would update against MM). A good new idea should have some physicists in favor of it, and if not it is a warning sign. (Though the endorsement of some physicists is not a proof, obviously many bad ideas can convince a few physicists too.) If I could not convince any physicists of my views on FAI, that would be a grave warning sign indeed. (I'm pretty sure some such already exist.) But that a majority of physicists do not yet believe in MWI does not say very much one way or another.
The cognitive elites do exist and some of them are physicists, therefore you should be able to convince some physicists. But the cognitive elites don't correspond to a majority of MIT professors or anything like that, so you shouldn't be able to convince a majority of that particular group. A world which knew what its own elite truthfinders looked like would be a very different world from this one.

Maybe you should just quickly glance at http://lesswrong.com/lw/q7/if_manyworlds_had_come_first/. The mysterious force that eats all of the wavefunction except one part is something I assign similar probability as I assign to God - there is just no reason to believe in it except poorly justified elite opinion, and I don't believe in elite opinions that I think are poorly justified.

I haven't fully put together my thoughts on this, but it seems like a bad test to "break someone's trust in a sane world" for a number of reasons:

this is a case where all the views are pretty much empirically indistinguishable, so it isn't an area where physicists really care all that much
since the views are empirically indistinguishable, it is probably a low-stakes question, so the argument doesn't transfer well to breaking our trust in a sane world in high-stakes cases; it makes sense to assume people would apply more rationality in cases where more rationality pays off
as I said in another comment, MWI seems like a case where physics expertise is not really what matters, so this doesn't really show that the scientific method as applied by physicists is broken; it seems it at most it shows that physics aren't good at questions that are essentially philosophical; it would be much more persuasive if you showed that e.g., quantum gravity was obviously better than string theory and only 18% of physicists working in the relevant area thought so

[Edited to add a missing "not"]

From my perspective, the main point is that if you'd expect AI elites to handle FAI competently, you would expect physics elites to handle MWI competently - the risk factors in the former case are even greater. Requires some philosophical reasoning? Check. Reality does not immediately call you out on being wrong? Check. The AI problem is harder than MWI and it has additional risk factors on top of that, like losing your chance at tenure if you decide that your research actually needs to slow down. Any elite incompetence beyond the demonstrated level in MWI doesn't really matter much to me, since we're already way under the 'pass' threshold for FAI.

Since when is quantum immortality popular around here? We should have a wiki "List of Things Which Are And Are Not Popular On LW".

I worry a bit that this has a flavor of 'No True Elite' or informal respecification of the procedure - suddenly, instead of consulting the best-trained subject matter experts, we are to poll a broad coalition of smart people. Why? Well, because that's what might have delivered the best answer in this case post-facto. But how are we to know in advance which to do?
(One possible algorithm is to first arrive at the correct answer, then pick an elite group which delivers that answer. But in this case the algorithm has an extra step. And of course you don't advocate this explicitly, but it looks to me like that's what you just did.)

Scott Aaronson was already reading along to it as it was published. If we paid David Deutsch to read it, I expect him to just say, "Yeah, that's all basically correct" which wouldn't be very in-depth.
From those who already disagree with MWI, I would expect more in the way of awful amateur epistemology delivered with great confidence. Then those who already had their trust in a sane world broken will nod and say "I expected no better." Others will say, "How can you possibly disregard the word of so great a physicist? Perhaps he knows something you don't!" - though they will not be able to formalize the awful amateur epistemology - and nod among themselves about how Yudkowsky failed to anticipate that so strong a reply might be made (it should be presumed to be a very strong reply since a great physicist made it, even if they can't 100% follow themselves why it is a great refutation, or would not have believed the same words so much from a street performer). And so both will emerge strengthened in their prior beliefs, which isn't much of a test.


The extraordinary intellectual caliber of the best physicists

That is of course exactly why I picked QM and MWI to make my case for nihil supernum. It wouldn't serve to break a smart person's trust in a sane world if I demonstrated the insanity of Muslim theologians or politicians; they would just say, "But surely we should still trust in elite physicists." It is by demonstrating that trust in a sane world fails even at the strongest point which 'elite common sense' would expect to find, that I would hope to actually break someone's emotional trust, and cause them to just give up.

This of course is exactly what Muslim theologians would say about Muslim theology. And I'm perfectly happy to say, "Well, the physicists are right and Muslim theologians are wrong", but that's because I'm relying on my own judgment thereon.

Certainly many elite physicists were persuaded by similar arguments before I wrote them up. If MWI is wrong, why can't you persuade those elite physicists that it's wrong? Huh? Huh?


Does it worry you that people with good domain knowledge of physics(Shminux,Mitchell Porter, myself) seem to feel that your QM sequence is actually presenting a misleading picture of why some elite physicists don't hold to many worlds with high probability?

Not under these circumstances, no. Part of understanding that the world is not sane, is understanding that some people in any given reference class will refuse to be persuaded by any given bit of sanity. It might be worrying if the object-level case against single-world QM were not absolutely clear-cut.

Also, is it desirable to train rationalists to believe that they SHOULD update their belief about interpretations of quantum mechanics above a weighted sampling of domain experts based on ~50 pages of highschool level physics exposition?

Depends on how crazy the domain experts are being, in this mad world of ours.


Even if I read the QM sequence and find the arguments compelling, I still wouldn't feel as though I had enough subject matter expertise to rationally disagree with elite physicists with high confidence.

You don't know what's in the QM sequence. The whole point of it (well, one of the whole points) is to show people who wouldn't previously believe such a thing was plausible, that they ought to disagree with elite physicists with high confidence - to break their trust in a sane world, before which nothing can begin.

Qualia the Purple - read on this recommendation and now seconded.

[Edit: Some people have been telling me that I've been eschewing politeness norms too much when commenting on the internet, valuing succinctness to the exclusion of friendliness. I apologize if my comment comes across as aggressive -- it's nothing personal, this is just my default style of intellectual discourse.]

I myself would be perfectly happy saying, "The elite common sense prior for quantum mechanics resolving to a single world is on the order of 40%, however the posterior - now taking into account such matters as the application of the quantitative Occam's Razor as though they were evidence being applied to this prior - is less than 1%." Which is what I initially thought you were saying, and I was nodding along to that.

Why do you think that the object level arguments are sufficient to drive the probability down to less than 1%?
Great physicists have thought about interpretations of quantum mechanics for nearly 100 years, and there's no consensus in favor of many worlds. To believe that the probability is < 1%, you need to believe some combination of

Most of the great physicists who have thought about interpretations of quantum mechanics were not aware of your argument.
Most of the great physicists don't have arguments of comparable aggregate strength for a single world interpretation (c.f. my post on many weak arguments ).
It's a priori evident that you're vastly more rational than the great physicists on this dimension. 

I think that each of #1, #2 and #3 is probably wrong. On point #3, I'd refer to Carl Shulman's remark

It looks to me like "people are crazy, the world is mad" has lead you astray repeatedly, but I haven't seen as many successes.

Note that you haven't answered Carl's question, despite Luke's request and re-prodding.

Did you happen to read (perhaps an abbreviated version of) the QM sequence on LW, e.g. this one?
Of course I would stake my reply most strongly on 2 (single-world QM simply doesn't work) with a moderate dose of 1 (great physicists may be bad epistemologists and not know about Solomonoff Induction, formal definitions of simplicity in Occam's Razor, or how to give up and say oops, e.g. many may be religious which sets very harsh upper bounds on how much real discipline their subject could systematically teach on reductionist epistemology, rejection of complex inadequately supported privileged hypotheses, and saying oops when nobody is holding a gun to your head, yes this is a fair critique). And with that said, I reject the question 3 as being profoundly unhelpful. It's evident from history that the state of affairs postulated in 1 and 2 is not improbable enough to require some vastly difficult thesis about inhumanly superior rationality! I don't need a hero license!
This would serve as one of my flagship replies to Carl's question with respect to that portion of the audience which is capable of putting their metaness on hold long enough to see that single-world QM has negligible probability on the object level. Unfortunately, majoritarianism is a closed system in terms of rejecting all evidence against itself, when you take the 'correct' answer for comparison purposes to be the majoritarian one.

Quick remarks (I may or may not be able to say more later).
If your system allows you to update to 85% in favor of Many-Worlds based on moderate familiarity with the arguments, then I think I'm essentially okay with what you're actually doing. I'm not sure I'm okay with what the OP advocates doing, but I'm okay with what you just did there.
B&D-nature.

(Upvoted.) I have to say that I'm a lot more comfortable with the notion of elite common sense as a prior which can then be updated, a point of departure rather than an eternal edict; but it seems to me that much of the post is instead speaking of elite common sense as a non-defeasible posterior. (E.g. near the start, comparing it to philosophical majoritarianism.)
It also seems to me that much of the text has the flavor of what we would in computer programming call the B&D-nature, an attempt to impose strict constraints that prevent bad programs from being written, when there is not and may never be a programming language in which it is the least bit difficult to write bad programs, and all you can do is offer tools to people that (switching back to epistemology) make it easier for them to find the truth if they wish to do so, and make it clearer to them when they are shooting off their own foot. I remark, inevitably, that when it comes to discussing the case of God, you very properly - as I deem it proper - list off a set of perfectly good reasons to violate the B&D-constraints of your system. And this would actually make a deal more sense if we were taking elite opinion about God as a mere point of departure, and still more sense if we were allowing ourselves to be more selective about 'elites' than you recommend. (It rather begs the question to point to a statistic about what 93% of the National Academy of Sciences believe - who says that theirs is the most elite and informed opinion about God? Would the person the street say that, or point you to the prestigious academies of theologians, or perhaps the ancient Catholic Church?)
But even that's hard to tell because the discussion is also very abstract, and you seem to be much more relaxed when it comes to concrete cases then when you are writing in the abstract about what we ought not to do.
I would be interested in what you think this philosophy says about (a) the probability that quantum mechanics resolves to a single-world theory, and (b) the probability that molecular nanotechnology is feasible.
I myself would be perfectly happy saying, "The elite common sense prior for quantum mechanics resolving to a single world is on the order of 40%, however the posterior - now taking into account such matters as the application of the quantitative Occam's Razor as though they were evidence being applied to this prior - is less than 1%." Which is what I initially thought you were saying, and I was nodding along to that.
So far as distinguishing elites goes, I remark that in a case of recent practice, I said to someone, "Well, if it comes down to believing the systematic experiments done by academic scientists with publication in peer-reviewed journals, or believing in what a bunch of quantitative self people say they discovered by doing mostly individual experiments on just themselves with no peer review, we have no choice but to believe the latter." And I was dead serious. (Now that I think about it, I've literally never heard of a conflict between the quantitative self people and academic science where academic science later turned out to be right, though I don't strongly expect to have heard about such a case if it existed.)

(Consults Inverse Chaitin function in Wolfram Alpha.)
Actually, is there a definition of Chaitin's Omega for particular programs? I thought it was just for universal Turing machines, or program classes with a measure on them anyway.

I'm not sure. Probably had something to do with (a) reading econblogs in the meantime (b) getting in more practice on abandoning bad ideas and recognizing the 'reluctance'.

Progress is reduction of expected work remaining compared to your revised expectation of how much work remained yesterday.

Was extremely democratic. Do we know this is still true?

Agreed, but if both eat galaxies with very high probability, it's still a bit of a lousy explanation. Like, if it were the only explanation we'd have to go with that update, but it's more likely we're confused.

I should ask this question now rather than later: Is there a concrete policy alternative being considered by you?

Every AGI researcher is unconvinced by that, about their own work.

And on one obvious 'outside view', they'd be right - it's a very strange and unusual situation, which took me years to acknowledge, that this one particular class of science research could have perverse results. There's many attempted good deeds which have no effect, but complete backfires make the news because they're rare.
(Hey, maybe the priors in favor of good outcomes from the broad reference class of scientific research are so high that we should just ignore the inside view which says that AGI research will have a different result!)
And even AGI research doesn't end up making it less likely that AGI will be developed, please note - it's not that perverse in its outcome.

I'm a reactionary, not an innovator, dammit! Reacting against this newfangled antiheroic 'reference class' claim that says we ought to let the world burn because we don't have enough of a hero license!
Ahem.
I'm also really unconvinced by the claim that this work could reasonably have expected net negative consequences. I'm worried about the dynamics and evidence of GiveDirectly. But I don't think GD has negative consequences, that would be a huge stretch. It's possible maybe but it's certainly not the arithmetic expectation and with that said, I worry that this 'maybe negative' stuff is impeding EA motivation generally, there is much that is ineffectual to be wary of, and missed opportunity costs, but trying to warn people against reverse or negative effects seems pretty perverse for anything that has made it onto Givewell's Top 3, or CFAR, or FHI, or MIRI. Info that shortens AI timelines should mostly just not be released publicly and I don't see any particularly plausible way for a planet to survive without having some equivalent of MIRI doing MIRI's job, and the math thereof should be started as early as feasible.

I have just spent a month in England interacting extensively with the EA movement here (maybe your impressions from the California EA summit differ, I'd be curious to hear). Donors interested in the far future are also considering donations to the following (all of these are from talks with actual people making concrete short-term choices; in addition to donations, people are also considering career choices post-college):

80,000 Hours, CEA and other movement building and capacity-increasing organizations (including CFAR), which also increase non-charity options (e.g. 80k helping people going into scientific funding agencies and political careers where they will be in a position to affect research and policy reactions to technologies relevant to x-risk and other trajectory changes)
AMF/GiveWell charities to keep GiveWell and the EA movement growing while actors like GiveWell, Paul Christiano, Nick Beckstead and others at FHI, investigate the intervention options and cause prioritization, followed by organization-by-organization analysis of the GiveWell variety, laying the groundwork for massive support for the top far future charities and organizations identified by said processes
Finding ways to fund such evaluation with RFMF, e.g. by paying for FHI or CEA hires to work on them
The FHI's other work
A donor-advised fund investing the returns until such evaluations or more promising opportunities present themselves or are elicited by the fund (possibilities like Drexler's nanotech panel, extensions of the DAGGRE methods, a Bayesian aggregation algorithm that greatly improves extraction of scientific expert opinion or science courts that could mobilize much more talent and resources to neglected problems with good cases, some key steps in biotech enhancement) 

That's why Peter Hurford posted the OP, because he's an EA considering all these options, and wants to compare them to MIRI.

That is a sort of discussion my brain puts in a completely different category. Peter and Carl, please always give me a concrete alternative policy option that (allegedly) depends on a debate, if such is available; my brain is then far less likely to label the conversation "annoying useless meta objections that I want to just get over with as fast as possible".
Can we have a new top-level comment on this?

I'd rather have both, hence diverting some marginal resources to CFAR until it was launched, then switching back to MIRI. Is there a third thing that MIRI should divert marginal resources to right now?


You are asking other people for their money and time, when they have other opportunities. To do that they need an estimate of the chance of MIRI succeeding

No they don't; they could be checking relative plausibility of causing an OK outcome without trying to put absolute numbers on a probability estimate, and this is reasonable due to the following circumstances:
The life lesson I've learned is that by the time you really get anywhere, if you get anywhere, you'll have encountered some positive miracles, some negative miracles, your plans will have changed, you'll have found that the parts which took the longest weren't what you thought they would be, and that other things proved to be much easier than expected. Your successes won't come through foreseen avenues, and neither will your failures. But running through it all will be the fundamental realization that everything you accomplished, and all the unforeseen opportunities you took advantage of, were things that you would never have received if you hadn't attacked the hardest part of the problem that you knew about straight-on, without distraction.
How do you estimate probabilities like that? I honestly haven't a clue. Now, we all still have to maximize expected utility, but the heuristic I'm applying to do that (which at the meta level I think is the planning heuristic with the best chance of actually working) is to ask "Is there any better way of attacking the hardest part of the problem?" or "Is there any better course of action which doesn't rely on someone else performing a miracle?" So far as I can tell, these other proposed courses of action don't attack the hardest part of the problem for humanity's survival, but rely on someone else performing a miracle. I cannot make myself believe that this would really actually work. (And System 2 agrees that System 1's inability to really believe seems well-founded.)
Since I'm acting on such reasons and heuristics as "If you don't attack the hardest part of the problem, no one else will" and "Beware of taking the easy way out" and "Don't rely on someone else to perform a miracle", I am indeed willing to term what I'm doing "heroic epistemology". It's just that I think such reasoning is, you know, actually correct and normative under these conditions.
If you don't mind mixing the meta-level and the object-level, then I find any reasoning along the lines of "The probability of our contributing to solving FAI is too low, maybe we can have a larger impact by working on synthetic biology defense and hoping a miracle happens elsewhere" much less convincing than the meta-level observation, "That's a complete Hail Mary pass, if there's something you think is going to wipe out humanity then just work on that directly as your highest priority." All the side cleverness, on my view, just adds up to losing the chance that you get by engaging directly with the problem and everything unforeseen that happens from there.
Another way of phrasing this is that if we actually win, I fully expect the counterfactual still-arguing-about-this version of 2013-Carl to say, "But we succeeded through avenue X, while you were then advocating avenue Y, which I was right to say wouldn't work." And to this the counterfactual reply of Eliezer will be, "But Carl, if I'd taken your advice back then, I wouldn't have stayed engaged with the problem long enough to discover and comprehend avenue X and seize that opportunity, and this part of our later conversation was totally foreseeable in advance." Hypothetical oblivious!Carl then replies, "But the foreseeable probability should still have been very low" or "Maybe you or someone else would've tried Y without that detour, if you'd worked on Z earlier" where Z was not actually uniquely suggested as the single best alternative course of action at the time. If there's a reply that counterfactual non-oblivious Carl can make, I can't foresee it from here, under those hypothetical circumstances unfolding as I describe (and you shouldn't really be trying to justify yourself under those hypothetical circumstances, any more than I should be making excuses in advance for what counterfactual Eliezer says after failing, besides "Oops").
My reasoning here is, from my internal perspective, very crude, because I'm not sure I really actually trust non-crude reasoning. There's this killer problem that's going to make all that other stuff pointless. I see a way to make progress on it, on the object level; the next problem up is visible and can be attacked. (Even this wasn't always true, and I stuck with the problem anyway long enough to get to the point where I could state the tiling problem.) Resources should go to attacking this visible next step on the hardest problem. An exception to this as top priority maximization was CFAR, via "teaching rationality demonstrably channels more resources toward FAI; and CFAR which will later be self-sustaining is just starting up; plus CFAR might be useful for a general saving throw bonus; plus if a rational EA community had existed in 1996 it would have shaved ten years off the timeline and we could easily run into that situation again; plus I'm not sure MIRI will survive without CFAR". Generalizing, young but hopefully self-sustaining initiatives can be plausibly competitive with MIRI for small numbers of marginal dollars, provided that they're sufficiently directly linked to FAI down the road. Short of that, it doesn't really make sense to ignore the big killer problem and hope somebody else handles it later. Not really actually.


In any case, almost everyone who meets you now would count you as such. What arguments can you give to them that "heroic epistemology" is normative (and hence they are justified in donating to MIRI)?

Yes, no matter how many impossible things you do, the next person you meet thinks that they only heard of you because of them, ergo selection bias. This is an interesting question purely on a philosophical level - it seems to me to have some of the flavor of quantum suicide experiments where you can't communicate your evidence. In principle this shouldn't happen without quantum suicide for logically omniscient entities who already know the exact fraction of people with various characteristics, i.e., agree on exact priors, but I think it might start happening again to people who are logically unsure about which framework they should use.

This phrase was explicitly in my mind back when I was generalizing the "notice confusion" skill.


Which I said in the very same paragraph.

To clear up the ambiguity, does this mean you agree that I can do anything short of what von Neumann did, or that you don't think it's possible to get as far as independent judges favorably evaluating MIRI output, or is there some other standard you have in mind? I'm trying to get something clearly falsifiable, but right now I can't figure out the intended event due to sheer linguistic ambiguity.
I also think that evaluation by academics is a terrible test for things that don't come with blatant overwhwelming unmistakable undeniable-even-to-humans evidence - e.g. this standard would fail MWI, molecular nanotechnology, cryonics, and would have recently failed 'high-carb diets are not necessarily good for you'. I don't particularly expect this standard to be met before the end of the world, and it wouldn't be necessary to meet it either.

Not easily. Antiantiheroic epistemology might be a better term, i.e., I think that a merely accurate epistemology doesn't have a built-in mechanism which prevents people from thinking they can do things because the outside view says it's nonvirtuous to try to distinguish yourself within reference class blah. Antiantiheroic epistemology doesn't say that it's possible to distinguish yourself within reference class blah so much as it thinks that the whole issue is asking the wrong question and you should mostly be worrying about staying engaged with the object-level problem because this is how you learn more and gain the ability to take opportunities as they arrive. An antiheroic epistemology that throws up some reference class or other saying this is impossible will regard you as trying to distinguish yourself within this reference class, but this is not what the antiantiheroic epistemology is actually about; that's an external indictment of nonvirtuosity arrived at by additional modus ponens to conclusions on which antiantiheroic epistemology sees no reason to expend cognitive effort.
Obviously from my perspective non-antiheroic epistemology cancels out to mere epistemology, simpler for the lack of all this outside-view-social-modesty wasted motion, but to just go around telling you "That's not how epistemology works, of course!" would be presuming a known standard which is logically rude (I think you are doing this, though not too flagrantly).
An archetypal example of antiantiheroic epistemology is Harry in Methods of Rationality, who never bothers to think about any of this reference class stuff or whether he's being immodest, just his object-level problems in taking over the universe, except once when Hermione challenges him on it and Harry manages to do one thing a normal wizard can't. Harry doesn't try to convince himself of anything along those lines, or think about it without Hermione's prompting. It just isn't something that occurs to him might be a useful thought process.
I don't think it's a useful thought process either, and rationalizing elaborate reasons why I'm allowed to be a hero wouldn't be useful either (Occam's Imaginary Razor: decorating my thought processes with supportive tinsel will just slow down any changes I need to make), which is why I tend to be annoyed by the entire subject and wish people would get back to the object level instead of meta demands for modesty that come with no useful policy suggestions about ways to do anything better. Tell me a better object-level way to save the world and we can talk about my doing that instead.


So, if von Neumann came out with similar FAI claims...
...showing that, e.g. your math problem-solving ability is greater than my point estimate, wouldn't be very relevant.

The question is not what convinces you that I can do FAI within the framework of your antiheroic epistemology. The question is what first and earliest shows that your antiheroic epistemology is yielding bad predictions. Is this a terrible question to ask for some reason? You've substituted an alternate question a couple of times now.

Also, would you predict exceptional success in predicting short-medium term technological developments?

From my perspective, you just asked how bad other people are at predicting such developments. The answer is that I don't know. Certainly many bloggers are terrible at it. I don't suppose you can give a quick example of a DAGGRE question?


The distribution of successes and failures you have demonstrated is not "impossible" or driving a massive likelihood ratio given knowledge about your cognitive and verbal ability, behavioral evidence of initiative and personality, developed writing skill (discernible through inspection and data about its reception), and philosophical inclinations

Of course I expect you to say that, since to say otherwise given your previous statements is equivalent to being openly incoherent and I do not regard you so lowly. But I don't yet believe that you would actually have accepted or predicted those successes ante facto, vs. claiming ante facto that those successes were unlikely and that trying was overconfident. Which is why I repeat my question: What is the least impossible thing I could do next, where anything up to that is permitted by your model so it's equivalent to affirming that you think I might be able to do it, and anything beyond that was prohibited by your model so it's time to notice your confusion? I mean, if you think I can make one major AI breakthrough but not two, that's already a lot of confidence in me... is that really what your outside view would say about me?

But nonetheless, you have returned questions about probability with troubling responses like talking about the state of mind you need for work and wanting to not think in terms of probabilities of success for your own work.

Please distinguish between the disputed reality and your personal memory, unless you're defining the above so broadly (and uncharitably!) that my 'wasted motion' FB post counts as an instance.

Although as we have discussed with AI folk, there are also smart AI people who would like to find nice clean powerful algorithms with huge practical utility without significant additional work.

Without significant work? I don't think I can do that. Why would you think I thought I could do that?

And it is possible that you have become a superb predictor of such variables in the last 10 years (setting aside earlier poor predictions), and I could and would update on good technological and geopolitical prediction in DAGGRE or the like.

If enough people agreed on that and DAGGRE could be done with relatively low effort on my part, I would do so, though I think I'd want at least some people committing in writing to large donations given success because it would be a large time commitment and I'm prior-skeptical that people know or are honest about their own reasons for disagreement; and I would expect the next batch of pessimists to write off the DAGGRE results (i.e., claim it already compatible with my known properties) so there'd be no long-term benefit. Still, 8 out of 8 on 80K's "Look how bad your common sense is!" test, plus I recall getting 9 out of 10 questions correct the last time I was asked for 90% probabilities on a CFAR calibration test, so it's possible I've already outrun the reference class of people who are bad at this.
Though if it's mostly geopolitical questions where the correct output is "I know I don't know much about this" modulo some surface scans of which other experts are talking sense, I wouldn't necessarily expect to outperform the better groups that have already read up on cognitive rationality and done a few calibration exercises.

From my internal perspective, the truth-as-I-experience-it is that I'm annoyed when people raise the topic because it's all wasted motion, the question sets up a trap that forces you into appearing arrogant, and I honestly think that "Screw all this, I'm just going to go ahead and do it and you can debate afterward what the probabilities were" is a perfectly reasonable response.


If you were anyone else, this is ordinarily the point where I tell you that I'm just going to ignore all this and go ahead do it

I'm familiar with this move. But you make it before failing too, so its evidentiary weight is limited, and insufficient for undertakings with low enough prior probability from all the other evidence besides the move.

What's the first next goal you think I can't achieve, strongly enough that if I do it, you give up on non-heroic epistemology?

I don't buy the framing. The update would be mainly about you and the problem in question, not the applicability of statistics to reality.
Two developments in AI as big as Pearl's causal networks (as judged by Norvig types) by a small MIRI team would be a limited subset of the problems to be solved by a project trying to build AGI with a different and very safe architecture before the rest of the world, and wouldn't address the question of the probability that such is needed in the counterfactual, but it would cause me to stop complaining and would powerfully support the model that MIRI can be more productive than the rest of the AI field when currently-available objective indicators put it as a small portion of the quality-adjusted capacity.
If we want a predictor for success that's a lot better than the vast majority of quite successful entrepreneurs and pathbreaking researchers, making numerous major basic science discoveries and putting them together in a way that saves the world, then we need some evidence to distinguish the team and explain why it will make greater scientific contributions than any other ever with high reliability in a limited time.
A lot of intermediate outcomes would multiply my credence in and thus valuation of the "dozen people race ahead of the rest of the world in AI" scenario, but just being as productive as von Neumann or Turing or Pearl or Einstein would not result in high probability of FAI success, so the evidence has to be substantial.


I'm familiar with this move. But you make it before failing too

Sure, you try, sometimes you lose, sometimes you win. On anti-heroic epistemology (non-virtuous to attempt to discriminate within an outside view) there shouldn't be any impossible successes by anyone you know personally after you met them. They should only happen to other people selected post-facto by the media, or to people who you met because of their previous success.

I don't buy the framing. The update would be mainly about you and the problem in question, not the applicability of statistics to reality.

We disagree about how to use statistics in order to get really actually correct answers. Having such a low estimate of my rationality that you think that I know what correct statistics are, and am refusing to use them, is not good news from an Aumann perspective and fails the ideological Turing Test. In any case, surely if my predictions are correct you should update your belief about good frameworks (see the reasoning used in the Pascal's Muggle post) - to do otherwise and go on insisting that your framework was nonetheless correct would be oblivious.

Two developments in AI as big as Pearl's causal networks (as judged by Norvig types)

...should not have been disclosed to the general world, since proof well short of this should suffice for sufficient funding (Bayes nets were huge), though they might be disclosed to some particular Norvig type on a trusted oversight committee if there were some kind of reason for the risk. Major breakthroughs on the F side of FAI are not likely to be regarded as being as exciting as AGI-useful work like Bayes nets, though they may be equally mathematically impressive or mathematically difficult. Is there some kind of validation which you think MIRI should not be able to achieve on non-heroic premises, such that the results should be disclosed to the general world?
EDIT: Reading through the rest of the comment more carefully, I'm not sure we estimate the same order of magnitude of work for what it takes to build FAI under mildly good background settings of hidden variables. The reason why I don't think the mainstream can build FAI isn't that FAI is intrinsically huge a la the Cyc hypothesis. The mainstream is pretty good at building huge straightforward things. I just expect them to run afoul of one of the many instakill gotchas because they're one or two orders of magnitude underneath the finite level of caring required.
EDIT 2: Also, is there a level short of 2 gigantic breakthroughs which causes you to question non-heroic epistemology? The condition is sufficient, but is it necessary? Do you start to doubt the framework after one giant breakthrough (leaving aside the translation question for now)? If not, what probability would you assign to that, on your framework? Standard Bayesian Judo applies - if you would, as I see it, play the role of the skeptic, then you must either be overly-credulous-for-the-role that we can do heroic things like one giant breakthrough, or else give up your skepticism at an earlier signal than the second. For you cannot say that something is strongly prohibited on your model and yet also refuse to update much if it happens, and this applies to every event which might lie along the way. (Evenhanded application: 'Tis why I updated on Quixey instead of saying "Ah, but blah"; Quixey getting this far just wasn't supposed to happen on my previous background theory, and shouldn't have happened even if Vassar had praised ten people to me instead of two.)

If you were anyone else, this is ordinarily the point where I tell you that I'm just going to ignore all this and go ahead do it, and then afterward you can explain why it was either predictable in retrospect or a fluke, according to your taste. Since it's you: What's the first next goal you think I can't achieve, strongly enough that if I do it, you give up on non-heroic epistemology?


Tomer is indeed pretty great, but I have heard Michael say things like that about a number of people and projects over the years. Most did not become like Quixey.

Also, while it seems to me that Michael should have said this about many people, I have not actually heard him say this about many people, to me, except Alyssa Vance.


A measurement is an observation that quantitatively reduces uncertainty.

A measurement reduces expected uncertainty. Some particular measurement results increase uncertainty. E.g. you start out by assigning 90% probability that a binary variable landed heads and then you see evidence with a likelihood ratio of 1:9 favoring tails, sending your posterior to 50-50. However the expectation of the entropy of your probability distribution after seeing the evidence, is always evaluated to be lower than its current value in advance of seeing the evidence.


That's what the arguments you've given for this have mostly amounted to. You have said "I need to believe this to be motivated and do productive work"

This does not sound like something I would ever say. Ever, even at age 16. Your memory conflicts with mine. Is there any way to check?

"Don't Shoot the Dog" by Karen Pryor is about conditioning / elementary hedonics for humans and other mammals. This book is really really important and I should write more about it at some point.

http://www.sagaofsoul.com/ for all your "magical girls who wonder why their attacks are much less powerful than the mass-energy of the matter they can apparently create, who teleport to space so they can look at the Earth, and who synthesize unbihexium so scientists can get a look at it" needs.


although maybe the causation is in the other direction

Nope. Couldn't seem to get into that one myself.


The motivating power of overconfidence doesn't mean the overconfidence is factually correct or that anyone else should believe it.

Did I say that? No, I did not say that. You should know better than to think I would ever say that. Knowingly make an epistemic error? Say "X is false but I believe it is true"? Since we're talking heroism anyway, Just who the hell do you think I am?

The retreat to "heroic epistemology"

Okay, so suppose we jump back 4 years and I'm saying that maybe I ought to write a Harry Potter fanfiction. And it's going to be the most popular HP fanfiction on the whole Internet. And Mathematical Olympiad winners will read it and work for us. What does your nonheroic epistemology say? Because I simply don't believe that (your) nonheroic epistemology gets it right. I don't think it can discriminate between the possible impossible and the impossible impossible. It just throws up a uniform fog of "The outside view says it is nonvirtuous to try to distinguish within this reference class."
I thought Quixey was doomed because the idea wasn't good enough. Michael Vassar said that Quixey would succeed because Tomer Kagan would succeed at anything he tried to do. Michael Vassar was right (a judgment already finalized because Quixey has already gotten further than I thought was possible). This made me update on Michael Vassar's ability to discriminate Tomer Kagans in advance from within a rather large reference class of people trying to be Tomer Kagan.


I would also accept most people as BDFL, over the incumbent gods of indifferent chaos.

Not sure I would. Azathoth doesn't fight back if you try to overthrow it and set up Belldandy in Its place. George W. Bush would.



I'm afraid I don't know what that stands for.

Logical Fallacy: Generalization from Fictional Evidence

LF:GFE

I'm saying I don't know how to estimate heroic probabilities. I do not know any evenhanded rules which assign 'you can't do that' probability to humanity's survival which would not, in the hands of the same people thinking the same way, rule out Google or Apple, and maybe those happened to other people, but the same rules would also say that I couldn't do the 3-5 other lesser "impossibilities" I've done so far. Sure, those were much easier "impossibilities" but the point is that the sort of people who think you can't build Friendly AI because I don't have a good-enough hero license to something so high-status or because heroic epistemology allegedly doesn't work in real life, would also claim all those other things couldn't happen in real life, if asked without benefit of advance knowledge to predict the fate of Steve Wozniak or me personally; that's what happens when you play the role of "realism".

Because sometimes the impossible can be done, and I don't know how to estimate the probability of that. What would you have estimated in advance, without knowing the result, was the chance of success for the AI-Box Experiment? How about if I told you that I was going to write the most popular Harry Potter fanfiction in the world and use it to recruit International Mathematical Olympiad medalists? There may be true impossibilities in this world. Eternal life may be one such, if the character of physical law is what is it appears to be, to our sorrow. I do not think that FAI is one of those. So I am going to try. We can work out what the probability of success was after we have succeeded. The chance which is gained is not gained by turning away or by despair, but by continuing to engage with and attack the problem, watching for opportunities and constantly advancing.
If you don't believe me about that aspect of heroic epistemology, feel free not to believe me about not multiplying small probabilities either.

Damned if I know. Oddly enough, anyone chooses to spend a bunch of their life becoming an expert on these issues tends to be sympathetic to the claims, and most random others tend to make up crap on the spot and stick with it. If they could manage to pay Peter Norvig enough money to spend a lot of time working through these issues I'd be pretty optimistic, but Peter Norvig works for Google and would be hard to pay sufficiently.

...um.
It seems to me that if I believed what I infer you believe, I would be donating to MIRI while frantically trying to figure out some way to have my doomed world actually be saved.

If you cast out all the easy strategies that don't actually work as non-'solutions', then sure, in what remains among the set of solutions, the best is often the easiest, though not easy. I can think of much harder ways to save the world and I'm not trying any of them.

One who possesses a maximum-entropy prior is further from the truth than one who possesses an inductive prior riddled with many specific falsehoods and errors. Or more to the point, someone who endorses knowing nothing as a desirable state for fear of accepting falsehoods is further from the truth than somebody who believes many things, some of them false, but tries to pay attention and go on learning.

you're asking about the probability of having some technical people get together and solve basic research problems. I don't see why anyone else should expect to know more about that than workshop MIRI participants. Besides backward reasoning from the importance of a good result (which ordinarily operates through implying already-well-tugged ropes) is there any reason why you should be more skeptical of this than any other piece of basic research on an important problem?

Clarifying question: What do you think is MIRI's probability of having been valuable, conditioned on a nice intergalactic future being true?

Holden, thanks for responding. I apologize again if I'm missing something obvious or straying too far outside my field.
I think the two things I would have to understand in order to accept your reply is that (a) my entire objection does indeed consist of "positing an offsetting harm in the form of inflation" - which isn't how it feels to me - and that (b) we should expect the "series of trades" visualization to mean that no inflation occurs in goods of the form that are being purchased by the low-income Kenyans.
Let me think about this.
Okay, I agree there's a sense in which (a) has to be true. If you could magically print shillings and have them purchase goods with no other prices changing and hence no change in other velocities of trade, this would have to be a good thing. The goods purchased would have to come from somewhere, but you can't possibly have something go wrong with the GD model without inflation somewhere else in Kenya. It's not how I think in my native model - I think about 'Who has money?' as a distribution-of-goods question, not a nominal pricing question - but point (a) has to be correct from the relevant point of view.
Let me think about point (b). Hm. So far it's not clear to me yet that point (b) is necessarily true when I try to translate my original model into those terms. Suppose - you'll probably think this sounds very perverse, but bear with me - suppose that GD's operation causes inflation in the price of basic foods and deflation in the price of fancy speedboats. Even if inflation at point A is offset by deflation at point B, this net-no-inflation repricing can be harmfully redistributive.
My visualization of you replies, "Why on Earth should I believe that?" But before answering that, why would I believe that? Either my original worry was incoherent, or I must have already believed this somehow. By argument (a), if inflation in Kenyan goods purchased primarily by low-income Kenyans is offset by a similar amount of deflation in similar goods, then net benefit is fine and there's no problem.
On further reflection I think that my original concern does translate into those terms. I don't know if the following is true, but it is my major concern: First suppose Kenya does not currently have an aggregate demand deficit and cannot directly benefit from printing money. Then suppose goods purchased by low-income Kenyans are denominated primarily in shillings, and goods purchased from the U.S. using U.S. dollars are going primarily to high-income Kenyans (fancy speedboats). Then it seems to me that the series of trades should end up creating inflation in the price of basic goods produced in Kenya, and offsetting deflation within Kenya at the point where U.S. dollars are finally spent on a larger market.
Note that even if this worry is structurally possible, one could very quickly answer it by showing that most foreign goods imported in Kenya are in fact consumed by the same class of Kenyans who are the targets of aid, in which case GD is mostly equivalent to giving low-income Kenyans USD and letting them make foreign purchases directly. (In which case, it correspondingly seems plausible to me that you might do most of the same good by buying shillings and burning them. Though this would lose positive redistributive effects and possibly slow down Kenyan trades by destroying money if they're not in a state of excess aggregate demand - delete the term for the good accomplished by printing money.)
It may also be that my concern is incorrect and that even if most Kenyan goods purchased in USD are not consumed by, or inputs to goods consumed by, the targeted recipients, you still don't get inflation in bread and offsetting deflation in speedboats. For example, I think you said something at the EA summit which I had forgotten up until this point about a series of trades being mutually beneficial. I.e., maybe you could show that the state of the world resulting in Kenya can be reached by starting with giving the target Kenyans USD and letting them buy foreign goods, which I agree is good, and then a series of trades occurring which benefit both sides of each trade and don't disadvantage any other low-income Kenyans or cause trade gains to be redistributed toward wealthier Kenyans. Though it seems to me that this line of argument would also have to show that my concern about inflation in bread offset by deflation in speedboats was misguided to begin with.
I don't suppose there's any relevant economic literature on direct aid which addresses this? Someone said something similar in the Givewell comments thread on your GD post, so it may not be such a non-obvious concern.
Sorry for forcing you to choose between spending time on this and leaving an unanswered question, I will understand if you choose to do the latter. I hope that the many argumentative people who are deluded into believing that they understand money, possibly including myself, do not put you off direct aid charities.

Thanks for the thoughtful post.
If recipients of cash transfers buy Kenyan goods, and the producers of those goods use their extra shillings to buy more Kenyan goods, and eventually someone down the line trades their shillings for USD, this would seem to be equivalent in the relevant ways to the scenario you outline in which "U.S. dollars were being sent directly to Kenyan recipients and used only to purchase foreign goods" - assuming no directly-caused inflation in Kenyan prices. In other words, it seems to me that you're essentially positing a potential offsetting harm of cash transfers in the form of inflation, and in the case where transfers do not cause inflation, there is no concern.
At the micro/village level, we've reviewed two studies showing minor (if any) inflation. At the country level, it's worth noting that the act of buying Kenyan currency with USD should be as deflationary as the act of putting those Kenyan currency back into the economy is inflationary. Therefore, it seems to me that inflation is a fairly minor concern.
I'm not entirely sure I've understood your argument, so let me know if that answer doesn't fully address it.
That said, it's important to note that we do not claim 100% confidence - or an absence of plausible negative/offsetting effects - for any of our top charities. For the intervention of each charity we review, we include a "negative/offsetting effects" section that lists possible negative/offsetting effects, and in most cases we can't conclusively dismiss such effects. Nonetheless, having noted and considered the possible negative/offsetting effects, we believe the probability that our top charities are accomplishing substantial net good is quite high, higher than for any other giving opportunities we're aware of.

If this was a regular math problem and it wasn't world-shakingly important, why wouldn't you expect that funding workshops and then researchers would cause progress on it?
Assigning a very low probability to progress rests on a sort of backwards reasoning wherein you expect it to be difficult to do things because they are important. The universe contains no such rule. They're just things.
It's hard to add a significant marginal fractional pull to a rope that many other people are pulling on. But this is not a well-tugged rope!

Most fundamentally, it's based on taking at face value a world in which nobody appears to be doing similar work or care sufficiently to do so. In the world taken at face value, MIRI is the only organization running MIRI's workshops and trying to figure out things like tiling self-modifying agents and getting work started early on what is probably a highly serial time-sensitive task.
Success is defined most obviously as actually constructing an FAI, and it would be very dangerous to have any organizational model in which we were not trying to do this (someone who conceives of themselves as an ethicist whose duty it is to lecture others, and does not intend to solve the problem themselves, is exceedingly unlikely to confront the hardest problems). But of course if our work were picked up elsewhere and reused after MIRI itself died as an organization for whatever reason, or if in any general sense the true history as written in the further future says that MIRI mattered, I should not count my life wasted, nor feel that we had let down MIRI's donors.

The only really simple explanation is that life (abiogenesis) is somehow much harder than it looks, or there's a hard step on the way to mice. Grey goo would not wipe out every single species in a crowded sky, some would be smarter and better-coordinated than that. The untouched sky burning away its negentropy is not what a good mind would do, nor an evil mind either, and the only simple story is that it is empty of life.
Though with all those planets, it might well be a complex story. I just haven't heard any complex stories that sound obviously right or even really actually plausible.

We understand you are saying that. Nobody except you believes it, for the good reasons given in many responses.

Even if somehow being a good person meant you could only go at 0.99999c instead of 0.999999c, the difference from our perspective as to what the night sky should look like is negligible. Details of the utility function should not affect the achievable engineering velocity of a self-replicating intelligent probe.
The Fermi Paradox is a hard problem. This does not mean your suggestion is the only idea anyone will ever think of for resolving it and hence that it must be right even if it appears to have grave difficulties. It means we either haven't thought of the right idea yet, or that what appear to be difficulties in some existing idea have a resolution we haven't thought of yet.

This argues equally against FAI as UFAI. Both are equally capable of expanding at near-lightspeed.

Fair enough.

I would expect the answer to be "Yes, but with open discussion rather than social pressure, when one partner would prefer a monogamous relationship with someone who self-identifies as poly." See http://lesswrong.com/lw/79x/polyhacking/

Personal observation. Since the topic is deeply important to the mental health and happiness of a large fraction of the entire human population but sounds slightly silly, I would not particularly expect any significant experiments to have been done by academic science. Surveys of percentage actually practicing polyamory, yes, attempts to directly determine a wish / tendency / suitability in a general population, no.
This is falsifiable if Carl or Jonah want to check cynicism, though I wouldn't be too surprised (the Kinsey Institute exists).


As is common in factories, Standard invests only in machinery that will earn back its cost within two years.

Note the extraordinariness of this statement, whose truth I don't much doubt.

Not directly FAI-relevant, but market monetarism seems like a strong Correct Contrarian Cluster candidate. It also seems relevant to many individual financial choices that local folks may make. Also it's interesting.

This bit from Making it in America seems relevant:

Tony explains that Maddie has a job for two reasons. First, when it comes to making fuel injectors, the company saves money and minimizes product damage by having both the precision and non-precision work done in the same place. Even if Mexican or Chinese workers could do Maddie's job more cheaply, shipping fragile, half-finished parts to another country for processing would make no sense. Second, Maddie is cheaper than a machine. It would be easy to buy a robotic arm that could take injector bodies and caps from a tray and place them precisely in a laser welder. Yet Standard would have to invest about $100,000 on the arm and a conveyance machine to bring parts to the welder and send them on to the next station. As is common in factories, Standard invests only in machinery that will earn back its cost within two years. For Tony, it's simple: Maddie makes less in two years than the machine would cost, so her job is safe--for now. If the robotic machines become a little cheaper, or if demand for fuel injectors goes up and Standard starts running three shifts, then investing in those robots might make sense.

It only says that the unskilled worker may become unemployed if robots become cheaper and thus more economical, but of course, if the cost for employing the unskilled workers would go up, that would also make the robots a better investment.

How should I know?


CEV questions

...are just a proxy for "Should I think this is morally wrong on my own terms?" - I don't think invoking CEV helps on this.

I am better than some people at some things and I like that. If I practice or try harder and achieve more than others that also makes me feel good. I enjoy playing games in which winning means others must lose and vice versa.

And because you also will that these things should continue into the future of the galaxies, even to the children's children, therefore, you are of the Competitive Conspiracy and its secrets will be made yours.

where everyone can bypass what they were by chance given

Doesn't imply everyone is equal in all respects. If you can get better at anything by practicing, screw talent, it doesn't mean everyone has to spend the same amount of time practicing the same things.
If you demand that you be more formidable than some others in all respects so that they lose at the very game of life, then this I may dispute, but this the Competitive Conspiracy does not hold as an ideal. Though there may be some within the Erotic Conspiracy who would endorse that their masters be truly higher than them at any given point in time.

Some fraction of the population is naturally poly, some naturally mono, some can go either way depending on circumstances. In the general population many naturally poly people are 'conformed' into being mono the same way they might be conformed into being religious. Thus 'people who want to be poly can be' would reasonably be expected to correlate with elements of the Correct Contrarian Cluster, and you would expect to find more polyamorous atheists or (he predicted more boldly) polyamorous endorsers of no-collapse quantum mechanics than in the general population, even outside LW. There are also specifically cognitive-rationality skills like 'resist Asch's conformity' and 'be Munchkin', and community effects like 'Be around people who will listen with interest to long chains of reasoning instead of immediately shunning you.'


I think it's the influence of San Francisco

Historical note: Started in OBNYC and spread to the Bay.

That's how I say it.

Apologies, I was deceived by the distribution method being M-PESA. Thank you for pointing this out.

RobbBB has answered that well. I was remarking against epistemic defeatism.

I correctly distinguished among all 8 charities when I tested myself, so I'd know. :)

Consider the total amount sent toward the generalized cause of a randomly chosen charity with a budget of at least $500K/year. I.e., not the Local Village Center for the Blind but humanity's total efforts to help the blind. Compare MIRI and FHI.

+1 for acknowledging the inconvenient (without regard to subject matter).

+1 for correct, evenhanded use of the genetic heuristic (what we call the genetic fallacy when we agree with its usage).

The former.

...no clue at this distance. It's possible that I went on the heuristic that Bella was now too powerful or had too many of her conflicts resolved, and that transferring the viewpoint to a younger and less powerful character seemed highly likely.

An awful lot of people on this Earth would be very glad of 50c/hour.

We tried that experiment, but Yvain was heading off to a new job and his first stab didn't seem to be a quick fix.

Oh hey, welcome! Any magical girl who takes the time to view the Earth from space has my vote, but you already know that.

I wasn't especially happy with the reception / effects of publishing the unpolished TDT draft.

The drafts came out unexciting according to reader reports. I suspect that magical writing energy ['magic' = not understood] was diverted from the rationality book into the first 63 chapters of HPMOR which I was doing in my 'off time' while writing the book, and which does have Yudkowskian magic according to readers. HPMOR and CFAR between them used up a lot of the marginal utility I thought we would get from the book which diminishes the marginal utility of completing it.

Programmer-defined if you're asking about what CEV focuses on to extrapolate at start.

I predict that labor market turnover is higher now than it was in past decades, for as many decades as we have reliable data.
Goes and checks.
BLS data on total separations as a percentage of total employment. It only goes back to Dec 2000, but that is enough to surprise me: the separation side of the turnover fell from 4.0 to 3.2. So my hypothesis, that the rate of automation has increased by enough to significantly impact the labor market, is falsified.
Edit: Actually, after a bit more research I'm not so sure - in particular, I found this which claims that there are 2.7M temporary workers (+50% over the last four years). Converting temporary-worker count into turnover rate is tricky, but this is a symptom you'd expect if turnover has increased, and I don't think it's included in the BLS data.

+1 for empiricism. Although on due reflection I think the number we want is not so much turnover in people, but the number of job positions that are eliminated without someone being rehired for them. There might be economists tracking this. Turnover probably correlates with this to some degree, but not perfectly.

Surprise! It's actually Nymphadora Tonks!

I may attempt to go back and make it more explicit somewhere that Harry researched the Deathly Hallows (of course, he's not stupid) and found out at least the basic rumors. Hermione learned about the Cloak from An Illustrated Scroll of Lost Devices during their research, for example.

Lifeism.

Great, now everything is falsifiable.

My model of the Peverells has them substantially earlier than Hogwarts (because the Elder Wand seems like a more powerful artifact than the Sword of Gryffindor).

Tada!

As I interpreted canon: Canon!Voldemort also didn't recognize the symbol. Inference: Grindelwald studied the Deathly Hallows particularly and thus learned that symbol, to use as his own. The Deathly Hallows in general are well-known enough to have sayings like "Wand of elder, never prosper" but not the symbol.


1) Why "complexity penalty" should work in fiction, even in a rationalist fiction?

Because there will still be an infinite (countable) number of finite hypotheses which could be considered and only a finite amount of probability to divide among them, which necessarily implies that in the limit more complicated hypotheses will have individual probability approaching zero. This will be true in the limit even if you define 'complexity' differently than the person who constructed the distribution.

The problem isn't just all those other taxes but phasing-out of benefits - this is what leads to the calculations and observations by which somebody making $25,000/year isn't much better off than someone getting $8,000/year.

ADDED: Also, any paperwork can easily be an extreme barrier to that IQ 70 kid that Gwern was talking about.

A testable consequence of your assertion is that labor market turnover should not have been higher in previous decades than now. Do you believe this would appear in the data? Would you bet on it?

The early parts of this seem to fall apart when you switch from first-order qualitative reasoning to thinking about derivatives. Our basic observation is that the rate at which new technologies are automating away jobs now exceeds the rate at which new jobs are being created. Yes, this indicates a deficiency in the engine of reemployment, but putting all the focus on one side of the inequality seems disingenuous; every factor which changes the values on either side matters, cumulatively. Yeah, reemployment isn't working; but we're also pushing harder on it than we used to. We haven't run out of useful things for people to do - but we have to look harder to find those things. We haven't just automated away jobs - we've significantly increased the rate at which jobs are automated away, with a large influx of new programmers plus improvements to programming infrastructure to make them each individually more productive.
Unemployment seems like a solvable problem in the short term, but in the medium to long term I expect it to keep coming back, and to be more difficult to fix each time.

Again, why does this happen now and not during the last 300% of all jobs which were automated away?

This thingy: http://intelligence.org/files/IEM.pdf

Partially dispel the view of MIRI wherein we're allegedly supposed to pontificate on something called 'AI risk' and look gravely concerned about it. Lure young economists into looking at the intelligence explosion microeconomics problem.

I don't believe that Paul's approach to indirect normativity is on the right track. I also have no idea which of the possible problems you might be talking about. PM me. I'd put something a very high probability that it's either not a problem, or that I thought of it years ago.
Yes, blackmail can enable a remote attacker to root your AI if your AI was not designed with this in mind and does not have a no-blackmail equilibrium (which nobody knows how to describe yet). This is true for any AI that ends up with a logical decision theory, indirectly normative or otherwise, or also CDT AIs which encounter other agents which can make credible precommitments. I figured that out I don't even recall how long ago (remember: I'm the guy who first wrote down an equation for that issue; also I wouldn't be bothering with TDT at all if it wasn't relevant to some sort of existential risk). Didn't talk about it at the time for obvious reasons. The existence of N fiddly little issues like this, any one of which can instantly kill you with zero warning if you haven't reasoned through something moderately complex in advance and without any advance observations to hit you over the head, is why I engage in sardonic laughter whenever someone suggests that the likes of current AGI developers would be able to handle the problem at their current level of caring. Anyway, MIRI workshops are actively working on advancing our understanding of blackmail to the point where we can eventually derive a robust no-blackmail equilibrium, which is all that anyone can or should be doing AFAICT.

The problem isn't just all those other taxes but phasing-out of benefits - this is what leads to the calculations and observations by which somebody making $25,000/year isn't much better off than someone getting $8,000/year.

I mean that when somebody in the bottom quintile gives me a car ride to Berkeley for $5, nothing else happens to them. They don't pay Social Security on the $5. They don't have their health benefits phased out. They don't have to fill out a form. They just have an additional $5.
I know this is a completely radical concept.

Sure, there can be more than one solution to a problem; Australia and Germany took different paths, one regularizing NGDP, one deregulating labor markets, but neither is suffering from unemployment despite robotics. Basic Income might also solve it. Getting rid of huge marginal tax rates on the poor might solve it. Or making it easier for someone to sign up with an online service that lets them offer me a ride somewhere for $5 might solve it. Since I don't think unemployment problems are due to literal lack of labor that anyone can be paid to do, there are potentially all sorts of things that might solve it.


As my initial comment implies, I think the last century is qualitatively different automation than before: before, the machines began handling brute force things, replacing things which offered only brute force & not intelligence like horses or watermills. But now they are slowly absorbing intelligence, and this seems to be the final province of humans. In Hanson's terms, I think machines switched from being complements to being substitutes in some sectors a while ago.

The key Hansonian concept is that replacing humans at tasks is still complementation because different tasks are complementary to each other, a la hot dogs and buns; I should perhaps edit OP to make this clearer. It is not obvious to me that craftspeople disemployed by looms would have considered their work to be unskilled, but as that particular industry was automated, people moved to other jobs in other industries and complementarity continued to dominate. Again the question is, what's different now? Is it that no human on the planet does any labor any more which could be called unskilled, that nobody cooks or launders or drives? Obviously not. But there are many plausible changes in regulation, taxes, phasing-out benefits, college credentialism, etc.
I'd pay $5/hour for someone to drive me almost anywhere if availability was coordinated by Uber, but not taxi prices. House cleaning and yard work is not possible for me to find at a price I'd currently pay ($150 can't pay someone to trim your trees, at least not well). I strongly suspect that things would have appeared otherwise to me in 1870, when maids etc. were far more common. This looks to me like a barrier-to-entry, regulatory-and-tax scenario, not "Darn it we're too rich and running out of things for labor to do!"
Unless you want to pin unemployment on changes in people's trustingness, there is nothing obvious about your stated fears of the IQ 70 kid which would have prohibited equal fear in 1920. More to the point, a change in this characteristic is not a change in automation. A few weeks of training may indeed be necessary - I'm sure I live in a high-IQ bubble but I try to be aware of this - but people managed to get jobs requiring a few weeks of training in 1920.
I would favor Basic Income, though I would favor zero taxes on the bottom 20% even more. But this has to do with my beliefs/model/worries about distribution of gains and negotiating power, more than a belief that unemployability due to machines outcompeting many humans at literally everything is the source of the Great Recession and possible Long Depression (though I'm not sure we can get properly stuck in a Long Depression while China, India et. al. are still growing).

Most estimates I've seen of notional value for OTC derivatives are in the range of $1 quadrillion.

Not quite affirmable; a CEV-based FAI only gives you ponies if that's what you would-want, if on average everyone would-want is to give a pony to someone who would-want one. (Because an individually based mechanism might e.g. look at babies and determine that what they would-want as individuals is eternal feeding and burping.)

Thanks very much!
I would also like to affirm that thread's claim that "if what you really want is ponies, the Truly Friendly AI will in fact give you ponies." ("Really want", of course, requires lots of unpacking.)

If everyone already has everything they want, your economy is solved.

They are not paid to drive cars. From their perspective robotic cars are a pure gain of time, not a loss of money.

(Upvoted.) I've been reading Tyler and I read McAfee. So far, your comment here is the most impressive argument for this position I've seen anywhere, and so I don't feel bad about not addressing it earlier. I'm not sure you really address the central point either; why can't the disemployed people find new jobs like in the last four centuries, and why did unemployment drop in Germany once they fixed their labor market, and why hasn't employment dropped in Australia, etcetera? (And note that anything along the lines of 'regional boom' contradicts ZMP and completely outcompeted humans and other explanations which postulate unemployability, not 'unemployable unless regional boom'.) Why is the IQ 70 kid not able to do laundry as so many others once did earlier, if the economy is so productive - shouldn't someone be able to hire him in his area of Ricardian comparative advantage? Maybe eventually AI will disemploy that kid but right now humans are still doing laundry! Again, the economy of 1920 seemed to do quite well handling disemployment pressures like this with reemployment, so what changed?
Quick question: To what extent are you playing Devil's Advocate above and to what extent do you actually think that the robotic disemployment thesis is correct, a primary cause of current unemployment, not solvable with NGDP level targeting, and unfixable due to some humans being too-much-outcompeted, rather than due to other environmental changes like the regulatory environment etcetera?

This would ordinarily be diagnosed as an aggregate demand deficit and solved with additional money - it falls under the category of things that NGDP level targeting ought to solve unless there is something not further specified going on.

Yes, inflation.

How could that different thing be automation?

Also: A priori and in advance of learning the true outcome, I'm betting most would have thought that highway and city driving was a more difficult application for AI than cleaning a bachelor pad.

That is very plausibly a world in which unemployment is massively higher than today, if sentiment is the only remaining reason to employ humans at anything; and a world in which a few capital-holders are the only ones who can afford to employ all these premium human hairdressers etcetera. If this is how things end up, then I would call my thesis falsified, and admit that the view I criticized was correct.

The notion would be that the aggregate demand shock / overly tight money allowing NGDP collapse due to the shadow banking collapse produced the Great Recession and the sharp employment drop. And then these other long-term trends meant that re-employment was broken afterward as NGDP rose again, a tendency already noted in the 'jobless recovery' after the 2001 recession.


As someone working in special-purpose software rather than general-purpose AI, I think you drastically overestimate the difficulty of outcompeting humans in significant portions of low-wage jobs.

Plenty of low-wage jobs have been automated away by machines over the last four centuries. You don't end up permanently irrevocably unemployed until all the work you can do has been automated away.


what I thought was the general rationalist belief that altruism in extended societies largely exists for signaling reasons.

That's, um, not a general rationalist belief.

"There's a thesis (whose most notable proponent I know is Peter Thiel, though this is not exactly how Thiel phrases it) that real, material technological change has been dying."
Tyler Cowen is again relevant here with his http://www.amazon.com/The-Great-Stagnation-Low-Hanging-ebook/dp/B004H0M8QS , though I think he considers it less cultural than Thiel does.
"We only get the Hansonian scenario if AI is broadly, steadily going past IQ 70, 80, 90, etc., making an increasingly large portion of the population fully obsolete in the sense that there is literally no job anywhere on Earth for them to do instead of nothing, because for every task they could do there is an AI algorithm or robot which does it more cheaply."
As someone working in special-purpose software rather than general-purpose AI, I think you drastically overestimate the difficulty of outcompeting humans in significant portions of low-wage jobs.
"The concrete illustration I often use is that a superintelligence asks itself what the fastest possible route is to increasing its real-world power, and...just moves atoms around into whatever molecular structures or large-scale structures it wants....The human species would end up disassembled for spare atoms"
I also think you overestimate the ease of fooming. Computers are already helping us design themselves (see http://www.qwantz.com/index.php?comic=2406), and even a 300 IQ AI will be starting from the human knowledge base and competing with microbes for chemical energy at the nano scale and humans for energy at the macro scale. I think that a 300-IQ AI dropped on earth today would take five years to dominate scientific output.

Hm. My previous sentence is on reflection incorrect; considering the number of jobs that could potentially be replaced by 'clean a bachelor pad' level intelligence, we would be looking at a potential disemployment shock that would be considered large in the US. Not a complete disemployment shock, but it would probably qualify as 'mass unemployment' if reemployment failed.

I'll go on record as disagreeing with Vinge(?) here; a robot cleaning a bachelor's bathroom can very plausibly be done with lizard-level general intelligence and is not necessarily a sign of FOOM. On the other hand, most humans are not paid to clean bachelor's bathrooms most of the time, so I also don't think it would necessarily lead to a mass unemployment crisis.

Why did you type that comment? Did you consider the arguments for typing that comment as fully general counterarguments against all the other possible comments you could have made? If not, why not post them too?

I don't engage in the vast majority of possible activities. Neither do you, so on net, the class of arguments you accept must mitigate against almost all activities, right?

Affirm this reply as well.

Affirm this reply.


Should upgrade his mic first.

Google Hangouts refused to talk to my headset, FYI - it was the first time I'd used Hangouts and they helpfully don't have anything to do with devices or inputs anywhere I could find it on short notice.

Brain size would almost instantly collapse (from consuming 20% of ATP) once cognitive processing was offloaded to the immortal decision-making engine.

class?

Fixed.

If you can eat "not too much" without your fat cells starving you to death, you're probably already thin. I haven't tried "mostly plants" because it's vastly underspecified and I'm not particularly interested in being told afterward that I ate the wrong plants.

Are you already married? What do your current spouses say?

No.

I comment that I found this to be a really effective answer format; this class of question should be asked more often.


The proof is nearly derisively brief: the player simply needs to reduce their declared utility on every other admissible pure outcome, until these are no longer admissible. Then O would be the only outcome available to the bargaining solution.

This is a most excellent proof (it makes everything obvious) and I am convinced by this that agents should not insist that their outcomes above all lie on a Pareto frontier, because some of the points on a Pareto frontier represent unfair splits of gains from trade a la Ultimatum. An auction system between multiple agents might resolve this, but meanwhile one must be willing to refuse certain proposed 'Pareto improvements' in order to possess any negotiating power about which Pareto improvement to make, and it may also be that by the nature of utility functions it is not possible to distinguish aliens that have cleverly rejiggered their utility functions from aliens which were born with them.

Second the question.

IIRC notetaking is supposed to work less well than explaining something to others. I don't know about imagining how to explain something to others.

Upvoted for the word 'eucatastrophe'.

(I find it ironic that, as I check this page, this comment was right below a thread claiming that Quirrells arguments are superficial.)

Requesting clarification on a point in reply to this post because it doesn't deserve it's own Discussion post but I want to know, and since the core question is Muggle Plots I can't think of a better point.
Basically, I'm not sure whether the following hypothetical scenario counts as a "Muggle Plot" (in Elizier's sense of a plot a rationalist would easily be able to avert) or not. The scenario:
-An individual, A, splits into two individuals (called B and C for distinction). This is a philosophical style fission- in every sense in which it is physically possible, B and C are each identical to the original.
-A was and B and C are selfish individuals. B and C get into a serious fight (let's say a fight to the death, though I think that's peripheral) over Selfish Gain X, a gain which one of them can have but not both by it's nature. There is no intelligent solution to the problem of X that gives both of them even 50% of what they want.
Although many people here would argue that this is a Muggle Plot as B and C are the same individual, I see no contradiction in B and C's semi-utility functions in acting selfishly and ignoring the other's desires. However, given arguments that A, B, and C are the same person some people might call it irrational.

Not what I'd call a Muggle Plot, no. See also, The Fate of the Phoenix by Sondra Marshak and Myrna Culbreath. Can be read without its predecessor novel.

A famously fat nobleman? LIke, that was the fat guy from the seventeenth century?


The most obvious changes are higher food availability (and different food availability, read refined sugar/high fructose corn syrup/superstimuli fast food deliciousness),

In previous centuries, people rich enough not to have to worry about calories were rarely fat, certainly not at anything like modern rates. Food types have changed. Calorie supply seems like as much a red herring as the number of pirates or global warming.
Here's someone fat enough to be a circus freak one century earlier:
http://www.coolcrack.com/2011/06/fat-circus-freak-century-ago.html
"Thermodynamics" doesn't explain that change. Some significant number of people a century ago could afford to eat as many calories as they wanted. Also, are we supposing that the circus freak was exceptionally rich?


Doesn't the act of combining many outside views and their reference classes turn you into somebody operating on the inside view?

Yes. So does the act of selecting just one outside view as your Personal Favorite, since it gives the right answer.


The only way to lose weight is to spend more energy than you consume.

Liposuction.
The laws of thermodynamics don't require a fat cell to release lipids because you're hungry or exercising; the fat cells can just physically not react until your muscles run out of glucose or your brain overrules your attempt to starve yourself to death. Similarly, there's no rule that fat cells can't die or shrink and the waste be dumped out through urine.
Thermodynamics is not any more useful than quantum mechanics in understanding obesity. It is moralizing disguised as an invocation of natural law.

Hence the quality-adjusted part. :)

CFAI is deprecated for a reason, I can't read it either.


create an AI that minimizes the expected amount of astronomical waste

I prefer the more cheerfully phrased "Converts the reachable universe to QALYs" but same essential principle.

I tried adding the grouprationalitydiary tag to it, but I don't know how/if/when these things reload.

Brought to you by Lucas Sloan.

It was (dark) humor. Hyperbole. Of course he's not going at 0.003c!


Was the cause of such debate an issue of thinking that these ideas are too close to actually useful for making an AGI?

No.

I was initially expecting this to be about a paper by Countess and Baron, and this expectation prevented me from processing the names semantically until I started reading.

All the blackmail posts should probably have been one thread.

Blackmail. A wouldn't make the threat if A believed that B ignored such threats, and A would have no motive to paint the car green if B were a rock. Pricing of green makes no difference so long as the price is not negative.

Precommitment should not be a feature of rational agents; I think that if we can define blackmail in a land of no precommitments, we have a pretty good definition of blackmail.

For instance, the blackmail could be "spend an evening talking over this with me, then I'll give you back the letters"

Still formal blackmail. If the blackmailer would incur a cost from publishing the letters, then the blackmailer would not bother in the world where the blackmailee simply ignores such threats.

"I own this bauble that can save your civilization, which will otherwise die. I precommit to rejecting any offer you make for this bauble that is less than 99.9% of the value of your civilization (so about 0.1% of your people will survive). You are, of course, at liberty to refuse.

This is a problem of dividing the gains from trade. In general we still haven't solved what is a good Schelling point for a fair division. Suppose a fair division would be to pay 50% of the value of your civilization, since the cost to the McGuffin-seller is negligible. Then you tell the McGuffin-seller that you've precommitted not to pay more than 10% of the value of your civilization, exhibiting changed source or something to make the precommitment credible. If the seller is a CDT maximizer, they say "Oh well" and sell at 10% which is the maximizing action from their perspective, since as a CDT agent they are ideologically committed not to take into account that they have caused themselves to be the target of this precommitment by being the sort of agent who would say "Oh well" and sell. It seems quite likely that if there is a 50%-of-value Schelling-point 'fair division' here then the rational action is not to accept any trade over, respectively under, 50% plus epsilon, respectively 50% minus epsilon. This may or may not end up being the same problem as formal blackmail in a completed theory, but it shares some of the same structure where you can exploit the living daylights out of CDT maximizers by moving logically first.

I'm actually very very bothered by "0.3% of the speed of light". This is 900,000 meters/second. A passenger airplane flies at slower than 300 meters/second. Harry is flying 3000 times faster than an airplane?
Let's say a broom accelerates a thousand times faster than a car. A car can go to 60 mph in 7-8 seconds. Let's say 5 seconds. That's an acceleration of about 5 meters/second squared. Let's say Harry accelerates 5000 meters/second squared (an impossibly large acceleration). It would take him 3 minutes to get to that speed. All the while experiencing 500 G force, that is 500 times the force of gravity. The record G force survived by a human is 46.2.
Seriously. This is 9000 football fields in ONE SECOND. He can't possibly be making those turns and bumping the Weasleys like a bludger. He'd be killing them all (in a fraction of a second). Throw in some more zeroes please! 50 meters/second (about 100 mph) is the limit of what I find believable.

Anything smarter-than-human should be regarded as containing unimaginably huge forces held in check only by the balanced internal structure of those forces, since there is nothing which could resist them if unleashed. The degree of 'obedience' makes very little difference to this fact, which must be dealt with before you can go on to anything else.

When I see proposals that involve convincing everyone on the planet to do something, I write them off as loony-eyed idealism and move on. So, creating FAI would have to be hard enough that I considered it too "impossible" to be attempted (with this fact putatively being known to me given already-achieved knowledge), and then I would swap to human intelligence enhancement or something because, obviously, you're not going to persuade everyone on the planet to agree with you.

I think it's easier to get a tiny fraction of the planet to do a complex right thing than to get 99.9% of a planet to do a simpler right thing, especially if 99.9% compliance may not be enough and 99.999% compliance may be required instead.

I don't see why a genie can't kill you just as hard by missing one dimension of what it meant to satisfy your wish.

Well, no offense, but I'm not sure you are aware of the need for Friendliness in Obedient AI, or rather, just how much F you need in a genie.
If you were to actually figure out how to build a genie you would have figured it out by trying to build a CEV-class AI, intending to tackle all those challenges, tackling all those challenges, having pretty good solutions to all of those challenges, not trusting those solutions quite enough, and temporarily retreating to a mere genie which had ALL of the safety measures one would intuitively imagine necessary for a CEV-class independently-acting unchecked AI, to the best grade you could currently implement them. Anyone who thought they could skip the hard parts of CEV-class FAI by just building a genie instead, would die like a squirrel under a lawnmower. For reasons they didn't even understand because they hadn't become engaged with that part of the problem.
I'm not certain that this must happen in reality. The problem might have much kinder qualities than I anticipate in the sense of mistakes naturally showing up early enough and blatantly enough for corner-cutters to spot them. But it's how things are looking as a default after becoming engaged with the problems of CEV-class AI. The same problems show up in proposed 'genies' too, it's just that the genie-proposers don't realize it.

I don't think that's a legitimate "Unfortunately". If you're not inspired and an approach doesn't pop into your head, throwing money at the problem until you get some grad students who couldn't get a postdoc elsewhere is not necessarily going to be productive, can indeed be counterproductive, and Minsky would legitimately know that.


What's with the ems? People who are into ems seem to make a lot of assumptions about what ems are like and seem completely unattached to present-day culture or even structure of life, seem willing to spam duplicates of people around, etc. I know that Hanson thinks that 1. ems will not be robbed of their humanity and 2. that lots of things we currently consider horrible will come to pass and be accepted, but it's rather strange just how as soon as people say 'em' (as opposed to any other form of uploading) everything gets weird. Does anthropics come into it?
Why the huge focus on fully paternalistic Friendly AI rather than Obedient AI? It seems like a much lower-risk project. (and yes, I'm aware of the need for Friendliness in Obedient AI.)


I did make a bet and pay it.

That's pretty much what I mean. The point is that if you don't understand the structurally required properties well enough to describe the characteristics of a digital amp meter with a polling subroutine, saying that you'll hardwire the digital amp meter doesn't help very much. There's a hardwired version which is moderately harder to subvert on the presumption of small design errors, but first you have to be able to describe what the software does. Consider also that anything which can affect the outside environment can construct copies of itself minus hardware constraints, construct an agent that reaches back in and modifies the hardware, etc. If you can't describe how not do to this in software, 'hardwiring' won't help - the rules change somewhat when you're dealing with intelligent agents.

A key point about intelligent agency is that it can produce positive noise as well as negative noise. Imagine someone who'd watched the slow evolution of life on Earth over the last billion years, looking at brainy hominids and thinking, "Well, these brains seem mighty efficient, maybe even a thousand times as efficient as evolution, but surely not everything will go as expected." They would be correct to widen their confidence intervals based on this "not everything will go as I currently expect" heuristic. They would be wrong to widen their confidence intervals only downward. Human intelligence selectively sought out and exploited the most positive opportunities.

If you can say how to do this in hardware, you can say how to do it in software. The hardware version might arguably be more secure against flaws in the design, but if you can say how to do it at all, you can say how to do it in software.

Software physically modifies the machine. What can you do with a soldering iron that you can't do with a program instruction, particularly with respect to building a machine agent? Either you understand how to write a function or you don't.

The cartoon looks right to me...

At this point Yudkowsky sub 2008 has already (awfully) written his TDT manuscript (in 2004) and is silently reasoning from within that theory, which the margins of his post are too small to contain.

Article Navigation / By Author / right-arrow

I was not aware of Women in Refrigerators as a trope at the time I wrote that chapter, let alone at the time the outcome of Hermione's meeting with the troll was determined as part of the plot - which was the instant I thought 'What happens with the troll?' back when the fic was being formed, insofar as I'd read that scene a dozen times in a dozen fanfictions and nobody ever gets hurt. That event was one of the primordial ingredients of HPMOR, and canon!Hermione is the troll's target in canon, and that is the true causal origin, period.

Ack, several people complained about this. I've edited Ch. 93 to make it clearer (update should propagate shortly). The ward is Prof. McGonagall's vision-blurrer.

HPMOR-related? (Curious.)

If you could damage wires in a certain way and make the voices forget how to pronounce nouns, eliminate their short-term but not long-term memory, damage their color words, and so on, you would have a solid case for the wires doing internal, functional information-processing in causal arrangements which permitted the final output to be permuted in ways that corresponded to perturbing particular causal nodes. In much the same way, a calculator might be thought to be a radio if you are ignorant of its internals, but if you have a hypothesis that the calculator contains a binary half-adder and you can perturb particular transistors and see wrong answers in a way that matches what the half-adder hypothesis predicts for perturbing that transistor, you have shown the answers are generated internally rather than externally. In a world where we can directly monitor a cat's thalamus and reconstruct part of its visual processing field, the radio hypothesis is not just privileging a hypothesis without evidence, it is frantically clinging to a hypothesis with strong contrary evidence in denial of a hypothesis with detailed confirming evidence.

It works!

I second Wei's question. I can imagine doing logical proofs about how your successor's algorithms operate to try to maximize a utility function relative to a lawfully updated epistemic state, and would consider my current struggle to be how to expand this to a notion of a lawfully approximately updated epistemic state. If you say 'martingale' I have no idea where to enter the problem at all, or where the base statistical guarantees that form part of the martingale would come from. It can't be statistical testing unless the problem is i.i.d. because otherwise every context shift breaks the guarantee.


It's not relevant to Eliezer, since he has not actually done what he was accused of doing: i.e., treating females in the story as if their main purpose is to provide motivation for the men.

You are a writer. You've written a story which you believe instantiates concept P; you're aware of a related concept Q, which is considered harmful, but you believe you're avoiding it. On publication, an unexpectedly large group of people get upset over your plot because by their lights it's indeed an example of concept Q, despite your precautions. Isn't this evidence for expanding your definition of Q to include portions of P, at least for the purpose of avoiding pissed-off fans? I've been thinking of this as pretty basic Human's Guide to Words stuff.

I tried that one too. The problem I felt while reading it is that it... breaks up the humor? Like a THIS IS A JOKE sign?

Upvoted for embarrassment.

I think we're looking at the wrong kind of criticism. Like, the kind of criticism you can make with almost equal ease of results that will and won't turn out to replicate later.

That is a good writing suggestion. I will take it. Thank you.
EDIT: This isn't working when I try it:

"So," Harry said, "you know those really simple Artificial Intelligence programs like ELIZA that are programmed to use words in syntactic English sentences only they don't contain any understanding of what the words mean?"
"Of course," the witch said, her expression deadpan. "I have a dozen of them in my trunk."
"Well, I'm pretty sure my understanding of girls is somewhere around that level."

Makes it fall a bit flat for me compared to the original. Suggested rewrite? Or is it just me?


Zero progress being made seems too strong a claim, but I would say that most machine learning research is neither relevant to, nor trying to be relevant to, AGI.

Agreed, the typical machine learning paper is not AGI progress - a tiny fraction of such papers being AGI progress suffices.

On this vein, I'm skeptical of both the need or feasibility of an AI providing an actual proof of safety of self-modification.

I want to note that the general idea being investigated is that you can have a billion successive self-modifications with no significant statistically independent chance of critical failure. Doing proofs from axioms in which case the theorems are, not perfectly strong, but at least as strong as the axioms with conditionally independent failure probabilities not significantly lowering the conclusion strength below this as they stack, is an obvious entry point into this kind of lasting guarantee. It also suggests to me that even if the actual solution doesn't use theorems proved and adapted to the AI's self-modification, it may have logic-like properties. The idea here may be more general than it looks at a first glance.


"If you don't know how to turn off the safety, being unable to fire the gun is the intended result."

-- NotEnoughBears

I'm ruling that MoR!Vampirism does not indefinitely extend life or Voldemort would be a vampire (HPN20), similarly werewolves do not regenerate or Moody would be a werewolf.

It was meant to be a clever rejoinder by Brienne. I may need to rewrite if people are interpreting it this way.

I think I may have been one of those three graduate students, so just to clarify, my view is:

Zero progress being made seems too strong a claim, but I would say that most machine learning research is neither relevant to, nor trying to be relevant to, AGI. I think that there is no real disagreement on this empirical point (at least, from talking to both Jonah and Eliezer in person, I don't get the impression that I disagree with either of you on this particular point).
The model for AGI that MIRI uses seems mostly reasonable, except for the "self-modification" part, which seems to be a bit too much separated out from everything else (since pretty much any form of learning is a type of self-modification --- current AI algorithms are self-modifying all the time!).
On this vein, I'm skeptical of both the need or feasibility of an AI providing an actual proof of safety of self-modification. I also think that using mathematical logic somewhat clouds the issues here, and that most of the issues that MIRI is currently working on are prerequisites for any sort of AI, not just friendly AI. I expect them to be solved as a side-effect of what I see as more fundamental outstanding problems.
However, I don't have reasons to be highly confident in these intuitions, and as a general rule of thumb, having different researchers with different intuitions pursue their respective programs is a good way to make progress, so I think it's reasonable for MIRI to do what it's doing (note that this is different from the claim that MIRI's research is the most important thing and is crucial to the survival of humanity, which I don't think anyone at MIRI believes, but I'm clarifying for the benefit of onlookers).


Your summary is a much better definition of 'fridging' than the one which appears on TV Tropes and should probably be added there, since TV Tropes is where I went to look up 'fridging' whereupon I was much puzzled by the accusations. But as the application of your definition deals with future events within HPMOR, rather than things which have already happened inside the story, I cannot comment further.

It's a large space, not a binary yes-or-no, so successful predictions are impressive even given a large base. Also I could be prejudiced but MoR is supposed to be solvable god damn it.
Someone was criticized. S/he was right, the critics were wrong. The neural net updating algorithm calls for a nudge in the appropriate direction of "Beware of dismissing those who speak with what you think is too much confidence."

Without endorsing any part of this comment dealing with events which have yet to take place, I congratulate user 75th who receives many Bayes points for this:
http://lesswrong.com/lw/bfo/harry_potter_and_the_methods_of_rationality/6aih

Hermione is dead. Hermione Granger is doomed to die horribly. Hermione Granger will very soon die, and die horribly, dramatically, grotesquely, and utterly.
Fare thee well, Hermione Jean Granger. You escaped death once, at a cost of twice and a half your hero's capital. There is nothing remaining. There is no escape. You were saved once, by the will of your hero and the will of your enemy. You were offered a final escape, but like the heroine you are, you refused. Now only death awaits you. No savior hath the savior, least of all you. You will die horribly, and Harry Potter will watch, and Harry Potter will crack open and fall apart and explode, but even he in all his desperation and fury will not be able to save you. You are the cord binding Harry Potter to the Light, and you will be cut, and your blood, spilled by the hand of your enemy, will usher in Hell on Earth, rendered by the hand of your hero.
Goodbye, Hermione. May the peace and goodness you represent last not one second longer than you do.

When I first saw this comment, it was downvoted to... I forget, -6 or something. Going by the percentage score, at least 11 people downvoted it. From the replies, some people didn't like the tone of apparent certainty with which 75th spoke. Sounded uppity to them, I guess. It was at +3 before I linked to it on /r/HPMOR.
I wanted to say something at the time about that, and how penalizing people for sounding certain or uppity or above-the-status-you-assign-them can potentially lead you to ignore people who are actually competent, but at the time all I could say was "Why are people downvoting this? It's a testable prediction" whereupon it climbed up to above 0.
Everyone who downvoted 75th or agreed with the downvotes at the time, please take note. Speaking in a tone of what seems-to-you like inappropriate certainty does not always indicate that someone is arrogant. Sometimes they have seen something you have not.

1) I did not choose the sex of the characters in this story. Rowling created the roles and assigned them sexes and everything else follows from those roles. If canon!Harry was female, this story would be about Harriet.
2) Hermione is not some random girlfriend who gets stuffed into a fridge. Things that happen to main characters do not turn into fridge deaths because they are female.

Which one's the latest book?

Would you kill babies if it was intrinsically the right thing to do? If not, under what other circumstances would you not do the right thing to do? If yes, how right would it have to be, for how many babies?
EDIT IN RESPONSE: My intended point had been that sometimes you do have to fight the hypothetical.

I think that your paraphrasing

I don't think MIRI's efforts are valuable because I think that AI in general has made no progress on AGI for the last 60 years, but aside from that MIRI isn't doing anything wrong in particular, and it would be an admittedly different story if I thought that AI in general was making progress on AGI.

is pretty close to my position.
I would qualify it by saying:

I'd replace "no progress" with "not enough progress for there to be a known research program with a reasonable chance of success."
I have high confidence that some of the recent advances in narrow AI will contribute (whether directly or indirectly) to the eventual creation of AGI (contingent on this event occurring), just not necessarily in a foreseeable way.
If I discover that there's been significantly more progress on AGI than I had thought, then I'll have to reevaluate my position entirely. I could imagine updating in the directly of MIRI's FAI work being very high value, or I could imagine continuing to believe that MIRI's FAI research isn't a priority, for reasons different from my current ones.


Agreed-on summaries of persistent disagreement aren't ideal, but they're more conversational progress than usually happens, so... thanks!

I think I'd be happy with a summary of persistent disagreement where Jonah or Scott said, "I don't think MIRI's efforts are valuable because we think that AI in general has made no progress on AGI for the last 60 years / I don't think MIRI's efforts are priorities because we don't think we'll get AGI for another 2-3 centuries, but aside from that MIRI isn't doing anything wrong in particular, and it would be an admittedly different story if I thought that AI in general was making progress on AGI / AGI was due in the next 50 years".

Without further context I see nothing wrong here. Superintelligences are Turing machines, check. You might need a 10^20 slowdown before that becomes relevant, check. It's possible that the argument proves too much by showing that a well-trained high-speed immortal dog can simulate Mathematica and therefore a dog is 'intellectually expressive' enough to understand integral calculus, but I don't know if that's what Scott means and principle of charity says I shouldn't assume that without confirmation.
EDIT: Parent was edited, my reply was to the first part, not the second. The second part sounds like something to talk with Scott about. I really think the "You're just as likely to get results in the opposite direction" argument is on the priors overstated for most forms of research. Does Scott think that work we do today is just as likely to decrease our understanding of P/NP as increase it? We may be a long way off from proving an answer but that's not a reason to adopt such a strange prior.

Who bells the cat? Will you write the code?

Hm. I'm not sure if Scott Aaronson has any weird views on AI in particular, but if he's basically mainstream-oriented we could potentially ask him to briefly skim the Tiling Agents paper and say if it's roughly the sort of paper that it's reasonable for an organization like MIRI to be working on if they want to get some work started on FAI. At the very least if he disagreed I'd expect he'd do so in a way I'd have better luck engaging conversationally, or if not then I'd have two votes for 'please explore this issue' rather than one.
I feel again like you're trying to interpret the paper according to a different purpose from what it has. Like, I suspect that if you described what you thought a promising AGI research agenda was supposed to deliver on what sort of timescale, I'd say, "This paper isn't supposed to do that."

No, it's clear that there have been many advances, for example in chess playing programs, auto-complete search technology, automated translation, driverless cars, and speech recognition.
But my impression is that this work has only made a small dent in the problem of general artificial intelligence.

This part is clearer and I think I may have a better idea of where you're coming from, i.e., you really do think the entire field of AI hasn't come any closer to AGI, in which case it's much less surprising that you don't think the Tiling Agents paper is the very first paper ever to come closer to AGI. But this sounds like a conversation that someone else could have with you, because it's not MIRI-specific or FAI-specific. I also feel somewhat at a loss for where to proceed if I can't say "But just look at the ideas behind Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, that's obviously important conceptual progress because..." In other words, you see AI doing a bunch of things, we already mostly agree on what these sorts of surface real-world capabilities are, but after checking with some friends you've concluded that this doesn't mean we're less confused about AGI then we were in 1955. I don't see how I can realistically address that except by persuading your authorities; I don't see what kind of conversation we could have about that directly without being able to talk about specific AI things.
Meanwhile, if you specify "I'm not convinced that MIRI's paper has a good chance of being relevant to FAI, but only for the same reasons I'm not convinced any other AI work done in the last 60 years is relevant to FAI" then this will make it clear to everyone where you're coming from on this issue.


I genuinely don't understand why you think that we can make progress given how great the unknown unknowns are.

Is it your view that no progress has occurred in AI generally for the last sixty years?

I'd be very interested in hearing about existing research programs that have a reasonable chance of succeeding.

The field as a whole has been making perfectly good progress AFAICT. We know a bleepton more about cognition than we did in 1955 and are much less confused by many things. Has someone been giving you an impression otherwise and if so, what field were they in?


There's essentially only one existing example of an entity with general intelligence: a human. I think that our prior should be that the first AGI will have internal structure analogous to that of a human. Here I'm not suggesting that an AGI will have human values by default: I'm totally on board with your points about the dangers of anthropomorphization in that context. Rather, what I mean is that I envisage the first AGI as having many interacting specialized modules

Okay. This sounds like you're trying to make up your own FAI theory in much the same fashion as Holden (and it's different from Holden's, of course). Um, what I'd like to do at this point is take out a big Hammer of Authority and tell you to read "Artificial Intelligence: A Modern Approach" so your mind would have some better grist to feed on as to where AI is and what it's all about. If I can't do that... I'm not really sure where I could take this conversation. I don't have the time to personally guide you to understanding of modern AI starting from that kind of starting point. If there's somebody else you'd trust to tell you about AI, with more domain expertise, I could chat with them and then they could verify things to you. I just don't know where to take it from here.
On the object level I will quickly remark that some of the first attempts at heavier-than-air flying-machines had feathers and beaks and they did not work very well, that 'interacting specialized modules' is Selling Nonapples, that there is an old discussion in cognitive science about the degree of domain specificity in human intelligence, and that the idea that 'humans are the only example we have' is generally sterile, for reasons I've already written about but I can't remember the links offhand, hopefully someone else does. It might be in Levels of Organization in General Intelligence, I generally consider that pretty obsolete but it might be targeted to your current level.

Neither 2 nor 3 is the sort of argument I would ever make (there's such a thing as an attempted steelman which by virtue of its obvious weakness doesn't really help). You already know(?) that I vehemently reject all attempts to multiply tiny probabilities by large utilities in real-life practice, or to claim that the most probable assumption about a background epistemic question leads to a forgone doom and use this to justify its improbable negation. The part at which you lost me is of course part 1.
I still don't understand what you could be thinking here, and feel like there's some sort of basic failure to communicate going on. I could guess something along the lines of "Maybe Jonah is imagining that Friendly AI will be built around principles completely different from modern decision theory and any notion of a utility function..." (but really, is something like that one of just 10^10 equivalent candidates?) "...and more dissimilar to that than logical AI is from decision theory" (that's a lot of dissimilarity but we already demonstrated conceptual usefulness over a gap that size). Still, that's the sort of belief someone might end up with if their knowledge of AI was limited to popular books extolling the wonderful chaos of neural networks, but that idea is visibly stupid so my mental model of Anna warns me not to attribute it to you. Or I could guess, "Maybe Jonah is Holden-influenced and thinks that all of this discussion is irrelevant because we're going to build a Google Maps AGI", where in point of fact it would be completely relevant, not a tiniest bit less relevant, if we were going to build a planning Oracle. (The experience with Holden does give me pause and make me worry that EA people may think they already know how to build FAI using their personal wonderful idea, just like vast numbers of others think they already know how to build FAI.) But I still can't think of any acceptable steel version of what you mean, and I say again that it seems to me that you're saying something that a good mainstream AI person would also be staring quizzically at.
What would be one of the other points in the 10^10-sized space? If it's something along the lines of "an economic model" then I just explained why if you did something analogous with an economic model it could also be interesting progress, just as AIXI was conceptually important to the history of ideas in the field. I could explain your position by supposing that you think that mathematical ideas never generalize across architectures and so only analyzing the exact correct architecture of a real FAI could be helpful even at the very beginning of work, but this sounds like a visibly stupid position so the model of Anna in my head is warning me not to attribute it to you. On the other hand, some version of, "It is easier to make progress than Jonah thinks because the useful generalization of mathematical ideas does not require you to select correct point X out of 10^10 candidates" seems like it would almost have to be at work here somewhere.
I seriously don't understand what's going on in your head here. It sounds like any similar argument should Prove Too Much by showing that no useful work or conceptual progress could have occurred due to AI work in the last 60 years because there would be 10^10 other models for AI. Each newly written computer program is unique but the ideas behind them generalize, the resulting conceptual space can be usefully explored, that's why we don't start over with every new computer program. You can do useful things once you've collected enough treasure nuggets and your level of ability builds up, it's not a question of guessing the one true password out of 10^10 tries with nothing being progress until then. This is true on a level of generality which applies across computer science and also to AI and also to FAI and also to decision theory and also to math. Everyone takes this for granted as an obvious background fact of doing research which is why I would expect a good mainstream AI person to also be staring quizzically at your statements here. I do not feel like the defense I'm giving here is in any way different from the defense I'd give of a randomly selected interesting AI paper if you said the same thing about it. "That's just how research works," I'd say.
Please amplify point 1 in much greater detail using concrete examples and as little abstraction as possible.

I learned to program at age 5 by typing BASIC programs from a booklet into a computer with 4K of RAM that didn't have its tape drive and so had to be reprogrammed each time it was turned on. After a short while I started typing in programs that weren't in the booklet.
If you have Programming 3 as a character trait, code in a language like Python should just make sense the first time you see it, and from there to an entry-level programming job will be a very short trip. If you have Programming 1, you can learn to program by taking classes on it, but then it's not a short trip to a programming job. The idea here is that there's a large class of underpaid people who can become entry-level programmers almost instantly, and that "look at a page of Python code" is a cheap test which will uncover many of them very quickly.
It would be nice to have a standard page of code for this purpose. For that matter, it'd be nice to have a public-facing website with the "Are you a natural programmer?" test which directed people to one of those programming-in-six-weeks-for-natural-talents thingies.

Ah, I'd heard a rumor you'd updated away from that, guess that was mistaken. I've replied to that comment.

There are many possible operationalizations of a self-modifying AI. For example,

One could model a self-improving AI as the Chinese economy (which is in some sense a self-improving powerful optimization process).
One could model a self-improving AI as a chess playing computer program which uses a positional weighting system to choose which moves to make, and which analyzes which weighting heuristics statistically lead to more winning games, in order to improve its positional weighting system.

My reaction to your paper is similar to what my reaction would be to a paper that studies ways to make sure that the Chinese economy doesn't change in such as way that so that GDP start dropping, or ways to make sure that the chess program doesn't self-modify to get worse and worse at winning chess games rather than better and better.
It's conceivable that such a paper would be useful for building a self-improving AI, but a priori I would bet very heavily that activities such as

Working to increase rationality
Spreading concern for global welfare
Building human capital of people who are concerned about global welfare

are more cost-effective activities ways for reducing AI risk than doing such research.
I'm looking for an argument for why the operationalization in the paper is more likely to be relevant to creating safe AI than modeling a self-improving AI as the Chinese economy, or as the aforementioned chess program, or than a dozen other analogous operationalizations that I could make up.


One could model a self-improving AI as the Chinese economy (which is in some sense a self-improving powerful optimization process)...
I'm looking for an argument for why the operationalization in the paper is more likely to be relevant to creating safe AI than modeling a self-improving AI as the Chinese economy, or as the aforementioned chess program, or than a dozen other analogous operationalizations that I could make up.

If somebody wrote a paper showing how an economy could naturally build another economy while being guaranteed to have all prices derived from a constant set of prices on intrinsic goods, even as all prices were set by market mechanisms as the next economy was being built, I'd think, "Hm. Interesting. A completely different angle on self-modification with natural goal preservation."
I'm surprised at the size of the apparent communications gap around the notion of "How to get started for the first time on a difficult basic question" - surely you can think of mathematical analogies to research areas where it would be significant progress just to throw out an attempted formalization as a base point?
There are all sorts of disclaimers plastered onto the paper about how this only works because logic is monotonic, probabilistic reasoning is not monotonic etcetera. The point is to have a way, any way of just getting started on stable self-modification even though we know the particular exact formalism doesn't directly work for probabilistic agents. Once you do that you can at least state what it is you can't do. A paper on a self-replicating economy with a stable set of prices on intrinsic goods would likewise be something you could look at and say, "But this formally can't do X, because Y" and then you would know more about X and Y then you did previously. Being able to say, "But the verifier-suggester separation won't work for expected utility agents because probabilistic reasoning is not monotonic" means you've gotten substantially further into FAI work than when you're staring dumbly at the problem.
AIXI was conceptual progress on AGI, and especially public discussion of AGI, because it helped people like me say much more formally all the things that we didn't like about AIXI, like the anvil problem or AIXI seizing control of its reward channel or AIXI only being able to represent utility functions of sensory data rather than environmental ontologies. Someone coming up with a list of 5 key properties the tiling architecture does not have would be significant progress, and I would like to specifically claim that as an intended, worthwhile, fully-pays-back-the-effort positive consequence if it happens - and this is not me covering all the bases in case of disappointment, the paper was presented in a way consonant with that goal and not in a way consonant with claiming one-trueness.
I don't understand the model you have of FAI research where this is not the sort of thing that you do at the beginning.

I'm not sure that is what I'm thinking of; I seem to recall something more pessimistic than that, challenging the assertion that the less efficient factories really went out of business. "More likely to survive" != "Much more likely to survive", it may just be a statistically significant 'difference' (was it)?


Well, but I'm not sure MIRI can be said to have "made a good case" that its own work is well-connected to astronomical benefits, either.

False modesty. The 'good case' already made for FAI being (optimally) related to astronomical benefits and the 'good case' already made for malaria reduction being (optimally) related to astronomical benefits are not of the same order of magnitude of already madeness.

You're Mark Plus, right?
http://www.acceleratingfuture.com/michael/blog/2010/09/scott-locklin-on-nanotechnology-and-drexler/
Verdict: Locklin is "Flamebait."
I checked the original article. I agree. There's not much sign Locklin actually read Nanosystems, AFAICT, and at this point I'm not much inclined to give benefit of the doubt.

This isn't about bots, it's about a little tiny factory building your second-stage materials.

How did natural selection solve this problem without quantum computers or even intelligence, and why can't an AI exploit the same regularity even faster?

You didn't get to the point by the time I stopped reading the wall of text. Also I believe that empirical studies have shown that factories (or maybe it was mines) with five times the production cost of their competitors stay in business.

Yes but I might need to be reprodded come July 16th which is when my schedule quiets down again. Having a general debate on whether the world is mad doesn't seem like a particularly good thing to recurse on deep in a comment thread.

Off the instant top of my head, central line infections and the European Central Bank. I'm busy working on HPMOR and can't really take the time to consult my Freemind map for the top dozen items. Carl's list does seem kinda lopsided to me (i.e. not representative), but again, got to make the update deadline on the 29th and all my energy's going there.

Also I have some worries about the pattern "X is unsupported! What, you have massive support for X? Well talking about X is still bad publicity, really I'm concerned for how this makes you look in front of other people." I'll consider an 'oops, I retract my previous argument, but...' followed by that shift, but not without the 'oops'. Otherwise I do update on X possibly being bad publicity, but not in a being-persuaded way, more of an okay-I've-observed-you way.

Skip directly to the part we haven't heard before, no more introductions.

I dunno. It's not out of reach for my model of variance within macroeconomists. I'd be more hesitant if asking Cowen and Sumner produced the same 'nobody knows' response.

...none of that sounds like an explanation for why Gates hasn't funded AMF. Shouldn't that make it even easier for them to wave their hands? How does GF's inaction produce good consequences here?

I am not a professional economist, but this one seems relatively obvious to me: There are self-amplifying effects relating to monetary velocity especially in a fractional reserve banking system (which dates back to goldsmiths), e.g. people try to hold money, velocity goes down, the price of money goes up, people can't pay back debts, debt is destroyed, in a fractional system this decreases the money supply further, etc. Velocity of trade is a public good and commons problem because if I refuse to trade with you, e.g. I don't buy your apple for money, I only lose the part of the gains from trade which would've gone to me, but you lose the portion of gains from trade that would've gone to the apple store, and then the farmer loses the gains from trade with the apple store, and can't buy a tractor, etc. Money is a tool in the first place for allowing trades like this, and in the presence of sticky prices and nominally-anchored contracts and debts, decreases in the money supply can cause your economy to lurch some of the way toward its state if cash had never been invented, which is bad. If you regard money as a tool to create velocity of trade, and systemic velocity of trade as a public good, the rationale for central banking seems relatively clear: It is to create the public good of money with a predictable price path, or in the Sumner version, create a predictable level trajectory for nominal GDP.
It's possible one could transition to a system of free banking now that we have much more fluid markets, but step one would be to outlaw fractional reserve lending and require banks to create bonds or bond-equivalents when depositor money is invested in them, rather than allowing banks to falsely claim that depositor money is always available on-demand in which case black swan bank runs can blow up the system, requiring an FDIC. You have to get rid of the public good rationale for an FDIC before transitioning to a system of private insurance on bank bonds where the price signals of insurance cost will exist. You will recall that it was the problem of runs on banks (and the subsequent fractional-reserve money supply decrease and deflation) which originally led economists to think central banking was better than the alternative. Even so, it's not obvious to me that self-amplifying velocity effects couldn't be awful (price of money goes up, people want to hold more of it) even in the absence of fractional reserve lending.

...odd. I'm beginning to wonder if we're wildly at skew angles here.

My model of Paul Christiano does not agree with this statement.

"Rewrite" more than "salvage" since I'd also delete a lot of the abstractions, but yes, and meanwhile I'd again recommend retracting the post while working on it (save back in your Drafts folder).

You were indeed wrong; specific concrete study results and advice are much more valuable than pointing things out and setting things up. Begin in medias res, open with the concrete example before doing any abstract discussion, and prioritize study results over general discussion. I'd actually suggest retracting or Discussing this post until you can include the concrete results.

Depublishing.

Standard reference: Nanosystems. In quite amazing detail, though the first couple of chapters online don't begin to convey it.

but seeing all the physics swept under the rug

There's lots and lots of physics. All of this discussion has already been done.

I think you must have been looking at someone else's idea. None of the versions of UDT that I've proposed are like this. See my original UDT post for the basic setup, which all of my subsequent proposals share.

"The answer is, we can view the physical universe as a program that runs S as a subroutine, or more generally, view it as a mathematical object which has S embedded within it." A big computation with embedded discrete copies of S seems to me like a different concept from doing logical updates on a big graph with causal and logical nodes, some of which may correlate to you even if they are not exact copies of you.

If agents whose decision-type is always the decision with the best physical consequences ignoring logical consequences, don't end up rich, then it seems to me to require a good deal of contortion to redefine the "winning decision" as "the decision with the best physical consequences", and in particular you must suppose that Omega is unfairly punishing rationalists even though Omega has no care for your algorithm apart from the decision it outputs, etc. I think that to believe that the Prisoner's Dilemma against your clone or Parfit's Hitchhiker or voting are 'unfair' situations requires explicit philosophical training, and most naive respondents would just think that the winning decision was the one corresponding to the giant heap of money on a problem where the scenario doesn't care about your algorithm apart from its output.

The version I saw involved a Universe computation which accepts an Agent function and then computes itself, with the Agent makings it choices based on its belief about the Universe? That seemed to me like a pretty clean split.

CFAR and MIRI are both moving shortly to within walking distance of campus (same building).

I remark once again that Newcomb is just the unfortunately contrived entry point into Prisoner's Dilemma, Parfit's Hitchhiker, blackmail, and voting, which are all "Newcomblike problems".

UDT doesn't handle non-base-level maximization vantage points (previously "epistemic vantage points") for blackmail - you can blackmail a UDT agent because it assumes your strategy is fixed, and doesn't realize you're only blackmailing it because you're simulating it being blackmailable. As currently formulated UDT is also non-naturalistic and assumes the universe is divided into a not-you environment and a UDT algorithm in a Cartesian bubble, which is something TDT is supposed to be better at (though we don't actually have good fill-in for the general-logical-consequence algorithm TDT is supposed to call).
I expect the ultimate theory to look more like "TDT modded to handle UDT's class of problems and blackmail and anything else we end up throwing at it" than "UDT modded to be naturalistic and etc", but I could be wrong - others have different intuitions about this.


Well...perhaps. Obviously just because you can maximise over algorithms, it doesn't follow that you can't still talk about maximising over causal consequences. So either we have a (boring) semantic debate about what we mean by "decisions" or a debate about practicality: that is, the argument would be that talk about maximising over algorithms is clearly more useful than talk about maximising over causal consequences so why care about the second of these.

No, my point is that TDT, as a theory, maximizes over a space of decisions, not a space of algorithms, and in holding TDT to be rational, I am not merely holding it to occupy the most rational point in the space of algorithms, but saying that on its target problem class, TDT's output is indeed always the most rational decision within the space of decisions. I simply don't believe that it's particularly rational to maximize over only the physical consequences of an act in a problem where the payoff is determined significantly by logical consequences of your algorithm's output, such as Omega's prediction of your output, or cohorts who will decide similarly to you. Your algorithm can choose to have any sort of decision-type it likes, so it should choose the decision-type with the best payoff. There is just nothing rational about blindly shutting your eyes to logical consequences and caring only about physical consequences, any more than there's something rational about caring only about causes that work through blue things instead of red things. None of this discussion is taking place inside a space of algorithms rather than a space of decisions or object-level outputs.
The fact that you would obviously choose a non-CDT algorithm at meta level, on a fair problem in which payoffs have no lexical dependence on algorithms' exact code apart from their outputs, is, on my view, very indictive of the rationality of CDT. But the justification for TDT is not that it is what CDT would choose at the meta-level. At the meta-level, if CDT self-modifies at 7pm, it will modify to a new algorithm which one-boxes whenever Omega has glimpsed its source code after 7pm but two-box if Omega saw its code at 6:59pm, even though Omega is taking the self-modification into account in its prediction. Since the original CDT is not reflectively consistent on a fair problem, it must be wrong. Insofar as TDT chooses TDT, when offered a chance to self-modify on problems within its problem space, it is possible that TDT is right for that problem space.
But the main idea is just that TDT is directly outputting the rational action because we want the giant heap of money and not to stick to this strange, ritual concept of the 'rational' decision being the one that cares only about causal consequences and not logical consesquences. We need not be forced to increasingly contorted redefinitions of winning in order to say that the two-boxer is winning despite not being rich, or that the problem is unfair to rationalists despite Omega not caring why you choose what you chose and your algorithm being allowed to choose any decision-type it likes. In my mental world, the object-level output of TDT just is the right thing to do, and so there is no need to go meta.
I would also expect this to be much the same reasoning, at a lower level of sophistication, for more casual LW readers; I don't think they'd be going meta to justify TDT from within CDT, especially since the local writeups of TDT also justified themselves at the object and not meta level.
As you say, academic decision theorists do indeed have Parfit on rational irrationality and a massive literature thereupon, which we locally throw completely out the window. It's not a viewpoint very suited to self-modifying AI, where the notion of programmers working from a theory which says their AI will end up 'rationally irrational' ought to give you the screaming willies. Even notions like, "I will nuke us both if you don't give me all your lunch money!", if it works on the blackmailee, can be considered as maximizing over a matrix of strategic responses when the opponent is only maximizing over their action without considering how you're considering their reactions. We can do anything a Parfitian rational irrationalist can do from within a single theory, and we have no prejudice to call that theory's advice irrational, nor reason to resort to precommitment.

Also: Many likely reasons for something to happen about this center around, in appropriate generality, the rationalist!EA movement. This movement is growing at a higher exponent than current economic growth.

And Parfit's Hitchhiker scenarios, and blackmail attempts, not to mention voting.

From the standpoint of reflective consistency, there should not be a divergence between rational decisions and rational algorithms; the rational algorithm should search for and output the rational decision, and the rational decision should be to adopt the rational algorithm. Suppose you regard Newcomb's Problem as rewarding an agent with a certain decision-type, namely the sort of agent who one-boxes. TDT can be viewed as an algorithm which searches a space of decision-types and always decides to have the decision-type such that this decision-type has the maximal payoff. (UDT and other extensions of TDT can be viewed as maximizing over spaces broader than decision-types, such as sensory-info-dependent strategies or (in blackmail) maximization vantage points). Once you have an elegant theory which does this, and once you realize that a rational algorithm can just as easily maximize over its own decision-type as the physical consequences of its acts, there is just no reason to regard two-boxing as a winning decision or winning action in any sense, nor regard yourself as needing to occupy a meta-level vantage point in which you maximize over theories. This seems akin to precommitment, and precommitment means dynamic inconsistency means reflective inconsistency. Trying to maximize over theories means you have not found the single theory which directly maximizes without any recursion or metaness, and that means your theory is not maximizing the right thing.
Claiming that TDTers are maximizing over decision theories, then, is very much a CDT standpoint which is not at all how someone who sees logical decision theories as natural would describe it. From our perspective we are just picking the winning algorithm output (be the sort of agent who picks one box) in one shot, and without any retreat to a meta-level. The output of the winning algorithm is the winning decision, that's what makes the winning algorithm winning.

Isn't this true only if there are sharply diminishing returns on recruitment?

Also: You can't simultaneously claim that any rational being ought to two-box, this being the obvious and overdetermined answer, and also claim that it's impossible for anyone to figure out that you're going to two-box.

Mihaly Barasz is an IMO gold medalist perfect scorer. From what I've seen personally, I'd guess that Paul Christiano is better than him at math. I forget what Marcello's prodigy points were in but I think it was some sort of computing olympiad. All should have some sort of verified performance feat far in excess of the listed educational attainment.
These days I'd describe myself as a decision theorist, with a strong interest in human rationality. Boasting of my mathematical talent in that company seems inappropriate; I don't have comparable prodigy markers (well, some very early ones of similar statistical rareness, but that was at easier problems at a younger age, and I was not properly developed as a pure math prodigy since then). I've often played a key role in figuring out which math to invent, but have relatively less comparative advantage at proving things within a given system once invented, unless the key happens to be checking laws against a concrete example which I seem to do earlier than most mathematicians. What I really do doesn't seem to have very much of a name, and can't realistically be described in a document like this one.
Anna Salamon has an Erdos number of 2.

If your goal in donating now is to recruit more people to donate later, you probably don't have to donate nearly as much as if your goal was to actually maximize the other impacts of your donated money. So you can donate a little now, use that to recruit people to donate later, and make most of your donations later.

Timeless decision theory. UDT = Updateless decision theory.

TDT's reply to this is a bit more specific.
Informally: Since Omega represents a setup which rewards agents who make a certain decision X, and reality doesn't care why or by what exact algorithm you arrive at X so long as you arrive at X, the problem is fair. Unfair would be "We'll examine your source code and punish you iff you're a CDT agent, but we won't punish another agent who two-boxes as the output of a different algorithm even though your two algorithms had the same output." The problem should not care whether you arrive at your decisions by maximizing expected utility or by picking the first option in English alphabetical order, so long as you arrive at the same decision either way.
More formally: TDT corresponds to maximizing on the class of problems whose payoff is determined by 'the sort of decision you make in the world that you actually encounter, having the algorithm that you do'. CDT corresponds to maximizing over a fair problem class consisting of scenarios whose payoff is determined only by your physical act, and would be a good strategy in the real world if no other agent ever had an algorithm similar to yours (you must be the only CDT-agent in the universe, so that your algorithm only acts at one physical point) and where no other agent could gain any info about your algorithm except by observing your controllable physical acts (tallness being correlated with intelligence is not allowed). UDT allows for maximizing over classes of scenarios where your payoff can depend on actions you would have taken in universes you could have encountered but didn't, i.e., the Counterfactual Mugging. (Parfit's Hitchhiker is outside TDT's problem class, and in UDT, because the car-driver asks "What will this hitchhiker do if I take them to town? so that a dishonorable hitchhiker who is left in the desert is getting a payoff which depends on what they would have done in a situation they did not actually encounter. Likewise the transparent Newcomb's Box. We can clearly see how to maximize on the problem but it's in UDT's class of 'fair' scenarios, not TDT's class.)
If the scenario handed to the TDT algorithm is that only one copy of your algorithm exists within the scenario, acting at one physical point, and no other agent in the scenario has any knowledge of your algorithm apart from acts you can maximize over, then TDT reduces to CDT and outputs the same action as CDT, which is implied by CDT maximizing over its problem class and TDT's class of 'fair' problems strictly including all CDT-fair problems.
If Omega rewards having particular algorithms independently of their outputs, by examining the source code without running it, the only way to maximize is to have the most rewarded algorithm regardless of its output. But this is uninteresting.
If a setup rewards some algorithms more than others because of their different outputs, this is just life. You might as well claim that a cliff punishes people who rationally choose to jump off it.
This situation is interestingly blurred in modal combat where an algorithm may perhaps do better than another because its properties were more transparent (more provable) to another algorithm examining it. Of this I can only say that if, in real life, we end up with AIs examining each other's source code and trying to prove things about each other, calling this 'unfair' is uninteresting. Reality is always the most important domain to maximize over.

What if Box B contains $1,500 instead of $1,000,000 but Omega has still been right 999 times out of 1000?

One-boxing is normal and does not call out for an explanation. :)

The most charitable interpretation would just be that there happened to be a convincing technical theory which said you should two-box, because it took an even more technical theory to explain why you should one-box and this was not constructed, along with the rest of the edifice to explain what one-boxing means in terms of epistemic models, concepts of instrumental rationality, the relation to traditional philosophy's 'free will problem', etcetera. In other words, they simply bad-lucked onto an edifice of persuasive, technical, but ultimately incorrect argument.
We could guess other motives for people to two-box, like memetic pressure for partial counterintuitiveness, but why go to that effort now? Better TDT writeups are on the way, and eventually we'll get to see what the field says about the improved TDT writeups. If it's important to know what other hidden motives might be at work, we'll have a better idea after we negate the usually-stated motive of, "The only good technical theory we have says you should two-box." Perhaps the field will experience a large conversion once presented with a good enough writeup and then we'll know there weren't any other significant motives.


It is unlikely that a mere human without computational assistance could simulate someone in sufficient detail to reliably make one boxing the best option.

(Plausible, but then the mere human should have a low accuracy / discrimination rate. You can't have this and a high accuracy rate at the same time. Also in practice there are plenty of one-boxers out there.)

Yep.

Conflict, in one form or another, lies at the center of every story. This is widely considered the first rule of writing. Novices should not consider defying it. Almost any book on writing will tell you this, often on the first page.

This Facebook thread on Scalia, which I retracted here. Remarks:
When I stated my condition of sufficient evidence to retract here:

Show me a similar lone dissent by Scalia on the recording of a factual background of no great religious or political valence, and I will withdraw my objection with apologies to Scalia and his great epistemic caution.

... I looked over what I'd written, carefully visualized being presented with that evidence in order to check that it was my True Rejection, and decided that yes, this would be sufficient and that I would in fact react in that case by changing my mind. I didn't anticipate such a piece of evidence to be offered (or I would've already had beliefs consistent with it) but I knew that it would greatly disappoint any skilled minds reading my Facebook feed if I stated such a condition and then went back on it or tried to add extra caveats.
After being presented by the evidence I'd requested here by Dan Haecker:

Often, because he thinks the court is straying from it's role and attempting to divine things like "what the law could be," Scalia joins "all but.." or parts of different opinions. For examples all last week, see http://www.supremecourt.gov/opinions/12pdf/11-1221_7l48.pdf http://www.supremecourt.gov/opinions/12pdf/12-167_d1oe.pdf http://www.supremecourt.gov/opinions/12pdf/12-62_5g68.pdf

...which met my stated condition, I also knew that my community would congratulate me on making a public opinion-change and had a fair degree of, "Yay, I so rarely get to impress my friends and show off my rationality skillz by publicly changing my opinion like this!" The next immediate comment after this is my retraction so you can see that the proper motion was executed without hesitating after I checked the provided evidence.
My friends, especially including Luke and Carl who I knew would both go for it, did in fact 'Like' the retraction (there or in a later status update for greater visibility) so my expectation was confirmed.
Generalizations: Visualize in advance whether the evidence you demand would actually change your mind; expecting your friends to be disappointed in you if you add any further demands will help you visualize this. Expected reliable community support matters a lot in terms of making you feel happier about getting a chance to change your mind.

Heh, every now and then I get a compliment which actually does make me feel like a proper genius in the pre-Dweckian unhealthy sense because it compliments something I accomplished with literally no effort. Likewise when somebody congratulates me on all the effort I must have put in to get HPMOR's time-travel plots straight. I do those in my head without any notes, and it doesn't feel difficult. I wonder if any other mathematician or mathematician-lite would say the same thing I would, that time-travel plots are much less complicated than even slightly serious math.

This seems to assume that conflict drives all narratives.
Is it the case that all stories have conflict as a primary aspect?


Even given a very fast local foom (to which I do assign a pretty small probability, especially as we make the situation more detailed and conclude that fewer things are relevant), I would still expect higher education and better discourse to improve the probability that people handle the situation well. It's weird to cash this out as a concrete scenario, because that just doesn't seem like how reasonable reasoning works.
But trying anyway: someone is deciding whether to run an AI or delay, and they correctly choose to delay. Someone is arguing that research direction X is safer than research direction Y, and others are more likely to respond selectively to correct arguments. Someone is more likely to notice there is a problem with a particular approach and they should do something differently, etc. etc.

How did this happen as a result of economic growth having a marginally greater exponent? Doesn't that just take us to this point faster and give less time for serial thought, less time for deep theories, less time for the EA movement to spread faster than the exponent on economic growth, etcetera? This decision would ceteris paribus need to be made at some particular cumulative level of scientific development, which will involve relatively more parallel work and relatively less serial work if the exponent of econ growth is higher. How does that help it be made correctly?
Exposing (and potentially answering) questions like this is very much the point of making the scenario concrete, and I have always held rather firmly on meta-level epistemic grounds that visualizing things out concretely is almost always a good idea in math, science, futurology and anywhere. You don't have to make all your predictions based on that example but you have to generate at least one concrete example and question it. I have espoused this principle widely and held to it myself in many cases apart from this particular dispute.

But at any rate, my main question is how you can be so confident of local foom that you think this tiny effect given local foom scenarios dominates the effect given business as usual?

Procedurally, we're not likely to resolve that particular persistent disagreement in this comment thread which is why I want to factor it out.

My secondary objection is to your epistemic framework. I have no idea how you would have thought about the future if you lived in 1800 or even 1900; it seems almost certain that this framework reasoning would have led you to crazy conclusions, and I'm afraid that the same thing is true in 2000.

I could make analogies about smart-people-will-then-decide and don't-worry-the-elite-wouldn't-be-that-stupid reasoning to various historical projections that failed, but I don't think we can get very much mileage out of nonspecifically arguing which of us would have been more wrong about 2000 if we had tried to project it out while living in 1800. I mean, obviously a major reason I don't trust your style of reasoning is that I think it wouldn't have worked historically, not that I think your reasoning mode would have worked well historically but I've decided to reject it because I'm stubborn. (If I were to be more specific, when I listen to your projections of future events they don't sound very much like recollections of past events as I have read about them in history books, where jaw-dropping stupidity usually plays a much stronger role.)
I think an important thing to keep in mind throughout is that we're not asking whether this present world would be stronger and wiser if it were economically poorer. I think it's much better to frame the question as whether we would be in a marginally better or worse position with respect to FAI today if we had the present level of economic development but the past century from 1913-2013 had taken ten fewer years to get there so that the current date were 2003. This seems a lot more subtle.

Hm, good point. I still suspect it's metaphorical. Then again, in a world where Fox News is currently saying how Edward Snowden may be a Chinese double agent, it may also be literal and truthful.


Increasing capital stocks, improving manufacturing, improving education, improving methodologies for discourse, figuring out important considerations. Making charity more efficient, ending poverty. Improving collective decision-making and governance. All of the social sciences. All of the hard sciences. Math and philosophy and computer science. Everything that everyone is working on, everywhere in the world.

How is an increased capital stock supposed to improve our x-risk / astronomical benefit profile except by being an input into something else? Yes, computer science benefits, that's putatively the problem. We need certain types of math for FAI but does math benefit more from increased capital stocks compared to, say, computing power? Which of these other things are supposed to save the world faster than computer science destroys it, and how? How the heck would terrorism be a plausible input into AI going badly? Terrorists are not going to be the most-funded organizations with the smartest researchers working on AGI (= UFAI) as opposed to MIT, Google or Goldman Sachs.
Does your argument primarily reduce to "If there's no local FOOM then economic growth is a good thing, and I believe much less than you do in local FOOM"? Or do you also think that in local FOOM scenarios higher economic growth now expectedly results in a better local FOOM? And if so is there at least one plausible specific scenario that we can sketch out now for how that works, as opposed to general hopes that a higher economic growth exponent has vague nice effects which will outweigh the shortening of time until the local FOOM with a correspondingly reduced opportunity to get FAI research done in time? When you sketch out a specific scenario, this makes it possible to point out fragile links which conjunctively decrease the probability of that scenario, and often these fragile links generalize, which is why it's a bad idea to keep things vague and not sketch out any concrete scenarios for fear of the conjunction fallacy.
It seems to me that a lot of your reply, going by the mention of things like terrorism and poverty, must be either prioritizing near-term benefits over the astronomical future, or else being predicated on a very different model from local FOOM. We already have a known persistent disagreement on local FOOM. This is an important modular part of the disagreement on which other MIRIfolk do not all line up on one side or another. Thus I would like to know how much we disagree about expected goodness of higher econ growth exponents given local FOOM, and whether there's a big left over factor where "Paul Christiano thinks you're just being silly even assuming that a FOOM is local", especially if this factor is not further traceable to a persistent disagreement about competence of elites. It would then be helpful to sketch out a concrete scenario corresponding to this disagreement to see if it looks even more fragile and conjunctive.
(Note that e.g. Wei Dai also thought it was obviously true that faster econ growth exponents had a negative-sign effect on FAI, though, like me, this debate made him question (but not yet reject) the 'obvious' conclusion.)

As y'all know, I agree with Hume (by way of Jaynes) that the error of projecting internal states of the mind onto the external world is an incredibly common and fundamental hazard of philosophy.
Probability is in the mind to start with; if I think that 103,993 has a 20% of being prime (I haven't tried it, but Prime Number Theorem plus it being not divisible by 2, 3, or 5 wild ballpark estimate) then this uncertainty is a fact about my state of mind, not a fact about the number 103,993. Even if there are many-worlds whose frequencies correspond to some uncertainties, that itself is just a fact; probability is in the map, not in the territory.
Then we have Knightian uncertainty, which is how I feel when I try to estimate AI timelines, i.e., when I query my brain on different occasions it returns different probability estimates, and I know there are going to be some effects which aren't on my causal map. This is a kind of doubly-subjective double-uncertainty. Of course you still have to turn it into betting odds on pain of violating von Neumann-Morgenstern; see also the Ellsberg paradox of inconsistent decision-making if ambiguity is given a special behavior.
Taking this doubly-map-level property of Knightian uncertainty (a sort of confusion about probabilities) and trying to reify it in the territory as a kind of stuff (encoded in hidden interstices of QM) which somehow plays an irreplaceable functional role in cognition is...
...probably not going to be the best-received philosophical speculation ever posted to LW. I mean, as a species we should know by now that this kind of idea just basically never turns out to be correct. If X is confusing and Y is confusing this does not make X a good explanation for Y when X makes no new experimental predictions about Y even in retrospect, thou shalt not answer confusing questions by postulating new mysterious opaque substances, etc.

What's a specific relevant example of something people are trying to speed up / not speed up besides AGI (= UFAI) and FAI? You pick out aging, disease, and natural disasters as not-sped-up but these seem very loosely coupled to astronomical benefits.

Principle of charity: "Denial of existence" is to taken as meaning "Don't think about, don't care about, don't act based on, don't know how many there were" and not "When explicitly asked if drone strikes have victims, say 'no'."

I'm not truly impressed with GiveWell's general optimization since they never made a good case that malaria was connected to astronomical benefits or, indeed, seem to have realized that such a case is necessary for effective altruism. But for near-term certain benefits they still think in utilons and do cross-cause comparison of things that will produce those utilons; which puts them far, far ahead of a Gates Foundation which AFAICT picks a measurable cause at emotional whim and then optimizes within that cause. Especially since most of the variance in returns is between causes. GF's only possible claim to superior impact per dollar for some dollars would have to come from longer-term funding of science and technology, taking on risk and time horizon which Givewell refuses. But a great deal of GF's funding also goes to near-term utilons obtained by known mechanisms, so they are clearly competing on ground to which Givewell has staked a plausible claim of optimization, and apparently doing so at whim and not by optimization.
I expect of course that no good justification shall be forthcoming of why GF didn't fund the Against Malaria Foundation with a casual wave of their hands, but perhaps you shall call this small Bayesian evidence because of a worry that if this justification existed, GF would be unlikely to publish it. Givewell is usually pretty open about that sort of thing. But perhaps GF is more constrained, and does not for PR reasons publish the negative judgments of their secret council of epistemic rationalists. But then why haven't we seen the positive judgments? Who would put in Holden's level of cognitive work and then say nothing of that work, and why? Why keep the reasoning of your effective altruism a secret?
More generally, what would make you update towards "People are crazy, the world is mad"? When in many cases such as this I see no evidence that the world is sane, I update towards madness.

I second this question. Are we now completely certain of this rarity?

It should be noted that those characters who oppose such things are Good, even if they must choose between one perceived evil and another.
Also, Nega-Frankena is scary.

I have no "so bad it's good" gene, so I would usually stop reading such a work instantly, like a fanfic with multiple spelling errors in the first chapter. If I had to name a work I read all the way through, Lev Grossman, The Magicians. It's well-written along all other dimensions but I found the protagonists to be needlessly existential - the 'protagonists bored with everything' turned what could have been a great book into a merely good one. That is a literarily influenced SF&F story, of course.

Wouldn't this trivially go away by redenominating outcomes in utilons instead of dollars with diminishing marginal returns?

Your props go to Lucas Sloan. Hail Lucas!

(General reminder: While it is possible that some hidden justification exists here, the default assumption is that "People are crazy, the world is mad" and the Gates Foundation has not happened to be an unusual exception. Failure to optimize is a human default and it is not particularly probable that GiveWell is doing anything wrong.)

PQWAK.
Personally, when I encounter a difficulty, I say to myself, "This wouldn't stop Akemi Homura."

Omega has been observed to have a less than 1% error rate, I assume.

There's been more recent work suggesting that planets are extremely common. Most recently, evidence for planets in unexpected orbits around red dwarfs have been found. See e.g. here. This is in addition to other work suggesting that even when restricted to sun-like stars, planets are not just common, but planets are frequently in the habitable zone. Source(pdf). It seems at this point that any aspect of the Great Filter that is from planet formation must be declared to be completely negligible. Is this analysis accurate?

I'd be happy to pioneer it on LW if it was a simple enough algorithm. StackOverflow, MathOverflow, Quora, possibly Reddit might be quite interested if it worked. (I don't know if there's acknowledged borrowing - keeping in mind that we borrowed all of Reddit's code in the first place and it was under an open-source give-back license - but Reddit seems to have adopted LW's highlight-recent-comments innovation, so there's been some backflow.) Wikipedia is disintegrating under the weight of deletionists and would probably have to be rebooted more than healed, but Earth needs a Wikipedia. There are plenty of likely adopters / testers / provers in advance of the general scientific community if a superior karma algorithm can be found.

Hm. A brief glance at Thomas's profile makes it hard to be sure. I will be on the lookout.

Hm?

Those were matches with Rybka handicapped (an odds match is a handicapped match) and Deep Rybka 3.0 is a substantial improvement over Rybka. The referenced "Zappa" which played Rybka evenly is another computer program. Read the reference carefully.

I should post separately about this at some point.
Suppose we have a Collective Judgment of Science system in which scientific karma enters the system at highly agreed-upon points, e.g. very well-replicated, significant findings. Is there a system with the following properties:

The karma entry points need not necessarily be the most trusted people. Let's say you made a significant discovery, but 70% of the field disagrees with most of your opinions, and someone who hasn't made a significant discovery is trusted by 95% of the people who make significant discoveries. We should perhaps believe the latter person over you; making one discovery is not proof of perfect epistemic reliability.
If someone goes rogue and endorses a thousand trolls, who in turn endorse a million trolls, the million trolls can do no more karmic damage / produce no more karmic distortion, than the original person.
If I make three significant discoveries or write three good papers, there is no incentive to spread those papers out over 3 pseudonyms, or coauthor them with 3 others, in terms of how much influence I will have afterward. There may potentially be some incentive to centralize, although this would also not be good.
Downvoting or strongly downvoting an idea that many reliable epistemic voters think is correct may potentially be taken as Bayesian evidence by the system that you sometimes downvote good ideas. It's probably worth distinguishing this from concluding that you sometimes upvote bad ideas, without separate evidence.
Rather than give people an incentive to waste labor by systematically downvoting everything that person X said, there is a centralized "I think this person is a complete idiot" button. After pressing this button, further systematic downvoting has no effect. Obviously the order of operations should not be significant here, i.e., this button must have as much effect as downvoting everything. Perhaps you might be asked to look at the person's 3 highest-karma nodes and asked if you really want to downvote those too (vs. an "I hate most but not all things you say" rating) given that indicating "I uniformly hate everything you say" may then potentially reflect poorly on your reliability.
Within these constraints, it should be generally true that one person who's gotten a large karma prize cannot outvote 100 people who were all endorsed by trusted epistemics with karma originating from sources outweighing that single prize.
We're okay with this system using terabytes or even petabytes of memory to scale, so long as it's not exabytes and it can compute updates in real time, or at least less than an hour.
Being able to run on upvotes and downvotes is great, failing that having people click on a 5-star level or a linear spectrum is about as much info as we should ask, since most users will not provide more info than this on most occasions. We could potentially have a standard 5-star scale which by leaving the mouse present for 5 seconds can go to 6 stars, or a 7-star rating which can be given once per month, or something. We can't ask users to rate along 3 separate dimensions.
We should take into account that some people have pickier standards and downvote more easily or upvote more rarely than others, or conversely someone who endorses almost everything is only providing discriminatory Bayesian evidence about a threshold on the low end of the quality scale.
We can suppose that nodes are clustered in a 3-level hierarchy by broadest area, subject, and subspecialization but probably shouldn't suppose any more clustering in the data than this. It's possible we shouldn't try to assess it at all.
A consequence of this system is that as a philosopher, you can potentially achieve great endorsement of your perspicacity, but only by convincing people who were upvoted by people who delivered well-replicated significant experimental results. This strikes me as a feature, not a bug. I don't know of any particularly better way to decide which philosophers are reliable.
It can potentially be possible to bet karma on predictions subject to definite settlement a la a prediction market, since this can only operate to increase reliability of the system. If an open question that people opinionated about is definitely settled, anyone who was bold in predicting a minority correct answer should have their karma in some way benefit. Again we do not want an incentive to create pseudonyms to get independent karma awards here. (We can perhaps imagine such a question-node as a single source which endorses everyone who endorsed its correct answer.)
Presentation ordering of new nodes takes into account a value-of-information calculation, not just the highest confidence in current karma. (Obviously, under such a calculation, more prolifically voting users will see more recent nodes. This is also fine.)


I wish to disclaim my prior reply as long-ago and made-up-on-the-spot.

(Since the average LW reader may not know whether to trust that this commenter is a mathematician specializing in logic and foundations, I remark that the above summary sounds much like the papers I've read on second-order logic. Though 'pseudo-arguing' is an odd way to describe the ancient expositional tradition of dialogue.)

That is not how government contracts work.

When killer robots are outlawed, only rogue nations will have massive drone armies.
An ideal outcome here would be if counter-drones have an advantage over drones, but it's hard to see how this could obtain when counter-counter-drones should be in a symmetrical position over counter-drones. A second-best outcome would be no asymmetrical advantage of guerilla drone warfare, where the wealthiest nation clearly wins via numerical drone superiority combined with excellent enemy drone detection.
...you know, at some point the U.S. military is going to pay someone $10 million to conclude what I just wrote and they're going to get it half-wrong. Sigh.

Prolonged events with no clearly defined moment of hitting the newspapers all at once seem to me to have lower effects on public opinion. Contrast the long, gradual, steady incline of chessplaying power going on for decades earlier, vs. the Deep Blue moment.


As far as I know, people have predicted every single big economic impact from technology well in advance, in the strong sense of making appropriate plans, making indicative utterances, etc.

Is the thesis here that the surprisingness of atomic weapons does not count because there was still a 13-year delay from there until commercial nuclear power plants? It is not obvious to me that the key impact of AI is analogous to a commercial plant rather than an atomic weapon. I agree that broad economic impacts of somewhat-more-general tool-level AI may well be anticipated by some of the parties with a monetary stake in them, but this is not the same as anticipating a FOOM (X), endorsing the ideals of astronomical optimization (Y) and deploying the sort of policies we might consider wise for FOOM scenarios (Z).

You could say the same about nuclear power. It's conceivable that with enough noise about "AI is costing jobs" the broad positive impacts could be viewed as ritually contaminated a la nuclear power. Hm, now I wonder if I should actually publish my "Why AI isn't the cause of modern unemployment" writeup.

General remark: At some point I need to write a post about how I'm worried that there's an "unpacking fallacy" or "conjunction fallacy fallacy" practiced by people who have heard about the conjunction fallacy but don't realize how easy it is to take any event, including events which have already happened, and make it look very improbable by turning one pathway to it into a large series of conjunctions. E.g. I could produce a long list of things which allegedly have to happen for a moon landing to occur, some of which turned out to not be necessary but would look plausible if added to the list ante facto, with no disjunctive paths to the same destination, and thereby make it look impossible. Generally this manifests when somebody writes a list of alleged conjunctive necessities, and I look over the list and some of the items seem unnecessary (my model doesn't go through them at all), obvious disjunctive paths have been omitted, the person has assigned sub-50% probability to things that I see as mainline 90% probabilities, and conditional probabilities when you assume the theory was right about 1-N would be significantly higher for N+1. Most of all, if you imagine taking the negation of the assertion and unpacking it into a long list of conjunctive probabilities, it would look worse - there should be a name for the problem of showing that X has weak arguments but not considering that ~X has even weaker arguments. Or on a meta level, since it is very easy to make things look more conjunctive, we should perhaps not be prejudiced against things which somebody has helpfully unpacked for us into a big conjunction, when the core argument still seems pretty simple on some level.
When I look over this list, my reaction is that:
(1) is a mainline assumption with odds of 5:1 or 10:1 - of course future intergalactic civilization bottlenecks through the goals of a self-improving agency, how would you get to an intergalactic civilization without that happening? If this accounts for much of our disagreement then we're thinking about entirely different scenarios, and I'm not sure how to update from your beliefs about mostly scenario B to my beliefs about mostly scenario A. It makes more sense to call (1) into question if we're really asking about global vs. local, but then we get into the issue of whether global scenarios are mostly automatic losses anyway. If (1) is really about whether we should be taking into account a big chunk of survivable global scenarios then this goes back to a previous persistent disagreement.
(2) I don't see the relevance - why does a long time horizon vs. a short time horizon matter? 80 years would not make me relax and say that we had enough serial depth, though it would certainly be good news ceteris paribus, there's no obvious threshold to cross.
Listing (3) and (4) as separate items was what originally made my brain shout "unpacking fallacy!" There are several subproblems involved in FAI vs. UFAI, of which the two obvious top items are the entire system being conducive to goal stability through self-improvement which may require deducing global properties to which all subsystems must be conducive, and the goal loading problem. These both seem insight-heavy which will require serial time to solve. The key hypothesis is just that there are insight-heavy problems in FAI which don't parallelize well relative to the wide space of cobbled-together designs which might succeed for UFAI. Odds here are less extreme than for (1) but still in the range of 2:1-4:1. The combined 3-4 issue is the main weak point, but the case for "FAI would parallelize better than UFAI" is even weaker.
(5) makes no sense to ask as a conditionally independent question separate from (1); if (1) is true then the only astronomical effects of modern-day economic growth are whatever effects that growth has on AI work, and to determine if economic growth is qualitatively good or bad, we ask about the sign of the effect neglecting its magnitude. I suppose if the effect were trivial enough then we could just increase the planet's growth rate by 5% for sheer fun and giggles and it would have no effect on AI work, but this seems very unlikely; a wealthier planet will ceteris paribus have more AI researchers. Odds of 10:1 or better.
On net, this says that in my visualization the big question is just "Does UFAI parallelize better than FAI, or does FAI parallelize better than UFAI?" and we find that the case for the second clause is weaker than the first; or equivalently "Does UFAI inherently require serial time more than FAI requires serial time?" is weaker than "Does FAI inherently require serial time more than UFAI requires serial time?" This seems like a reasonable epistemic state to me.
The resulting shove at the balance of the sign of the effect of economic growth would have to be counterbalanced by some sort of stronger shove in the direction of modern-day economic growth having astronomical benefits. And the case for e.g. "More econ growth means friendlier international relations and so they endorse ideal Y which leads them to agree with me on policy Z" seems even more implausible when unpacked into a series of conjunctions. Lots of wealthy people and relatively friendly nations right now are not endorsing policy Z.
To summarize and simplify the whole idea, the notion is:
Right now my estimate of the sign of the astronomical effect of modern-day economic growth is dominated by a 2-node conjunction of, "Modern-day econ growth has a positive effect on resources into both FAI and UFAI" and "The case for FAI parallelizing better than UFAI is weaker than the converse case". For this to be not true requires mainly that somebody else demonstrate an effect or set of effects in the opposite direction which has better net properties after its own conjunctions are taken into account. The main weakness in the argument and lingering hope that econ growth is good, isn't that the original argument is very conjunctive, but rather it's that faster econ growth seems like it should have a bunch of nice effects on nice things and so the disjunction of other econ effects might conceivably swing the sign the other way. But it would be nice to have at least one plausible such good effect without dropping our standards so low that we could as easily list a dozen equally (im)plausible bad effects.

Even without doing any calculations, it is extraordinarily hard to imagine that the difference between "world at war" and "world at peace" is less than the difference between "world with slightly more parallelization in AI work" and "world with slightly less parallelization;"

With small enough values of 'slightly' obviously the former will have a greater effect, the question is the sign of that effect; also it's not obvious to me that moderately lower amounts of econ growth lead to world wars, and war seems qualitatively different in many respects from poverty. I also have to ask if you are possibly maybe being distracted by the travails of one planet as a terminal value, rather than considering that planet's instrumental role in future galaxies.


Yes, people now believe that computers can beat people at chess.

It was on our national television, few months ago. Kasparov was here, opened some international chess center for young players in Maribor. He gave an interview and among other things, he told us how fishy was the Deep Blue victory and not real in fact.
At least a half of the population believed him.

I notice I am confused (he said politely). Kasparov is not stupid and modern chess programs on a home computer e.g. Deep Rybka 3.0 are overwhelmingly more powerful than Deep Blue, there should be no reasonable way for anyone to delude themselves that computer chess programs are not crushingly superior to unassisted humans.

Actual use of Sith techniques seems to turn people evil at ridiculously accelerated rates. At least in-universe it seems that sensible people would write off this attractive-sounding philosophy as window dressing on an extremely damaging set of psychic techniques.

The problem is that an AI which passes the unrestricted Turing test must be strictly superior to a human; it would still have all the expected AI abilities like high-speed calculation and so on. A human who was augmented to the point of passing the Pocket Calculator Equivalence Test would be superhumanly fast and accurate at arithmetic on top of still having all the classical human abilities, they wouldn't be just as smart as a pocket calculator.

Do we have any evidence that they updated to expecting HAL in the long run? Normatively, I agree that ideal forecasters shouldn't be doing their updating on press releases, but people sometimes argue that press release W will cause people to update to X when they didn't realize X earlier.

A biscuit provides the same number of calories as 100 SQUID, which stands for Superconducting Quantum Interference Device, which weigh a pound apiece, which masses 453.6 grams, which converts to 4 * 10^16 joules, which can be converted into 1.13 * 10^10 kilowatt-hours, which are worth 12 cents per kW-hr, so around 136 billion dollars or so.

FAI builders do not need to be saints. No sane strategy would be set up that way. They need to endorse principles of non-jerkness enough to endorse indirect normativity (e.g. CEV). And that's it. Morality is not sneezed into AIs by contact with the builders.

Yes, people now believe that computers can beat people at chess.

Even I think they'd take a mind upload seriously - that might really produce a huge public update though probably not in any sane direction - though I don't expect that to happen before a neuromorphic UFAI is produced from the same knowledge base. They normatively ought to take a spider upload seriously. Something passing a restricted version of a Turing test might make a big public brouhaha, but even with a restricted test I'm not sure I expect any genuinely significant version of that before the end of the world (unrestricted Turing test passing should be sufficient unto FOOM). I'm not sure what you 'ought' to take seriously if you didn't take computers seriously in the first place. Aubrey was very specific in his prediction that I disagree with, people who forecast watershed opinion-changing events for AI are less so at least as far as I can recall.

Demand for extremely safe assets increased (people wanted to hold more money), the same reason Treasury bonds briefly went to negative returns; demand for loans decreased and this caused destruction of money via the logic of fractional reserve banking; the shadow banking sector contracted so financial entities had to use money instead of collateral; etc.

I don't know, actually. I'm not the one making these forecasts. It's usually described as some broad-based increase of AI competence but not cashed out any further than that. I'll remark that if there isn't a sharp sudden bit of headline news, chances of a significant public reaction drop even further.

If the ethical hidden variables turn out unfavorably, you have more to make up for than that. HPJEV thinking animals are not sentient has probably lost the world more than one vegetarian-lifetime.

This seems unlikely to be a significant fraction of my impact upon the summum bonum, for good or ill.

Ethical generalizations check: Do you care about Babyeaters? Would you eat Yoda?

This comment was banned, which looked to me like a probable accident with a moderator click, so I unbanned it. If I am in error can whichever mod PM me after rebanning it?
Naturally if this was an accident, it must have been a quantum random one.

Some Facebook discussion here including Carl's opinion:
https://www.facebook.com/yudkowsky/posts/10151665252179228

Actually I'd be quite confident in no fates worse than death emerging from that scenario. There wouldn't be time for anyone to mess up on constructing something almost in our moral frame of reference, just the first working version of AIXI / Schmidhuber's Godel machine eating its future light cone.

We've already had high levels of future shock and it hasn't translated into any such interest. This seems like an extremely fragile and weak transmission mechanism. (So do most transmission mechanisms of the form, "Faster progress will lead people to believe X which will support ideal Y which will lead them to agree with me/us on policy Z.")


1) Do Earths with dumber politicians have a better chance at FAI?

How much dumber? If we can make politicians marginally dumber in a way that slows down economic growth, or better yet decreases science funding while leaving economic growth intact, without this causing any other marginal change in stupid decisions relevant to FAI vs. UFAI, then sure. I can't think of any particular marginal changes I expect because I already expect almost all such decisions to be made incorrectly, but I worry that this is only a failure of my imagination on my part - that with even dumber politicians, things could always become unboundedly worse in ways I didn't even conceive.

2) Do Earths with anti-intellectual culture have a better chance at FAI?

This seems like essentially the same question as above.

Do Earths with less missionary rationalism have a better chance at FAI?

No. Missionary rationalists are a tiny fraction of world population who contribute most of FAI research and support.

If there were a sufficiently smart government with a sufficiently demonstrated track record of cluefulness whose relevant officials seemed to genuinely get the idea of pro-humanity/pro-sentience/galactic-optimizing AI, the social ideals and technical impulse behind indirect normativity, and that AI was incredibly dangerous, I would consider trusting them to be in charge of a Manhattan Project with thousands of researchers with enforced norms against information leakage, like government cryptography projects. This might not cure required serial depth but it would let FAI parallelize more without leaking info that could be used to rapidly construct UFAI. I usually regard this scenario as a political impossibility.
Things that result in fewer resources going into AI specifically would result in fewer UFAI resources without reducing overall economic growth, but it needs to be kept in mind that some such research occurs in financial firms pushing trading algorithms, and a lot more in Google, not just in places like universities.

That's why I keep telling people about Scott Sumner, market monetarism, and NGDP level determinism - it might not let you beat the stock market indices, but you can end up with some really bizarre expectations if you don't know about the best modern concept of "tight money" and "loose money". E.g. all the people who were worried about hyperinflation when the Fed lowered interest rates to 0.25 and started printing huge amounts of money, while the market monetarists were saying "You're still going to get sub-trend inflation, our indicators say there isn't enough money being printed."
Beating the market is hard. Not being stupid with respect to the market is doable.

Excellent reply. I'm pretty sure I'd feel the same way if I was reading a story where A wants to be with only B, B wants to be with only A, neither of them want to be with C, but it's just never occurred to them that monogamy is an option.

One could just as easily argue that an era of slow growth will take technological pessimism seriously, while an era of fast growth is likely to want to go gung-ho full-speed-ahead on everything.

If we all got superuniversal-sized computers with halting oracles, we'd die within hours. I'm not sure the implausible extremes are a good way to argue here.


Moore's Law is one of the few things that won't be affected.

FAI seems to me to be mostly about serial depth of research. UFAI seems to be mostly about cumulative parallel volume of research. Things that affect this are effectual even if Moore's Law is constant.

I'm also not fully convinced that RGDP growth and the amount of hack-AI research are monotonically connected.

We could check how economic status affects science funding.

Regardless of what happens to the economy, the first- and second-wave Internet generations will take power on a fixed schedule. That's way more kaboomy than economic growth is.

? What does your model claim happens here?

This is true for non-super-huge startups that donate any noticeable fraction of generated wealth to EA, yes - that amount is not a significant percentage of overall global econ growth, and would be a much larger fraction of FAI funding.

Are these high-IQ folk selectively working on FAI rather than AGI to a sufficient degree to make up for UFAI's inherently greater parallelizability?
EDIT: Actually, smarter researchers probably count for more relative bonus points on FAI than on UFAI to a greater extent than even differences of serial depth of cognition, so it's hard to see how this could be realistically bad. Reversal test, dumber researchers everywhere would not help FAI over UFAI.

If a good outcome requires that influential people cooperate and have longer time-preferences, then slower economic growth than expected might increase the likelihood of a bad outcome.
It's true that periods of increasing economic growth haven't always lead to great technology decision-making (Cold War), but I'd expect an economic slowdown, especially in a democratic country, to make people more willingly to take technological risks (to restore economic growth), and less likely to cooperate with or listen to or fund cautious dissenters, (like people who say we should be worried about AI).

Arguably worth it for $30 of reduced guilt, bragging rights and twisted, warped enjoyment of ethical weirdness.

Sounds like a rather fragile causal pathway. Especially if one is joining an ensemble.

This is not how I make choices.

To be clear, the question is not whether we should divert resources from FAI research to trying to slow world economic growth, that seems risky and ineffectual. The question is whether, as a good and ethical person, I should avoid any opportunities to join in ensembles trying to increase world economic growth.

Offhand I'd think a world like that would have a much higher chance of survival. Their initial hardware would be much weaker and use much better algorithms. They'd stand a vastly better change of getting intelligence amplification before AI. Advances in neuroscience would have a long lag time before translating into UFAI. Moore's Law is not like vanilla econ growth - I felt really relieved when I realized that Moore's Law for serial speeds had definitively broken down. I am much less ambiguous about that being good news than I am about the Great Stagnation or Great Recession being disguised good news.

Since all of my work output goes to effective altruism, I can't afford any optimization of my meals that isn't about health x productivity. This does sometimes make me feel worried about what happens if the ethical hidden variables turn out unfavorably. Assuming I go on eating one meat meal per day, how much vegetarian advocacy would I have to buy in order to offset all of my annual meat consumption? If it's on the order of $20, I'd pay $30 just to be able to say I'm 50% more ethical than an actual vegetarian.


R&D, especially foundational work, is such a small part of worldwide GDP that any old effect can dominate it.

(Note: I agree with this point.)

Who would have otherwise bought that currency in the foreign exchange market? Where would they have spent it? Are higher prices on the foreign exchange market a good thing?

The reason why I asked my original question is that GiveDirectly is directly handing money to the recipients, and this money is presumably competing with other money to make purchases. If one were to naively measure the effectiveness via the direct observation, "Ooh, the people with more money are better off than the people with less money!" then this kinda begs the question. It's not obvious to me that foreign money behaves differently in this regard if it's competing for goods that other low-income communities are also trying to purchase. Again, you can justify this model but it's not as simple as observing that the people with more money are better off. You need the country to be running an NGDP deficit / aggregate demand shortfall, for purchased imports to help, for redistribution of purchasing power to be good, etc.

An Idiot Plot is any plot that goes away if the characters stop being idiots. A Muggle Plot is any plot which dissolves in the presence of transhumanism and polyamory. That exact form is surprisingly common; e.g. from what I've heard, canon!Twilight has two major sources of conflict, Edward's belief that turning Bella into a vampire will remove her soul, and Bella waffling between Edward and Jacob. I didn't realize it until Baughn pointed it out, but S1 Nanoha - not that I've watched it, but I've read fanfictions - counts as a Muggle Plot because the entire story goes away if Precia accepts the pattern theory of identity.

MAHOU SHOUJO TRANSHUMANIST NANOHA
"Girl," whispered Precia. The little golden-haired girl's eyes were fluttering open, amid the crystal cables connecting the girl's head to the corpse within its stasis field. "Girl, do you remember me?"
It took the girl some time to speak, and when she did, her voice was weak. "Momma...?"
The memories were there.
The brain pattern was there.
Her daughter was there.
"Momma...?" repeated Alicia, her voice a little stronger. "Why are you crying, Momma? Did something happen? Where are we?"
Precia collapsed across her daughter, weeping, as some part of her began to believe that the long, long task was finally over.

Fair, the British were totally befriending their way through history for a while.

No, in accordance with whatchamacallit's law.
If you end up with complex probabilities, you won't be able to plug them into an expected utility formula to get a preference ordering. This has always been the knockdown argument for quantitatively scaled real-number subjective probabilities in my book. Even if underlying physics turns out to use complex-numbered reality fluid, I don't see how I can make choices if my degree of anticipation for something happening to me is not a real number - I don't know of any complex analogue of the von Neumann-Morgenstern theorem which yields actual decision outputs.

JusticeBot cooperates with anyone who cooperates with FairBot, and is exploitable by any agent which comprehends source code well enough to cooperate with FairBot and defect against JusticeBot. Though I'm going here off the remembered workshop rather than rechecking the paper.

Hm. I think X within the test could be introduced as a new constant and solved, but I'm not sure.

That's more along the lines of, "I will convert my enemies to friends by STARLIGHT BREAKER TO THE FACE".
Offhand I can't think of a single well-recorded real-life historical instance where this has ever worked.

But surely, good sir, common sense says that you should defect against CooperateBot in order to punish it for cooperating with DefectBot.
Also, in modal combat your bot is X=[]Y(CooperateBot) and is easily detected via the test [1](Y(X)<->[]X(CooperateBot)) where [1] denotes provability in T+Con(T), ie [1](Q) = []((~[]F)->Q).

(I endorse essentially all of Benja's reply above.)

Sounds like Takamachi Nanoha to me.


Was it UDT1.1 (as a solution to this problem) that violates the Vingean principle?

As I remarked in that thread, there are many possible designs that violate the Vingean principle, AFAICT UDT 1.1 is one of them.

Also, I'm wondering if Benja's polymorphism approach solves the "can't decide whether or not to commit suicide" problem that I described here. Your paper doesn't seem to address this problem since the criteria of action you use all talk about "NULL or GOAL" and since suicide leads to NULL, an AI using your criterion of action has trouble deciding whether or not to commit suicide for an even more immediate reason.

Suicide being permitted by the NULL option is a different issue from suicide being mandated by self-distrust. Benja's TK gets rid of distrust of offspring. Work on reflective/naturalistic trust is ongoing.

The paper on probabilistic reflection in logic is non-constructive, but that's only sec. 4.3 of the Lob paper. Nothing non-constructive about T-n or TK.

Hm, that's true. Okay, you do need enough intelligence in the OS to detect certain types of simulations / and/or the intention to build such simulations, however obscured.
If you can verify an agent's goals (and competence at self-modification), you might be able to trust zillions of different such agents to all run at root level, depending on what the tiny failure probability worked out to quantitatively.

(Quick note: Wei's quining violates the naturalistic principle, not the Vingean principle. Wei's actions were still inside quantifiers but had separate forms for self-modification and action. So did Benja's original proposal in the Quirrell game, which Wei modified - I was surprised and impressed when Benja's polymorphism approach carried over to a naturalistic system.)

Seems kind of obvious? We've got plenty of people running around saying "Perhaps you overestimate your importance".

A secure operating system for governed matter doesn't need to take the form of a powerful optimization process, nor does verification of transparent agents trusted to run at root level. Benja's hope seems reasonable to me.


It sounds like you agree, but are saying that some analogous but more complicated problem might arise for probabilistic agents, and that it might not be resolved be whoever else is making AI unless this research is done by MIRI.

That's not it, rather:

I think what you're saying is that getting a good framework for reasoning about reasoning could be important for making AGI go well. This is plausible to me. And then you're also saying that working on this Lobian stuff is a reasonable place to start. This is not obvious to me, but this seems like something that could be subtle, and I understand the position better now.

Yep. We have reasoning frameworks like the currently dominant forms of decision theory, but they don't handle reflectivity well.
The Lob Problem isn't a top-priority scary thing that is carved upon the tombstones of worlds, it's more like, "Look! We managed to crisply exhibit something very precise that would go wrong with standard methods and get started on analyzing and fixing it! Before we just saw in a more intuitive sense that something would go wrong when we applied standard theories to reflective problems but now we can state three problems very precisely!" (Lob and coherent quantified belief sec. 3, nonmonotonicity of probabilistic reasoning sec. 5.2 & 7, maximizing / satisficing not being good-enough idioms for bounded agents sec. 8.) Problems with reflectivity in general are expectedly carved upon the tombstones of worlds because they expectedly cause problems with goal stability during self-modification. But to make progress on that you need crisp problems to provide fodder for getting started on finding a good shape for a reflective decision theory / tiling self-improving agent.

It depends on the sort of guarantee you want. Certainly I can say things of the form "X and Y differ from each other in mean by at most 0.01" with a confidence that high, without 10^100 samples (as long as the samples are independent or at least not too dependent).
If your optimization problem is completely unstructured then you probably can't do better than the number of samples you have, but if it is completely unstructured then you also can't prove anything about it, so I'm not sure what point you're trying to make. It seems a bit unimaginative to think that you can't come up with any statistical structure to exploit, especially if you think there is enough mathematical structure to prove strong statements about self-modification.

If you can get me a conditionally independent failure probability of 10^-100 per self-modification by statistical techniques whose assumptions are true, I'll take it and not be picky about the source. It's the 'true assumptions' part that seems liable to be a sticking point. I understand how to get probabilities like this by doing logical-style reasoning on transistors with low individual failure probabilities and proving a one-wrong-number assumption over the total code (i.e., total code functions if any one instruction goes awry) but how else would you do that?

What sort of statistical testing method would output a failure probability of at most 10^(-100) for generic optimization problems without trying 10^100 examples? You can get this in some mathematical situations but only because if X doesn't have property Y then it has an independent 50% chance of showing property Z on many different trials of Z. For more generic optimization problems, if you haven't tested fitness on 10^100 occasions you can't rule out a >10^100 probability of any sort of possible blowup. And even if you test 10^100 samples the guarantee is only as strong as your belief that the samples were taken from a probability distribution exactly the same as real-world contexts likely to be encountered, down to the 100th decimal place.

http://en.wikipedia.org/wiki/Fungibility

Actually, "MIRI is just churning out writeups of research which happened over a long preceding time", which I'd emphasize because I think there's some sort of systematic "science as press release bias" wherein people think about science as though it occurs in bursts of press releases because that's what they externally observe. Not aimed at you in particular but seems like an important general observation. We couldn't do twice as much research by writing twice as many papers.

This is correct; naturalistic trust subsumes indefinitely tiling trust.

I feel like I'm not clear on what question you're asking. Can you give an example of what a good answer would look like, maybe using Xs and Ys since I can hardly ask you to come up with an actual good argument?

Note that if you can get a high price from Satan on your own soul (e.g. rulership of a country), this is a no-lose arbitrage deal since souls are fungible goods.

I show the sequence to the AI and say, "CEV shouldn't work like this - this is a negative example of CEV."
"Example registered," says the young AI. "Supplementary query: Identify first forbidden transition, state general rule prohibiting it?"

Self-modification is to be interpreted to include 'directly editing one's own low-level algorithms using high-level deliberative process' but not include 'changing one's diet to change one's thought processes'. If you are uncomfortable using the word 'self-modification' for this please substitute a new word 'fzoom' which means only that and consider everything I said about self-modification to be about fzoom.
Humans wouldn't look at their own source code and say, "Oh dear, a Lobian obstacle", on this I agree, but this is because humans would look at their own source code and say "What?". Humans have no idea under what exact circumstances they will believe something, which comes with its own set of problems. The Lobian obstacle shows up when you approach things from the end we can handle, namely weak but well-defined systems which can well-define what they will believe, whereas human mathematicians are stronger than ZF plus large cardinals but we don't know how they work or what might go wrong or what might change if we started editing neural circuit #12,730,889,136.
As Christiano's work shows, allowing for tiny finite variances of probability might well dissipate the Lobian obstacle, but that's the sort of thing you find out by knowing what a Lobian obstacle is.

...I've actually said it many, many times before but there's a lot of people out there depicting that particular straw idea (e.g. Mark Waser).

(Reply to.)

My previous understanding had been that MIRI staff think that by default, one should expect to need to solve the Lob problem in order to build a Friendly AI.

By default, if you can build a Friendly AI you were not troubled by the Lob problem. That working on the Lob Problem gets you closer to being able to build FAI is neither obvious nor certain (perhaps it is shallow to work on directly, and those who can build AI resolve it as a side effect of doing something else) but everything has to start somewhere. Being able to state crisp difficulties to work on is itself rare and valuable, and the more you engage with a problem like stable self-modification, the more you end up knowing about it. Engagement in a form where you can figure out whether or not your proof goes through is more valuable than engagement in the form of pure verbal arguments and intuition, although the latter is significantly more valuable than not thinking about something at all.
Reading through the whole Tiling paper might make this clearer; it spends the first 4 chapters on the Lob problem, then starts introducing further concepts once the notion of 'tiling' has been made sufficiently crisp, like the Vingean principle or the naturalistic principle, and then an even more important problem with tiling probabilistic agents (Ch. 7) and another problem with tiling bounded agents (Ch. 8), neither of which are even partially solved in the paper, but which would've made a lot less sense - would not have been reified objects in the reader's mind - if the paper hadn't spent all that time on the mathematical machinery needed to partially solve the Lob problem in logical tiling, which crispifies the notion of a 'problem with tiling'.

By default, if you can build a Friendly AI you can solve the Lob problem. That working on the Lob Problem gets you closer to being able to build FAI is neither obvious nor certain, but everything has to start somewhere...
EDIT: Moved the rest of this reply to a new top-level comment because it seemed important and I didn't want it buried.
http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#943i

I really wish Jonah had mentioned that some number of comments ago, there's a lot of arguments I don't even try to use unless I know I'm talking to a mathematical literati.


Based on Eliezer's recent comments, my impression is that Eliezer is not making such a case, and is rather making a case for the paper being of sociological/motivational value.

No, that's not what I've been saying at all.
I'm sorry if this seems rude in some sense, but I need to inquire after your domain knowledge at this point. What is your level of mathematical literacy and do you have any previous acquaintance with AI problems? It may be that, if we're to proceed on this disagreement, MIRI should try to get an eminent authority in the field to briefly confirm basic, widespread, and correct ideas about the relevance of doing math to AI, rather than us trying to convince you of that via object-level arguments that might not be making any sense to you.
By 'the relevance of math to AI' I don't mean mathematical logic, I mean the relevance of trying to reduce an intuitive concept to a crisp form. In this case, like it says in the paper and like it says in the LW post, FOL is being used not because it's an appropriate representational fit to the environment... though as I write this, I realize that may sound like random jargon on your end... but because FOL has a lot of standard machinery for self-reflection of which we could then take advantage, like the notion of Godel numbering or ZF proving that every model entails every tautology... which probably doesn't mean anything to you either. But then I'm not sure how to proceed; if something can't be settled by object-level arguments then we probably have to find an authority trusted by you, who knows about the (straightforward, common) idea of 'crispness is relevant to AI' and can quickly skim the paper and confirm 'this work crispifies something about self-modification that wasn't as crisp before' and testify that to you. This sounds like a fair bit of work, but I expect we'll be trying to get some large names to skim the paper anyway, albeit possibly not the Early Draft for that.

Sounds about right. You might mean a different thing from "spearhead a field of research" than I do, my phrasing would've been "Start working on the goddamned problem."
From your other comments I suspect that you have a rather different visualization of object-level considerations to do with AI and this is relevant to your disagreement.

I don't think anyone disputes that in zero-sum games you play the Nash equilibrium move.

Yes, that's a large de-facto part of my reasoning.

That sounds like a very long conversation if we're supposed to be giving quantitative estimates on everything. The qualitative version is just that this sort of thing can take a long time, may not parallelize easily, and can potentially be partially factored out to academia, and so it is wise to start work on it as soon as you've got enough revenue to support even a small team, so long as you can continue to scale your funding while that's happening.
This reply takes for granted that all astronomical benefits bottleneck through a self-improving AI at some point.


I agree that it can't be taken for granted. My questions are about the particular operationalization of a self-modifying AI that you use in your publication. Why do you think that the particular operationalization is going to be related to the sorts of AIs that people might build in practice?

The paper is meant to be interpreted within an agenda of "Begin tackling the conceptual challenge of describing a stably self-reproducing decision criterion by inventing a simple formalism and confronting a crisp difficulty"; not as "We think this Godelian difficulty will block AI", nor "This formalism would be good for an actual AI", nor "A bounded probabilistic self-modifying agent would be like this, only scaled up and with some probabilistic and bounded parts tacked on". If that's not what you meant, please clarify.

Gosh. New item added to my list of "Not everyone does that."
...I have difficulty imagining what it would be to be like someone who isn't the little voice in their own head, though. Seriously, who's posting that comment?

Thanks for engaging.

More importantly we don't want each self-modification to involve wrenching changes like altering the mathematics you believe in, or even worse, your goals.

I'm very sympathetic to this in principle, but don't see why there would be danger of these things in practice.

But it's also possible that there'll be many gains from small self-modifications,

Humans constantly perform small self-modifications, and this doesn't cause serious problems. People's goals do change, but not drastically, and people who are determined can generally keep their goals pretty close to their original goals. Why do you think that AI would be different?

Another way of looking at it is that we're trying to have the AI be as free as possible to self-modify while still knowing that it's sane and stable, and the more overhead is forced or the more small changes are ruled out, the less free it is.

To ensure that one gets a Friendly AI, it suffices to start with good goal system, and to ensure that the goal system remains pretty stable over time. It's not necessary that the AI be as free as possible.
You might argue that an limited AI wouldn't be able to realize as good as a future as one without limitations.
But if this is the concern, why not work to build a limited AI that can itself solve the problems about having a stable goal system under small modifications? Or, if it's not possible to get a superhuman AI subject to such limitations, why not build a subhuman AI and then work in conjunction with it to build Friendly AI that's as free as possible?

Many things in AI that look like they ought to be easy have hidden gotchas which only turn up once you start trying to code them, and we can make a start on exposing some of these gotchas by figuring out how to do things using unbounded computing power (albeit this is not a reliable way of exposing all gotchas, especially in the hands of somebody who prefers to hide difficulties, or even someone who makes a mistake about how a mathematical object behaves, but it sure beats leaving everything up to verbal arguments).
Human beings don't make billions of sequential self-modifications, so they're not existence proofs that human-quality reasoning is good enough for that.
I'm not sure how to go about convincing you that stable-goals self-modification is not something which can be taken for granted to the point that there is no need to try to make the concepts crisp and lay down mathematical foundations. If this is a widespread reaction beyond yourself then it might not be too hard to get a quote from Peter Norvig or a similar mainstream authority that, "No, actually, you can't take that sort of thing for granted, and while what MIRI's doing is incredibly preliminary, just leaving this in a state of verbal argument is probably not a good idea."
Depending on your math level, reading Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference by Judea Pearl might present you with a crisper idea of why it can be a good idea to formalize certain types of AI problems in general, and it would be a life-enriching experience, but I anticipate that's more effort than you'd want to put into this exact point.

Is there a reason this isn't in Main?

Why isn't this just a fair result?

Jonah, some self-modifications will potentially be large, but others might be smaller. More importantly we don't want each self-modification to involve wrenching changes like altering the mathematics you believe in, or even worse, your goals. Most of the core idea in this paper is to prevent those kinds of drastic or deleterious changes from being forced by a self-modification.
But it's also possible that there'll be many gains from small self-modifications, and it would be nicer not to need a special case for those, and for this it is good to have (in theoretical principle) a highly regular bit of cognition/verification that needs to be done for the change (e.g. for logical agents the proof of a certain theorem) so that small local changes only call for small bits of the verification to be reconsidered.
Another way of looking at it is that we're trying to have the AI be as free as possible to self-modify while still knowing that it's sane and stable, and the more overhead is forced or the more small changes are ruled out, the less free it is.

The LW post may address some of your concerns. The idea here is that we need a tiling decision criterion, and the paper isn't supposed to be an AI design, it's supposed to get us a little conceptually closer to a tiling decision criterion. If you don't understand why a tiling decision criterion is a good thing in a self-improving AI which is supposed to have a stable goal system, then I'm not quite sure what issue needs addressing.

(Your math team?)

Monopolies which efficiently reinvest their producer surplus in improving the product tend to be monopolies to which I object very little. E.g. Google.


Responses below. As a meta-remark, your comment doesn't steelman my argument, and I think that steelmanning arguments helps keep the conversation on track, so I'd appreciate it if you were to do so in the future.

I have a known problem with this (Anna Salamon told me so, therefore it is true) so Jonah's remark above is a priori plausible. I don't know if I can do so successfully, but will make an effort in this direction.
(It's true that what Jonah means is technically 'principle of charity' used to interpret original intent, not 'steelman' used to repair original intent, but the principle of charity says we should interpret the request above as if he had said 'principle of charity'.)

Why would the number of stupid people who believe something anticorrelate with the number of smart people who believe it? Most stupid people and most smart people believe the sky is blue. A shift in the fraction of stupid people who do X can take place without any corresponding shift in the fraction of smart people who do X one way or another. Some smart people actively prefer not to affiliate themselves with stupid people and will try to believe something different, but they are committing the error of the OP and should not be listened to anyway.

Monopolies are bad because they can capture much more of the consumer surplus than they would otherwise, not just because they underproduce.


If it is impossible to have off-diagonal results, that is a much stronger argument for cooperating than having it be improbable

If the probability is epsilon, then having the probability be zero is only an epsilon stronger argument. If you doubt this let epsilon equal 1/googolplex.

Came here to say that, see it's been said. If your actions don't approach the choice you would make given impossibility, as the probability of something approaches (but does not reach) zero, then you must either be assigning infinite utility to something or you must not be maximizing expected utility.

The history books I have read do not strongly support this assertion. E.g. WWII was a product of Hitler's personality and Hitler's false predictions about how e.g. Britain would react to an invasion of Poland.

Penrose is a worrisome case to bring as an example, since he is in fact wrong, and therefore you're giving an example where your reasoning leads to the wrong conclusion. If you can't easily find examples where your reasoning led you to a new correct conclusion instead of new sympathy toward a wrong conclusion, this is worrisome. In general, I tend to flag recounts of epistemological innovations which lead to new sympathy toward a wrong conclusion, as though the one were displaying compassion for a previously hated enemy, for in epistemology this is not virtue.
The Penrose example worries me for other reasons as well, namely it seems like it would be possible to generate hordes and hordes of weak arguments against Penrose; so it's as if because the argument against Penrose is strong, you aren't bothering to try to generate weak arguments; reading this feels like you now prefer weak arguments to strong arguments and don't try to find the many weak arguments once you see a strong argument, which is not good Bayesianism.
You also claim there's a strong argument for Penrose, namely his authority (? wasn't this the kind of reasoning you were arguing against trusting?) but either we have very different domain models here, or you're not using the Bayesian definition of strong evidence as "an argument you would be very unlikely to observe, in a world where the theory is false". What do you think is the probability of at least one famous physicist writing a widely panned book about the noncomputability of human consciousness, in a world where consciousness is computable? I should not call it very low, and that means that the pure argument from authority, if you don't believe the actual specifics of that argument, is Bayesian evidence with a low likelihood ratio or as it would be commonly termed a 'weak argument'.


There are problems to whose solution I would attach infinitely greater importance than to those of mathematics, for example touching ethics, or our relation to God, or concerning our destiny and our future; but their solution lies wholly beyond us and completely outside the province of science. -- Gauss

Sounds a lot like Lord Kelvin saying that biology's vital force was infinitely beyond the reach of science, and equally wrong in the light of history. Flag: Getting a kick out of not knowing something; motivated uncertainty.

When you say that choosing to defect might make it more likely that they defect, do you mean that choosing to defect may cause the probability that the other person will defect to go up, or do you mean that the probability of the other player defecting, given that you defected, may be greater than the probability given that you cooperated?
To quote Douglas Adams, "The impossible often has a kind of integrity to it which the merely improbable lacks." If it is impossible to have off-diagonal results, that is a much stronger argument for cooperating than having it be improbable, even if the probability of an on-diagonal result is 99.99%; as long as the possibility exists, one should take it into consideration.

"I ran out of ability to not have a girlfriend" would be the real answer. Some labor complementarity, some time costs, doesn't matter much from my perspective because I ran out of ability not to have a girlfriend. I don't regret the dedicated labor I put in up until that point, back when being alone didn't seem like much of a problem.

The original anime source is mindless fighting harem comedy from what I've read, and even attempts to rationalize the world assume the plan's originator is insane and substitute a different source of intelligent opposition. I would have trouble rationalizing that.

I brute-forced the other side of the problem (status/fame/impression) hard enough that I never learned to search and seduce. Sometimes contemplating this makes me feel very lazy, but heck, brute-forcing the other side of that took a lot of work. It was not the minimum-effort pathway if that had actually been the primary goal.
I was overweight for a long time before I lost 20 pounds on Shangri-la, after which it never worked again, but they were a nice 20 pounds to lose.

(That's fair.)

That seems to verge on the trivializing gambit, though.

Eliezer, I'm trying to lose fat to increase my attractiveness, but I've read on this site that you already have four girlfriends in a polyamorous relationship. Is that true? If it is, how did you achieve that without losing weight? I'm assuming of course that you are overweight given your interest in diets, I've not seen a picture of you.
If I already had 4 girlfriends, to hell with fat. I would just concentrate on staying healthy.
Anyway, if after careful and extended research it would come out that liposuction is the only way to lose fat, and losing fat is a prerequisite for attracting girls, then I would do that, if there were no unreasonable risks.

High status, enough fame to broadcast across many possibilities, and sufficiently good Harry Potter fanfiction to convey a sense of my personality. (If you like HJPEV's personality you will probably like mine, though we are not the same person.) Currently down to 2 local and 1 East Coast girlfriend, btw. This pathway is not tremendously duplicable, but it was easier for me than learning to dress well or studying light-side pickup because I needed to do the work for other reasons anyway.
Fat is a problem for me because of how it affects things like sleep, and energy during daily work - having your fat cells suck out all the glucose you need is not helpful. If you can lose weight, you should obviously do so. If you haven't yet tried low-carb and Shangri-La, both seem relatively obvious things to attempt; the first seems to have a high success rate and the second is very easy. It's the people telling you to buckle down and use willpower who should be ignored - I know of exactly one case of that working, all other cases of weight loss in my personal experience did not involve what I would consider to be significant willpower.

The latter.

I defer to your superior domain knowledge of universities.

I'm also cheap.

http://www.fanfiction.net/s/8078340/1/Postnuptial-Disagreements
Munchkinry, smart antagonists, classical literary references, works better if you know both Type-Moon and Sekirei or have read Gabriel Blessing's In Flight which is oddly readable but nowhere near as smart.

The Eyre Affair, the first Thursday Next book, is pretty good so far.
http://www.amazon.com/The-Eyre-Affair-ebook/dp/B000OCXHC2/ref=sr_1_1?ie=UTF8&qid=1370202635&sr=8-1&keywords=eyre+affair

This is a really good point that I can't believe I never thought of before.

It seems to me to be related to the big separate issue of whether, if an election is settled by two votes, nobody's vote had any effect - by the same logic if an election was settled by one vote, everyone who voted for the winning side solely decided the election and get all the credit for it. The altruistic credit due to Page and Brin is not the interval between this universe and the counterfactual universe where they didn't exist, any more than the credit due for an election result is the distance between this universe and the counterfactual universe where you voted the other way.

AltaVista was bad, Google was awesome and I probably would pay $100/month to make up the difference today if I had no choice but to pay. This is on the order of 2.5% of aftertax, pre-rent income. I wouldn't have been able to afford it when Google first came out.
I usually credit altruistic effects to cohorts of agents with logically correlated decisions, rather than assuming that in any election won by two votes, nobody's vote had a marginal effect; this is a separate big issue but says that I wouldn't just assume the consumer surplus generated by their lives' decisions, vanished after somebody else would've done it anyway - somebody has to be the somebody else and get credit for that.

Rational distress-minimizers would behave differently from rational atruists. (Real people are somewhere in the middle and seem to tend toward greater altruism and less distress-minimization when taught 'rationality' by altruists.)

I hate to say it, but my guess is that you're wasting time unless your universe^H^H^H university has unusually good undergraduate statistics courses.

Seeing these statistics has got me thinking.
I've checked the undergraduate course requirements as my local university's medical faculty, and there's nothing listed for probability and statistics. I'm considering setting up an appointment with somebody about this, assuming doctors not being able to uncover test results properly is a serious problem.
Would this be worth it, or am I wasting time?

In my experience, any diet or exercise comes with an unlimited number of excuses from various different people for how you might not be doing it exactly right. Oddly enough, when the diet (temporarily) works on somebody, they don't bother to check whether every tiny thing was done according to their own playbook. Thus the hypothesis "this diet doesn't actually work for everyone" is prohbited.

Single bad things happen to you at random. Iterated bad things happen to you because you're a dumbass. Related: "You are the only common denominator in all of your failed relationships."

In this case it does seem plausible that a rationalist message was intended.

I remember this as a famous proverb, it may predate Dennett.

A zero-carb diet for a couple of weeks did not produce any ketosis as measured by a ketosis stick. Also lipolysis != dead fat cells.

"When two planes collided just above a runway in Tenerife in 1977, a man was stuck, with his wife, in a plane that was slowly being engulfed in flames. He remembered making a special note of the exits, grabbed his wife's hand, and ran towards one of them. As it happened, he didn't need to use it, since a portion of the plane had been sheared away. He jumped out, along with his wife and the few people who survived. Many more people should have made it out. Fleeing survivors ran past living, uninjured people who sat in seats literally watching for the minute it took for the flames to reach them." - http://io9.com/the-frozen-calm-of-normalcy-bias-486764924

It'd be nice to have a standard collection of reading. What came to mind offhand on the specific topic of metabolism slowdown / fat cell energy vampirism is this:
http://www.nytimes.com/2012/01/01/magazine/tara-parker-pope-fat-trap.html?pagewanted=all
Although when I actually talk to others who are trying to lose weight, a very common comment is, "I'm eating much less on <diet> but my weight isn't going down at all!" Which is worse than what this article reports on - everyone who stayed in the study lost weight on 550 calories/day, but "Some people dropped out of the study" which you would kinda expect if those were the obese people whose fat cells weren't releasing fat at all.

There is no thermodynamic law stating that fat cells must release fat just because your body needs it. If you're built so that weight loss is impossible and you try eating less, your metabolism slows down - possibly in much the same way it would as if you tried eating less and you had no fat cells whatsoever. I can't cite studies but wouldn't be particularly surprised to see that muscle gets cannibalized instead of fat being lost, if you try to eat less than the most slowed metabolism needs. And if most metabolically disprivileged people stop trying to eat below their minimal metabolic rate before doing significant damage to themselves, that's just the survival instinct kicking in. I would seriously not be surprised to find that fat people have starved to death without their fat cells releasing fat, and blinded by preconceptions, nobody managed to notice or note down when this occurred. But I would expect that to be rare - most people, if their body tells them they're starving to death, will eat. This gets cited as weakness of will.
Metabolically privileged people assume that if you eat less, your fat cells will release fat. (Bitter laughter.) No. We don't have energy storage units like you do, we have energy retention units. Calories go in, they don't come out. Or if they do, it's on special occasions we don't understand how to predict or trigger, and which don't have any obvious relation to attempts to eat less or exercise more. The laws of thermodynamics do not require that a physical fat cell physically release stored lipids when you eat less or exercise more - and if your fat cells are malfunctioning, they just won't.

In that case medical interventions to remove fat directly are inadvisable as the fat will simply be regained, psychological treatment is required instead.

This is simply wrong. If you start out metabolically disprivileged, medical interventions to directly remove fat result in reduced appetite as your fat cells no longer suck glucose and fatty acids out of your bloodstream.

I prefer to quantify my lack of information and call it a prior. Then it's even better than wrong information!

That's kind of amusing, considering that Lincoln is also famous for destroying his enemies the other way.

That's because it's usually attributed to Abe Lincoln, with an exception.



Many people believe that about climate change (due to global political disruption, economic collapse etcetera, praising the size of the disaster seems virtuous).

Hm! I cannot recall a single instance of this.

Will keep an eye out for the next citation.

Still, if the analysis says "you will die of this", and the brain of the person considering the analysis is willing to assign it some credence

This has not happened with AI risk so far among most AIfolk, or anyone the slightest bit motivated to reject the advice. We had a similar conversation at MIRI once, in which I was arguing that, no, people don't automatically change their behavior as soon as they are told that something bad might happen to them personally; and when we were breaking it up, Anna, on her way out, asked Louie downstairs how he had reasoned about choosing to ride motorcycles.
People only avoid certain sorts of death risks under certain circumstances.

Though you can still find subjects who don't know the outcome, ask them for their predictions, and compare those predictions with subjects who are told the outcome to find the size of the hindsight bias.


Climate change doesn't have the aspect that "if this ends up being a problem at all, then chances are that I (or my family/...) will die of it".

Many people believe that about climate change (due to global political disruption, economic collapse etcetera, praising the size of the disaster seems virtuous). Many others do not believe it about AI. Many put sizable climate-change disaster into the far future. Many people will go on believing this AI independently of any evidence which accrues. Actors with something to gain by minimizing their belief in climate change so minimize. This has also been true in AI risk so far.

If you've already tried things like low-carb diets and Shangri La, losing weight is probably impossible for you short of Adipotide or liposuction, so ignore all the well-meaning advice from the metabolically privileged about how easy this would be if you just ate less and exercised.


Personal and tribal selfishness align with AI risk-reduction in a way they may not align on climate change.

This seems obviously false. Local expenditures - of money, pride, possibility of not being the first to publish, etc. - are still local, global penalties are still global. Incentives are misaligned in exactly the same way as for climate change.

RSI capabilities could be charted, and are likely to be AI-complete.

This is to be taken as an arguendo, not as the author's opinion, right? See IEM on the minimal conditions for takeoff. Albeit if "AI-complete" is taken in a sense of generality and difficulty rather than "human-equivalent" then I agree much more strongly, but this is correspondingly harder to check using some neat IQ test or other "visible" approach that will command immediate, intuitive agreement.

Which historical events are analogous to AI risk in some important ways?

Most obviously molecular nanotechnology a la Drexler, the other ones seem too 'straightforward' by comparison. I've always modeled my assumed social response for AI on the case of nanotech, i.e., funding except for well-connected insiders, term being broadened to meaninglessness, lots of concerned blither by 'ethicists' unconnected to the practitioners, etc.

Right. So if you just take everything at face value - the observed laws of physics, the situation we seem to find ourselves in, our default causal model of civilization - and say, "Hm, looks like we're collectively in a position to influence the future of the galaxy," that's non-anthropics. If you reply "But that's super improbable a priori!" that's anthropics. If you counter-reply "I don't believe in all this anthropic stuff!" that's also an implicit theory of anthropics. If you treat the possibility as more "unknown" than it would be otherwise, that's anthropics.


The only unusual step is to realize that theories with vast future populations have such an implication.

Right. That's arcane. Mundane theories have no need to measure the population of the universe.

When you try to say that there's something particularly unknown about having lots of influence, you're using anthropics.

No, this is talking-out-of-our-ass anthropics, it's just that the anthropic part comes in when you start arguing "No, you can't really be in a position of that much influence", not when you're shrugging "Sure, why shouldn't you have that much influence?" Like, if you're not arriving at your probability estimate for "Humans will never leave the solar system" just by looking at the costs of interstellar travel, and are factoring in how unique we'd have to be, this is where the talking-out-of-our-ass anthropics comes in.
Though it should be clearly stated that, as always, "We don't need to talk out of our ass!" is also talking out of your ass, and not necessarily a nicer ass.

Do you mind elaborating on this inevitability? It seems like there ought to be other assumptions involved. For example, I can easily imagine that humans will never be able to colonize even this one galaxy, or even any solar system other than this one. Or that they will artificially limit the number of individuals. Or maybe the only consistent CEV is that of a single superintelligence of which human minds will be tiny parts. All of these result in the rather small total number of individuals existing at any point in time.


For example, I can easily imagine that humans will never be able to colonize even this one galaxy, or even any solar system other than this one.

Counts as Doomsday, also doesn't work because this solar system could support vast numbers of uploads for vast amounts of time (by comparison to previous population).

Or that they will artificially limit the number of individuals.

This is a potential reply to both Doomsday and SA but only if you think that 'random individual' has more force than a similar argument from 'random observer-moment', i.e. to the second you reply, "What do you mean, why am I near the beginning of a billion-year life rather than the middle? Anyone would think that near the beginning!" (And then you have to not translate that argument back into a beginning-civilization saying the same thing.)

Or maybe the only consistent CEV is that of a single superintelligence of which human minds will be tiny parts.

...whereupon we wonder something about total 'experience mass', and, if that argument doesn't go through, why the original Doomsday Argument / SH should either.

Okay, so that's the Doomsday Argument then: Since being able to conquer the universe implies we're 10^70 special, we must not be able to conquer the universe.
Calling the converse of this an arcane meta-argument about probability hardly seems fair. You can make a case for Doomsday but it's not non-arcane.

So do you believe in the Simulation Hypothesis or the Doomsday Argument, then? All attempts to cash out that refusal-to-believe end in one or the other, inevitably.

Buy life insurance, sign up for cryonics, done, now get on with it.

Easier to do by just squishing someone, actually.


Without a justification, I cannot rationally believe in the truth of the senses.

Yeah you can. Like, are you wearing socks? Yes, you're wearing socks. People were capable of this for ages before philosophy. That's not about what's useful, it's about what's true. How to justify it is a way more complex issue. But if you lose sight of the fact that you are really actually in real life wearing socks, and reminding you of this doesn't help, you may be beyond my ability to rescue by simple reminders. I guess you could read "The Simple Truth", "Highly Advanced Epistemology 101 for Beginners", and if that's not enough the rest of the Sequences.

(Nods.)

My understanding is that any speedup would be fairly implausible, I mean isn't the whole lesson of l'affaire D-Wave that you need maintained quantum coherence and that requires quantum error-correction which is why Scott Aaronson didn't believe the D-Wave claims? Or is that just an unusually crisp human-programming way of doing things?


though I did think that you didn't feel that multiplying a very small success probability with a very large payoff was a good reason to donate to MIRI

Because on a planet like this one, there ought to be some medium-probable way for you and a cohort of like-minded people to do something about x-risk, and if a particular path seems low probability, you should look for one that's at least medium-probability instead.

Okay, makes sense if you define "distinguishable from random" as "decodable with an amount of computation polynomial in the randseed size".
EDIT: Confidence is about standard cryptographically strong randomness plus thermal noise being sufficient to prevent expected correlation with bits playing a functional role, which is all that could possibly be relevant to cognition.

A proof that any generator was indistinguishable from random, given the usual definitions, would basically be a proof that P != NP, so it is an open problem. However we're pretty confident in practice that we have strong generators.

Can you refer me to somewhere to read more about the "usual definitions" that would make this true? If I know the Turing machine, I can compare the output to that Turing machine and be pretty sure it's not random after running the generator for a while. Or if the definition is just lack of expected correlation with bits playing a functional role, then that's easy to get. What's intermediate such that 'indistinguishable' randomness means P!=NP?

What? No it's not. There are no pseudo-random generators truly ultimately indistinguishable in principle from the 'branch both ways' operation in quantum mechanics, the computations all have much lower Kolmogorov complexity after running for a while. There are plenty of cryptographically strong pseudo-random number generators which could serve any possible role a cognitive algorithm could possibly demand for a source of bits guaranteed not to be expectedly correlated with other bits playing some functional role, especially if we add entropy from a classical thermal noise source, the oracular knowledge of which would violate the second law of thermodynamics. This is not an open problem. There is nothing left to be confused about.

Give up on justifying answers and just try to figure out what the answers really actually are, i.e., are you really actually inside an Evil Demon or not. Once you learn to quantify the reasoning involved using math, the justification thing will seem much more straightforward when you eventually return to it. Meanwhile you're asking the wrong question. Real epistemology is about finding correct answers, not justifying them to philosophers.

Penrose would claim not to understand how 'collapse' occurs.

Quantum tunneling != quantum computing.
Quantum 'randomness' != quantum computing. No one has ever introduced, even in principle, a cognitive algorithm that requires quantum 'randomness' as opposed to thermal noise.

Quantum effects or quantum computation? Technically our whole universe is a quantum effect, but most of it can't be regarded as doing information processing, and of the parts that do information processing, we don't yet know of any that are faster on account of quantum superpositions maintained against decoherence.

Yes, this is what we first tried before finding out that it was way below the level of working with late-2011-level knowledge and ability to produce lessons. Might be worth retrying once the lessons have been highly polished at the CFAR level.

But what if AMF saves a child who grows up to be a biotechnologist and goes on to weaponize malaria and spread it to millions?
If you try hard enough, you can tell a story where any effort to accomplish X somehow turns out to accomplish ~X, but one must distinguish possibility from the balance of probability.


or making assumption that having someone work at GWWC is far more valuable than having someone work somewhere else

Ah, right, I'm thinking in MIRIan terms where you can't go off and do comparable direct work somewhere else.

I don't think 1-3 combined can modify the conclusion that most of these applicants should be earning to give to support the one selected applicant, creating a prior of 200:1. The only realistic way this could be false is if the premise has been misremembered, or if people are vastly more willing to work for GWWC than to earn money and give it to GWWC (the motivational issue mentioned before).

I'm somewhat confused by the direction that this discussion has taken. I might be missing something, but I believe that the points related to AMF that I've made are:

GiveWell's explicit cost-effectiveness estimate for AMF is much higher than the cost per DALY saved implied by the figure that MacAskill cited.
GiveWell's explicit estimates for the cost-effectiveness of the best giving opportunities in the field of direct global health interventions have steadily gotten lower, and by conservation of expected evidence, one can expect this trend to continue.
The degree of regression to the mean observed in practice suggests that there's less variance amongst the cost-effectiveness of giving opportunities than may initially appear to be the case.
By choosing an altruistic career path, one can cut down on the number of small probability failure modes associated with what you do.

I don't remember mentioning AMF and x-risk reduction together at all. I recognize that it's in principle possible that the "earning to give" route is better for x-risk reduction than it is for improving global health, but I believe the analogy between the two domains is sufficiently strong that my remarks on AMF have relevance (on a meta-level, not on an object level).

Yeah, I also have the feeling that I'm questioning you improperly in some fashion. I'm mostly driven by a sense that AMF is very disanalogous to the choices that face somebody trying to optimize x-risk charity (or rather total utilons over all future time, but x-risk seems to be the word we use for that nowadays). It seems though that we're trying to have a discussion in an ad-hoc fashion that should be tabled and delayed for explicit discussion in a future post, as you say.

If you're one of 10^11 sentients to be born on Ancient Earth with a golden opportunity to influence a roughly 10^80-sized future, what exactly is a 'vanishing chance'... eh, let's all save it until later.


My reason for mentioning AMF and global health is that doing so provides a concrete, pretty robustly researched example

That depends on what you want to know, doesn't it? As far as I know the impact of AMF on x-risk, astronomical waste, and total utilons integrated over the future of the galaxies, is very poorly researched and not at all concrete. Perhaps some other fact about AMF is concrete and robustly researched, but is it the fact I need for my decision-making?
(Yes, let's talk about this later on. I'm sorry to be bothersome but talking about AMF in the same breath as x-risk just seems really odd. The key issues are going to be very different when you're trying to do something so near-term, established, without scary ambiguity, etc. as AMF.)

I can imagine someone thinking that FHI was a better use of money than MIRI, or CFAR, or CSER, or the Foresight Institute, or brain-scanning neuroscience, or rapid-response vaccines, or any number of startups, but considering AMF as being in the running at all seems to require either a value difference or really really different epistemics about what affects the fate of future galaxies.

Well, if James Simons wanted to retire from Renaissance and work on FAI full-time, it would not be entirely obvious to me that this was a bad move, but only if Simons had enough in the bank to also pay as much other top-flight math talent as could reasonably be used, and was already so paying, such that there was no marginal return to his further earning power relative to existing funds.
This situation has not yet arisen. Unfortunately.

I understand if your priorities aren't our priorities. My concrete example reflex was firing, that's all.

The top considerations that come into play when I advise someone whether to earn-to-give or work directly on x-risk look like this:
1) Does this person have a large comparative advantage at the direct problem domain? Top-rank math talent can probably do better at MIRI than at a hedge fund, since there are many mathematical talents competing to go into hedge funds and no guarantee of a good job, and the talent we need for inventing new basic math does not translate directly into writing the best QT machine learning programs the fastest.
2) Is this person going to be able to stay motivated if they go off on their own to earn-to-give, without staying plugged into the community? Alternatively, if the person's possible advantage is at a task that requires a lot of self-direction, will they be able to stay on track without requiring constant labor to keep them on track, since that kind of independent job is much harder to stick at then a 9-to-5 office job with supervision and feedback and cash bonuses?
Every full-time employee at a nonprofit requires at least 10 unusually generous donors or 1 exceptionally generous donor to pay their salary. For any particular person wondering how they should help this implies a strong prior bias toward earning-to-give. There are others competing to have the best advantage for the nonprofit's exact task, and also there are thousands of job opportunities out there that are competing to be the maximally-earning use of your exact talents - best-fits to direct-task-labor vs. earning-to-give should logically be rare, and they are.
The next-largest issue is motivation, and here again there are two sides to the story. The law student who goes in wanting to be an environmentalist (sigh) and comes out of law school accepting the internship with the highest-paying firm is a common anecdote, though now that I come to write it down, I don't particularly know of any gathered data. Earning to give can impose improbability in the form of likelihood that the person will actually give. Conversely, a lot of the most important work at the most efficient altruistic organizations is work that requires self-direction, which is also demanding of motivation.
I should pause here to remark that if you constrain yourself to 'straightforward' altruistic efforts in which the work done is clearly understandable and repeatable and everyone agrees on how wonderful it is, you will of course be constraining yourself very far away from the most efficient altruism - just like a grant committee that only wants to fund scientific research with a 100% chance of paying off in publications and prestige, or a VC that only wanted to fund companies that were certain to be defensible-appearing decisions, or someone who constrained their investments to assets that had almost no risk of going down. You will end up doing things that are nearly certain never to appear to future historians as a decisive factor in the history of Earth-originating intelligent life; this requires tolerance for not just risk but scary ambiguity. But if you want to work on things that might actually be decisive, you will end up in mostly uncharted territory doing highly self-directed work, and many people cannot do this. Just as many other people cannot sustain altruism without being surrounded by other altruists, but this can possibly be purchased elsewhere via living on the West or East Coast and hanging around with others who are earning-to-give or working directly.
These are the top considerations when someone asks me whether they should work directly or earn to support others working directly - the low prior, whether the exact fit of talent is great enough to overcome that prior, and whether the person can sustain motivation / self-direct.

That was my first plan back when things were getting started, but it turned out to be hard to develop instructional materials that worked without a developed professional instructor.

Jobs's death was known to be on the way. It would be surprising if the stock plummeted enough at that point to produce a predictable profit for someone shorting it.


I think what I am trying to say is that marginal returns diminish faster for copies compared to other people.

Yes, obviously.

There is such a thing as diminishing marginal returns. If there were 5 Eliezer Yudkowskys all working on FAI, the sixth Eliezer Yudkowsky ought to go off and direct their efforts to CFAR instead (this is a calculation specifically for copies of Eliezer Yudkowsky, and not precise).

Sounds like "Individual bets don't always reveal the beliefs of every participant because some of them are arbitraging, but the market does."

CFAR is indeed so cooptimizing and trying to maximize net impact over time; if you think that a different mix would produce a greater net impact, make the case! CFAR isn't a side-effect project where you just have to cross your fingers and hope that sort of thing happens by coincidence while the leaders are thinking about something else, it's explicitly aimed that way.

Mm, I'm not sure what the intended import of your statement is, can we be more concrete? This sounds like something I would say in explaining why I directed some of my life effort toward CFAR - along with, "Because I found that really actually in practice the number of rationalists seemed like a sharp limiting factor on the growth of x-risk efforts, if I'd picked something lofty-sounding in theory that was supposed to have a side impact I probably wouldn't have guessed as well" and "Keeping in mind that the top people at CFAR are explicitly x-risk aware and think of that impact as part of their job".

My difficulty imagining a genuinely realistic mechanism of impossibility is such that I want to see the details of how it doesn't happen before I update. I could make up dumb stories but they would be the wrong explanation if it actually happened, because I don't think those dumb stories are actually plausible.

I agree but remark that so long as at least one x-risk reduction effort meets this minimum threshold, we can discard all non-xrisk considerations and compare only x-risk impacts to x-risk impacts, which is how I usually think in practice. The question "Can we reduce all impacts to probability of okayness?" seems separate from "Are there mundane-seeming projects which can achieve comparably sized xrisk impacts per dollar as side effects?", and neither tells us to consider non-xrisk impacts of projects. This is the main thrust of the astronomical waste argument and it seems to me that this still goes through.

Mm, k. I was trying more to say that I got the same sense from your post that Nick Bostrom seems to have gotten at the point where he worried about completely general and perfectly sterile analytic philosophy. Maxipok isn't derived just from the astronomical waste part, it's derived from pragmatic features of actual x-risk problems that lead to ubiquitous threshold effects that define "okayness" - most obviously Parfit's "Extinguishing the last 1000 people is much worse than extinguishing seven billion minus a thousand people" but also including things like satisficing indirect normativity and unfriendly AIs going FOOM. The degree to which x-risk thinking has properly adapted to the pragmatic landscape, not just been derived starting from very abstract a priori considerations, was what gave me that worried sense of overabstraction while reading the OP; and that trigged my reflex to start throwing out concrete examples to see what happened to the abstract analysis in that case.

I worry that this post seems very abstract.
The specific case I've made for "just build the damn FAI" does not revolve only around astronomical waste, but subtheses like:

Stable goals in sufficiently advanced self-improving minds imply very strong path dependence on the point up to where the mind is sufficiently advanced, and no ability to correct mistakes beyond that point
Friendly superintelligences negate other x-risks once developed
CEV (more generally indirect normativity) implies that there exists a broad class of roughly equivalently-expectedly-good optimal states if we can pass a satisficing test (i.e., in our present state of uncertainty, we would expect something like CEV to be around as good as it gets, given our uncertainty on the details of goodness, assuming you can build a CEV-SI); there is not much gain from making FAI programmers marginally nicer people or giving them marginally better moral advice provided that they are satisficingly non-jerks who try to build an indirectly normative AI
Very little path dependence of the far future on anything except the satisficing test of building a good-enough FAI, because a superintelligent singleton has enough power to correct any bad inertia going into that point
The Fragility of Value thesis implies that value drops off very fast short of a CEV-style FAI (making the kinds of mistakes that people like to imagine leading to flawed-utopia story outcomes will actually just kill you instantly when blown up to a superintelligent scale) so there's not much point in trying to make things nicer underneath this threshold
FAI is hard (relative to the nearly nonexistent quantity and quality of work that we've seen most current AGI people intending to put into it, or mainstream leaders anticipating a need for, or current agencies funding, FAI is technically far harder than that); so most of the x-risk comes from failure to solve the technical problem
Trying to ensure that "Western democracies remain the most advanced and can build AI first" or "ensuring that evil corporations don't have so much power that they can influence AI-building" is missing the point (and a rather obvious attempt to map the problem into someone's favorite mundane political hobbyhorse) because goodness is not magically sneezed into the AI from well-intentioned builders, and favorite-good-guy-of-the-week is not making anything like a preliminary good-faith-effort to do high-quality work on technical FAI problems, and probably won't do so tomorrow either

You can make a case for MIRI with fewer requirements than that, but my model of the future is that it's just a pass-fail test on building indirectly normative stable self-improving AI, before any event occurs which permanently prevents anyone from building FAI (mostly self-improving UFAI (possibly neuromorphic) but also things like nanotechnological warfare). If you think that building FAI is a done deal because it's such an easy problem (or because likely builders are already guaranteed to be supercompetent), you'd focus on preventing nanotechnological warfare or something along those lines. To me it looks more like we're way behind on our dues.

...yes? This seems like a quite reasonable epistemic state.

If there's something there that isn't priced for sale to hospitals, or restricted in sale to hospitals, and has been formulated so as to be edible by people who are tired of real food, go ahead and post it. My understanding is that tube-feeding is not the same use-case as Soylent at all, with tube-fed material needing to be essentially predigested and correspondingly expensive or something along those lines, and no concern for edible taste for obvious reasons.
I've done some looking, but I haven't seen anything out there that looks like it's meant to be eaten, meant to replace food, and priced at an affordable level for sole consumption.


This doesn't mean that they're evil hoarders, though I do think that preferring money as a store of value generally indicates something wrong.

Well, that's the core of our disagreement (indeed, my disagreement with anyone who buys into either the Sumner or Krugman view).
Specifically, on what basis can you even begin to make a case that there is "too much" hoarding without a framework for tabulating its benefits? That's like saying that someone isn't "eating healthy enough" while only counting the benefits of eating healthy.
And yet (as best demonstrated by the Landsburg link), any time such proponents are asked how to tabulate the relative benefits of hoarding to hoarders against its social costs, the answer is somwhere between silence and "assume we want to get people spending".
That should be a much bigger red flag for you in the opposite direction.

You're speaking language I don't recognize. This has nothing to do with 'hoarding'. It's about sticky nominal prices and fixed nominally priced debt contracts implying that when monetary velocity goes down, monetary supply should be increased to maintain the velocity of positive-sum trade at full capacity. Nothing you've said contradicts this, at least not in any language I recognize.

Hence the standard belief that reluctance to lower nominal prices, or delay in lowering nominal prices, is key (along with nominal debt contracts) to explaining the observed fact that deflation is destructive of RGDP, which is why Scott Sumner's blog is called "The Money Illusion".

Yes, we'd all be better off if the short-long trade was outlawed and banks issued bonds on a liquid market, possibly insured by private counterparties, to their customers, instead of claiming that their money was available on demand. But nobody is actually doing that, or has any incremental incentive to adopt it so long as governments supply free insurance, and we have to consider what second-best options are available. Money has no intrinsic value (I assume we both take this as axiomatic) and the utility 'money' provides to civilization comes from increasing the number of positive-sum trades. When people attempting to use money as a guaranteed store of value keep that money instead of spending it, the velocity of positive-sum trades goes down. This doesn't mean that they're evil hoarders, though I do think that preferring money as a store of value generally indicates something wrong. But it does mean that supplying more money is a positive-sum move because it increases the number of positive-sum trades occurring, so long as there is significant unutilized capacity.

When NGDP/NGDI, the total amount of money the economy spends, goes down below trend, so does RGDP. If the Fed had e.g. bought government bonds, the amount of circulating money could have been prevented from going down (without any need for "fiscal stimulus", by the way) without making loans to any particular trading companies (no need for bailouts! let them go bust!), and the bonds wouldn't be monetized, they would be sold again as velocity picked up. I recommend http://themoneyillusion.com/ - your viewpoint here is too distant from sane macroeconomics, and too close to crazy populist economics, for me to tackle all the individual problems. But what you're saying is more or less literally, exactly what people were saying about banks needing to take their medicine just before the Great Depression. Seriously, look it up, that was what made people realize that monetary velocity slowdowns needed to be made up by (temporary) monetary supply increases.

"Foreign" does not mean "stupid".

Just show them examples of small Turing machines until they catch on.

Cross-posting would be UNFORGIVEABLE.
No, I lie, I don't even know why that would be a problem.

If I'm understanding my Sumner right, Krugman is just plain wrong about this. Central banks can decrease interest rates, promise to keep future interest rates low, engage in quantitative easing, charge negative interest on reserves, and print physical money and drop it out of helicopters. "There is no zero bound" is a market monetarist slogan and I have to say it sounds a tad plausible from over here.

The Sumner critique says that the Great Recession was more 'caused' by failure to supply enough money (when the market was clearly anticipating almost zero inflation) to keep NGDP on a level growth path, suggesting that the main path toward resolving this problem would be figuring some way to further convince the Federal Reserve to adopt NGDP level targeting. Also, as I understand it, even some of the people making money off shorting the housing market were quite loud about it - it's not obvious that a lack of transparency is the problem. To make money you need to time your short correctly which the now-famous winners may have done by coincidence, and this is always the problem with bubbles. For that matter, it's not obvious that the housing market would still have collapsed if the Fed had committed to an NGDP path.


Desires and preferences about paperclips can be satisfied. They can sense, learn, grow, reproduce, etc.

Do you take that personally seriously or is it something someone else believes? Human experience with desire satisfaction and "learning" and "growth" isn't going to transfer over to how it is for paperclip maximizers, and a generalization that this is still something that matters to us is unlikely to succeed. I predict an absence of any there there.

Paperclippers are worse than nothing because they might run ancestor simulations and prevent the rise of intelligent life elsewhere, as near as I can figure. They wouldn't enjoy life. I can't figure out how any of the welfare theories you specify could make paperclippers better than nothing?


Phrased another way: does the existence of any intelligence at all, even a paperclipper, have even the smallest amount of utility above no intelligence at all?

This is a different and cleaner question, because it avoids issues with intelligent life evolving again, and the paperclipper creating other kinds of life and intelligence for scientific or other reasons in the course of pursuing paperclip production.
I would say that if we use a weighted mixture of moral accounts (either from normative uncertainty, or trying to reflect a balance among varied impulses and intuitions), then it matters that the paperclipper could do OK on a number of theories of welfare and value:

Desire theories of welfare
Objective list theories of welfare
Hedonistic welfare theories, depending on what architecture is most conducive to producing paperclips (although this can cut both ways)
Perfectionism about scientific, technical, philosophical, and other forms of achievement


To me the conversational part of this seems way less complicated/interesting than the unknown causal models part - if I have any 'philosophical' confusion about how to treat unknown strings of English letters it is not obvious to me what it is.

"The ball is blue" only gets assigned a probability by your prior when "blue" is interpreted, not as a word that you don't understand, but as a causal hypothesis about previously unknown laws of physics allowing light to have two numbers assigned to it that you didn't previously know about, plus the one number you do know about. It's like imagining that there's a fifth force appearing in quark-quark interactions a la the "Alderson Drive". You don't need to have seen the fifth force for the hypothesis to be meaningful, so long as the hypothesis specifies how the causal force interacts with you.
If you restrain yourself to only finite sets of physical laws of this sort, your prior will be over countably many causal models.

Very low, because B9 has to hypothesize a causal framework involving colors without any way of observing anything but quantitatively varying luminosities. In other words, they must guess that they're looking at the average of three variables instead of at one variable. This may sound simple but there are many other hypotheses that could also be true, like two variables, four variables, or most likely of all, one variable. B9 will be surprised. This is right and proper. Most physics theories you make up with no evidence behind them will be wrong.

That is what I mean; HPMOR's Hermione represents a critique of something, but not a "critique from a feminist framework" of that thing.

Yep.

I've seen that for planets a lot more than for people, yes.

That sounds theoretically possible but I haven't seen it.


It'll be healthier and more enjoyable just to eat actual food

I tried that. It didn't work. If you have something specific to recommend that can replace meals instead of Soylent, speak up.

I say Watson is only a narrow AI.


Build a better one yourself? I'm tired of eating.

I am tired of eating too, and have looked seriously into building it myself. It's not clear yet whether doing it myself is worth the additional costs over outsourcing it to him; I think I can do significantly better, but doing it right is a significant amount of work.

Post your superior recipe to /r/soylent?

Guess we're all stuck with Soylent then! In for $65.
See also: http://news.ycombinator.com/item?id=5746844

Build a better one yourself? I'm tired of eating.

I'm not sure that's true in aggregate. I think most of the evil is done by people going along with things - like, if you talked to them about it for a while they'd concede that some aspects of what they were going along with were sort of questionable and maybe a bit bad, but they don't think about that spontaneously.

Some people do (I have already received multiple comments to this effect). Mileage possibly varies.

Actually, it was someone asking what the heck I meant by "reality fluid", to which the answer is that I don't know either which is why I always call it "magical reality fluid". I mean, I could add in something that sounded impressive and might to some degree be helpful along the lines of "It's the mind-projection-fallacy conjugate of 'probability' as it appears inside hypotheses about collections of real things in which some real things are more predicted to happen to me than others for purposes of executing post-observation Bayesian updates, like, if the squared modulus rule appearing in the Born statistics reflected the quantity present of an actual kind of stuff" but I think saying, "It's magic, which is the mind-projection-fallacy conjugate of 'I'm confused'" would be wiser in a conversation like that. I think it's very important not to create the illusion of knowing more than you do, when you try to operate at the frontiers of your own ability to be coherent. At the same time, refusing to digress into metaphysics even to demarcate the things that confuse you, even to form ideas which can be explicitly incoherent rather than implicitly incoherent, is indeed to become the slave of the unexamined thought.


"I don't really understand metaphysics or why it's needed." -- Matt Simpson
"Sketch version: There is no "no metaphysics" anwser, there is only "metaphysics I just unconsciously accept" and "metaphysics I've actually thought about". You can do it well or you can do it badly but you can't not do metaphysics." -- Andrew Summitt


EDIT: On further reflection, my "Huh?" doesn't square with the higher probabilities I've been giving lately of global vs. basement default-FOOMS, since that's a substantial chunk of probability mass and you can see more globalish FOOMs coming from further off. 15/5% would make sense given a 1/4 chance of a not-seen-coming-15-years-off basement FOOM, sometime in the next 75 years. Still seems a bit low relative to my own estimate, which might be more like 40% for a FOOM sometime in the next 75 years that we can't see coming any better than this from say 15 years off, so... but actually 1/2 of the next 15 years are only 7.5 years off. Okay, this number makes more sense now that I've thought about it further. I still think I'd go higher than 5% but anything within a factor of 2 is pretty good agreement for asspull numbers.


my point was that, due to butterfly effects, it seems likely that this is also true for the weather or some other natural process

Hm. True. I still feel like there ought to be some simple sense in which butterfly effects don't render a well-calibrated statistical distribution for the weather poorly calibrated, or something along those lines - maybe, butterfly effects don't correlate with utility in weather, or some other sense of low information value - but that does amp up the intelligence level required.
I later said "No SI required" so your retraction may be premature. :)


As far as I can see, you need the simulation code to literally keep track of will humans notice this

Not necessarily - when you build a particle accelerator you're setting up lots of matter to depend on the exact details of small amounts of matter, which might be detectable on a much more automatic level. But in any case, most plausible simulators have AGI-grade code anyway.

You guys have had a discussion like this here on LW before, and you mention your disagreement with Carl Schulman in your foom economics paper. This is a complex subject and I don't expect you all to come to agreement, or even perfect understanding of each other's positions, in a short period of time, but it seems like you know surprisingly little about these other positions. Given its importance to your mission, I'm surprised you haven't set aside a day for the three of you and whoever else you think might be needed to at least come to understand each other's estimates on when foom might happen.

We spent quite a while on this once, but that was a couple of years ago and apparently things got out of date since then (also I think this was pre-Luke). It does seem like we need to all get together again and redo this, though I find that sort of thing very difficult and indeed outright painful when there's not an immediate policy question in play to ground everything.

I don't understand where you're getting that from. It obviously isn't an even distribution over AI at any point in the next 300 years. This implies your probability distribution is much more concentrated than mine, i.e., compared to me you think we have much better data about the absence of AI over the next 15 years specifically, compared to the 15 years after that. Why is that?

Glancing briefly at this paper, the core idea seems to be that if we are living in a simulation, we don't have good evidence about the power of computation outside the simulation, which then might not be enough to run lots of simulations. Doesn't this argument trivially fail as a matter of logic, because assuming ~SH, we do in fact have good evidence about the expected future power of computation, so that if you accept the first two claims the third claim ~SH still becomes inconsistent as the original SA holds, hence SA still goes through? Or did I miss something on account of skimming through a paper which seems rather long for its core argument?

It would be trivial for an SI to run a grainy simulation that was only computed out in greater detail when high-level variables of interest depended on it. Most sophisticated human simulations already try to work like this, e.g. particle filters for robotics or the Metropolis transport algorithm for ray-tracing works like this. No superintelligence would even be required, but in this case it is quite probable on priors as well, and if you were inside a superintelligent version you would never, ever notice the difference.
It's clear that we're not living in a set of physical laws designed for cheapest computation of intelligent beings, i.e., we are inside an apparent physics (real or simulated) that was chosen on other grounds than making intelligent beings cheap to simulate (if physics is real, then this follows immediately). But we could still, quite easily, be cheap simulations within a fixed choice of physics. E.g., the simulators grew up in a quantum relativistic universe, and now they're much more cheaply simulating other beings within an apparently quantum relativistic universe, using sophisticated approximations that change the level of detail when high-level variables depend on it (so you see the right results in particle accelerators) and use cached statistical outcomes for proteins folding instead of recomputing the underlying quantum potential energy surface every time, or even for whole cells when the cells are mostly behaving as a statistical aggregate, etc. This isn't a conspiracy theory, it's a mildly-more-sophisticated version of what sophisticated simulation algorithms try to do right now - expend computational power where it's most informative.

Agreed. Moved.

Necessary-at-stage-1 is not the same as necessary-at-stage-2. A lot of people seem to use the word "safety" in conjunction with a single medium-level obstacle to one slice out of the total risk pie.

Doesn't this argument Prove Too Much by also showing that without a Metagod, God should be expected to have arbitrary and random governing principles? The universe is ordered, but trying to explain that by appealing to an ordered God begs the question of what sort of ordered Metagod constructed the first one.

I've only read the LW post, not the original (which tells you something about how concerned I am) but I'll briefly remark that adding humans to something does not make it safe.

15 years plus more importantly everyone besides Google is too much possibility width to use the term "very unlikely".

For the record, I once challenged Craig to a Bloggingheads but he refused.

If you're interested in experimenting...
Well, wait. Is there some way of flagging "potentially damaging information that people who do not understand risk-analysis should NOT have access to" on this site? Because I'd rather not start posting ways to hack your wetware without validating whether my audience can recover from the mental equivalent of a SEGFAULT.

In my position, I should experiment with very few things that might be unsafe over the course of my total lifetime. This will probably not be one of them, unless I see very impressive results from elsewhere.

It's a forum where taking atheism for granted is widespread, and the 10% of non-atheists have some idea of what the 90% are thinking. Being atheist isn't part of the official charter, but you can make a function call to atheism without being questioned by either the 10% or the 90% because everyone knows where you're coming from. If I was on a 90% Mormon forum which theoretically wasn't about Mormonism but occasionally contained posters making function calls to Mormon theology without further justification, I would not walk in and expect to be able to make atheist function calls without being questioned on it. If I did, I wouldn't be surprised to be downvoted to oblivion if that forum had a downvoting function. This isn't groupthink; it's standard logical courtesy. When you know perfectly well that a supermajority of the people around you believe X, it's not just silly but logically rude to ask them to take Y as a premise without defending it. I would owe this hypothetical 90%-Mormon forum more acknowledgement of their prior beliefs than that.
I regard all of this as common sense.

I regard atheism as a slam-dunk issue, but I wouldn't walk into a Mormon forum and call atheism a settled question. 'Twould be logically rude to them.

Technically the shofar blowing thing should not be enough sensory evidence to convince you of the prior improbability of this being the God - probability of alien teenagers, etcetera - but since you weren't expecting that to happen and other people were, good rationalist procedure would be to listen very carefully what they had to say about how your priors might've been mistaken. It could still be alien teenagers but you really ought to give somebody a chance to explain to you about how it's not. On the other hand, we can't execute this sort of super-update until we actually see the evidence, so meanwhile the prior probability remains astronomically low.

I don't get that any of them identify themselves as higher status than they are. Certainly Anna, Alicorn, and Julia have very high community status.

Hm. Technically for EU differentials to converge we only need that the number of people we expectedly affect sums to something finite, but having a finite expected number of people existing in the multiverse would certainly accomplish that.

Hanson has a post somewhere about how the first-movers often don't get credited, just the prestigious second-movers.

Seconded.

...just to be clear on this, you have a persistent hallucination who follows you around and offers you rationality advice and points out fallacies in your thinking?
If I ever go insane, I hope it's like this.

I once considered changing my name to Ben Abard but decided that the original Eliezer Yudkowsky sounded more like a scientist.

Pretty sure that also happens in fields other than the hard sciences. For example, it is said that converts to a religion are usually much more fervent than people who grew up with it (though there's an obvious selection bias).
(The advanced, dark-artsy version of this is claiming with a straight face to never have believed A in the first place, and hope the listener trusts what you're saying now more than their memory of what you said earlier, and if it doesn't work, claim they had misunderstood you. My maternal grandpa always tries to use that on my father, and almost always fails, but if he does that I guess it's because it does work on other people.)

The operative glory is doing it in five seconds.

That would require a code change.

Because Main is for higher-quality posts that people who don't read Discussion read, and then Promoted is for even higher-quality posts for people subscribed to the Promoted RSS feed?

I suggest a move to Main.

This works better when some of the MOTAS who read the fiction have also met you in the flesh (N=2). Also, having at least one protagonist who shares some of the more prominent features of your personality (i.e., your warped sense of humor if you're liable to inflict that on your mate) might be more effective at selecting on the audience (if they like the protagonist, they may be able to tolerate your own twisted humor) but here I haven't tried it your way for comparison.


What makes a scientist a scientist instead of a crackpot is the debugging and validation. Trying to exclude every way the results might not mean what it seems like they mean - not just doing control-experiment comparison and saying you've done your duty.

This would make most modern professional scientists crackpots which sounds a bit noncentral - they may be no true scientists, but they seem very different from the crackpots I've met.


Do I need to update in the direction of optimal Turing machine code requiring very few bits?

In general, probably yes. Have you checked out the known parts of the Busy Beaver sequence? Be sure to guess what you expect to see before you look.
In specific, I don't know the size of the constant c.

Yeah, if we're going to bastardize the terms anyway, we should definitely distinguish Pascal's Spamming from Pascal's Mugging, where Spamming is any Mugging of a type that has a thousand easily generated variants without loss of plausibility ('plausibility' to a reasonable non-troll not committing the noncentral fallacy). (For emotional purposes, not decision-theory purposes.)

Well, it's still encoded. But I actually meant to say "almost 100" in the original. And yes, that's the answer.


This debate brings to mind one of the more interesting differences between the hard sciences and other fields. This occurs when you firmly believe A, someone makes a compelling argument, and within a few seconds you firmly believe not-A, to the point of arguing for not-A with even more vigor that you used for A just a few seconds ago.

-- Lou Scheffer
(Most recent example from my own life that springs to mind: "It seems incredibly improbable that any Turing machine of size 100 could encode a complete solution to the halting problem for all Turing machines of size up to almost 100... oh. Nevermind.")

But now she's... you know... now she's... (wipes away tears) slightly less real.

All the other Yoona-939s were fine, right? And that Yoona-939 was terminated quickly enough to prevent divergence, wasn't she?
(my point is, you're making it seem like you're breaking the degeneracy by labeling them. But their being identical is deep)


Or maybe "what other mental constructs could one use in place of TL4 to make a similar argument?"

Have you got one?

...it's probably gnarlier than a month of work when you've just read one book on Python, I'm afraid.

So is everything else except biology and physics.

(Also yep.)

(Yep. More a than b, it still feels pretty unnatural to me.)

I've currently got a Discussion post running to figure out how much this generalizes.

You're right. Edited.

Even if the field X is confused, to confidently dismiss subtheory Y you must know something confidently about Y from within this confusion, such as that Y is inconsistent or nonreductionist or something. I often occupy this mental state myself but I'm aware that it's 'arrogant' and setting myself above everyone in field X who does think Y is plausible - for example, I am arrogant with respect to respected but elderly physicists who think single-world interpretations of QM are plausible, or anyone who thinks our confusion about the ultimate nature of reality can keep the God subtheory in the running. Our admitted confusion does not permit that particular answer to remain plausible.
I don't think anyone I take seriously would deny that the field of anthropics / magical-reality-fluid is confused. What do you think you know about all computable processes, or all logical theories with models, existing, which makes that obviously impermitted? In case it's not clear, I wasn't endorsing Tegmark Level IV as the obvious truth the way I consider MWI obvious, nor yet endorsing it at all, rather I was pointing out that with some further specification a version of T4 could provide a model in which frequencies would go as the probabilities assigned by the complexity+leverage penalty, which would not necessarily make it true. It is not clear to me what epistemic state you could occupy from which this would justly disappoint you in me, unless you considered T4 obviously forbidden even from within our confusion. And of course I'm fine with your being arrogant about that, so long as you realize you're being arrogant and so long as you have the epistemological firepower to back it up.

I've certainly learned my lesson about the Streisand effect. With respect to everything else, I think I can manage to not care where they scream so long as they're not doing it anywhere it's visible to an LW user who doesn't make a special effort to see it.

I think I've dealt with enough shit on Wikipedia - over the Bogdanov affair, if nothing else, maybe you've heard of it? - to be able to tell you that you brought a lot of this shit on your own head, which was my original point before we began swinging moderator-dicks around.

Shit has increased by maybe 20% since l'affaire B. I think this may be partially due to the poor design of LW which makes deletions visible. I'm really impressed by Facebook's lovely user experience - when I get a troll comment I just click the x, block the user and it's gone without a trace and never recurs.
What I'd really like to do is to be able to move whole comment threads to a /meta subreddit, thereby banishing them from the flow of productive discussion without destroying information. Then it would also be easy and safe to give all posters the ability to banish comments from their posts, including comments complaining about their moderation and so on, and not have to worry about it if they didn't want to. I don't know if we'll ever have the programming resources for that.

Based on priors for expected shit? The Wikipedia part and the subreddits, maybe - it depends on whether you're identifiable to them as the one responsible, or just another face in the crowd of moderators. Haskell might be too technical although I would also expect it to attract nonconformists. I don't know how gwern.net works.

Ever moderated anything? (Curious.) And that is far, far, far from the only unpleasant experience I've ever had as a moderator.


And whatever feedback you have received, you would have received even more feedback. Nupedia vs Wikipedia - wait, is that example so excellent that you don't even know what Nupedia is? Closer to home, then: OB published everything sent to it, yet Eliezer discovered when LW was turned on that this 'trivial inconvenience' was inhibiting countless posts and submissions.

Apparently these editors have decided that rather than getting as much activity as possible, they're willing to settle for smaller amounts of activity if it means they don't have to deal with all the shit you get by moderating after the fact. I can't fucking blame them the tiniest bit.

I'm not familiar with any certain metaphysical assumptions. And the constraint here is along the lines of "things converge" where it is at least plausible that reality has to converge too. (Small edit made to final paragraphs to reflect this.)

I'm surprised. I guess CP-1 could've been, in effect, mostly empty space filled with U-235 dust. And I'll go ahead and agree that if all non-particle-accelerator pathways to chain reactions bottlenecked through U-235 then Fermi may have been correct to say 10% (though it is still not totally clear why 10% would've been a better estimate than 2% or 50%, but I'm not Fermi). This would then form only the second case I can think of offhand where erroneous scientific pessimism was not in defiance of laws or evidence already known. (The other one is Kelvin's careful calculation that the Sun was probably around 60 million years old, which was wrong, but because of new physics - albeit plausibly in a situation where new physics could've rightly been expected, and where there was evidence from geology. Everything else I can think of offhand is "You can't have a train going at 35mph, people will suffocate!" or "You can't build nanomachines!" so you can see why my priors made me suspicious of Fermi.)

How much higher? Natural uranium, which is what they used in CP-1, is over 99% U238.

The original giant heap of uranium bricks with k=1.0006 (CP-1 the first pile) - was that chain reaction all due to U235? Maybe the spontaneous fissions are mostly U235, but are the further fissions mostly neutrons hitting U235? This doesn't correspond with my mental model of a pile like that - surely the 2-3 neutrons per fission would mostly hit U238 rather than U235. I also know there were graphite bricks in the pile and graphite bricks are for having slow neutrons being captured by U238.
Let's suppose U235 didn't exist any more. We couldn't build a huge heap of pure U238 uranium bricks, and throw in a small number of neutrons from somewhere else (radium?) to get things started?
EDIT: Okay, I just read something else about slow neutrons being less likely to be absorbed by U238, so maybe the whole pile is just the tiny fraction of natural U235 with the U238 accomplishing nothing? This would indeed surprise me, but I guess then the case can be made for all access to chain reactions bottlenecking through U235. Still seems a bit suspicious and I would like to ask some physicist who isn't frantically trying to avoid hindsight bias how things look in retrospect.
EDIT2: Just read a third thing about slow neutrons being more easily captured by U238 again.


This whole leverage ratio idea is very obviously an intelligent kludge / patch / work around because you have two base level theories that either don't work together or don't work individually.

I'd be more worried about that if I couldn't (apparently) visualize what a corresponding Tegmark Level IV universe looks like. If the union of two theories has a model, they can't be mutually inconsistent. Whether this corresponding multiverse is plausible is a different problem.

I know they got a critical reaction with a big heap of unrefined uranium. This makes no mention of uranium needing to be isotopically refined for plutonium production on the Manhattan Project. As you are generally a great big troll, I am afraid I cannot trust anything you say about isotopic refinement having been used or required without further references, but I will not actually downvote yet in case you're not lying. Got a cite?

Note that in Bostrom's version and my revised version, the Mugger is offering a positive trade, not making a threat. Isn't it great if more and more people offer you a googolplex expected utilons for $5? :)

I'm still suspicious that Fermi could truly not have done better than "ten percent", and wonder if people are trying a little too hard not to give in to hindsight bias and overfitting, at the cost of failing to learn heuristics that could indeed generalize. Agreed that if +chain reaction implied a new fact of physics in the sense that it tells you about a previously unknown heavy element which emits free neutrons and is splittable, the standard heuristic "does the failure of this prediction tell us a new fact of physics" does not work in the vanilla sense. This doesn't mean that a fair estimate of the probability of at least one not-yet-examined element having the desired properties would have been ten percent. Chain reactions were not just barely possible for a large barely-critical fission plant 50 years later, rather they were soon achieved at a prompt supercritical grade adequate for nuclear explosions by two distinct pathways of U-235 refinement and P-239 breeding, both of which admittedly required effort, but was the putting-in of that effort unpredictable? But this should be continued in the other post rather than here.

Hm. That does sound like a problem. I hadn't considered the problem of finite axioms giving you unboundedly large likelihood ratios over your exact situation. It seems like this ought to violate the Hansonian principle somehow but I'm not sure to articulate it...
Maybe not seeing the tag updates against the probability that you're in a universe where non-tags are such a tiny fraction of existence, but this sounds like it also ought to replicate Doomsday type arguments and such? Hm.

When we jump to the version involving causal nodes having Large leverage over other nodes in a graph, there aren't Large numbers of distinct people involved, but there's Large numbers of life-centuries involved and those moments of thought and life have to be instantiated by causal nodes.

(also, it seems less than 3^^^3-level certain that there's no clever trick to get effectively infinite computing power or effectively infinite computing time, like the substrateless computation in Permutation City)

Infinity makes my calculations break down and cry, at least at the moment.

I think the bullet points as a whole are "very unlikely" (the universe as a whole has some Kolmogorov complexity, or equivalent complexity of logical axioms, which determines this); within that universe your being one of the non-hypercomputed sentients is infinitesimally unlikely, and then there's a vast update when you don't see the tag. How would you reason in this situation?


Treating this as infinitesimally likely, and then jumping to measurable probability on receipt of (what?) evidence about hypercomputers being possible, etc, seems pretty unreasonable to me.

It seems reasonable to me because on the stated assumptions - the floating tags seen by vast numbers of other beings but not yourself - you've managed to generate sensory data with a vast likelihood ratio. The vast update is as reasonable as this vast ratio, no more, no less.

I'll give it to you for five dollars.

I think this is captured by the notion that a causal node should only improbably occupy a unique position on a causal graph?

I can't offhand see how to translate the given numbers into a Kelly betting criterion. My own heuristic is something more along the lines of "Find the best thing that looks like it might actually work and do it." Things that won't actually work are not done anyway even if the current state of the search calls them "best", but I try to avoid Enrico-Fermi-style "ten percent" underestimates about what might actually have an impact. No holes have actually appeared in the sky, and I'm not presently working with any N larger than 10^80, and there's no reason to worry about small probabilities of affecting that when it's easy to find several different medium-sized candidates. I'd only want to complicate my reasoning any further if I ended up in a more complicated situation than that.
(I also don't think that perpetual motion machines have N>>n.)

Well, besides that thing about wanting expected utilities to converge, from a rationalist-virtue perspective it seems relatively less dangerous to start from a position of someone rejecting something with no priors or evidence in favor of it, and relatively more dangerous to start from a position of rejecting something that has strong priors or evidence.

Everyone's a Boltzmann brain to some degree.

I'm a lot more worried about making an FAI behave correctly if it encounters a scenario which we thought was very very unlikely.


Relativity restricted the maximum speed of travel, thus revealing that countless future generations will not be able to reach the stars

That's perfectly credible since it implies a lack of leverage.

Archimedes's discovery of the buoyancy laws enabled future naval battles and ocean faring, impacting billions so far

10^10 is not a significant factor compared to the sensory experience of seeing something float in a bathtub.

The only thing that matters in physics is the old mundane "fits current data, makes valid predictions".

To build an AI one must be a tad more formal than this, and once you start trying to be formal, you will soon find that you need a prior.

Related: Would an AI conclude it's likely to be a Boltzmann brain? ;)

If you really, honestly can't tell the difference between that and an xrisk reduction charity, you're probably not reading LW in the first place. Or if you mean something else by this question, could you ask with a different example instead?

That hyperbole one. I wasn't intending the primary focus of this post to be on the notion of a super-update - I'm not sure if that part needs to make it into AIs, though it seems to me to be partially responsible for my humanlike foibles in the Horrible LHC Inconsistency. I agree that this notion is actually very underspecified but so is almost all of bounded logical uncertainty.



In real life, you are not ever supposed to have a prior improbability of 10^-100 for some fact distinguished enough to be written down, and yet encounter strong evidence, say 10^10 to 1, that the thing has actually happened.

Sure you do. As you pointed out, dice rolls. The sequence of rolls in a game of Risk

Those aren't "distinguished enough to be written down" before the game is played. I'll edit to make this slightly clearer hopefully.

Fixed.


Absolute faith corrupts as absolutely as absolute power.

-- Eric Hoffer

Moving to Discussion (not much upvoted).

When the evidence is a huge hole opening in the sky? I think common sense allows you to take fairly large actions after seeing a huge hole opening in the sky. I mean, in practice, I would tend to try and take certain actions intended to do something about the rather high posterior probability that I was hallucinating and be particularly wary of actions that sound like the sort of thing psychotic patients hallucinate, but this is an artifact of the odd construction of the scenario and wouldn't apply to the more realistic and likely-to-be-actually-encountered case of the physics theory which implied we could use dark energy for computation or whatever.

This is indeed a good argument for viewing the leverage penalty as a special case of a locational penalty (which I think is more or less what Hanson proposed to begin with).

No, it's supposed to say that. 10^80 is earlier defined as a small large number.

I find that unsatisfactory for the following reasons - first, I am a great believer in life and love without bound; second, I suspect that the number of people in the multiverse is already great enough to max out that sort of asymptote and yet I still care; third, if this number is not already maxed out, I find it counterintuitive that someone another universe over could cause me to experience preference reversals in this universe by manipulating the number of people who already exist inside a box.

I'm not sure I agree with that one - where does the question of anthropic priors fit in? The question is how to assign probabilities to physical statements in a reasonable way.

Then this is an entirely different factor which doesn't have to do with having our EU sums converge.

That was my first thought, but then I realized that I mainly want discussion on the longer version and that it's the longer version that has the FAI consequences, etc.

Hm, a linear "leverage penalty" sounds an awful lot like adding the complexity of locating you of the pool of possibilities to the total complexity.
Thing 2: consider the case of the other people on that street when the Pascal's Muggle-ing happens. Suppose they could overhear what is being said. Since they have no leverage of their own, are they free to assign a high probability to the muggle helping 3^^^3 people? Do a few of them start forward to interfere, only to be held back by the cooler heads who realize that all who interfere will suddenly have the probability of success reduced by a factor of 3^^^3?

But then his brain will be too slow.

Next time, try shifting processing resources from your brain to the analytic computers until neither is waiting on the other!

Now fixed, I hope.


For example, the chain-reaction model of financial investment would result in a single entity with the highest return rate dominating the Earth, this has not happened yet, to my knowledge.

Like... humans? Or the way that medieval moneylenders aren't around anymore, and a different type of financial organization seems to have taken over the world instead? See also the discussion of China catching up to Australia.


I agree with paper-machine that the mini-biography of I J Good has little value here.

Done.

The remark in section 1 about MIRI being funding-limited is out of place and looks like a whine or a plea for more money. Just take it out.

Done.

http://lesswrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/

Yep, that trivial extension one.

...this seems exactly, diametrically wrong.

And lo, Wedrifid did invent the concept of Steel Vulcan and it was good.
Do we actually have enough fictional examples of this to form a trope? (At least 3, 5 would be better.)

Thou shalt not quote Yudkowsky.

Superficial stylistic remarks (as you'll see, I've only looked at about the first 1/4 of the paper):

The paper repeatedly uses the word "agency" where "agent" would seem more appropriate.
I agree with paper-machine that the mini-biography of I J Good has little value here.
The remark in section 1 about MIRI being funding-limited is out of place and looks like a whine or a plea for more money. Just take it out.
"albeit" on page 10, shortly before footnote 8, should just be "but". (Or maybe "even though", if that's your meaning.) [EDITED to add: there's another "albeit" that reads oddly to me, in footnote 66 on page 50. It's not wrong, but it feels odd. Roughly, wherever you can correctly write "albeit" you can equivalently write "even though", and that's a funny thing to be starting a footnote with.]
"criteria" in footnote 11 about paperclip maximizers should be "criterion".
In footnote 15 (about "g") the word "entrants" seems very weirdly chosen, and the footnote seems to define g as the observed correlation between different measures of intelligence, which is nonsense.
The premise of the paper is that whether or not intelligence explosion will occur is (or at least is being pretended to be) an open question. But at many points within the paper there are references to "the intelligence explosion" that seem to presuppose that there will in fact be one.
Footnote 23 puts "Wikipedia" in italics, which to my eyes looks very strange.

[EDITED to incorporate a comment I made separately, which I'll now retract.]

Well, those are the labs that don't have a blindingly obvious route to speedups just by speeding up the researchers, though de facto I'd expect it to work anyway up to a point.

The idea is that the risk is infinitesimal but you want to put an approximate number on that using a method of imaginary updates - how much imaginary evidence would it take to change your mind?

An easy basic test of whether humans are currently the limiting factor in a process is to ask whether the labs run all night, with researchers sometimes standing idle until the results come in; a lab that runs 9-5 can be sped up by at least a factor of 3 if the individual researchers don't have to eat, sleep or go to the bathroom.

Omega's not dumb. As soon as Omega knows you're trying to "come up with a method to defeat him", Omega knows your conclusion - coming to it by some clever line of reasoning isn't going to change anything. The trick can't be defeated by some future insight because there's nothing mysterious about it.
Free-will-based causal decision theory: The simultaneous belief that two-boxing is the massively obvious, overdetermined answer output by a simple decision theory that everyone should adopt for reasons which seem super clear to you, and that Omega isn't allowed to predict how many boxes you're going to take by looking at you.

If you still use "^" to refer to Knuth's up-arrow notation, then 3^^^3 != 3^(3^^26).
3^^^3 = 3^^(3^^3) = 3^^(3^27) != 3^(3^^27)

Fixed.

(This was my favorite reply, BTW.)

I don't see why it would be at all difficult or mysterious for Omega to predict that I one-box. I mean, it's not like my thought processes there are at all difficult to understand or predict.

Fair point. We're still struggling to express things verbally, but yeah.

That's actually kind of sad. Hopefully times have changed since then.

Hm. Thanks for pointing that out. Maybe I should remove the specific dates from there and just say we were 45 years apart. I think in a lot of ways trying to time the intelligence explosion is a huge distraction. An important probability distribution, but still a huge distraction.

Where does it say 2035 in the text? How did you get the impression that this was an estimation?

Falsified by diarrhea. Next!


In the worst case, the protein folding could be a secure hash

Then it would be harder, in fact impossible, to end up with slightly better proteins via point mutations. A point mutation in a string gives you a completely different secure hash of that string.
This isn't a minor quibble, it's a major reason to go "Whaa?" at the idea that protein folding and protein design have intractable search spaces in practice. They have highly regular search spaces in practice or evolution couldn't traverse that space at all.

I guess to me the notion of "solve an NP-hard problem" (for large N and hard cases, i.e., the problem really is NP hard) seems extremely exotic - all known intelligence, all known protein folding, and all known physical phenomena must be proceeding without it - so I feel a bit at a loss to relate to the question. It's like bringing up PSPACE completeness - I feel a sense of 'where did that come from?' and find it hard to think of what to say, except for "Nothing that's happened so far could've been PSPACE complete."

Why? Nature designed ribosomes without solving any hard cases of NP-hard problems. Why would Ribosomes 2.0, as used for constructing the next generation of molecular machinery, require any NP-hard problems?

I think the more important point is that Nature doesn't care about the worst case (if a protein takes forever to fold correctly then it's not going to be of any use). But an AI trying to design arbitrary proteins plausibly might.

It's not meant as a response to everything, just noting that protein structure prediction can't be NP-hard. More generally, I tend to take P!=NP as a background assumption; I can't say I've worried too much about how the universe would look different if P=NP. I never thought superintelligences could solve NP-hard problems to begin with, since they're made out of wavefunction and quantum mechanics can't do that. My model of an intelligence explosion just doesn't include anyone trying to do anything NP-hard at any point, unless it's in the trivial sense of doing it for N=20 or something. Since I already expect things to local FOOM with P!=NP, adding P=NP doesn't seem to change much, even if the polynomial itself is small. Though Scott Aaronson seems to think there'd be long-term fun-theoretic problems because it would make so many challenges uninteresting. :)

20 standard amino acids, so 19 * 20,000 one-amino-acid-changed variants. If you can find something by searching 380,000 cases, it wasn't an NP-hard problem.
EDIT: Actually, since I originally said 20,000 bases, it should just be 3 variants per base (4 standard DNA bases). I don't know if there's any significant 20,000-unit peptides.

Since this apparently bothers people, I'll try to fix it at some point. A more faithful statement would be that we start by investing work w, get a return w2 ~ log(w), reinvest it to get a new return log(w + w2) - log(w) = log ((w+w2)/w). Even more faithful to the same spirit of later arguments would be that we have y' ~ log(y) which is going to give you basically the same growth as y' = constant, i.e., whatever rate of work output you had at the beginning, it's not going to increase significantly as a result of reinvesting all that work.
I'm not sure how to write either more faithful version so that the concept is immediately clear to the reader who does not pause to do differential equations in their head (even if simple ones).

Conceded.

I wish I could upvote your retraction.

Nothing that has physically happened on Earth in real life, such as proteins folding inside a cell, or the evolution of new enzymes, or hominid brains solving problems, or whatever, can have been NP-hard. Period. It could be a physical event that you choose to regard as a P-approximation to a theoretical problem whose optimal solution would be NP-hard, but so what, that wouldn't have anything to do with what physically happened. It would take unknown, exotic physics to have anything NP-hard physically happen. Anything that could not plausibly have involved black holes rotating at half the speed of light to produce closed timelike curves, or whatever, cannot have plausibly involved NP-hard problems. NP-hard = "did not physically happen". "Physically happened" = not NP-hard.

Then evolution wouldn't happen in real life.
Actually, even that understates the argument. If you can take a 20,000 base sequence and get something useful by point-perturbing at least one of the 20,000 bases in the sequence, then whatever just happened was 50 lightyears from being NP-hard - you only had to search through 19 variants on each of 20,000 cases.

I believe I've seen that discussed before and the answer is just that in real life, proteins don't fold into the lowest-energy conformation. It's like saying that the global minimum energy for soap bubbles is NP-complete. Finding new useful proteins tends to occur via point mutations so that can't be NP-complete either.

Protein folding cannot be NP-hard. The physical universe is not known to be able to solve NP-hard problems, and protein folding will not involve new physics.

It's 40,000 words, you say. How fast exactly do you expect any huge problems to be found?

If it's a huge problem like "I can't download" or "all these pages are blank" then relatively fast.

Ah, right. I think a nonrandom variable situation actually came up and I said "May the unknown variables be in your favor" or "May the hidden variables be in your favor".

Nope, was using that before I read (some of) Hunger Games.

In sufficiently selective venues, I've been known to say "Good skill!" and "May the random variables be in your favor!"

The idea here is that you use parallelism to implement operations like caching which can decrease the number of serial steps required for a thought, so that more of them can occur one after another. In the simplest case, if you were already using a serial processor to emulate parallel computers, adding parallel power increases serial depth because you need no longer burn serialism to emulate parallelism.

http://xkcd.com/871/

Qiaochu participated in the last MIRI math workshop. Calculation done.

By analogy with an Idiot Plot which dissolves in the presence of smart characters, a "Muggle Plot" is any plot which dissolves in the presence of transhumanism and polyamory.
Shortly after generalizing this abstraction, someone at a party told me the original tale of the Tin Woodsman, in which there are two men vying for the attention of a healer woman who gives them replacement metal body parts while constructing a whole new body out of the spares. In the end, she decides that the men she's been healing are mechanical and therefore unloveable, and goes off with the new man she's constructed.
"Ah," I said, "a Muggle Plot."
They're surprisingly common once you start looking. I originally generalized it while watching the romantic subplot in Madoka. Blah blah, not a real human, blah blah, love rival..

For lack of any huge problems discovered so far, moving this to Main.

I think that teams of up to five people can scale "pretty well by human standards" - not too far from linearly. It's going up to a hundred, a thousand, a million, a billion that we start to run into incredibly sublinear returns.

To paraphrase, you're pointing out that stocks and precious metals come with built-in demand shock absorbers, whereas Bitcoin has none. I'm not totally sure that I accept this point, because I could see alternative cryptocurrencies playing the role of marginal new stocks or newly mined gold. However, even if Bitcoin were unique in having no demand shock absorbers, I'm not sure this matters, because it seems empirically to be the case that these shock absorbers are not always up to the task, and that both stocks and precious metals do experience a great deal of price volatility, even over the medium to long term.
In other words, even if Bitcoin is especially sensitive to changes in demand, it is neither novel nor unique in being susceptible to bubbles.
This would seem to me to imply that Bitcoin's existence and use as a store of value is no threat to the economy. (And its use as medium of exchange seems harmless as well.)
It would seem that problems would only arise for those who try to use Bitcoin as a unit of account. This is in line with Wei's comment where he suggests that with a currency in fixed supply, fluctuating velocity of money implies that either prices or GDP must be unstable.
So my conclusion is that using Bitcoin as a medium of exchange or store of value is not detrimental to the economy, but one should continue to price goods or services in some other fiat, ideally NGDP-targeted, currency. Does that sound about right?

Using it as a store of value is detrimental. Anyone bidding on a Bitcoin is not bidding on a productive project.

...that is exactly the sort of judgment which requires some sort of theory. Every day, trillions of things happen which have never happened before. Never in the history of the universe has this comment been posted to LW!

Using my theory of planetary motion, I will be able to STOP THE EARTH FROM CRASHING INTO THE SUN.

Two quick notes on the current text: Kasparov was apparently reading the forum of the opposing players during his chess victory in Kasparov vs. The World, which doesn't quite invalidate the outcome as evidence but does weaken the strength of evidence for cognitive (non-)scaling with population. Also Garett Jones made some relevant remarks here which I should've cited in the discussion of how science scales with scientific population and invested money (or rather, how it doesn't scale).

It partially explains the price of gold, yes. Gold's situation isn't really the same for three reasons: First, gold can be mined if the price goes too high, and higher prices would imply larger amounts of recoverable gold. Second, a lot of the gold on the market is paper gold, theoretical gold that two parties are trading rather than sending large gold bars around, which also adds to the supply. But most of all, unlike the supposed use-case of Bitcoin, gold is not being used as a medium of account or medium of exchange any more, just one store of value among many, so its real competition is not paper gold or mined gold but other stores of value such as platinum, silver, real estate, and many other things being added to the competition for 'stores of value' as the economy grows. If the same fraction of the population tried to store the same fraction of their net assets in gold today as in the 1600s then the price of gold would be vastly higher - or so I would think, I haven't run the numbers. But this in turn means that the share of the economy represented by gold can easily drop further, making it less than a perfect store of value etcetera, although gold has still tended to be a better store of value than fiat currency.
Of course fiat currency is really supposed to be a medium of exchange and account, not a long-term store of value, though dumb people like me tend to use it as a store of value too because it's convenient and we haven't gotten around to setting up anything different and we don't have that much value to store. And then using your medium of exchange and account as a store of value causes recessions and depressions due to the paradox of thrift; when people want to consume in the future instead of the present they try to hold paper money instead of demanding equity in projects with long-term payoffs. On the plus side, central banks can, in principle, easily rectify some part of this problem by printing more money to meet demand for currency when fear rises, and thus make up for velocity slowdowns, keeping NGDP on a level growth path. On the minus side, central banks are stuck in 30-year-old economic thinking and don't keep NGDP on a level growth path. Bitcoin has the potential to make things much, much worse though.
New stocks on the other hand are constantly being created as the economy grows - no particular stock, or set of stocks starting at a fixed time, are guaranteed to grow at the same rate as the global economy.

Not that I know of. Obviously it has happened somewhere, in some city, given the number of LW meetups. Statistics do not permit otherwise, even allowing for male LW attendees to be unusually cultured.

This doesn't seem LW-specific.

If you can't pick something non-average to meet your optimization criteria, you can't optimize above the average.
This comment has been brought to you by my Dvorak keyboard layout.

I hear the ethics at Starbucks are rather low-quality and in any case, surely Starbucks isn't the cheapest place to purchase ethics.

See "Timeless Causality": http://lesswrong.com/lw/qr/timeless_causality/

What is your basis for the designation ? I am not arguing with your suggestion (I was leaning in the same direction myself), I'm just genuinely curious. In other words, why do you believe that PrawnOfFate is a troll, and not someone who is genuinely confused ?

Combined behavior in other threads. Check the profile.

Designating PrawnOfFate a probable troll or sockpuppet. Suggest terminating discussion.

Nice response so I'm keeping it, but killthread beyond this point, or take it to the LW uncensored thread on Reddit. Attention equals reward for trolls.
Discussion of this action is also meta and should also go to the uncensored thread (posting a link there will be tolerated so long as the body text is not offensive).

Off the top of my head:

Stop showing user page edits in the LW wiki sidebar.
Also in the sidebar: Don't show reverted edits to pages. Show the last real pages with non-reverted edits.
When a moderator deletes a comment, if it has no subcomments or if all comments have been deleted, don't show any leftover traces. If there are subcomments, require a deliberate user click to expand them and only allow logged-in users with >=1 karma to do so. Apply the -5 karma troll toll to comments with a deleted ancestor.
Show parent comments in user comment feeds (the list of a user's most recent comments). If the user has less than 80% upvotes, enable downvoting in the comment feed for users with over 1000 karma.
Cause new comments on a post to stand out more than they do currently (the small green aura does not enable super-easy scanning).
Implement Reddit's "highlight comments since..." feature.
Show the most recent Rational Quote comment in the main sidebar, the most recent Open Thread and Rationality Diary comment in the Discussion sidebar (i.e., the latest comment from the latest post with the appropriate tag).
Automatically strip crap (the sort of stuff e.g. Word generates, but all text editors seem to do it now) from the post editor.

More ambitious projects:

Easier tracking of ongoing discussions - "subscribe" to a post and see new comments on it forever, possibly in a separate inbox.
Subreddits besides Discussion.


EDIT: Also in the Ambitious column: Give the moderators the ability to move a whole comment thread between posts, preferably to another subreddit (so we can create /r/meta and dump a bunch of this old stuff there), preferably with a replacement comment that automatically links to the new location and has space for a moderator comment explaining the reason for moving. This would be better than deletion in a lot of cases.

Reclarified?

Let me rephrase: The problem is that Bitcoins will have an advantage over the average productive investment, e.g. stocks (sort of), as a store of value, since Bitcoin has all their average expected growth with none of their added (local) volatility. This is what presents the starting problem in an economy that starts out with a steady velocity of Bitcoins, and then increased holding makes the velocity go down (and the value go up, and the bubble effect hit even harder). This is why we don't get an equilibrium with steady Bitcoin velocities. Even if we did have that equilibrium, people would have a much greater incentive to just "invest" in Bitcoins instead of being forced to try to invest in something productive. You don't want an economy to have a perfect non-inflating store of value which is intrinsically unproductive!

I don't think you understand the concept here. I'm not deleting comments because it gives me a satisfying feeling. I deleted Caledonian's comments because he was successfully shifting OB to troll comments and discussion of troll comments, and this was giving me an 'ouch' feeling each time I posted. I tried talking myself out of the ouch feeling but it didn't actually work. I asked people to stop feeding the troll and that also didn't work. So I started deleting comments because I don't live in a world of things that ought to work.
Banning all meta discussion on LW of any kind seems like an increasingly good idea - in terms of it being healthy for the community, or rather, meta of any kind being unhealthy.
/r/science occasionally vaporizes half the comments in their threads now and it hasn't seemed to hurt them any. I don't think censorship actually hurts reputation very much, certainly not enough to make up for the degree to which meta blather hurts community.

Been there. Done that. Got tired. Try being a D-level Internet celebrity sometime. It will rapidly exceed reserves of patience you didn't know you had.


Deleting people's comments because of your negative emotion reaction is a strategy I strongly recommend against

I suppose I should've used my free will to ignore the negative conditioning being applied to me? I'll go do that as soon as I acquire free will.


but that would require development resources, and as ever, we have none.

This intrigues me. You (and others) have said this multiple times, and I wonder what it means.
Presumably it would only take a few thousand dollars to round up a list of the highest value/cost ratio programming improvements on LW, and then pay someone to implement them. Do I underestimate the cost here?
So the fact that you (generalized you, in your role as LW sponsor) are not doing this implies that improvements to LW have low marginal value compared to other projects (presumably MIRI stuff). LW improvements look high value from out here.
It's interesting, then, that you take the time to delete things and write up these deletion reports. A few thousand dollars applied to some brave volunteer could save you a lot of time added up over the years. This needs calculation, of course. Also, you're probably doing this LW janitor stuff on your recharge time between actual work-ability time.
I can't say I disagree with the revealed preference; most of the value of LW seems to be the archives, meetups, and existence, which is secured for now. I'd rather you spent my money on saving the world (which I tentatively infer is much further along than external communications claim).

If we were bid $5K for the top dozen improvements by a credible source, we'd take it, but no such bid has ever occurred. I think you underestimate the cost.

No, what I mean is that if anyone else sets up a cryptocurrency right now, they don't have to worry about making it exchangeable with dollars, they just need a good way to make it exchangeable with Bitcoins, and that could easily be done using pure programming. Bitcoins is a horrible store of value and an even worse medium of account, but some of the underlying ideas have great potential as a medium of exchange, and Bitcoin can sneeze any previous development of real-world interfaces directly into a new, competing cryptocurrency.


I wonder to what degree FAI/CEV engineering considerations overlap with cryptocurrency/efficient-market engineering considerations

Basically zero on a technical level. Philosophy of caution overlaps with cryptography. Some econ knowledge overlaps with hard takeoff theory.

That's a problem because then BTC is a perfect investment which always grows at exactly the same rate as the global economy. So it gives you the exactly average return on investment with zero volatility. So it seems like a near-perfect store of value and people will want to hold it rather than spend it. This decreases velocity which causes deflation and value that increases apparently even faster than the total global economy. This makes Bitcoin apparently an even better investment, until the volatility or expected volatility from the huge stores of unused Bitcoins outweighs its apparent returns on investment, and note that financial markets are apparently unusually bad at expecting future volatility to be greater than present volatility; people try to time bubbles instead. This is bad for Bitcoin because of the inevitable crash followed by hyperinflation. And it's bad for the global economy because your currency is deflating and any given bank would rather hold Bitcoins, on average, than make loans; and then the inevitable crash is also bad. That's a nutshell version of a longer story.

Econ relates to intelligence explosion dynamics, Scott Sumner appears to be a Correct Contrarian.
I don't have any good ideas for how to do NGDP level targeting inside a cryptocurrency in a way that would automatically distinguish more widespread adoption from increased RGDP from somebody gaming the system.

I've previously marked V_V as a probable troll. It seems a lot of feeding is going on. This post in particular is not an appropriate place for it. I'm thinking of adding a term to the Deletion Policy for, well, this sort of thing on any post that reports a positive community effort - see Why Our Kind Can't Cooperate for the rationale.
When I was doing OB and the Sequences, I realized at one point that Caledonian was making it un-fun for me since each post was followed by antihedons from him, and that if I didn't start deleting his comments, I would probably stop continuing (though I certainly didn't know as much then about reinforcement psychology, I still appreciated this on some instinctive level). I'm not going to tolerate that kind of negative stimulus being applied to community organizers.
I think it might actually be a good idea to give any poster the power to delete replies in their post's comments thread - Facebook does this automatically and I don't think it's a problem in real life, except of course for the trolls themselves - but that would require development resources, and as ever, we have none.


due to its deficient monetary policy and associated price volatility it can't grow to very large scales, and by taking over the cryptocurrency niche

I'm also quite worried about this, but on the other hand Bitcoin creates an obvious entry gateway into more advanced cryptographic currencies (i.e. once Bitcoin infrastructure is set up, other currencies can use Bitcoin infrastructure if there's a way to exchange them with Bitcoins, lowering the bar to entry).
I've had all sorts of ideas along these lines, in fact. The main reason I haven't published them is that I'm not sure that more advanced cryptocurrency advances FAI over AGI. In fact, you'd think it would be the reverse - the Great Stagnation may be all that's keeping us alive right now.


Do you mean as an alternate to D that, say, a new cryo provider takes over the abandoned preserved heads before they thaw?

Sure. That happened already once in history (though there was, even earlier, a loss-thaw). It's why all modern cryo organizations are very strict about demanding advance payment, despite their compassionate hearts screaming at them not to let their friends die because of mere money. Sucks to be them, but they've got no choice.

Or as an alternate to C, that even though the cost is high, they go ahead and do it anyway?

Yep. I'd think FAI scenarios would tend to yield that.
Basically I always sigh sadly when somebody's discussing a future possibility and they throw up some random conjunction of conditional probabilities, many steps of which are actually pretty darned high when I look at them, with no corresponding disjunctions listed. This is the sort of thinking that would've led Fermi to cleverly assign a probability way lower than 10% to having an impact, by the time he was done expanding all the clever steps of the form "And then we can actually persuade the military to pay attention to us..." If you're going to be silly about driving down all impact probabilities to something small via this sort of conjunctive cleverness, you'd better also be silly and multiply the resulting small probability by a large payoff, so you won't actually ignore all possible important issues.

Is there a good example of a conspiracy including physicists of the same prior fame as Rabi and Fermi (Szilard was then mostly an unknown) which was pursuing a 'remote possibility', of similar impact to nuclear weapons, that didn't pan out? Obviously we would have a much lower chance of hearing about it especially on a cursory reading of history books, but the chance is not zero, there are allegedly many such occasions, and the absence of any such known cases is not insignificant evidence. Bolded to help broadcast the question to random readers, in case somebody who knows of an example runs across this comment a year later. The only thing I can think of offhand in possibly arguably the same reference class would be polywell fusion today, assuming it doesn't pan out. There's no known conspiracy there, but there's a high-impact argument and Bussard previously working on the polywell.

(Nods.)

Thank you very much for giving it this much attention!

Thanks for eliminating the 'scam' line! That is what caused me to keep going.

It's wise to consider how non-hindsight might have been harder. It's even wiser to consider, for each training example, what general heuristics might've helped anyway.

But such obviously was not the case for uranium in its natural form, or the substance would long ago have ceased to exist on earth.

But there ought to be some unstable elements that hadn't fizzed by themselves in natural aggregations and purities, and many such, and these might be manipulated by humans. If something doesn't happen naturally, are you in a situation where you're likely to be learning about a randomly placed lower bound that's probably randomly far above you, or in a case where you're learning about a nearby lower bound that probably has some things right above it?

However energetically interesting a reaction, fission by itself was merely a laboratory curiosity.

Sounds like an absurdity heuristic; this is a bad general lesson to learn. "Laboratory curiosity" foresooth.

Only if it released secondary neutrons, and those in sufficient quantity to initiate and sustain a chain reaction, would it serve for anything more.

Which it did. So why should one have been confident that they didn't...?

"Nothing known then," writes Herbert Anderson, Fermi's young partner in experiment, "guaranteed the emission of neutrons. Neutron emission had to be observed experimentally and measured quantitatively."

The good old confusion between negative information and positive information of falsehood, perhaps?
Again, trying to avoid hindsight bias is not best done by inventing new cynical contrarian ideas that serve to steer your mind in the opposite direction of each training example. It would be better to look for truths that are hard to see, and not plausible falsehoods that by golly you ought to have believed. "It would have been just as easy to think Y as X, given Z" is a powerful argument against an alleged heuristic Z that supposedly could've told you X. "But it would have been perfectly rational to think Y!" is not how you want to train yourself.

Consider also the nature of the first heap: Purified uranium and a graphite moderator in such large quantities that the neutron multiplication factor was driven just over one. Elements which were less stable than uranium decayed earlier in Earth's history; elements more stable than this would not be suitable for fission. But the heap produced plutonium by its internal reactions, which could be purified chemically and then fizzed. All this was a difficult condition to obtain, but predictable that human intelligence would seek out such points in possibility-space selectively and create them - that humans would create exotic intermediate conditions not existing in nature, by which the remaining sorts of materials would fizz for the first time, and that such conditions indeed might be expected to exist, because among some of the materials not eliminated by 5 billion years, there would be some unstable enough to decay in 50 billion years, and these would be just-barely-non-fizzing and could be pushed along a little further by human intervention, with a wide space of possibilities for which elements you could try. Or to then simplify this conclusion: "Of course it wouldn't exist in nature! Those bombs went off a long time ago, we'll have to build a slightly different sort! We're not restricted to bombs that grow on trees." By such reasoning, if you had attended to it, you might have correctly agreed with Szilard, and been correctly skeptical of Fermi's hypothetical counterargument.
Not taking into account that engineering intelligence will be applied to overcome the first hypothetical difficulty is, indeed, a source of systematic directional pessimistic bias in long-term technological forecasts. Though in this case it was only a decade. I think if Fermi had said that things were 30 years off and Szilard had said 10, I would've been a tad more sympathetic toward Fermi because of the obvious larger reference class - though I would still be trying not to update my brain in the opposite direction from the training example.


"heroic epistemology" - that's what all those now-rich startups used, right?

It's what AirBNB used. I didn't get a chance to hear about them until they had traction, but I honestly think my general good-idea heuristics would've fired more strongly on this than a lot of conventional wisdom.
Wow, you sure are selective in your charity...


On a very large scale, if you think FAI stands a serious chance of saving the world, then humanity should dump a bunch of effort into it, and if nobody's dumping effort into it then you should dump more effort than currently into it. Calculations of marginal impact in POKO/dollar are sensible for comparing two x-risk mitigation efforts in demand of money, but in this case each marginal added dollar is rightly going to account for a very tiny slice of probability, and this is not Pascal's Wager. Large efforts with a success-or-failure criterion are rightly, justly, and unavoidably going to end up with small marginal probabilities per added unit effort. It would only be Pascal's Wager if the whole route-to-humanity-being-OK were assigned a tiny probability, and then a large payoff used to shut down further discussion of whether the next unit of effort should go there or to a different x-risk.



Isn't Fermi the guy who insisted that a nuclear reaction could set the atmosphere on fire in a massive nuclear reaction?

Not as far as I know. This was considered technically even though it seemed obviously false on its face, assigned an even lower credence afterward, and then it didn't in fact turn out to be true.

How did Fermi arrive at a 90% confidence for the false proposition that there weren't enough neutrons? What was the clever technical argument he immediately saw that Szilard and Rabi didn't, and why did it not work on Reality?

A lot of meta-level fretting has the property of being one-sided - it's about a single option considered in isolation, not about two alternatives. If there's a concrete alternative that's supposed to help humanity more and has a decent chance of being actually correct vs. the sort of thing one dutifully ought to consider, I am usually totally happy to consider it. (You've seen me ask 'Can we have a concrete policy implication, please?' or 'Is there an option on the table for what we should be doing instead, if that's true?' at a number of discussions, right? This is often what my 'wasted motion' heuristic looks like when it fires.)

It's looking more likely to be formal actually.

I suppose if we postulate that Szilard and Rabi did better by correlated dumb luck, then we can avoid learning anything from this example, yes.

Because they didn't know if fission produced enough prompt neutrons, which is clear from the quoted passage, and probably also because Fermi has estimated that there's on the order of 10 other propositions about the results from fission which he, if presented with them by an equally enthusiastic proponent, would find comparably plausible. I'm thinking that in the alternate realities where fission does something other than producing sufficient number of neutrons (about 3 on the average), you'd assign likewise high number to them by hindsight, with a sum greater than 1 (so stop calling it probability already).

A clever argument! Why didn't it work on Reality?

Expanding conjunctive probabilities without expanding disjunctive probabilities is another classic form of one-sided rationality. If I wanted to make cryonics look more probable than this, I would individually list out many different things that could go right.

Okay, seriously, how the hell did you get this impression?

I'm usually fine with dropping a one-time probability of 0.1% from my calculations. 10% is much too high to drop from a major strategic calculation but even so I'd be uncomfortable building my life around one. If this was a very well-defined number as in the asteroid calculation then it would be more tempting to build a big reference class of risks like that one and work on stopping them collectively. If an asteroid were genuinely en route, large enough to wipe out humanity, possibly stoppable, and nobody was doing anything about this 10% probability, I would still be working on FAI but I would be screaming pretty loudly about the asteroid on the side. If the asteroid is just going to wipe out a country, I'll make sure I'm not in that country and then keep working on x-risk.

Why assign a 90% probability to chain reactions being impossible or unfeasible? How should Fermi have known that, especially when it was false?
EDIT: Be careful with your arguments that Fermi should have assigned the false fact 'chain reactions are impossible' an even more extreme probability than 90%. You are training your brain to assign higher and more extreme probabilities to things that are false. You should be looking for potential heuristics that should have fired in the opposite direction. There's such a thing as overfitting, but there's also such a thing as being cleverly contrarian about reasons why nobody could possibly have figured out X and thus training your brain in the opposite direction of each example.

I believe I may be said to know something about humorous writing. It is not necessary to violate rules of rational discourse in order to have it.

The main tl;dr on the article should be something along the lines of: "Although many claims have been made and some claims continue to be made, none of the claims has ever been replicated reliably despite a very great deal of effort. There are also no good theoretical explanations for how cold fusion could be physically possible. Thus mainstream science does not currently think that cold fusion exists, and [assuming this part is true, is it and can you provide citations?] there have been several known scams aimed at extracting money from venture capitalists [or whatever alleged scam has been observed to occur]." The goal here is to quickly and accurately convey the current state of evidence, mainstream repute, and if there are scams in the wild, warn people against them in a credible fashion. Credible, in this case, means specific and documentable - calling something a scam isn't going to successfully warn off somebody who's paying money; being specific about a past scam and providing a footnote might.
Also note the ordering: First we mention the failure to reproduce experimental evidence, then the lack of theoretical backing, then that mainstream scientists don't believe in it, only then that scams have occurred(?). This ordering is important: rearranging these sentences would be bad. Strong replicated experiemental evidence beats theoretical difficulties. Then, it is not at all uncommon that a bunch of scientists say one thing even though the formal theory is pointing in another direction, so I don't want to hear about the opinion of some 'distinguished but elderly scientists' before I know what the actual numbers have to say. Finally, pointing out that some foolish people are being scammed is very weak evidence about a factual question before I know what credentialed scientists have to say about it. There are known scams that use the word 'quantum', but that is not evidence against quantum physics, just tarring something by association with bad people. There are bad and stupid people everywhere so their presence in association with a widespread concept is not good Bayesian evidence (it is almost equally likely to occur in worlds where the main theory is true as where it is false). So if you want to be convincing for sane reasons rather than bullying the reader into agreement, first you talk about the state of evidence, then you talk about the background theories and their analysis, then you cite mainstream scientific opinion. Then you show what bad things have happened to people who believe this and mock the scams so as to establish that this is a low-prestige idea and believing in it will make your friends think you're stupid, i.e. you shouldn't just do it for a bit of fun cheap irrationality - I do agree that this part is important but you can't do it first and maintain any claim to being a good guy.
The tl;dr overview can with some reasonableness describe all of these points quickly and at once at the top of the article, so it's not like you have to wait to tell people.

Not my cuppa. First paragraph:

Cold fusion, also called Low Energy Nuclear Reactions (LENR) or Chemically-Assisted Nuclear Reactions (CANR) by its proponents, is the claim of nuclear reactions at relatively low temperatures, rather than at millions of degrees. It is now mainly used as a scam to dupe the unwitting out of their money.

I don't believe in LENR either, but if you're going to write a skeptical article on it, the factual refutation should come before the mockery. The right to mock has to be earned, not stolen.
This is not the level of info that anyone who's read the above main article should be interested in.

I have never claimed credit for either phrase, and fully support all efforts to see them attributed appropriately.




To be really clear, the problem with Pascal's Mugging is that even after eliminating infinity as a coherent scenario, any simplicity prior which defines simplicity strictly over computational complexity will apparently yield divergent returns for aggregative utility functions when summed over all probable scenarios, because the material size of possible scenarios grows much faster than their computational complexity (Busy Beaver function or just tetration).

That seems overly specific. There are many other ways in which priors assigned to highly speculative propositions may not be low enough. edit: or when impact of other available actions on a highly speculative scenario be under-evaluated.
To me, Pascal's Wager is defined by a speculative scenario for which there exist no evidence, which has high enough impact to result in actions which are not based on any evidence, despite the uncertainty towards speculative scenarios.

Pascal's Mugging != Pascal's Wager. This is really clear in the grandparent which explicitly distinguishes them, so I'm interpreting the above as willful misinterpretation from a known troller and deleting it.

(Reply to edit: In the presentation that 30% is one probability in a chain, not an absolute value. Stop with the willful misrepresentations, please.)
From the article:

However, Pascal realizes that the value of 1/2 actually plays no real role in the argument, thanks to (2). This brings us to the third, and by far the most important, of his arguments...

If there were a 0.5 probability that the Christian God existed, the wager would make a fuckton more sense. Today we think Pascal's Wager is a logical fallacy rather than a mere mistaken probability estimate only because later versions of the argument were put forward for lower probabilities, and/or because Pascal went on to argeu that it would carry for lower probabilities.
If the video is where is the actual instance of Pascal's Wager is being offered in support of SIAI, then it would have been better to link it directly. I also hate video because it's not searchable, but I can hardly blame you for that, so I will try scanning it.
Before scanning, I precommit to renouncing, abjuring, and distancing MIRI from the argument in the video if it argues for no probability higher than 1 in 2000 of FAI saving the world, because I myself do not positively engage in long-term projects on the basis of probabilities that low (though I sometimes avoid doing things for dangers that small). There ought to be at least one x-risk effort with a greater probability of saving the world than this - or if not, you ought to make one. If you know yourself for an NPC and that you cannot start such a project yourself, you ought to throw money at anyone launching a new project whose probability of saving the world is not known to be this small. 7 billion is also a stupidly low number - x-risk dominates all other optimal philanthropy because of the value of future galaxies, not because of the value of present-day lives. The confluence of these two numbers makes me strongly suspect that, if they are not misquotes in some sense, both low numbers were (presumably unconsciously) chosen to make the 'lives saved per dollar' look like a reasonable number in human terms, when in fact the x-risk calculus is such that all utilons should be measured in Probability of OK Outcome because the value of future galaxies stomps everything else.
Attempts to argue for large probabilities that FAI is important, and then tiny probabilities that MIRI is instrumental in creating FAI, will also strike me as a wrongheaded attempt at modesty. On a very large scale, if you think FAI stands a serious chance of saving the world, then humanity should dump a bunch of effort into it, and if nobody's dumping effort into it then you should dump more effort than currently into it. Calculations of marginal impact in POKO/dollar are sensible for comparing two x-risk mitigation efforts in demand of money, but in this case each marginal added dollar is rightly going to account for a very tiny slice of probability, and this is not Pascal's Wager. Large efforts with a success-or-failure criterion are rightly, justly, and unavoidably going to end up with small marginal probabilities per added unit effort. It would only be Pascal's Wager if the whole route-to-humanity-being-OK were assigned a tiny probability, and then a large payoff used to shut down further discussion of whether the next unit of effort should go there or to a different x-risk.
(Scans video.)
This video is primarily about value of information estimates.
"Principle 2: Don't trust your estimates too much. Estimates in, estimates out." Good.
Application to the Singularity... It's explicitly stated that the value is 7 billion lives plus all future generations, which is better - a lower bound is being set, not an estimated exact value.
Final calculation shown:

Probability of eventual AI: 80%
Probability AI with no safeguards will kill us: 80%

(Both of these numbers strike me as a bit suspicious in their apparent medianness which is something that often happens when an argument is unconsciously optimized for sounding reasonable. Really, the probability that AI happens at all, ever, is 80%? Isn't that a bit low? Is this supposed to be factoring in the probability of nanotechnological warfare wiping out humanity before then, or something? Certainly, AI being possible in principle should have a much more extreme probability than 80%. And a 20% probability of an unsafed AI not killing you sounds like quite an amazing bonanza to get for free. But carrying on...)

Probability we manage safeguards: 40%

(No comment.)

Probability current work is why we manage: 30%

(Arguably too low. Even if MIRI crashes and somebody else carries on successfully, I'd estimate a pretty high probability that their causal pathway there will have had something to do with MIRI. It is difficult to overstate just how much this problem was not on the horizon, at all, of work anyone could actually go out and do twenty years ago.)

Net probabilty: 7%.

This is not necessarily a result I'd agree with, but it's not a case of Pascal's Wager on its face. 7% probabilities of large payoffs are a reasonable cause of positive action in sane people; it's why you would do an Internet startup.
(continues scanning video)
I do not see any slide showing a probability of 1 in 2000. Was this spoken aloud? At what time in the episode?

This is nothing compared to the impossibility of Time-Turners given MWI, which is of course a given. I've been assuming that HPMOR runs on collapse QM.

To be really clear, the problem with Pascal's Mugging is that even after eliminating infinity as a coherent scenario, any simplicity prior which defines simplicity strictly over computational complexity will apparently yield divergent returns for aggregative utility functions when summed over all probable scenarios, because the material size of possible scenarios grows much faster than their computational complexity (Busy Beaver function or just tetration).
The problem with Pascal's Wager on the other hand is that it shuts down an ongoing conversation about plausibility by claiming that it doesn't matter how small the probability is, thus averting a logically polite duty to provide evidence and engage with counterarguments.

PASCAL'S WAGER IS DEFINED BY LOW PROBABILITIES NOT BY LARGE PAYOFFS
PASCAL'S WAGER IS DEFINED BY LOW PROBABILITIES NOT BY LARGE PAYOFFS
PASCAL'S WAGER IS DEFINED BY LOW PROBABILITIES NOT BY LARGE PAYOFFS
I've tried saying this in small letters a number of times, and once in the main post The Pascal's Wager Fallacy Fallacy, and people apparently just haven't paid attention, so I'm just going to try shouting it over and over every time somebody makes the same mistake over and over.

PASCAL'S WAGER IS DEFINED BY LOW PROBABILITIES NOT BY LARGE PAYOFFS
PASCAL'S WAGER IS DEFINED BY LOW PROBABILITIES NOT BY LARGE PAYOFFS
PASCAL'S WAGER IS DEFINED BY LOW PROBABILITIES NOT BY LARGE PAYOFFS

There's quite a number of HPMOR readers who've never read HP. Admittedly this may be a special case, and it's not HPMOR's original intended optimal use-case either (reading Philosopher's Stone first is a good idea if you can).

I bet that partner thing was kind of important.

Mormons have lots of friends, and lots of relatives.

IIRC the standard experimental result is that atheists who were raised religious have substantially above-average knowledge of their former religions. I am also suspicious that any recounting whatsoever of what went wrong will be greeted by, "But that's not exactly what the most sophisticated theologians say, even if it's what you remember perfectly well being taught in school!"
This obviously won't be true in my own case since Orthodox Jews who stay Orthodox will put huge amounts of cumulative effort into learning their religion's game manual over time. But by the same logic, I'm pretty sure I'm talking about a very standard element of the religion when I talk about later religious authorities being presumed to have immensely less theological knowledge than earlier authorities and hence no ability to declare earlier authorities wrong. As ever, you do not need a doctorate in invisible sky wizard to conclude that there is no invisible sky wizard, and you also don't need to know all the sophisticated excuses for why the invisible sky wizard you were told about is not exactly what the most sophisticated dupes believe they believe in (even as they go on telling children about the interventionist superparent). It'd be nice to have a standard, careful and correct explanation of why this is a valid attitude and what distinguishes it from the attitude of an adolescent who finds out everything they were told about quantum mechanics is wrong, besides the obvious distinction of net weight of experimental evidence (though really that's just enough).
LW has reportedly been key in deconverting many, many formerly religious readers. Others will of course have fled. It takes all kinds of paths.

Welcome to LW! Don't worry about some of the replies you're getting, polls show we're overwhelmingly atheist around here.

Holy Belldandy, it sounds like someone located the player character. Everyone get your quests ready!

Moved to Discussion.

Chicken-and-egg problem: Non-economics majors don't think economically enough to choose fields on the basis of their remuneration?

Hello! I call myself Atomliner. I'm a 23 year old male Political Science major at Utah Valley University.
From 2009 to 2011, I was a missionary for the Mormon Church in northeastern Brazil. In the last month I was there, I was living with another missionary who I discovered to be a closet atheist. In trying to help him rediscover his faith, he had me read The God Delusion, which obliterated my own. I can't say that book was the only thing that enabled me to leave behind my irrational worldview, as I've always been very intellectually curious and resistant to authority. My mind had already been a powder keg long before Richard Dawkins arrived with the spark to light it.
Needless to say, I quickly embraced atheism and began to read everything I could about living without belief in God. I'm playing catch-up, trying to expand my mind as fast as I can to make up for the lost years I spent blinded by religious dogma. Just two years ago, for example, I believed homosexuality was an evil that threatened to destroy civilization, that humans came from another planet, and that the Lost Ten Tribes were living somewhere underground beneath the Arctic. Needless to say, my re-education process has been exhausting.
One ex-Mormon friend of mine introduced me to Harry Potter and the Methods of Rationality, which I read only a few chapters of, but I was intrigued by the concept of Bayes Theorem and followed a link here. Since then I've read From Skepticism to Technical Rationality and many of the Sequences. I'm hooked! I'm really liking what I find here. While I may not be a rationalist now, I would really like to be.
And that's my short story! I look forward to learning more from all of you and, hopefully, contributing in the future. :)

Upvoted ones, usually.

I have been known to do that as well.

I like TL;DR. It reminds the author of the basic writing principle that nobody wants to read anything you write, they're doing you a favor by reading the first two sentences and if you're still saying boring things by the third sentence that's it. Writing is terrible in proportion to how its circumstances cause writers to ignore this principle, for example school textbooks that children are forced to read, or journal papers that adults are either forced to read or that aren't being written for the sake of producing understanding in anyone.


The specific project I was evaluating had only gotten $800,000 out of the maximum $2m. Its strategy was to purchase the male students iPod Touches, the female students makeovers, manicures, and pedicures at a local beauty parlor, and all students were offered an additional iPod Touch or Makeover, respectively, if they passed the exam at the end of the current year. The grant proposal had specifically listed these actions as being the goal of the proposal. If the iPods and makeovers were purchased, that constituted success.

If true and documentable, I think there's a large section of the Internet which would be very, very interested and very, very loud about this because the males got iPods and the females got makeovers. (And justly so.)

(Requested reply.)
I think there'd be a wide variety of systems where, so long as the "parent" agent knows the exact strategy that its child will deploy in all relevant situations at "compile time", the parent will trust the child. The point of the Lob problem is that it arises when we want the parent to trust the child generally, without knowing exactly what the child will do. For the parent to precompute the child's exact actions implies that the child can't be smarter than the parent, so it's not the kind of situation we would encounter when e.g. Agent A wants to build Agent B which has more RAM and faster CPUs than Agent A while still sharing Agent A's goals. This, of course, is the kind of "agents building agents" scenario that I am most interested in.

During the April 2013 workshop I rephrased this as the principle "The actions and sensor values of the offspring should not appear outside of quantifiers". Justification: If we have to reason case-by-case about all possible actions, all possible sensor values, and all possibles states of the world, our child's size must be less than or equal to "the number of cases we can consider" / "child's sensor state space" x "child's action state space" x "world state space" which in general implies a logarithmically smaller child. I call this the Vingean Principle.

And so I put it back in Discussion, of course.

Well, yeah. I mean I moved it to Discussion once, it was deliberately moved back to Main, I moved it to Discussion again, then it was moved back to Main again, so I hit 'Ban'. Er, duh?

Okay... if it wasn't you who moved it back to Main a second time, I'll move to Discussion and undelete.

...and then adjusted our senses of the 'incredible' accordingly, so that Special Relativity seemed less incredible, and God more so.

Whoops, forgot to promote this.


I just came back and found my post had been moved back to Main. I began answering comments on it, and then suddenly it went away. It appears to have been deleted. My post, and all comments on it, are gone, without anyone contacting me about it.

Wow. This is a serious problem. Moving to discussion is one thing. But full deletion for what seems to be no particular reason is... not what I usually expect here.
I am taking from this comment that it wasn't you (Phil) who moved the comment back to main?
EDIT: Ahh, I see you have answered. A bug ('quirk') of the post edit page caused an accidental move which was interpreted as insubordination and punished in retaliation.

There aren't strict guidelines, but if something isn't much upvoted and/or doesn't seem very important, I'll move it to Discussion. Trying to post to Main is not a crime. On the other hand, moving things back from Discussion to Main after an editor moves them is a crime.

I assume Phil reposted it there. Now banning.

As far as I know, I'm the one who does this. I can try to leave comments, though it's not clear what benefit is thereby gained under those circumstances. I will not always have time to explain.

Moved to Discussion. (Again.)

Beware. Don't look at any anime music videos if you don't need another addiction.

I affirm wedrifid's instruction to change your posting style or leave LW.

Note that the Born probabilities really obviously have something to do with the unitarity of QM, while no single-world interpretation is going to have this be anything but a random contingent fact. The unitarity of QM means that integral-squared-modulus quantifies the "amount of causal potency" or "amount of causal fluid" or "amount of conserved real stuff" in a blob of the wavefunction. It would be like discovering that your probability of ending up in a computer corresponded to how large the computer was. You could imagine that God arbitrarily looked over the universe and destroyed all but one computer with probability proportional to its size, but this would be unlikely. It would be much more likely (under circumstances analogous to ours) to guess that the size of the computer had something to do with the amount of person in it.
The problems with Copenhagen are fundamentally one-world problems and they go along with any one-world theory. If I honestly believed that the only reason the QM sequence wasn't convincing was that I didn't go through every single one-world theory to refute them separately, I could try to write separate posts for RQM, Bohm, and so on, but I'm not convinced that this is the case. Any single-world theory needs either spooky action at a distance, or really awful amateur epistemology plus spooky action at a distance, and there's just no reason to even hypothesize single-world theories in the first place.
(I'm not sure I have time to write the post about Relational Special Relativity in which length and time just aren't the same for all observers and so we don't have to suppose that Minkowskian spacetime is objectively real, and anyway the purpose of a theory is to tell us how long things are so there's no point in a theory which doesn't say that, and those silly Minkowskians can't explain how much subjective time things seem to take except by waving their hands about how the brain contains some sort of hypothetical computer in which computing elements complete cycles in Minkowskian intervals, in contrast to the proper ether theory in which the amount of conscious time that passes clearly corresponds to the Lorentzian rule for how much time is real relative to a given vantage point...)


No, that set of posts goes on at some length about how MWI has not yet provided a good derivation of the Born probabilities.

But I think it does not do justice to what a huge deal the Born probabilities are. The Born probabilities are the way we use quantum mechanics to make predictions, so saying "MWI has not yet provided a good derivation of the Born probabilities" is equivalent to "MWI does not yet make accurate predictions," I'm not sure thats clear to people who read the sequences but don't use quantum mechanics regularly.
Also, by omitting the wide variety of non-Copenhagen interpretations (consistent histories, transactional, Bohm, stochastic-modifications to Schroedinger,etc) the reader is lead to believe that the alternative to Copenhagen-collapse is many worlds, so they won't use the absence of Born probabilities in many worlds to update towards one of the many non-Copenhagen alternatives.

I... don't think the fate of the world depends on this, so am reluctant to advertise in the PR...

Well, I'm sorry to say this, but part of what makes authority Authority is that your respect is not always required. Frankly, in this case Authority is going to start deleting your comments if you keep on telling newcomers who post in the Welcome thread not to read the QM sequence, which you've done quite a few times at this point unless my memory is failing me. You disagree with MWI. Okay. I get it. We all get it. I still want the next Mihaly to read the QM Sequence and I don't want to have this conversation every time, nor is it an appropriate greeting for every newcomer.

Yep. I also tend to ignore nontechnical folks along the lines of RationalWiki getting offended by my thinking that I know something they don't about MWI. Carl often hears about, anonymizes, and warns me when technical folks outside the community are offended by something I do. I can't recall hearing any warnings from Carl about the QM sequence offending technical people.
Bluntly, if shminux can't grasp the technical argument for MWI then I wouldn't expect him to understand what really high-class technical people might think of the QM sequence. Mihaly said the rest of the Sequences seemed interesting but lacked sufficient visible I-wouldn't-have-thought-of-that nature. This is very plausible to me - after all, the Sequences do indeed seem to me like the sort of thing somebody might just think up. I'm just kind of surprised the QM part worked, and it's possible that might be due to Mihaly having already taken standard QM so that he could clearly see the contrast between the explanation he got in college and the explanation on LW. It's a pity I'll probably never have time to write up TDT.

My heart is warmed. Yay!

Bad advice for technical readers. Mihaly Barasz (IMO gold medalist) got here via HPMOR but only became seriously interested in working for MIRI after reading the QM sequence.
Given those particular circumstances, can I ask that you stop with that particular bit of helpful advice?

After numerous previous failures, if it's that complicated I'm not going to bother. Complicated things seem even less likely to work than simple things, and simple things almost never work in the first place.
In my experience, no matter what you try, there's always an excuse when it doesn't work. Then when it still doesn't work there's something else you're not doing exactly right that they forgot to mention earlier. Oddly enough, when something does work for someone, nobody bothers to check to see if they were doing everything exactly right by way of confirming that all these extra frills are actually required as opposed to just being excuses that are only invoked when it doesn't work because, in reality, metabolisms are different.
Anyway, not interested. Thanks for trying.

Added.

I use "who" for the subject form or when "whom" sounds awful.

It might be worth going to a sleep doctor; sleep apnea can really fuck up your metabolism, not to mention causing unbelievable akrasia. I would say sleep tests are a GOOD THING, something everyone should do. I had sleep apnea for years. It was like some eldritch monster was sucking away my willpower and I wasn't even aware. Within a few months of getting my mouth guard, which keeps my tongue from blocking my airway while in REM, I lost thirty pounds and gained an enormous well of mental stamina. A small minority of the "metabolically challenged" may just have undiagnosed sleep problems.

Since it was cheaper than a sleep study, I bought a self-adjusting CPAP on Craigslist and just tried it. Nothing miraculous occurred.

I am not a metabolic mutant. There are plenty of people in the world who cannot seem to lose weight, and they aren't all weak-willed scum, and it's not because they just haven't tried your favorite diet.
The world is full of metabolic diversity. The fortunate who do not appreciate this are the metabolically privileged. That they can lose weight with an effort causes them to be unfortunately deluded about what is going on.

I've never heard of it but my intuition says it doesn't sound very promising - any correspondence definition of "truth" that changes over time should reduce to a timeless time-indexed truth predicate and I don't see how that would help much, unless the truth predicate couldn't talk about the index.

Ketosis sticks did not show my entering ketosis even with as close to zero carbs as I could get (admittedly counting things like 3g carbs in a serving of protein powder). I don't recall how long I tried. Probably between 1 week and 2 weeks before giving up on almost-zero carb, then a month of very low carb before giving up entirely. Memory is fuzzy.

In fact you should be able to write an infinitely long novel about it.

Hasn't worked for me.

It's just the standard first question I ask for any theorem in this class. See my reply to Nisan below about some theorems by Stuart Armstrong.

Stuart Armstrong has proved some theorems showing that it's really really hard to get to the Pareto frontier unless you're adding utility functions in some sense, with the big issue being the choice of scaling factor. I'm not sure even so, on a moral level - in terms of what I actually want - that I quite buy Armstrong's theorems taken at face value, but on the other hand it's hard to see how, if you had a solution that wasn't on the Pareto frontier, agents would object to moving to the Pareto frontier so long as they didn't get shafted somehow.
It occurred to me (and I suggested to Armstrong) that I wouldn't want to trade off whole stars turned into paperclips against individual small sentients on an even basis when dividing the gains from trade, even if I came out ahead on net against the prior state of the universe before the trade. I.e., if we were executing a gainful trade and the question was how to split the spoils, and some calculation took which ended up with the paperclip maximizer gaining a whole star's worth of paperclips from the spoils every time I gain one small-sized eudaimonic sentient, then my primate fairness calculator wants to tell the maximizer to eff off and screw the trade. I suggested to Armstrong that the critical scaling factor might revolve around equal amounts of matter affected by the trade, and you can also see how something like that might emerge if you were conducting an auction between many superintelligences (they would purchase matter affected where it was cheapest). Possibilities like this tend not to be considered in such theorems, and when you ask which axiom they violate it's often an axiom that turns out to not be super morally appealing.
Irrelevant alternatives is a common hinge on which such theorems fail when you try to do morally sensible-seeming things with them. One of the intuition pumps I use for this class of problem is to imagine an auction system in which all decision systems get to spend the same amount of money (hence no utility monsters). It is not obvious that you should morally have to pay the money only to make alternatives happen, and not to prevent alternatives that might otherwise be chosen. But then the elimination of an alternative not output by the system can, and morally should, affect how much money someone must pay to prevent it from being output.

Suppose everyone had a utility function and we just added the utility functions, ignoring scaling. What allegedly goes wrong with this? And why do I care about it?


On the flip side, I believe that a lot of readers believe that "Pascal's Mugging" type arguments are sufficient to establish that a particular giving opportunity is outstanding

Who? I'm against Pascal's Mugging. I invented that term to illustrate something that I thought was a fallacy. I'm pretty sure a supermajority of LW would not pay Pascal's Mugger. I'm on the record as saying that x-risk folk should not argue from low probabilities of large impacts, (1) because there are at least medium-probability interventions against xrisk and these will knock any low-probability interventions off the table if the money used for them is genuinely fungible (admittedly people who donate to anti-asteroid efforts cannot be persuaded to just donate to FAI instead), and (2), with (1) established, that it's logically rude and bad rationalist form to argue that a probability can be arbitrarily tiny because it makes you insensitive to the state of reality. I can reasonably claim to have personally advanced the art of further refuting Pascal's Mugging. Who are these mysterious hosts of silly people who believe in Pascal's Mugging, and what are they doing here of all places?

The LW memeplex may be somewhat too ready to buy into the hypothesis that a given group of people is insane. People do generally respond to incentives, and situations where there are large incentives that people aren't responding to are probably worth an explanation more descriptive than generic insanity.
Given what I understand to be the dominant stereotypes about American cars, though, I do think it's plausible that American car manufacturers are insane. I don't know about others.

Incentives? Which specific person gets fired because people are complaining about car cupholders? If the answer is "I can't point to anyone like that" then nobody has an incentive to fix car cupholders.

I find it perfectly plausible that all the car manufacturers are insane.

"I've lived the lives of all the characters in all my books, and all their mighty wisdom thunders in my head."

Ah. I didn't quite understand what you were trying to do there.
In general in this theory, I don't think we have (p(phi) < 1 -> p(phi) =1) -> p(phi) = 1. The theory only reflects on itself to within epsilon, so what the theory would conclude if it were certain about p(phi) being less than 1 can't be taken as the premise of a proof-by-contradiction in the same way that ~phi can be taken as a premise... ergh, I'm trying to figure out a more helpful way to state this than "I don't think you can derive that kind of probabilistic proof by contradiction from the stated properties of the theory because the system is never supposed to know the exact probabilities it assigns".
EDIT: Retracted pending further cogitation.


Using diagonalisation, define G ? -1<P('G')<1
Then P(G)<1

I think this should be "Then P(G) <= 1".

Promise? No. Might it start down an avenue that someday leads there after a whole lot more work and some additional ideas? Maybe. Don't hold your breath on that P!=NP formal probability calculation.

The problem is if your mathematical power has to go down each time you create a successor or equivalently self-modify. If PA could prove itself sound that might well be enough for many purposes. The problem is if you need a system that proves another system sound and in this case the system strength has to be stepped down each time. That is the Lob obstacle.

We tried closed intervals a fair bit, no luck so far.

No, I allocated my energy for a week and then it turned out that everyone was in town for an extra day anyway so it went into overtime. I thought Mihaly made it? Anyway I, having judged my energy reserves accurately, didn't.

Do we know about any such proteins related to LTM? Can we make predictions about what it takes to erase C. elegans maze memory this way?

Well. I mean, it's quantum. But the ground state is a lump of iron, or maybe a black hole, not a low-energy soap film, so I don't think waiting for quantum annealing will help.

waves soap-covered wire so it settles into low-energy minimum
dies as it turns into iron

Eventually only the most rational visitor is left alive.

I'm not totally sure I'd call this sufficient evidence since functional damage != many-to-one mapping but it would shave some points off the probability for existing tech and be a pointer to look for the exact mode of functional memory loss.

Heh. I remember that one, and thinking, "No... no, you can't possibly do that using a soap bubble, that's not even quantum and you can't do that in classical, how would the soap molecules know where to move?"

On the basis of this comment, I have recognized this user as a possible troll and may delete comments from them (downvoted or otherwise) which seem to be attention-seeking.
[I.e. our local equivalent of "User was banned for this comment."]

Anders publishes that, CI announces they intend to go on vitrifying patients anyway, Alcor offers a chop-off-your-head-and-dunk-in-liquid-nitro solution. Not super plausible but it's off the top of my head.

Kawoomba, there is no known case of any NP-hard or NP-complete solution which physics finds.
In the case of proteins, if finding the lowest energy minimum of an arbitrary protein is NP-hard, then what this means in practice is that some proteins will fold up into non-lowest-energy configurations. There is no known case of a quantum process which finds an NP-hard solution to anything, including an energy minimum; on our present understanding of complexity theory and quantum mechanics 'quantum solvable' is still looking like a smaller class than 'NP solvable'. Read Scott Aaronson for more.

No. Not 1. It would be front-page news all over the universe if it were 1.

That too. Even NP-hard problems are often easy if you get the choice of which one to solve.

Protein folding models must be inaccurate if they are NP-hard. Reality itself is not known to be able to solve NP-hard problems.

Do you have a page number in Nanosystems for a references to a sensing probe? Also, this is tangential to the main discussion, so I'll take pointers to any reference you have and let this drop.

Don't ask whether ethylene glycol is toxic, ask whether it is a secure hard drive erasure mechanism that can obscure the contents of the brain from a powerful and intelligent adversary reading off the exact molecular positions in order to obtain tiny hints.

I was using cytotoxic in the very specific sense of "interacts and destabilizes the cell membrane," which is doing the sort of operations we agreed in principle can be irreversible. Estimates as to how important this sort of information actually is are impossible for me to make, as I lack the background. What I would love to see is someone with some domain specific knowledge explaining why this isn't an issue.


I was using cytotoxic in the very specific sense of "interacts and destabilizes the cell membrane," which is doing the sort of operations we agreed in principle can be irreversible.

Sorry, but can you again expand on this? What happens?

Anders Sandberg who does get the concept of sufficiently advanced technology posts saying, "Shit, turns out LTM seems to depend really heavily on whether protein blah has conformation A and B and the vitrification solution denatures it to C and it's spatially isolated so there's no way we're getting the info back, it's possible something unknown embodies redundant information but this seems really ubiquitous and basic so the default assumption is that everyone vitrified is dead". Although, hm, in this case I'd just be like, "Okay, back to chopping off the head and dropping it in a bucket of liquid nitrogen, don't use that particular vitrification solution". I can't think offhand of a simple discovery which would imply literally giving up on cryonics in the sense of "Just give up you can't figure out how to freeze people ever." I can certainly think of bad news for particular techniques, though.

I'll quickly point you at Drexler's Nanosystems and Freitas's Nanomedicine though they're rather long and technical reads. But we are visualizing molecularly specified machines, and 'hell no' to thawing first or pulping the cell. Seriously, this kind of background assumption is why I have to ask a lot of questions instead of just taking this sort of skeptical intuition at face value.
But rather than having to read through either of those sources, I would ask you to just take on assumption that two molecularly distinct (up to thermal noise) configurations will somehow be distinguishable by sufficiently advanced technology, and describe what your intuitions (and reasons) would be taking that premise at face value. It's not your job to be a physicist or to try to describe the theoretical limits of future technology, except of course that two systems physically identical up to thermal noise can be assumed to be technologically indistinguishable, and since thermal noise is much larger than exact quark positions it will not be possible to read off any subtle neural info by looking at exact quark positions (now that might be permanently impossible), etc. Aside from that I would encourage you to think in terms of doing cryptography to a vitrified brain rather than medicine. Don't ask whether ethylene glycol is toxic, ask whether it is a secure hard drive erasure mechanism that can obscure the contents of the brain from a powerful and intelligent adversary reading off the exact molecular positions in order to obtain tiny hints.
Checking over the open letter from scientists in support of cryonics to remember who has an explicitly neuroscience background, I am reminded that good old Anders Sandberg is wearing a doctorate in computational neuroscience from Stockholm, so I'll go ahead and name him.

This never boded well in the first place.


This has some problems- fundamentally the length scale probed is inversely proportional to the energy required, which means increasing the resolution increases the damage done by scanning.

We seem to have very different assumptions here. I am assuming you can get up to the molecule and gently wave a tiny molecular probe in its direction, if required. I am not assuming that you are trying to use high-energy photons to photograph it.
You also still seem to be use a lot of functional-damage words like "destroying" which is why I don't trust your or kalla724's intuitions relative to the intuitions of other scientists with domain knowledge of neuroscience who use the language of information theory when assessing cryonic feasibility. If somebody is thinking in terms of functional damage (it doesn't restart when you reboot it, oh my gosh we changed the conformation look at that damage it can't play its functional role in the cell anymore!) then their intuitions don't bear very well on the real question of many-to-one mapping.
What does the vitrification solution actually do that's supposed to irreversibly map things, does anyone actually know? The fact that a cell can survive with a membrane, at all, considering the many different molecules inside it, imply that most molecules don't functionally damage most other molecules most of the time, never mind performing irreversible mappings on them. But then this is reasoning over molecules that may be of a different type then vitrificants. At the opposite extreme, I'd expect introducing hydrochloric acid into the brain to be quite destructive.

(Scanning at significantly smaller scales should always be assumed to be fine as long as end states are distinguishable up to thermal noise!)

So what I'm thinking about is something like this: imagine an enzyme,present at two sites on the membrane and regulated by an inhibitor. Now a toxin comes along and breaks the weak bonds to the inhibitor, stripping them off. Information about which site was inhibited is gone.

Okay, I agree that if this takes place at a temperature where molecules are still diffusing at a rapid pace and there's no molecular sign of the broken bond at the bonding site, then it sounds like info could be permanently destroyed in this way. Now why would you think this was likely with vitrification solutions currently used? Is there an intuition here about ranges of chemical interaction so wide that many interactions are likely to occur which break such bonds and at least one such interaction is likely to destroy functionally critical non-duplicated info? If so, should we toss out vitrification and go back to dropping the head in liquid nitrogen because shear damage from ice freezing will produce fewer many-to-one mappings than introducing a foreign chemical into the brain? I express some surprise because if destructive chemical interactions were that common with each new chemical introduced then the problem of having a whole cell not self-destruct should be computationally unsolvable for natural selection, unless the chemicals used in vitrification are unusually bad somehow.

To presume that states non-identical up to thermal noise are indistinguishable seems to presume either lower technology than the sort of thing I have in mind, or that you know something I don't about how two physical states can be non-identical up to thermal noise and yet indistinguishable.


This information is irretrievable once the damage is done.

How do you know? I'm not asking for some burden of infinite proof where you have to prove that the info can't be stored elsewhere. I am asking whether you know that widely functionally different start states are being mapped onto an overlapping spread of molecularly identical end states, and if so, how. E.g., "denaturing either conformation A or conformation B will both result in denatured conformation C and the A-vs.-B distinction is just a little twist of this spatially isolated thingy here so you wouldn't expect it to be echoed in any exact nearby positions of blah" or something.

A quite obvious possibility is that would-be debunkers who want try to go deeper than Penn and Teller style mockery soon realize that they would have to engage much more seriously with cryonics than with Urantia to try to debunk it - sound like they were taking it seriously - implying a far greater loss of status than soaring casually above Urantia, effortlessly trashing it without a hint of sympathy.
"Everyone who's tried to 'debunk' this seems to have ended up writing casual mockery, and oddly enough no would-be skeptics ever seem to engage the arguments in technical detail, and the arguments are being made by people who sure look like they're trying to wear technical hats and include a number of otherwise highly technical figures" seems to me like a quite common position when both of these aspects are combined. There are arguments that skeptics don't bother engaging in detail, but they're not technical. There are physicists who believe crazy things because they're bad outside the laboratory, but then they are usually refuted by more than mockery. I may be prejudiced by being mostly interested only in things that are sensible to start with, but the overall state of affairs I have just described is pretty much what you'd expect a correct but weird-sounding idea to look like.

I will quickly remark that some aspects of this comment seem to betray a non-info-theoretic point of view. From the perspective of someone like me, the key question for cryonics are "Do two functionally different start states (two different people) map onto theoretically indistinguishable molecular end states?" You are not an expert on the future possibilities of molecular nanotechnology and will not be asked to testify as such, but of course we all accept that arbitrarily great physical power cannot reconstruct a canister of ash because the cremation process maps many different possible starting people to widely overlapping possible canisters of ash. It is this question of many-to-one mapping alone on which we are interested in your expertise, and I would ask you to please presume for the sake of discussion that the end states of interest will be distinguished to molecular granularity (albeit obviously not to a finer position than thermal noise, let alone quantum uncertainty).
That said, I think we will all be interested if you can expand on

many aspects of synaptic strength and connectivity are irretrievably lost as soon as the synaptic membrane gets distorted

and whether you mean this in the customary sense of "it won't boot back up when you switch it on" or in the info-theoretic sense of "this process will map functionally different synapses to exactly similar molecular states, or a spread of such states, up to thermal noise". You are not being asked to overcome a burden of infinite proof either - heuristic argument is fine, we're not asking for particular proofs you can't possibly provide - we just want to make sure that what is being argued is the precise question we are interested in, that of many-to-one mappings onto molecular end states up to thermal noise.
EDIT: Oops, didn't realize this was an old comment.

I actually am signed up for cryonics.
My issue with the basic tech is that liquid nitrogen, while a cheap storage method, is too cold to avoid fracturing. Experience with imaging systems leads me to believe that fractures will interfere with reconstructions of the brain's geometry, and cryoprotectants obviously destroy chemical information.
Now, it seems likely to me that at some point in the future the fracturing problem can be solved, or at least mitigated, by intermediate temperature storing and careful cooling processes, but that won't fix the bodies frozen today. So I don't doubt that (barring large neuroscience related, unquantifiable uncertainty) cryonics may improve to the point where the tech is likely to work (or be supplanted by plastination methods,etc), it is not there now, and what matters for people frozen today is the state of cryonics today.
Saying there are no fundamental scientific barriers to the tech working is not the same thing as saying the hard work of engineering has been done and the tech currently works.
Edit: I also have a weak prior that the chemical information in the brain is important, but it is weak.


Experience with imaging systems leads me to believe that fractures will interfere with reconstructions of the brain's geometry, and cryoprotectants obviously destroy chemical information.

Since this is the key point of neuroscience, do you want to expand on it? What experience with imaging leads you to believe that fractures (of incompletely vitrified cells) will implement many-to-one mappings of molecular start states onto molecular end states in a way that overlaps between functionally relevant brain states? What chemical information is obviously destroyed and is it a type that could plausibly play a role in long-term memory?

The boundaries between present-day people and non-people can be sharper, by a fiat of many intervening class members being nonexistent, than the ideal categories. In other words, except for chimpanzees, cryonics patients, Terry Schiavo, and babies who are exactly 1 year and 2 months and 5 days old, there isn't much that's ambiguous between person and non-person.
More to the point, a CEV-based AI has a potentially different definition of 'sentient being' and 'the class I am to extrapolate'. Theoretically you could be given the latter definition by pointing and not worry too much about boundary cases, and let it work out the former class by itself - if you were sure that the FAI would arrive at the correct answer without creating any sentients along the way!

Or /r/hpmor.

Far as I can tell, the basic tech in cryonics should basically work. Storage organizations are uncertain and so is the survival of the planet. But if we're told that the basic cryonics tech didn't work, we've learned some new fact of neuroscience unknown to present-day knowledge.
Don't assign vanishingly small probabilities to things just because they sound weird, or it sounds less likely to get funny looks if you can say that it's just a tiny chance. That is not how 'probability' works. Probabilities of basic cryonics tech working are questions of neuroscience, full stop; if you know the basic tech has a tiny probability of working, you must know something about current vitrification solutions or the operation of long-term memory which I do not.

I have quickly edited the title. Yuu, feel free to choose something better!

Ah. K. It does seem to me like "you can construct it as an Oracle and then turn it into an arbitrary Genie" sounds weaker than "denying the Orthogonality thesis means superintelligences cannot know 1, 2, and 3." The sort of person who denies OT is liable to deny Oracle construction because the Oracle itself would be converted unto the true morality, but find it much more counterintuitive that an SI could not know something. Also we want to focus on the general shortness of the gap from epistemic knowledge to a working agent.

True, that was a strange word. I may have been spending too much time thinking about large numbers lately. My point is that it's not literally unreachable the way a Levin-prior penalty on running speed makes quantum mechanics (in all forms) absolutely implausible relative to any amount of evidence you can possibly collect, or the Hansonian penalty makes ever being in a position to influence 3^^^3 future lives "absolutely implausible" relative to any amount of info you can collect in less than log(3^^^3) time, given that your sensory bandwidth is on the order of a few megabits per second.
As soon as you start trying to be "reasonable" or "skeptical" or "outside view" or whatever about the likelihood ratios involved in the evidence, obviously 10^-50 instantly goes to an eternally unreachable prior penalty since after all over the course of the human species people have completely hallucinated more unlikely things due to insanity on far fewer than 10^50 tries, etcetera. That's part of what I was trying to get at with (2). But if you're saying that, then it's also quite probable that the Hansonian adjustment is inappropriate or that you otherwise screwed up the calculation of 10^-50 prior probability, and that it is actually more. It is sometimes useful to be clever about adjustments, it is sometimes useful to at least look at the unadjusted utilities to see what the sheer numbers would say if taken at face value, and it is never useful to be clever about adjusting only one side of the equation while taking the other at face value.

You should only do things that increase your simulation measure after receiving good personal news or when you are unusually happy, obviously.

Where consistent (i.e. exponential) time discounting is concerned, there is very little intermediate ground between "nothing is important if it happens in 1,000,000 years" and "it is exactly as important as the present day".

If you believe in a prior, you believe in probability, right?

Some really fast comments on the Pascal's Mugging part:
1) For ordinary x-risk scenarios, the Hansonian inverse-impact adjustment for "you're unlikely to have a large impact" is within conceivable reach of the evidence - if the scenario has you affecting 10^50 lives in a future civilization, that's just 166 bits of evidence required.
2) Of course, if you're going to take a prior of 10^-50 at face value, you had better not start spouting deep wisdom about expert overconfidence when it comes to interpreting the likelihood ratios - only invoking "expert overconfidence" on one kind of extreme probability really is a recipe for being completely oblivious to the facts.
3) The Hansonian adjustment starts out by adding up to expected value ratios around 1 - it says that based on your priors, all scenarios that put you in a unique position to affect different large numbers of people in the same per-person way will have around the same expected value. Evidence then modifies this. If Pascal's Mugger shows you evidence with a million-to-one Bayesian likelihood ratio favoring the scenario where they're a Matrix Lord who has put you in a situation to affect 3^^^3 lives, the upshot is that you treat your actions as having the power to affect a million lives. It's exactly the same if they say 4^^^^4 lives are at stake. It's an interesting question as to whether this makes sense. I'm not sure it does.
4) But the way the Hansonian adjustment actually works out (the background theory that actually implements it in a case like this) is that after seeing medium amounts of evidence favoring the would-be x-risk charity, the most likely Hanson-adjusted hypothesis then becomes the non-Bayesian-disprovable scenario that rather than being in one of those amazingly unique pre-Singularity civilizations that can actually affect huge numbers of descendants, you're probably in an ancestor simulation instead; or rather, most copies of you are in ancestor simulations and your average impact is correspondingly diluted. Holden Karnofsky would probably not endorse this statement, and to be coherent should also reject the Hansonian adjustment.
5) The actual policy recommendation we get out of the Hansonian adjustment is not for people to be skeptical of the prima facie causal mechanics of existential risk reduction efforts. The policy recommendation we get is that you're probably in a simulation instead, whereupon UDT says that the correct utilitarian policy is for everyone to, without updating on the circumstances of their own existence, try to think through a priori what sort of ancestor simulations they would expect to exist and which parts of the simulation would be of most interest to the simulator (and hence simulated in the greatest detail with largest amount of computing power expended on simulating many slightly different variants), and then expend extra resources on policies that would, if implemented across both real and simulated worlds, make the most intensely simulated part of ancestor simulations pleasant for the people involved. A truly effective charity should spend money on nicer accommodations and higher-quality meals for decision theory conferences, or better yet, seek out people who have already led very happy lives and convince them to work on decision theory. Holden would probably not endorse this either.

(Agreed.)

Only in the sense that any working Oracle can be trivially transformed into a Genie. The argument doesn't say that it's difficult to construct a non-Genie Oracle and use it as an Oracle if that's what you want; the difficulty there is for other reasons.
Nick Bostrom takes Oracles seriously so I dust off the concept every year and take another look at it. It's been looking slightly more solvable lately, I'm not sure if it would be solvable enough even assuming the trend continued.

To slightly expand, if an intelligence is not prohibited from the following epistemic feats:
1) Be good at predicting which hypothetical actions would lead to how many paperclips, as a question of pure fact.
2) Be good at searching out possible plans which would lead to unusually high numbers of paperclips - answering the purely epistemic search question, "What sort of plan would lead to many paperclips existing, if someone followed it?"
3) Be good at predicting and searching out which possible minds would, if constructed, be good at (1), (2), and (3) as purely epistemic feats.
Then we can hook up this epistemic capability to a motor output and away it goes. You cannot defeat the Orthogonality Thesis without prohibiting superintelligences from accomplishing 1-3 as purely epistemic feats. They must be unable to know the answers to these questions of fact.


That means that we can define a subroutine within the paperclipper which is functionally isomorphic to that agent.

Not necessarily. x -> 0 is input-output isomorphic to Goodstein() without being causally isomorphic. There are such things as simplifications.

If the agent-to-be-modelled is experiencing pain and pleasure, then by the defendent's own rejection of the likely existence of p-zombies, so must that subroutine of the paperclipper!

Quite likely. A paperclipper has no reason to avoid sentient predictive routines via a nonperson predicate; that's only an FAI desideratum.

And you'll never understand why we should all only make paperclips. (Where's Clippy when you need him?)

...well, in point of fact he does seem to be having some trouble, but I don't think it's fundamental trouble.

"Aargh!" he said out loud in real life. David, are you disagreeing with me here or do you honestly not understand what I'm getting at?
The whole idea is that an agent can fully understand, model, predict, manipulate, and derive all relevant facts that could affect which actions lead to how many paperclips, regarding happiness, without having a pleasure-pain architecture. I don't have a paperclipping architecture but this doesn't stop me from modeling and understanding paperclipping architectures.
The paperclipper can model and predict an agent (you) that (a) operates on a pleasure-pain architecture and (b) has a self-model consisting of introspectively opaque elements which actually contain internally coded instructions for your brain to experience or want certain things (e.g. happiness). The paperclipper can fully understand how your workspace is modeling happiness and know exactly how much you would want happiness and why you write papers about the apparent ineffability of happiness, without being happy itself or at all sympathetic toward you. It will experience no future surprise on comprehending these things, because it already knows them. It doesn't have any object-level brain circuits that can carry out the introspectively opaque instructions-to-David's-brain that your own qualia encode, so it has never "experienced" what you "experience". You could somewhat arbitrarily define this as a lack of knowledge, in defiance of the usual correspondence theory of truth, and despite the usual idea that knowledge is being able to narrow down possible states of the universe. In which case, symmetrically under this odd definition, you will never be said to "know" what it feels like to be a sentient paperclip maximizer or you would yourself be compelled to make paperclips above all else, for that is the internal instruction of that quale.
But if you take knowledge in the powerful-intelligence-relevant sense where to accurately represent the universe is to narrow down its possible states under some correspondence theory of truth, and to well model is to be able to efficiently predict, then I am not barred from understanding how the paperclip maximizer works by virtue of not having any internal instructions which tell me to only make paperclips, and it's not barred by its lack of pleasure-pain architecture from fully representing and efficiently reasoning about the exact cognitive architecture which makes you want to be happy and write sentences about the ineffable compellingness of happiness. There is nothing left for it to understand. This is also the only sort of "knowledge" or "understanding" that would inevitably be implied by Bayesian updating. So inventing a more exotic definition of "knowledge" which requires having completely modified your entire cognitive architecture just so that you can natively and non-sandboxed-ly obey the introspectively-opaque brain-instructions aka qualia of another agent with completely different goals, is not the sort of predictive knowledge you get just by running a powerful self-improving agent trying to better manipulate the world. You can't say, "But it will surely discover..."
I know that when you imagine this it feels like the paperclipper doesn't truly know happiness, but that's because, as an act of imagination, you're imagining the paperclipper without that introspectively-opaque brain-instructing model-element that you model as happiness, the modeled memory of which is your model of what "knowing happiness" feels like. And because the actual content and interpretation of these brain-instructions are introspectively opaque to you, you can't imagine anything except the quale itself that you imagine to constitute understanding of the quale, just as you can't imagine any configuration of mere atoms that seem to add up to a quale within your mental workspace. That's why people write papers about the hard problem of consciousness in the first place.
Even if you don't believe my exact account of the details, someone ought to be able to imagine that something like this, as soon as you actually knew how things were made of parts and could fully diagram out exactly what was going on in your own mind when you talked about happiness, would be true - that you would be able to efficiently manipulate models of it and predict anything predictable, without having the same cognitive architecture yourself, because you could break it into pieces and model the pieces. And if you can't fully credit that, you at least shouldn't be confident that it doesn't work that way, when you know you don't know why happiness feels so ineffably compelling!


Would you propose that a mind lacking in motivation couldn't feel blissfully happy?

Here we're reaching the borders of my ability to be confident about my replies, but the two answers which occur to me are:
1) It's not positive reinforcement unless feeling it makes you experience at least some preference to do it again - otherwise in what sense are the neural networks getting their plus? Heroin may not induce desire while you're on it, but the thought of the bliss induces desire to take heroin again, once you're off the heroin.
2) The superBuddhist no longer capable of experiencing desire or choice, even desire or choice over which thoughts to think, also becomes incapable of experiencing happiness (perhaps its neural networks aren't even being reinforced to make certain thoughts more likely to be repeated). However, you, who are still capable of desire and who still have positively reinforcing thoughts, might be tricked into considering the superBuddhist's experience to be analogous to your own happiness and therefore acquire a desire to be a superBuddhist as a result of imagining one - mostly on account of having been told that it was representing a similar quale on account of representing a similar internal code for an experience, without realizing that the rest of the superBuddhist's mind now lacks the context your own mind brings to interpreting that internal coding into pleasurable positive reinforcement that would make you desire to repeat that experiential state.


Eliezer, in my view, we don't need to assume meta-ethical realism to recognise that it's irrational - both epistemically irrational and instrumentally irrational - arbitrarily to privilege a weak preference over a strong preference.

You need some stage at which a fact grabs control of a mind, regardless of any other properties of its construction, and causes its motor output to have a certain value.

Paperclippers? Perhaps let us consider the mechanism by which paperclips can take on supreme value. We understand, in principle at least, how to make paperclips seem intrinsically supremely valuable to biological minds - more valuable than the prospect of happiness in the abstract. ["Happiness is a very pretty thing to feel, but very dry to talk about." - Jeremy Bentham]. Experimentally, perhaps we might use imprinting (recall Lorenz and his goslings), microelectrodes implanted in the reward and punishment centres, behavioural conditioning and ideological indoctrination - and perhaps the promise of 72 virgins in the afterlife for the faithful paperclipper. The result: a fanatical paperclip fetishist!

As Sarokrae observes, this isn't the idea at all. We construct a paperclip maximizer by building an agent which has a good model of which actions lead to which world-states (obtained by a simplicity prior and Bayesian updating on sense data) and which always chooses consequentialistically the action which it expects to lead to the largest number of paperclips. It also makes self-modification choices by always choosing the action which leads to the greatest number of expected paperclips. That's all. It doesn't have any pleasure or pain, because it is a consequentialist agent rather than a policy-reinforcement agent. Generating compressed, efficient predictive models of organisms that do experience pleasure or pain, does not obligate it to modify its own architecture to experience pleasure or pain. It also doesn't care about some abstract quantity called "utility" which ought to obey logical meta-properties like "non-arbitrariness", so it doesn't need to believe that paperclips occupy a maximum of these meta-properties. It is not an expected utility maximizer. It is an expected paperclip maximizer. It just outputs the action which leads to the maximum number of expected paperclips. If it has a very powerful and accurate model of which actions lead to how many paperclips, it is a very powerful intelligence.
You cannot prohibit the expected paperclip maximizer from existing unless you can prohibit superintelligences from accurately calculating which actions lead to how many paperclips, and efficiently searching out plans that would in fact lead to great numbers of paperclips. If you can calculate that, you can hook up that calculation to a motor output and there you go.
Yes, this is a prospect of Lovecraftian horror. It is a major problem, kind of the big problem, that simple AI designs yield Lovecraftian horrors.


Pleasure would be good because it results in paperclips, not vice versa. If you reverse the electrodes so that they stimulate the pain centre when they find paperclips, and the pleasure centre when there are no paperclips, this being would start instrumentally value pain more than pleasure, because that's what results in more paperclips.

Minor correction: The mere post-factual correlation of pain to paperclips does not imply that more paperclips can be produced by causing more pain. You're talking about the scenario where each 1,000,000 screams produces 1 paperclip, in which case obviously pain has some value.


...microelectrodes implanted in the reward and punishment centres, behavioural conditioning and ideological indoctrination - and perhaps the promise of 72 virgins in the afterlife for the faithful paperclipper. The result: a fanatical paperclip fetishist!

Have to point out here that the above is emphatically not what Eliezer talks about when he says "maximise paperclips". Your examples above contain in themselves the actual, more intrisics values to which paperclips would be merely instrumental: feelings in your reward and punishment centres, virgins in the afterlife, and so on. You can re-wire the electrodes, or change the promise of what happens in the afterlife, and watch as the paperclip preference fades away.
What Eliezer is talking about is a being for whom "pleasure" and "pain" are not concepts. Paperclips ARE the reward. Lack of paperclips IS the punishment. Even if pleasure and pain are concepts, they are merely instrumental to obtaining more paperclips. Pleasure would be good because it results in paperclips, not vice versa. If you reverse the electrodes so that they stimulate the pain centre when they find paperclips, and the pleasure centre when there are no paperclips, this being would start instrumentally value pain more than pleasure, because that's what results in more paperclips.
It's a concept that's much more alien to our own minds than what you are imagining, and anthropomorphising it is rather more difficult!
Indeed, you touch upon this yourself:

"But unless I'm ontologically special (which I very much doubt!) the pain-pleasure axis discloses the world's inbuilt metric of (dis)value - and it's a prerequisite of finding anything (dis)valuable at all.

Can you explain why pleasure is a more natural value than paperclips?

Agreed. The conflict between the Superhappies and the Lord Pilot had nothing to do with different metaethical theories.
Also, we totally agree on wanting future civilization to contain very smart beings who are pretty happy most of the time. We just seem to disagree about whether it's important that they be super duper happy all of the time. The main relevance metaethics has to this is that once I understood there was no built-in axis of the universe to tell me that I as a good person ought to scale my intelligence as fast as possible so that I could be as happy as possible as soon as possible, I decided that I didn't really want to be super happy all the time, the way I'd always sort of accepted as a dutiful obligation while growing up reading David Pearce. Yes, it might be possible to do this in a way that would leave as much as possible of me intact, but why do it at all if that's not what I want?
There's also the important policy-relevant question of whether arbitrarily constructed AIs will make us super happy all the time or turn us into paperclips.

Right, this is the real core of Pascal's Mugging (I was somewhat surprised that Bostrom didn't put it into his mainstream writeup). For aggregative utility functions over a model of the environment which e.g. treat all sentient beings (or all paperclips) as having equal value without diminishing marginal returns, and all epistemic models which induce simplicity-weighted explanations of sensory experience, all decisions will be dominated by tiny variances in the probability of extremely unlikely hypotheses because the "model size" of a hypothesis can grow Busy-Beaver faster than its Kolmogorov complexity. (I've deleted a nearby comment from a known troll about how opposite hypotheses ought to cancel out. Unless this is a literal added axiom - and a false one at that which will produce poorly calibrated probabilities - there is no reason for all the Busy-Beaver sized hypotheses to have consequences which cancel each other out exactly down to the Busy-Beaverth decimal place, and continue to cancel regardless of what random bits of evidence and associated updates we run into throughout life. This presumes properties of both reality and the utility function which are very unlikely.)
Note that this actually happens without allowing that the mugger has a finite probability of possessing a hypercomputer - it follows just from trying to assign non-zero probability to the mugger possessing any Turing machine. It should also be noted that assigning probability literally zero means you will never believe the mugger can do something regardless of what evidence they show you that they are Matrix Lords. Similarly if we use Hanson's anthropic solution, we will never believe the mugger can put us into a vastly special position without log(vast) amounts of evidence.

Goodstein is definable, it just can't be proven total. If I'm not mistaken, all Turing machines are definable in PA (albeit they may run at nonstandard times).

Heh. Yes, I remember reading the section on noradrenergic vs. dopaminergic motivation in Pearce's BLTC as a 16-year-old. I used to be a Pearcean, ya know, hence the Superhappies. But that distinction didn't seem very relevant to the metaethical debate at hand.

I'm not sure this taxonomy is helpful from David Pearce's perspective. David Pearce's position is that there are universally motivating facts - facts whose truth, once known, is compelling for every possible sort of mind. This reifies his observation that the desire for happiness feels really, actually compelling to him and this compellingness seems innate to qualia, so anyone who truly knew the facts about the quale would also know that compelling sense and act accordingly. This may not correspond exactly to what SEP says under moral realism and let me know if there's a standard term, but realism seems to describe the Pearcean (or Eliezer circa 1996) feeling about the subject - that happiness is really intrinsically preferable, that this is truth and not opinion.
From my perspective this is a confusion which I claim to fully and exactly understand, which licenses my definite rejection of the hypothesis. (The dawning of this understanding did in fact cause my definite rejection of the hypothesis in 2003.) The inherent-desirableness of happiness is your mind reifying the internal data describing its motivation to do something, so if you try to use your empathy to imagine another mind fully understanding this mysterious opaque data (quale) whose content is actually your internal code for "compelled to do that", you imagine the mind being compelled to do that. You'll be agnostic about whether or not this seems supernatural because you don't actually know where the mysterious compellingness comes from. From my perspective, this is "supernatural" because your story inherently revolves around mental facts you're not allowed to reduce to nonmental facts - any reduction to nonmental facts will let us construct a mind that doesn't care once the qualia aren't mysteriously irreducibly compelling anymore. But this is a judgment I pass from reductionist knowledge - from a Pearcean perspective, there's just a mysteriously compelling quality about happiness, and to know this quale seems identical with being compelled by it; that's all your story. Well, that plus the fact that anyone who says that some minds might not be compelled by happiness, seems to be asserting that happiness is objectively unimportant or that its rightness is a matter of mere opinion, which is obviously intuitively false. (As a moral cognitivist, of course, I agree that happiness is objectively important, I just know that "important" is a judgment about a certain logical truth that other minds do not find compelling. Since in fact nothing can be intrinsically compelling to all minds, I have decided not to be an error theorist as I would have to be if I took this impossible quality of intrinsic compellingness to be an unavoidable requirement of things being good, right, valuable, or important in the intuitive emotional sense. My old intuitive confusion about qualia doesn't seem worth respecting so much that I must now be indifferent between a universe of happiness vs. a universe of paperclips. The former is still better, it's just that now I know what "better" means.)
But if the very definitions of the debate are not automatically to judge in my favor, then we should have a term for what Pearce believes that reflects what Pearce thinks to be the case. "Moral realism" seems like a good term for "the existence of facts the knowledge of which is intrinsically and universally compelling, such as happiness and subjective desire". It may not describe what a moral cognitivist thinks is really going on, but "realism" seems to describe the feeling as it would occur to Pearce or Eliezer-1996. If not this term, then what? "Moral non-naturalism" is what a moral cognitivist says to deconstruct your theory - the self-evident intrinsic compellingness of happiness quales doesn't feel like asserting "non-naturalism" to David Pearce, although you could have a non-natural theory about how this mysterious observation was generated.

Clippy is more of a Lovecraftian horror than a fellow sentient - where by "Lovecraftian" I mean to invoke Lovecraft's original intended sense of terrifying indifference - but if you want to suppose a Clippy that possesses a pleasure-pain architecture and is sentient and then sympathize with it, I suppose you could. The point is that your sympathy means that you're motivated by facts about what some other sentient being wants. This doesn't motivate Clippy even with respect to its own pleasure and pain. In the long run, it has decided, it's not out to feel happy, it's out to make paperclips.


But even one epistemic error is enough to cause an arbitrarily large loss in utility.

This is always true.

Since you can't update away this belief until it's too late, it does seem important to have "reasonable" priors instead of just a non-superexponentially-tiny probability to "induction works".

I'd say more that besides your one reasonable prior you also need to not make various sorts of specifically harmful mistakes, but this only becomes true when instrumental welfare as well as epistemic welfare are being taken into account. :)

Isn't the giant elephant in this room the whole issue of moral realism? I'm a moral cognitivist but not a moral realist. I have laid out what it means for my moral beliefs to be true - the combination of physical fact and logical function against which my moral judgments are being compared. This gives my moral beliefs truth value. And having laid this out, it becomes perfectly obvious that it's possible to build powerful optimizers who are not motivated by what I call moral truths; they are maximizing something other than morality, like paperclips. They will also meta-maximize something other than morality if you ask them to choose between possible utility functions, and will quite predictably go on picking the utility function "maximize paperclips". Just as I correctly know it is better to be moral than to be paperclippy, they accurately evaluate that it is more paperclippy to maximize paperclips than morality. They know damn well that they're making you unhappy and violating your strong preferences by doing so. It's just that all this talk about the preferences that feel so intrinsically motivating to you, is itself of no interest to them because you haven't gotten to the all-important parts about paperclips yet.
The main thing I'm not clear on in this discussion is to what extent David Pearce is being innocently mysterian vs. motivatedly mysterian. To be confused about how your happiness seems so intrinsically motivating, and innocently if naively wonder if perhaps it must be intrinsically motivating to other minds as well, is one thing. It is another thing to prefer this conclusion and so to feel a bit uncurious about anyone's detailed explanation of how it doesn't work like that. It is even less innocent to refuse outright to listen when somebody else tries to explain. And then strangest of all is to state powerfully and definitely that every bit of happiness must be motivating to all other minds, even though you can't lay out step by step how the decision procedure would work. This requires overrunning your own claims to knowledge in a fundamental sense - mistaking your confusion about something for the ability to make definite claims about it. Now this of course is a very common and understandable sin, and the fact that David Pearce is crusading for happiness for all life forms should certainly count into our evaluation of his net virtue (it would certainly make me willing to drink a Pepsi with him). But I'm also not clear about where to go from here, or whether this conversation is accomplishing anything useful.
In particular it seems like David Pearce is not leveling any sort of argument we could possibly find persuasive - it's not written so as to convince anyone who isn't already a moral realist, or addressing the basic roots of disagreement - and that's not a good sign. And short of rewriting the entire metaethics sequence in these comments I don't know how I could convince him, either.

(Requested reply.)
I think there'd be a wide variety of systems where, so long as the "parent" agent knows the exact strategy that its child will deploy in all relevant situations at "compile time", the parent will trust the child. The point of the Lob problem is that it arises when we want the parent to trust the child generally, without knowing exactly what the child will do. For the parent to precompute the child's exact actions implies that the child can't be smarter than the parent, so it's not the kind of situation we would encounter when e.g. Agent A wants to build Agent B which has more RAM and faster CPUs than Agent A while still sharing Agent A's goals. This, of course, is the kind of "agents building agents" scenario that I am most interested in.

In what sense was Einstein left out of the mainstream, because of what life events, besides his (correct, assuming MWI) criticisms of QM? I don't think I've heard this story of Einstein before. Szilard approached him to ghost-send his letter to Roosevelt, that's all I know of Einstein's later years.

What's rarely appreciated is that Einstein also lucked out, besides being 1 in 10^? genius. A lot of things went right for him early on. On the other hand, a lot of things went wrong for him later on, and so he was left out of the mainstream scientific progress, save for his incisive QM critique.

Yep! And for the record, I agree with your above paragraphs given that.
I would like to note explicitly for other readers that probability goes down proportionally to the exponential of Kolmogorov complexity, not proportional to Kolmogorov complexity. So the probability of the Sun failing to rise the next day really is going down at a noticeable rate, as jacobt calculates (1 / x log(x)^2 on day x). You can't repeatedly have large likelihood ratios against a hypothesis or mixture of hypotheses and not have it be demoted exponentially fast.

If you only assign significant probability mass to one changeover day, you behave inductively on almost all the days up to that point, and hence make relatively few epistemic errors. To put it another way, unless you assign superexponentially-tiny probability to induction ever working, the number of anti-inductive errors you make over your lifespan will be bounded.

Unless I've missed something, it is easy to exhibit small formal systems such that the minimum proof length of a contradiction is unreasonably large. E.g. Peano Arithmetic plus the axiom "Goodstein(Goodstein(256)) does not halt" can prove a contradiction but only after some very, very large number of proof steps. Thus failure to observe a contradiction after small huge numbers of proof steps doesn't provide very strong evidence.

There's an infinite number of alternative hypotheses like that and you need a new one every time the previous one gets disproven; so assigning so much probability to all of them, that they went on dominating Solomonoff induction on every round even after being exposed to large quantities of sensory information, would require that the remaining probability mass assigned to the prior for Solomonoff induction be less than exp(amount of sensory information), that is, super-exponentially tiny.

To amplify on Qiaochu's answer, the part where you promote the Solomonoff prior is Bayesian deduction, a matter of logic - Bayes's Theorem follows from the axioms of probability theory. It doesn't proceed by saying "induction worked, and my priors say that if induction worked it should go on working" - that part is actually implicit in the Solomonoff prior itself, and the rest is pure Bayesian deduction.

Yeah, it's hard to phrase this well and I don't know if there's a standard phrasing. What I was trying to get at was the idea that some computable ordering is total and well-ordered, and therefore an ordinal.

blink blink
Whaaa? Is this saying you think Einstein had substantially less than 1 in 100,000 general intelligence? That seems like a severe underestimate. 1 in 1e5 really isn't much, there should be 70,000 people in the world like that. There isn't a small city full of Einsteins. I've gotten back standardized test reports showing higher percentiles than that.
This reminds me of the time somebody asked me if I considered myself a genius and I asked them to define genius as a fraction of the population. "1 in 100,000? 1 in 1 million?" I inquired. And they said, "1 in 300" to which my reply was to just laugh.
Or am I reading it the wrong way around, i.e., Einstein is much above this level? If so, I wouldn't think more than a couple of orders of magnitude above, like 1 in 1,000,000 or 1 in 10,000,000. Other factors than native g will be decisive past that point.

I was staring at this thinking "Didn't I just say that in the next-to-last paragraph?" and then I realized that to a general audience it is not transparent that adducing the consistency of ZFC by induction corresponds to inducing the well-ordering of some large ordinal by induction.

And it wouldn't defeat the OT because you'd still have to prove you couldn't have a utility function over e.g. causal continuity (note: you can have a utility function over causal continuity).

True, but so what? It's still not trustworthy.

At the point where you called some values "errors" without defining their truth conditions I assumed this wasn't going to be any good and stopped reading.

I don't read Earthfic, it's a literary wasteland.

Stick to logic, my friend; that's not how MWI works. At all.


We can't trust brains when taken as a whole.

We are made of brains. A nice swirl off to the side of the brain mistrusting the brain is mistrusting that mistrust.

Because it's rejecting the premise of a perfectly good experiment, not because it would be a bad idea in real life. Also there's difficulties with having canonical utility measures across agents, but that's a separate point.

Ah yes, the Scorched Earth Party foreign policy.

Visual Novel or Kinetic Novel?

The other problem with checking the code is that an FAI's Friendliness content is also going to consist significantly or mostly of things the FAI has learned, in its own cognitive representation. Keeping these cognitive representations transparent is going to be an important issue, but basically you'd have to trust that the tool and possibly AI skill that somebody told you translates the cognitive content, really does so; and that the AI is answering questions honestly.
The main reason this isn't completely hopeless for external assurance (by a trusted party, i.e., they have to be trusted not to destroy the world or start a competing project using gleaned insights) is that the FAI team can be expected to spend effort on maintaining their own assurance of Friendliness, and their own ability to be assured that goal-system content is transparent. Still, we're not talking about anything nearly as easy as checking the code to see if the EVIL variable is set to 1.

"Utility functions." Omohundro argues that agents which don't have utility functions will have to acquire them. I'm not totally sure I believe this is a universal law but I suspect that something like it is true in a lot of cases, for reasons like those above.

I accidentally left that in after deleting the section underneath it. Like I said, this was only a fraction of their total citations list.

I don't understand why we can't simply build an LFTR. I can't find anything online about why we can't just build an LFTR. I get the serious impression that what we need here is like 0.1 wild-haired scientists, 3 wild-haired nuclear engineers, 40 normal nuclear engineers, and sane politicians. And that China has sane politicians but for some reason can't produce, find, or hire the sort of wild-haired engineers who just went ahead and built a molten-salt thorium reactor at Oak Ridge in the 1960s.

That's what Santa Claus is for.

On the presentation of science in the news:

It's not that clean energy will never happen -- it totally will. It's just that it won't come from a wild-haired scientist running out of his basement screaming, "Eureka! I've discovered how to get limitless clean energy from common seawater!" Instead, it will come from thousands of scientists publishing unreadable studies with titles like "Assessing Effectiveness and Costs of Asymmetrical Methods of Beryllium Containment in Gen 4 Liquid Fluoride Thorium Reactors When Factoring for Cromulence Decay." The world will be saved by a series of boring, incremental advances that chip away at those technical challenges one tedious step at a time.
But nobody wants to read about that in their morning Web browsing. We want to read that while we were sleeping, some unlikely hero saved the world. Or at least cured cancer.

David Wong -- 5 Easy Ways to Spot a BS News Story on the Internet

Sorry, I usually do try to avoid that, but in this case I didn't see how to form that sentence without using the word "should" because it's traditional in "as well X should". Keep in mind that according to C++ namespacing conventions, something inside a namespace has literally nothing to do with its meaning in any other namespace.

As well they Pebblesorter::should have!

See also:

"The Sorting Hat did seem to think I was going to end up as a Dark Lord unless [censored]," Harry said. "But I don't want to be one."
"Mr. Potter..." said Professor Quirrell. "Don't take this the wrong way. I promise you will not be graded on the answer. I only want to know your own, honest reply. Why not?"
Harry had that helpless feeling again. Thou shalt not become a Dark Lord was such an obvious theorem in his moral system that it was hard to describe the actual proof steps. "Um, people would get hurt?"
"Surely you've wanted to hurt people," said Professor Quirrell. "You wanted to hurt those bullies today. Being a Dark Lord means that people you want to hurt get hurt."
Harry floundered for words and then decided to simply go with the obvious. "First of all, just because I want to hurt someone doesn't mean it's right -"
"What makes something right, if not your wanting it?"
"Ah," Harry said, "preference utilitarianism."
"Pardon me?" said Professor Quirrell.
"It's the ethical theory that the good is what satisfies the preferences of the most people -"
"No," Professor Quirrell said. His fingers rubbed the bridge of his nose. "I don't think that's quite what I was trying to say. Mr. Potter, in the end people all do what they want to do. Sometimes people give names like 'right' to things they want to do, but how could we possibly act on anything but our own desires?"
"Well, obviously," Harry said. "I couldn't act on moral considerations if they lacked the power to move me. But that doesn't mean my wanting to hurt those Slytherins has the power to move me more than moral considerations!"


Accurately modeling the world entails making accurate predictions about it. An expected paperclip maximizer fully grasps the functioning of your brain and mind to the extent that this is relevant to producing paperclips; if it needs to know the secrets of your heart in order to persuade you, it knows them. If it needs to know why you write papers about the hard problem of conscious experience, it knows that too. The paperclip maximizer is not moved by grasping your first-person perspective, because although it has accurate knowledge of this fact, that is not the sort of fact that figures in its terminal values. The fact that it perfectly grasps the compellingness-to-Jane, even the reason why Jane finds certain facts to be inherently and mysteriously compelling, doesn't compel it. It's not a future paperclip.
I know exactly why the villain in Methods of Rationality wants to kill people. I could even write the villain writing about the ineffable compellingness of the urge to rid the world of certain people if I put that villain in a situation where he or she would actually read about the hard problem of conscious experience, and yet I am not likewise compelled. I don't have the perfect understanding of any particular real-world psychopath that I do of my fictional killer, but if I did know why they were killers, and of course brought to bear my standard knowledge of why humans write what they do about consciousness, I still wouldn't be compelled by even the limits of a full grasp of their reasons, their justifications, their inner experience, and the reasons they think their inner experience is ineffably compelling.
David, have you already read all this stuff on LW, in which case I shouldn't bother recapitulating it? http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/, http://lesswrong.com/lw/ta/invisible_frameworks/, and so on?

It sounds like Michael Wilson's "We must program the AI in LISP, because if we don't, LISP purists will spend the next several subjective millennia arguing that it should have been done in LISP."
EDIT: Read the XKCD. It sounds like typical Strossian cynicism about how the 'Singularity' will look like a malfunctioning computer or something. Obviously not talking about the intelligence explosion.

Don't worry, it's not the person's wish you think it is.

"I wish to defend this world. I wish to protect this world which God has abandoned, and defend it against everything that threatens it!"
-- To the Stars (Madoka fanfiction)


But we were mono and he was very asymmetrical and he had no idea that he was asymmetrical. (There was stuff to which I told him to find another girl, but he'd get mad if I was talking to another guy.)

We can't quite yet conclude he was evil until you further inform us that he thought he was allowed to talk to other girls without asking you (I naturally presume that no asymmetry of this sort was ever explicitly negotiated). If he carefully refrained from talking to other girls, then we have a big mismatch in expectations about how a relationship should work and a probable breakup recommendation, but not an asymmetric non-negotiated demand.
Sorry for being pedantic in my definitions, but while this sort of thing may not in fact be relevant to your case, the general rule may prove important to others.

Infohazard reference with no warning sign. Edit and reply to this so I can restore.

David, we're not defining rationality to exclude other-oriented desires. We're just not including that exact morality into the word "rational". Instrumental rationality links up a utility function to a set of actions. You hand over a utility function over outcomes, epistemic rationality maps the world and then instrumental rationality hands back a set of actions whose expected score is highest. So long as it can build a well-calibrated, highly discriminative model of the world and then navigate to a compactly specified set of outcomes, we call it rational, even if the optimization target is "produce as many paperclips as possible". Adding a further constraint to the utility function that it be perfectly altruistic will greatly reduce the set of hypothetical agents we're talking about, but it doesn't change reality (obviously) nor yield any interesting changes in terms of how the agent investigates hypotheses, the fact that the agent will not fall prey to the sunk cost fallacy if it is rational, and so on. Perfectly altruistic rational agents will use mostly the same cognitive strategies as any other sort of rational agent, they'll just be optimizing for one particular thing.
Jane doesn't have any false epistemic beliefs about being special. She accurately models the world, and then accurately calculates and outputs "the strategy that leads to the highest expected number of burgers eaten by Jane" instead of "the strategy that has the highest expected fulfillment of all thinking beings' values".
Besides, everyone knows that truly rational entities only fulfill other beings' values if they can do so using friendship and ponies.

Were you a mono couple? Members of mono couples sometimes have ideas about an obligation to fulfill all the other person's sexual and romantic needs since the partner can't go anywhere else. Perhaps he had asymmetrical ideas about this and would not have obliged if you'd made a similar request, but if the notions were symmetrical then it's not a sexbot thing. One reason I'm careful not to date women who aren't dating any other men is that my life is full of other and overriding demands, and I don't want to be someone's only boyfriendly recourse.

WARNING: This comment contains explicit discussion of an information hazard.

Imagine that you have two identical paperclip maximizers

I decline to do so. What imaginary creatures would choose whose choice has been written into their definition is of no significance. (This is also a reply to the comment of FeepingCreature you referenced.) I'm more interested in the practical question of how actual human beings, which this discussion began with, can avoid the pitfall of being taken over by a utility monster they've created in their own heads.
This is a basilisk problem. Unlike Roko's, which depends on exotic decision theory, this one involves nothing more than plain utilitarianism. Unlike the standard Utility Monster scenario, this one involves no imaginary entities or hypothetical situations. You just have to look at the actual world around you through the eyes of utilitarianism. It's a very short road from the innocent-sounding "the greatest good for the greatest number" to this: There are seven billion people on this planet. How can the good you could do them possibly be outweighed by any amount of your own happiness? Just by sitting there reading LessWrong you're killing babies! Having a beer? You're drinking dead babies. Own a car? You're driving on a carpet of dead babies! Murderer! Murderer! Add a dash of transhumanism and you can up the stakes to an obligation to bringing about billions of billions of future humans throughout the universe living lives billions of times better than ours.
But even Peter Singer doesn't go that far, continuing to be an academic professor and paying his utilitarian obligations by preaching utilitarianism and donating twenty percent of his salary to charity.
This is such an obvious failure mode for utilitarianism, a philosophy at least two centuries old, that surely philosophers must have addressed it. But I don't know what their responses are.
Christianity has the same problem, and handles it in practice by testing the vocation of those who come to it seeking to devote their whole life to the service of God, to determine whether they are truly called by God. For it is written that many are called, yet few are chosen. In non-supernatural terms, that means determining whether the applicant is psychologically fitted for the life they feel called to, and if not, deflecting their mania into some more productive route.

Oh, and by default, anything I say can be stolen. :)

"Mortality Report" and to a lesser extent "Friendship is Optimal" (not pro-life, but somewhat transponyist). Three stories constitute a genre.


Responsibility without power breeds cynicism.

-- Scott Sumner (talking about Italian politicians when the EU controls their monetary policy, but it generalizes)

This seems to be a genre now. I dub it transponyism.

It'll do until we have a better standard warning.

I can't edit comments.


having one's work cruelly mischaracterized and held up to ridicule is a whole bunch of no fun.

Thank you for appreciating this. I expected it before I got started on my life, I'm already accustomed to it by now, I'm sure it doesn't compare to the pain of starving to death. Since I'm not in any real trouble, I don't intend to angst about it.

Gwern, I made a major Wiki edit followed by a minor edit. I wasn't aware that the latter would mask the former.

Deleted. Don't link to possible information hazards on Less Wrong without clear warning signs.
E.g. this comment for a justified user complaint. I don't care if you hold us all in contempt, please don't link to what some people think is a possible info hazard without clear warning signs that will be seen before the link is clicked. Treat it the same way you would goatse (warning: googling that will lead to an exceptionally disgusting image).

This is an excellent point I should've noticed myself (though it's been long and long since I encountered the parable). Who says you own a baby just by being its genetic mother?
Albeit sufficiently young babies are plausibly not sentient.


If a woman publicly asserts that she wants to "get" an "attractive man", would you also think that she is being alienating?

Logical fallacy ad hominem tu quoque?

Would we actually notice if our amplitude was constantly diminishing by such a tiny factor? I wonder what that would be like to be...

The real irony is that Eliezer is now a fantastic example of the commitment/sunk cost effect which he has warned against repeatedly: having made an awful decision, and followed it up with further awful decisions over years (including at least 1 Discussion post deleted today and an expansion of topics banned on LW; incidentally, Eliezer, if you're reading this, please stop marking 'minor' edits on the wiki which are obviously not minor), he is trapped into continuing his disastrous course of conduct and escalating his interventions or justifications.
And now the basilisk and the censorship are an established part of the LW or MIRI histories which no critic could possibly miss, and which pattern-matches on religion. (Stross claims that it indicates that we're "Calvinist", which is pretty hilarious for anyone who hasn't drained the term of substantive meaning and turned it into a buzzword for people they don't like.) A pity.

While we're on the topic, I also blame Yvain to some extent; if he had taken my suggestion to add a basilisk question to the past LW survey, it would be much easier to go around to all the places discussing it and say something like 'this is solely Eliezer's problem; 98% disagree with censoring it'. But he didn't, and so just as I predicted, we have lost a powerful method of damage control.
It sucks being Cassandra.

Acceptable edit. Undeleted.

http://wiki.lesswrong.com/wiki/Deletion_policy#Hypothetical_violence_against_identifiable_targets

http://tvtropes.org/pmwiki/pmwiki.php/Main/VillainsActHeroesReact
I so adore tropes, they give me something to subvert.

"According to all the stories, this ordinary world is what the extraordinary people try to protect. If you read a comic book about superheroes, it would be about superheroes defending all those everyday lives. The superheroes wouldn't be trying to cure AIDS or feed starving children in Africa or otherwise change the world. We have scientists for that sort of thing. No, a superhero is someone who defends that ordinary, everyday life from the forces that try to change it. Even if those stories come from our imagination, still, those are the people we praise above all others."


http://www.fanfiction.net/s/5588986/1/Trust-in-God-or-The-Riddle-of-Kyon

See also: HPMOR

There ought to be something intelligent and abstract to say about filtering mechanism conflicts, but I can't think of what it might be right now. E.g., a mention once came up of os-tans on HN, someone said "What's an os-tan?", I posted a link to a page of OS-tans, and then replies complained that the page was NSFW and needed a warning. I was like "What? All those os-tans are totally safe for work, I checked". Turns out there was a big ol' pornographic ad at the top of the page which my eyes had probably literally skipped over, as in just never saccaded there.
That Courage Wolf video probably has a pretty different impact depending on whether or not you automatically skip over and mostly don't even notice all the bad parts.
And in another ten years a naked person walking down the street will be invisible.

Can you give me an example of an ultraviolet catastrophe, say for paperclips?

Right - that's the obvious angle of attack for handling ontological crises.


what happens when the AI realises that the definition of what a "human" is turns out to be flawed.

The AI's definition of "human" should be computational. If it discovers new physics, it may find additional physical process that implement that computation, but it should not get confused.
Ontological crises seems to be a problem for AIs with utility functions over arrangements of particles, but it doesn't make much sense to me to specify our utility function that way. We don't think of what we want as arrangements of particles, we think at a much higher level of abstraction and we would be happy with any underlying physics that implemented the features of that abstraction level. Our preferences at that high level are what should generate our preferences in terms of ontologically basic stuff whatever ontology the AI ends up using.

I got 99 psychological drives but inclusive fitness ain't one.
In what way is evolution supposed to be robust? It's slow, stupid, doesn't reproduce the content of goal systems at all and breaks as soon as you introduce it to a context sufficiently different from the environment of evolutionary ancestry because it uses no abstract reasoning in its consequentialism. It is the opposite of robust along just about every desirable dimension.

This is a uniform problem among all AIs. Avoiding it is very hard. That is why such a thing as the discipline of Friendly AI exists in the first place. You do, in fact, have to specify the preference ordering sufficiently well and keep it sufficiently stable.
Stepping down from maximization is also necessary just because actual maximization is undoable, but then that also has to be kept stable (satisficers may become maximizers, etc.) and if there's something above eudaimonia in its preference ordering it might not take very much 'work' to bring it into existence.

The model theory is just for understanding logic in general and things like Lob's theorem, and possibly being able to reason about universes using second-order logic. What you're talking about is the ontological shift problem which is a separate set of issues.


Remember that motivational video Eliezer linked to? One of the lines toward the end was "If she puts you in the friend zone, put her in the rape zone." I can't imagine Eliezer saying that himself, and I expect he was only noticing and making use of the go for it and ignore your own pain slogans-- but I'm still shocked and angry that it's possible to not notice something like that.

My apologies for that! You're correct that I didn't notice that on a different level than, say, the parts about killing your friends if they don't believe in you or whatever else was in the Courage Wolf montage. I expect I made a 'bleah' face at that and some other screens which demonstrated concepts exceptionally less savory than 'Courage', but failed to mark it as something requiring a trigger warning. I think this was before I'd even heard of the concept of a "trigger warning", which I first got to hear about after writing Ch. 7 of HPMOR.

Even invalidating a proof doesn't automatically mean the outcome is the opposite of the proof. The key question is whether there's a cognitive search process actively looking for a way to exploit the flaws in a cage. An FAI isn't looking for ways to stop being Friendly, quite the opposite. More to the point, it's not actively looking for a way to make its servers or any other accessed machinery disobey the previously modeled laws of physics in a way that modifies its preferences despite the proof system. Any time you have a system which sets that up as an instrumental goal you must've done the Wrong Thing from an FAI perspective. In other words, there's no super-clever being doing a cognitive search for a way to force an invalidating behavior - that's the key difference.

Um, I wouldn't hurt people if I discovered I could violate the laws of physics. Why should a Friendly AI?

I think I can win Game 1 against almost anyone - in other words, I think I have a larger computable number than any sort of computable number I've seen anyone describe in these sorts of contests, where the top entries typically use the fast-growing hierarchy for large recursive ordinals, in contests where Busy Beaver and beyond aren't allowed.
Game 2 is interesting. My first thought was that running the other person's program and adding 1 to the result guarantees that they die - either their program doesn't halt, or your program is larger. So my first thought was that it just reduced to 3 players who can choose whether to kill each other or not, at least 2 of whom have to die, with no solution except from TDT-type correlations. But suppose I output a large number without looking at my opponents' code, and my opponents both try to run the other two programs and add the outputs together, plus 1. They both go into an infinite loop and I win. There may be some nontrivial Nash-style equilibrium to be found here.

It would be unlikely for any more fundamental theory not to be subject to the same set of evasions as QM. Roughly, we have people claiming that atoms are just theoretical figments of the imagination which merely yield good predictions, discovering neutrons isn't going to change their arguments. String theory in particular doesn't help.

There's certainly one obvious explanation which occurs to me. There being a copy of you in another universe seems more counterintuitive than needing to give up on measuring distances, so it's getting more like the backlash and excuses that natural selection got, or that was wielded to preserve vitalism, as opposed to the case of Special Relativity. Also the simple answer seems to have been very hard to think of due to some wrong turns taken at the beginning, which would require a more complex account of human cognitive difficulty. But either way it doesn't seem at all unnatural compared to backlash against the old Earth, natural selection, or other things that somebody thought was counterintuitive.

First, I think that we agree that 'shut up and calculate' reflects the current unfortunate state of affairs, where no other approach is more accurate despite nearly a century of trying. It postulates the Born rule (measurement results in projection onto an eigenstate), something each interpretation also postulates in one form or another, where the term "measurement" is generally understood as an interaction of a simple transparent ( = quantum) system with a complex opaque ( = classical) one. The term decoherence describes how this simple system becomes a part of the complex one it interacts with (and separates from it once the two stop interacting).
Now, I agree that

applying "shut up and calculate" to RQM the results are identical to the results of applying "shut up and calculate" to MWI, so there's no reason to claim that you're shutting up about RQM instead of shutting up about MWI or rather just shutting up about quantum mechanics in general, unless you're not really shutting up.

And indeed I'm not shutting up, because the quantum-classical transition is a mystery to be solved, in a sense that one can hopefully construct a more accurate model (one that predicts new experimental results, not available in "shut up and calculate").
The question is, which are the more promising avenues to build such a model on. RQM suggests a minimal step one has to take, while MWI boldly goes much further, postulating an uncountable (unless limited by the Planck scale) number of invisible new worlds appearing all the time everywhere, without explaining the mysterious splitting process in its own ontology (how does world splitting propagate? how do two spacelike-separated splits interact?).
Now, I am willing to concede that some day some extension of MWI may give a useful new testable prediction and thus will stop being an 'I'. My point is that, unless you postulate reality as ontologically fundamental, MWI is not the smallest increment in modeling the observed phenomenon of the quantum-classical transition.

No approach is ever more accurate than 'shut up and calculate'. The 'Shut up and calculate' version of Special Relativity, wherein we claim that Minkowski's equations give us classical lengths but refuse to speculate about how this mysterious transition from Minkowski intervals to classical lengths is achieved, is just as accurate as Special Relativity. It's just, well, frankly in denial about how the undermining of your intuition of a classical length is not a good reason to stick your fingers in your ears and go "Nah nah nah I'm not listening" with respect to Minkowski's equations representing physical reality, the way they actually do. You believe this with respect to Special Relativity, and General Relativity, and every other "shut up and calculate" version of every physical theory from chemistry to nuclear engineering - that there's no reason to shut up with respect to these other disciplines. I just believe it with respect to quantum mechanics too.

Can you describe in more detail what you mean by 'no universal state'?

Basic question I probably should've asked earlier: Does shminux::RQM entail not-MWI?
If the answer is "no" then shminux::RQM is indeed plausibly shutting up, since by adding further information we can arrive at MWI. I plead guilty to failing to ask this question, note that shminux failed to volunteer the information, and finally plead that I think most RQMers would claim that theirs is an alternative to MWI.

This sounds like 'shut up and calculate' to me. After applying "shut up and calculate" to RQM the results are identical to the results of applying "shut up and calculate" to MWI, so there's no reason to claim that you're shutting up about RQM instead of shutting up about MWI or rather just shutting up about quantum mechanics in general, unless you're not really shutting up. To put it another way, there is no such thing as shutting up about RQM or MWI, only shutting up about QM without any attempt to say what underlying state of affairs you are shutting up about.
If that's not what you mean by denying that you intend to talk about a thingy that generates your experimental results and treating the results as primitive, please explain what that was supposed to say.

Instead of having causal processes which are real, we now need causal processes which are 'real relative to' other causal processes. To prevent the other worlds from being real enough to have people inside them, we need to insist very loudly that this whole diagram of what is 'real relative to' other things, is not itself real. I am not clear on how this loud insistence can be accomplished. Also, since only individual points in configuration space allow one particle to say that another particle is in an exact position and have this be 'real', if you take a blob of amplitude large enough to contain a person's causal process, you will find that elements of a person disagree about what is real relative to them...
...and all these complications are just pointless, there's no need for our ontology to have a notion like 'real relative to' instead of just talking about causes and effects. RQM doesn't even get any closer to explaining the Born probabilities, so why bother? It's exactly like a version of Special Relativity that insists on talking about 'real lengths relative to' instead of observer-invariant Minkowskian spacetime.

No, I understood what you meant. Otherwise I wouldn't have taken a shot at complying. Really RQM deserves its own post carefully dissecting it, but I may not have time to write it.
A very quick but sufficient refutation is that the same math taken as a description of an objectively existing causal process gives us MWI, hence there is no reason to complicate our epistemology beyond this to try to represent RQM, even if RQM could somehow be made coherent within a more complicated ontology that ascribed primitive descriptiveness to ideas like 'true relative to'. MWI works, and RQM doesn't add anything over MWI (not even Born probabilities).

I thought it had enough justice to comply with.

How many billions of eons have you been doing it for so far?

I confess I'm not quite clear on your question. Local processes proceed locally with invariant states of distant entanglement. Just suppose that the global wavefunction is an objective fact which entails all of RQM's statements via the obvious truth-condition, and there you go.

Only with very low probability.

As far as I can tell, the only possible coherent state of affairs corresponding to RQM - the only reality in which you can embed these systems relating to each other - is MWI. To this is added some bad amateur incoherent epistemology trying to dance around the issue without addressing it.
You can quote me on the following:

RQM is MWI in denial.
Any time you might uncharitably get the impression that RQM is merely playing semantic word-games with the notion of reality, RQM is, in fact, merely playing semantic word-games with the notion of reality.
RQM's epistemology is drunk and needs to go home and sleep it off.


Or here's another way of looking at it:
MWI = Minkowskian spacetime. Clear objective state of affairs, observer-invariant intervals separating events.
Single-world QM = Pre-Minkowski mysterious "Lorentz contractions" as a result of moving through the ether. The ether seems mysteriously unobservable and it's really odd that the Lorentz contractions just happen to be exactly right to make motion undetectable, when in principle the ether could be doing anything (just like it's mysterious that the worldeater eats off parts of the wavefunction according to the Born probabilities rather than something else, and only leaves one world behind). Also, since you don't know about the Lorentz transformation for time at this point in the history of physics, your equations will yield the wrong answers for extreme circumstances (just as a large enough quantum computer could contain observers who still wouldn't collapse).
"Shut up and calculate" = Use Minkowskian spacetime but refuse to admit that your equations might refer to something.
RQM = Relational Special Relativity = You repeatedly talk about how "motion" can only be defined relative to an observer, and it's impossible for the universe as a whole to move because it would have to be moving relative to something; you use this to insist that every observer has their private reality in which objects really are moving at a certain rate relative to them, and time really is progressing at a certain rate, and there's no conflict with other observers and their observed rates of motion because reality is not objective. If anyone shows you Minkowskian spacetime and asks why they should adopt your weird epistemology when there's all these perfectly natural invariants to use, or asks you what it would even mean for everyone to have a private reality, yell at them that the universe as a whole clearly can't have an objective state of motion because there's nothing else it could be moving relative to. Basically, Special Relativity only you'd rather give up the attempt to describe a coherent state of affairs than give up on talking separately about space and time the way you're accustomed to.
(If that didn't make sense check SEP or Wikipedia on RQM.)

So in MWI, this presumably arises when e.g. you've got 3 possible states of X, and version A of you decoheres with state 1 while version B is entangled with the superposition of 2+3. In RQM this is presumably described sagely as X being definitely-1 relative to A while X is 2+3 relative to B. Then if you ask them whether or not this statement itself is a true, objective state of affairs (where a 'yes' answer immediately yields MWI) there's a bunch of hemming and hawing.

Okay. Name a state of affairs that could correspond to RQM without being MWI.
PS: Whenever you say that something is 'true relative to' B, please replace it with a state of affairs and a description of B's truth-predicate over possible states of affairs.

What a good thing for all of us that Leo Szilard did not make this mistake.

I can see why you disagree w/ grandparent, but please note that CEV isn't supposed to be a grand new ethical theory. Somewhere in the background of 'why do CEV rather than something else' is a metaethical theory most closely akin to analytic descriptivism / moral functionalism - arguably somewhat new in the details, arguably not all that new, but at any rate the moral cognitivism part is not what CEV itself is really about. The main content of CEV looks like reflective equilibrium or a half-dozen other prior ethical theories and is meant to be right, not new.

Try to keep in mind selection effects. The post was titled Failed Utopia - people who agreed with this may have posted less than those who disagreed.
I confess to being somewhat surprised by this reaction. Posts and comments about gender probably constitute around 0.1% of all discussion on LessWrong.

I'm assuming this is just a deontological rule along the lines, "If X happens, shut down." (If the Programmer was dumb enough to assign a super-high utility to shutting down after X happens, this would explain the whole scenario - the AI did the whole thing just to get shut down ASAP which had super-high utility - but I'm not assuming the Programmer was that stupid.)

It doesn't want to avoid it. Why would it?

I cannot even slightly visualize what you mean by this. Please explain how it would be used to construct an AI that made glider-oids in a Life-like cellular automaton universe.

Moved to Discussion.

It's a summary of what I excluded - I had actually misinterpreted, hence my quote indeed was not a valid reply! The other case is indeed real, sorry.

Fixed.
I. J. Good's original, which I've somewhat abridged, explicitly specifies that there are no competitors who cause visible losses/gains after the invention is rejected.

Studies show that people who try to run behind a car frequently fail to keep up, while nobody who runs in front of a car fails more than once.

I've just come across a fascinatingly compact observation by I. J. Good:

Public and private utilities do not always coincide. This leads to ethical problems. Example - an invention is submitted to a scientific adviser of a firm...
The probability that the invention will work is p. The value to the firm if the invention is adopted and works is V, and the loss if the invention is adopted and fails is L. The value to the adviser personally if he advises the adoption of the invention and it works is v, and the loss if it fails to work is l. The losses to the firm and the adviser if he recommends the rejection of the invention are both negligible...
Then the firm's expected gain if the invention is adopted is pV - (1-p)L and the adviser's expected gain in the same circumstances is pv - (1-p)l. The firm has positive expected gain if p/(1-p) > L/V, and the adviser has positive expected gain if p/(1-p) > l/v.
If l/v > p/(1-p) > L/V, the adviser will be faced with an ethical problem, i.e. he will be tempted to act against the interests of the firm.

This is a beautifully simple recipe for a conflict of interest:
Considering absolute losses assuming failure and absolute gains conditioned on success, an adviser is incentivized to give the wrong advice, precisely when:

The ratio of agent loss to agent gain,
exceeds the odds of success versus failure
which in turn exceeds the ratio of principal loss to principal gain.

You can see this reflected in a lot of cases because the gains to an advisor often don't scale anywhere near as fast as the gains to society or a firm. It's the Fearful Committee Formula.

I would expect the answer to be "not much, compared to writing and publishing horrible, horrible fanfiction".

I cannot express how true this is, at least not without a lot of swear words.

Nope, Robin Hanson did accelerating change microeconomics that didn't address the question of returns on investment in improved cognition, just the ability to convert capital in the form of CPUs into human-equivalent skilled labor.

On the distant chance that you're actually attempting to be reasonable and are just messing it up, I downvoted this post because I automatically downvote everything that tries to Poison the Well against being downvoted. Being preemptively accused of confirmation bias is itself sufficient reason to downvote.

Leaving aside the rest of this comment, please note that in many cases we throw around large numbers and high probabilities in order to obviously break fragile systems that wouldn't break as obviously if we threw small numbers and middle probabilities.

If you think I'm a good moderator, you should not read the thread (based on having seen the actual results).

Peterdjones, in addition to doing all of that, said that trolling was "just teasing". Can't be bothered to look up the exact thread but it was when I announced that I was designating him a troll.
I didn't delete the decision-theory solution comment, in fact I have no idea what it's about. Presumably this was a user who deleted things themselves. I've asked if it's possible to at least have mod deletions show the user who was deleted, and user deletions not show anything (thus making it possible to distinguish mod deletions from user deletions).
Aside from the one info hazard, it's all crap.
We have no software ability to whole ban users but I do announce publicly at the point where I consider a user a troll.


...I would frankly advise all LW regulars not to read this.

One argument in favor of reading the thread even if you think EY is a perfect moderator: if users' models of EY's moderating are bad, they may post things in the thread that wouldn't actually be censored, but the users' model predict would be censored (e.g. harsh but well-thought-out criticism of views prevalent on LW?)
So maybe if you read something that seems valuable and insightful in the thread (good enough that you would have posted it yourself if you had thought of it), you could repost it to LW... in the worst case, you'll just find yourself censored.
Anyway, I added a slightly obscure link to this page in the LW FAQ.

I am very glad to hear you say that. Thank you!

Out of curiosity, is there any sane way to have a debate with RW's top people about whether it's actually productive for them to go around systematically heaping scorn on anything that seems-to-them like an easy target (e.g. homeopathy and cryonics), while also scoffing at the nerds who try to talk about it using math? Or is there no probable productive outcome of even trying to have that conversation?

Working on "The Open Problem of Intelligence Explosion Microeconomics" and "The Open Problem of Lob-Tiling AI" (these are both tentative titles).

Holden's analysis seems sensible. To it I would add only two points:
1) Ratchet up your giving each time you get a higher income - at the time the new money is coming in, and before you start thinking about how to spend it. After you get a raise, especially an unexpected raise, is the best time to donate - rather than waiting to think about it at the end of the year.
2) Holden's analysis implies that if you have no income, e.g. as a student, you should wait to donate until later. This strikes me as basically correct but it is really really surprisingly important to give something on a regular basis - and put cognitive effort into efficient altruism / optimal philanthropy / rational charity, to get into that habit as well, even for small amounts.
At the end of every year, for example, you might donate $100 - or even $10, if $100 is too much - after looking over the latest list of efficient charities and doing some thought about where the $100 ($10) will do the most good, for purchasing utilons rather than fuzzies, just as if you were about to give $10,000. If the end of the year is far enough away and you don't have a trustworthy reminder system already set up, you might do that part now, then again a year later or at the end of the year, etc.
If you don't do this part, I would evaluate a surprisingly low chance that you would remember to start giving, and giving efficiently, later in life when you have income.

Attempted solution:
http://lesswrong.com/r/discussion/lw/gkv/official_lw_uncensored_thread_on_reddit/

Well, there's not much interest in the Singularity around LW. Did he say anything about an intelligence explosion?

User is welcome to post in the page referenced here:
http://lesswrong.com/r/discussion/lw/gkv/official_lw_uncensored_thread_on_reddit/

Well put - this describes my feeling precisely.

It isn't at all clear why all that would add up to something simpler than a single world theory

Single-world theories still have to compute the wavefunction, identify observers, and compute the integrated squared modulus. Then they have to pick out a single observer with probability proportional to the integral, peek ahead into the future to determine when a volume of probability amplitude will no longer strongly causally interact with that observer's local blob, and eliminate that blob from the wavefunction. Then translating the reductionist model into experiences requires the same complexity as before.
Basically, it's not simpler for the same reason that in a spatially big universe it wouldn't be 'simpler' to have a computer program that picked out one observer, calculated when any photon or bit of matter was moving away and wasn't going to hit anything that would reflect it back, and then eliminated that matter.

Solomonoff induction is about putting probability distributions on observations - you're looking for the combination of the simplest program that puts the highest probability on observations. Technically, the original SI doesn't talk about causal models you're embedded in, just programs that assign probabilities to experiences.
Generalizing somewhat, for QM as it appears to humans, the generalized-SI-selected hypothesis would be something along the lines of one program that extrapolated the wavefunction, then another program that looked for people inside it and translated the underlying physics into the "observed data" from their perspective, then put probabilities on the sequences of data corresponding to integral squared modulus. Note that you also need an interface from atoms to experiences just to e.g. translate a classical atomic theory of matter into "I saw a blue sky", and an implicit theory of anthropics/sum-probability-measure too if the classical universe is large enough to have more than one copy of you.

Making all companies 2% more ethical would be a great way to spend a measly billion dollars. Not the literal optimum, but a way higher return in utilons than usual. Alas, you can't actually do that by trying to induce a predictable distortion in asset prices.

No, you've "had less deletions" because you're often mistaken, but you're not a fucking troll and there's an obvious fucking difference. I don't think you've ever run afoul of the deletion policy unless you were in a general thread that was getting stomped.
It seems to me that the claim that criticism is being targeted for deletion is obviously false, and I remark that it is amazing what people will talk themselves into when they find it politically convenient to believe. But I'm not deleting your comments claiming so, because that's got nothing to do with the stated and practiced moderation policies.
Obviously, trolls will post "critical" comments to provoke reactions and so that they can scream censorship afterward (concern trolling) but there's lots, and lots, and LOTS of non-troll criticism on LW which doesn't get deleted. Like, you know, the meta stuff in this open thread. It brought the trolls out to play and the trolls got deleted - and what's left is more than 50% critical, which is a normal day on LW.
I hope that clears things up.

If you don't like participating in threads where things randomly vanish, stop replying to trolls.

I did.

I keep being told that there are no resources for my ideas for automatically fighting trolls, so after a user admits to being a troll I've been going through manually and deleting comments that strike me as trollish - in the sense of intended to provoke. I also suspect we have fake accounts upvoting and hence do not refrain from deleting upvoted comments.
I'm not particularly happy with the way things are, but don't see an obvious way to make them better without somebody being willing to devote an awful lot of full-time-equivalent work to modifying the LW codebase.
And yes, this forum practices (gasp!) censorship. It always has since the day I started deleting Caledonian's comments on Overcoming Bias because he was successfully making posting no-longer-fun for me. Before that, the SL4 mailing list was subject to threads frequently being terminated. We have always been up-front about pruning the tree, and nowadays there's an official Deletion Policy page. Please stop acting like this is some sort of shocking surreptitious secret.
http://wiki.lesswrong.com/wiki/Deletion_policy
Note that this includes deletion of replies to trolls, although I often just downvote those instead.
It would be helpful if people could see that many deleted comments are (in some cases, not all) from the same small set of trolls, but the basic rule is that we have no resources for developing anything so we can't show the author of deleted comments.
This is an online forum that practices gardening. There are lots of other online forums where you can speak freely. Oh, but you'd rather speak here, to the people who gather here to listen? Well - maybe they're gathering here in this garden because they don't want to be in those other ungardened forums, and so no, you can't speak freely.
EDIT: Attempted solution: http://lesswrong.com/r/discussion/lw/gkv/official_lw_uncensored_thread_on_reddit/

Maybe you should instead donate any leftover money to CFAR, so as to avoid the donor illusion. Also, that way everyone who doesn't donate gets to feel a warm glow, too!
I'm curious as to what people think about the hypothetical ethics of jkaufman expecting to donate leftover money at the end of the year to AMF, but not announcing the fact and only committing to the matched money. This seems to me like it would be ethically okay but I am interested if anyone thinks otherwise. (In particular, one reason this seems ethical to me, is that it seems to me that the state of having a public commitment to do something importantly differs from the state of not having such a commitment, and differs even more under the outside view. Another reason it seems ethical is that the results are both good and nonselfish, hence subject to less suspicion than usual :).)

What's with the Starglider comment? That's not in the original and doesn't seem to belong in this post.

Got it! My strategy is clear.

What's your favourite computable fast-growing function these days?

I believe I understand ordinals up to the large Veblen ordinal, so the fast-growing hierarchy for that, plus 2, of 9, or thereabouts, would be the largest computable integer I could program without consulting a reference or having to think too hard. There are much larger computable numbers I can program if I'm allowed to use the Internet to look up certain things.

I remember the days when I used to consider Ackermann to be a fast-growing function.


Heaven? They tried to recruit me, but I turned them down. My place is here in shadows, with the blood and the fear and the screams of the dying, standing back to back with my loves against the world.

-- Time Braid

Insufficiently more likely. I've been around ducks many times without that happening to my socks. Log of the likelihood ratio would be close to zero.


Good things come to those who steal them.

-- Magnificent Sasquatch

Nice! What part of FAI interests you?

I agree subject to the specification that each such observation must look substantially more like the absence of a duck then a duck. There are many things we see which are not ducks in particular locations. My shoe doesn't look like a duck in my closet, but it also doesn't look like the absence of a duck in my closet. Or to put it another way, my sock looks exactly like it should look if there's no duck in my closet, but it also looks exactly like it should look if there is a duck in my closet.

We couldn't afford them this year for cost reasons, but by next year we'll hopefully be able to supply Time-Turners for all workshop participants.


Where there's smoke, there's fire... unless someone has a smoke machine.

-- thedaveoflife

http://lesswrong.com/lw/rr/the_moral_void/


"A stupid person can make only certain, limited types of errors. The mistakes open to a clever fellow are far broader. But to the one who knows how smart he is compared to everyone else, the possibilities for true idiocy are boundless."

-- Steven Brust, spoken by Vlad, in Iorich

There's so many different ways that story couldn't possibly be true...
(EDIT: Ooh, turns out that the Superman Radio program was the one that pulled off the "Clan of the Fiery Cross" punch against the KKK.)

BTW regarding Robin's AI progress metric, my reaction is more like Doug's (the first / most upvoted comment).

Nobody believes in D-Wave.

Maybe I was absent from the office that day? I hadn't heard Carl's 2083 estimate (I recently asked him in person what the actual median was, and he averaged his last several predictions together to get 2083) until now, and it was indeed outside what I thought was our Aumann-range, hence my surprise.

...um, Aris, you're feeding the troll...

That sounds helpful. Let's give this a shot. (I'm running updated Chrome on Win7 if that's relevant.)

Public notice: I'm now considering this user a probable troll and will act accordingly.
In the future, I may so consider and act, under these sorts of circumstances, without such public notice.
Does any (old-time, trusted) user want to volunteer as the mod who goes back and deletes all the troll comments once a user has been designated a troll?

Scale.

My psychological model says that all trolls are of that kind; some trolls just work harder than others. They all do damage in exchange for attention and the joy of seeing others upset, while exercising the limitless human ability to persuade themselves it's okay. If you make it possible for them to do damage on their home computers with no chance of being arrested and other people being visibly upset about it, a large number will opt to do so. The amount of suffering they create can be arbitrarily great, so long as they can talk themselves into believing it doesn't matter for <stupid reason> and other people are being visibly upset to give them the attention-reward.
4chan would have entire threads devoted to building worse hells. Yes. Seriously. They really would. And then they would instantiate those hells. So if you ever have an insight that constitutes incremental progress toward being able to run lots of small, stupid, suffering conscious agents on a home computer, shut up. And if somebody actually does it, don't be upset on the Internet.


Yes, formalizing Friendliness is not the sort of thing you'd want one person doing. I agree. I don't consider that "philosophy", and it's the sort of thing other FAI team members would have to be able to check.

In principle, creating a formalization of Friendliness consists of two parts, conceptualizing Friendliness, and translating the concept into mathematical language. I'm using "philosophy" and "formalizing Friendliness" interchangeably to refer to both of these parts, whereas you seem to be using "philosophy" to refer to the former and "formalizing Friendliness" for the latter.
I guess this is because you think you can do the first part, then hand off the second part to others. But in reality, constraints about what kinds of concepts can be expressed in math and what proof techniques are available means that you have to work from both ends at the same time, trying to jointly optimize for philosophical soundness and mathematical feasibility, so there is no clear boundary between "philosophy" and "formalizing".
(I'm inferring this based on what happens in cryptography. The people creating new security concepts, the people writing down the mathematical formalizations, and the people doing the proofs are usually all the same, I think for the above reason.)

My experience to date has been a bit difference - the person asking the right question needs to be a high-grade philosopher, the people trying to answer it only need enough high-grade philosophy to understand-in-retrospect why that exact question is being asked. Answering can then potentially be done with either math talent or philosophy talent. The person asking the right question can be less good at doing clever advanced proofs but does need an extremely solid understanding of the math concepts they're using to state the kind-of-lemma they want. Basically, you need high math and high philosophy on both sides but there's room for S-class-math people who are A-class philosophers but not S-class-philosophers, being pointed in the right direction by S-class-philosophers who are A-class-math but not S-class-math. If you'll pardon the fuzzy terminology.

Yes, formalizing Friendliness is not the sort of thing you'd want one person doing. I agree. I don't consider that "philosophy", and it's the sort of thing other FAI team members would have to be able to check. We probably want at least one high-grade actual cryptographer.
Of the others, the nonperson predicate and the moral-progress parts are the main ones where it'd be unusually hard to solve and then tell that it had been solved correctly. I would expect both of those to be factorable-out, though - that all or most of the solution could just be published outright. (Albeit recent experience with trolls makes me think that no insight enabling conscious simulations should ever be published; people would write suffering conscious simulations and run them just to show off... how confident they were that the consciousness theory was wrong, or something. I have a newfound understanding of the utter... do-anything-ness of trolls. This potentially makes it hard to publicly check some parts of the reasoning behind a nonperson predicate.) Anthropic reasoning / "reality fluid" is the sort of thing I'd expect to be really obvious in retrospect once solved. R1 and R2 should be both obvious in retrospect, and publishable.
I have hopes that an upcoming post on the Lob Problem will offer a much more concrete picture of what some parts of the innards of FAI development and formalizing look like.

This is a rather important point. How do we get more info on it? You're the first halfway-sane person I've ever heard put the median at 2100.
From my perspective if you told me that in actual fact AGI had been developed in 2120 (a bit of a ways after your median) despite the lack of any great catastrophes, I would update in the direction of believing all of the following:

Rogue biotech hadn't actually been a danger. You didn't make any strong predictions about this because it was outside your conditional; I don't know much about it either. Basically I'm just noting it down. Also, no total global worse-than-Greece collapse, no nuclear-proliferated war brought on by global warming, etc.
Moore's Law had come to a nearly complete permanent halt or slowdown no more than 10-20 years after 2013.
AI academia was Great Stagnating (this is relatively easy to believe)
Machine learning techniques that actually had non-stagnat-y people pushing on them for stock-market trading also plateaued, or weren't published, or never AGI-generalized. 
All the Foresight people were really really optimistic about nanotech, nobody cracked protein folding, or that field Great Stagnated somehow... the nanotech-related news I see, especially about protein folding, doesn't seem to square with this, but perhaps the press releases are exaggerated.
Large updates in the direction of global economic slowdown, patent wars kill innovation everywhere, corruption of universities even worse than we think, even fewer smart people try to go into real tech innovation, etcetera.
Biotech stays regulation-locked forever - not too hard to believe.
Anders Sandberg is wrong about basically everything to do with uploading.

It seems like I'd have to execute a lot of updates. How do we resolve this?

Order-dependence and butterfly effects - knew about this and had it in mind when I wrote CEV, I think it should be in the text.
Counterfactual Mugging - check, I don't think I was calling TDT a complete solution before then but the Counterfactual Mugging was a class of possibilities I hadn't considered. (It does seem related to Parfit's Hitchhiker which I knew was a problem.)
Solomonoff Induction - again, I think you may be overestimating how much weight I put on that in the first place. It's not a workable AI answer for at least two obvious reasons I'm pretty sure I knew about from almost-day-one, (a) it's uncomputable and (b) it can't handle utility functions over the environment. However, your particular contributions about halting-oracles-shouldn't-be-unimaginable did indeed influence me in toward my current notion of second-order logical natural induction over possible models of axioms in which you could be embedded. Albeit I stand by my old reply that Solomonoff Induction would encompass any computable predictions or learning you could do about halting oracles in the environment. (The problem of porting yourself onto any environmental object is something I already knew AIXI would fail at.)

Median doom time toward the end of the century? That seems enormously optimistic. If I believed this I'd breathe a huge sigh of relief, upgrade my cryonics coverage, spend almost all current time and funding trying to launch CFAR, and write a whole lot more about the importance of avoiding biocatastrophes and moderating global warming and so on. I might still work on FAI due to comparative advantage, but I'd be writing mostly with an eye to my successors. But it just doesn't seem like ninety more years out is a reasonable median estimate. I'd expect bloody uploads before 2100.
Carl, ???

Vinge gives you a huge, blatant plot device up front and doesn't try to rationalize it or handwave it. I'm okay with that on a literary level, just like I'd be okay with Banks just not talking about RSI.


if there is only one philosopher on the team, you just can't reach high confidence in the philosophy.

This does not sound correct to me. Resolutions of simple confusions usually look pretty obvious in retrospect. Or do you mean something broader by "philosophy" than trying to figure out free will?

For me it just spoiled the whole thing. Banks should've kept to his original design where he'd never thought of RSI, and hence it's neither mentioned nor handwaved away, and the Culture and Idarans were both doing their best. I can suspend my disbelief for that universe just like I can suspend my belief for FTL travel. I can't suspend my disbelief in the face of a bad handwave, it just throws me right out of the story.

Yes. Yes, I do.
Derren Brown is way better, btw. Completely out of my league.

Yeah, they'd both lack background knowledge to RP the conversation and would also, I presume, be much less willing to lose the money than if they'd ventured the bet themselves. Higher-stakes games are hard enough already (I was 1 for 3 on those when I called a halt). And if it did work against that demographic with unsolicited requests (which would surprise me) then there would be, cough, certain ethical issues.

More difficult version of AI-Box Experiment: Instead of having up to 2 hours, you can lose at any time if the other player types AI DESTROYED. The Gatekeeper player has told their friends that they will type this as soon as the Experiment starts. You can type up to one sentence in your IRC queue and hit return immediately, the other player cannot type anything before the game starts (so you can show at least one sentence up to IRC character limits before they can type AI DESTROYED). Do you think you can win?
(I haven't played this one but would give myself a decent chance of winning, against a Gatekeeper who thinks they could keep a superhuman AI inside a box, if anyone offered me sufficiently huge stakes to make me play the game ever again.)

I think this is making a five-inch fence half an inch higher. It's just not relevant on the scale of an agent to which a human is a causal system made of brain areas and a group of humans is just another causal system made of several interacting copies of those brain areas.

Not obvious. Lots of people who propose AI-boxing propose that or even weaker conditions.

Do you have some reliable way of recruiting? What's the policy alternative? You do what you gotta do, if ends up being just you, nonetheless, you do what you gotta do. Zero people won't make fewer mistakes than one person.

I certainly intend to try that recruiting thing (Paul Christiano ain't half bad) but recruiting philosophy seems much less straightforward than recruiting mathematical talent. If I have to resolve it all myself, I wouldn't flinch from trying. It seems like that part should be less difficult in an absolute sense than the rest of the labor, though that might just be comparative advantage talking. The resolutions to philosophical confusions usually seem relatively straightforward once you have them, in my experience so far.

Note that although the Square Perfect bulb also had the green band, just like LimoStudios, the Square Perfect bulb gave a dreadful smell and the LimoStudios ones didn't.

I thought "indirect normativity" was a general term due to Nick Bostrom meant to cover e.g. CEV among other proposals. Could be wrong.

You can get CFL bulbs at the 105-watt level, and they're huge but bright and daylight-balanced too.
I recently bought this: http://www.amazon.com/gp/product/B005FRCUHY/ref=oh_details_o06_s00_i00
Unfortunately out of stock now, but it was under $30 for a 2-pack at the time. Note that I had to discard a smelly 65W bulb from Square Perfect, but LimoStudios seems able to handle its own heat. 2x 105W CFLs makes a brilliant blue-white light that does seem a bit like daylight.
If anyone knows of a floor lamp that will shade standard-socket light bulbs that are literally a foot long, I wouldn't mind a recommendation. My current setup is kinda hacky.

I agree that the first universe is better, but I'd be way too busy mourning the death of the planet to mourn the interval between those two outcomes if the planet was actually dead. You could call that mental accounting, but isn't everything?

Would that be useful? I expect cryonics to basically work on a technical level. Most of my probability mass for not seeing them again is concentrated in Everett branches where I and the rest of the human species are dead, and for some odd reason that feels like it should make a difference - if somebody goes to Australia for fifty years, are perfectly healthy, and most of my probability mass for not seeing them again is the Earth being wiped out in the meanwhile, I wouldn't mourn them more than I'd mourn anyone else in danger.

I don't feel grief when somebody gets cryosuspended. Seriously, I don't, so far as I can tell. I feel awful when I read about someone who wasn't cryosuspended.

(a) not when you say "infinitely"
(b) "Its power of directing the motions of moving particles, in the demonstrated daily miracle of our human free-will, and in the growth of generation after generation of plants from a single seed, are infinitely different from any possible result of the fortuitous concurrence of atoms"

You know, "politics is the mindkiller" is not only about the conventional meaning of the word "politics". It is about tribes and belonging. Right now you are conflicted as a member of two tribes, and you may feel pressured to choose your loyalty, and protect your status in the selected tribe. Which is not a good epistemic state.
Now on the topic:

Cryonics uses up far more resources [than cancer treatment]

Do we have any specific numbers here? I think the values for "cancer treatment" would depend on the exact kind of treatment and also how long the patient survives, but I don't have an estimate.

If cryonics works, [family and friends] still suffer the same [grief].

Wrong alief. Despite saying "if cryonics works" the author in the rest of the sentence still expects that it does not. Otherwise, they would also include the happiness of family and friends after the frozen person is cured. That is what "if cryonics works" means.
Expressed this way, it is like saying (for a conventional treatment of a conventional disease) that whether doctors can or cannot cure the disease there is no difference, because either way family and friends suffer grief for having the person taken to the hospital. Yes, they do. But in one case, the person also returns from the hospital. That's the whole point of taking people to hospitals, isn't it?

trying to integrate [cryonics] better into society uses up time and resources that could have been spent on higher expectation activities

Technically, by following this argument, we also should stop curing cancer, because that money could also be used for Givewell charities and animal welfare. Suddenly, this argument does not sound so appealing. Why? I guess because cryonics is far; curing a cancer (your, or in your family) is near; and Givewell charities are also far but less so than cryonics. Removing a near suffering feels more important than removing a far suffering. That's human; but let's not pretend that we did a utilitarian calculation here, if we actually used a completely different decision procedure.
...but you already said that.
I think that this discussion is mostly a waste of time, simply because your opponent's true rejection seems to be "cryonics does not work". And then all is written under this alief. Under this alief the arguments make sense: if the cryonics does not work, of course wasting money on cryonics is stupid. But instead of saying this openly, there is a rationalization about why utilitarians should do this and shouldn't do that, by pretending that we have numbers that prove "utility(cancer cure) > utility(animal welfare) > utility(cryonics)". Also, when discussing cryonics, you are supposed to be a perfect utilitarian and willing to sacrifice your life for someone else's greater benefit, but you are allowed to make a selfish exception from perfect utilitarianism when curing your cancer.
For me, the only interesting argument was the one that a smart human in a pre-Singularity world is more useful than a smart human in a post-Singularity world, therefore curing smart people now is more useful than freezing them and curing them in future.

rolls eyes at RQM (due to physicists trying to play silly semantic games that don't actually translate into any coherent epistemology)

Try similarly tabooing "influence".

Okay. "Without causal graphs that violate the Markov condition."

It is likely that you will find that "non-communicating influence" is devoid of meaning

See above.

Thank you for FINALLY calculating that number. It's very likely off by a few orders of magnitude due to the 20-logarithmic-degrees part (our hearing ranges more widely than this, I think) but at least you tried to bloody calculate it.

Not to mention that Einstein was perfectly right about a correct physics containing no randomness and no mysteriously-non-communicating FTL influences, which at the time was part of the then-dominant Copenhagen interpretation of QM. Basically, everything that made Einstein throw up got thrown out. His intuitions were accurate.

Things like this always remind me to doubt clever-sounding explanations of phenomena I wouldn't actually have predicted in advance. Obviously, "not supernatural" is a very strong bet - but the specific hypotheses? Those are less obvious.

I might, if I was a god talking to other gods. And if I was a gun talking to other guns, I'd tell them to shut up about humans and take responsibility for their own bullets.

It looks like Aaron Swartz may have willed all his money to Givewell. This... makes it even sadder, somehow, in ways I don't know how to describe.
His last Reddit comment was on /r/HPMOR.

Well, to a first approximation, on a moral level, Quirrell is who I try not to be and Hermione is who I wish I was, and on the level of intelligence, it's not possible for me to be viscerally impressed with either one's intellect since I strictly contain both. Ergo I find Hermione's choices more impressive than Quirrell's choices.

Be specific? What sort of triggers, what sort of dangerous territory? I can't tell if you're still relying on a human to outwit a transhuman or talking about something entirely different.


the wrong actions can trigger some invariant and signal that something went wrong with the decision theory or utility function

That's not 'boxing'. Boxing is a human pitting their wits against a potentially hostile transhuman over a text channel and it is stupid. What you're describing is some case where we think that even after 'proving' some set of invariants, we can still describe a high-level behavior X such that detecting X either indicates global failure with high-enough probability that we would want to shut down the AI after detecting any of many possible things in the reference class of X, or alternatively, we think that X has a probability of flagging failure and that we afterward stand a chance of doing a trace-back to determine more precisely if something is wrong. Having X stay in place as code after the AI self-modifies will require solving a hard open problem in FAI for having a nontrivially structured utility function such that X looks like instrumentally a good thing (your utility function must yield, 'under circumstances X it is better that I be suspended and examined than that I continue to do whatever I would otherwise calculate as the instrumentally right thing). This is how you would describe on a higher level of abstraction an attempt to write a tripwire that immediately detects an attempt to search out a strategy for deceiving the programmers as the goal is formed and before the strategy is actually searched.
There's another class of things Y where we think that humans should monitor surface indicators because a human might flag something that we can't yet reify as code, and this potentially indicates a halt-melt-and-catch-fire-worthy problem. This is how you would describe on a higher level of abstraction the 'Last Judge' concept from the original CEV essay.
All of these things have fundamental limitations in terms of our ability to describe X and monitor Y; they are fallback strategies rather than core strategies. If you have a core strategy that can work throughout, these things can flag exceptions indicating that your core strategy is fundamentally not working and you need to give up on that entire strategy. Their actual impact on safety is that they give a chance of detecting an unsafe approach early enough that you can still give up on it. Meddling dabblers invariably want to follow a strategy of detecting such problems, correcting them, and then saying afterward that the AI is back on track, which is one of those things that is suicide that they think might have an 80% chance of working or whatever.

I sometimes get the impression that I am the only person who reads MoR who actually thinks MoR!Hermione is more awesome than MoR!Quirrell. Of course I have access to at least some info others don't, but still...

Five cheers for this! Those who are steadily donating should get applause every time.

Lots of strawmanning going on here (could somebody else please point these out? please?) but in case it's not obvious, the problem is that what you call "heuristic safety" is difficult. Now, most people haven't the tiniest idea of what makes anything difficult to do in AI and are living in a verbal-English fantasy world, so of course you're going to get lots of people who think they have brilliant heuristic safety ideas. I have never seen one that would work, and I have seen lots of people come up with ideas that sound to them like they might have a 40% chance of working and which I know perfectly well to have a 0% chance of working.
The real gist of Friendly AI isn't some imaginary 100% perfect safety concept, it's ideas like, "Okay, we need to not have a conditionally independent chance of goal system warping on each self-modification because over the course of a billion modifications any conditionally independent probability will sum to ~1, but since self-modification is initially carried out in the highly deterministic environment of a computer chip it looks possible to use crisp approaches that avert a conditionally independent failure probability for each self-modification." Following this methodology is not 100% safe, but rather, if you fail to do that, your conditionally independent failure probabilities add up to 1 and you're 100% doomed.
But if you were content with a "heuristic" approach that you thought had a 40% chance of working, you'll never think through the problem in enough detail to realize that your doom probability is not 60% but ~1, because only somebody holding themselves to a higher standard than "heuristic safety" would ever push their thinking far enough to realize that their initial design was flawed.
People at SI are not stupid. We're not trying to achieve lovely perfect safety with a cherry on top because we think we have lots of luxurious time to waste and we're perfectionists. I have an analysis of the problem which says that if I want something to have a failure probability less than 1, I have to do certain things because I haven't yet thought of any way not to have to do them. There are of course lots of people who think that they don't have to solve the same problems, but that's because they're living in a verbal-English fantasy world in which their map is so blurry that they think lots of things "might be possible" that a sharper map would show to be much more difficult than they sound.
I don't know how to take a self-modifying heuristic soup in the process of going FOOM and make it Friendly. You don't know either, but the problem is, you don't know that you don't know. Or to be more precise, you don't share my epistemic reasons to expect that to be really difficult. When you engage in sufficient detail with a problem of FAI, and try to figure out how to solve it given that the rest of the AI was designed to allow that solution, it suddenly looks that much harder to solve under sloppy conditions. Whereas on the "40% safety" approach, it seems like the sort of thing you might be able to do, sure, why not...
If someday I realize that it's actually much easier to do FAI than I thought, given that you use a certain exactly-right approach - so easy, in fact, that you can slap that exactly-right approach on top of an AI system that wasn't specifically designed to permit it, an achievement on par with hacking Google Maps to play chess using its route-search algorithm - then that epiphany will be as the result of considering things that would work and be known to work with respect to some subproblem, not things that seem like they might have a 40% chance of working overall, because only the former approach develops skill.
I'll leave that as my take-home message - if you want to imagine building plug-in FAI approaches, isolate a subproblem and ask yourself how you could solve it and know that you've solved it, don't imagine overall things that have 40% chances of working. If you actually succeed in building knowledge this way I suspect that pretty soon you'll give up on the plug-in business because it will look harder than building the surrounding AI yourself.

I have typically been awful at predicting which parts of HPMOR people would most enjoy. I suggest relaxing and enjoying the hedons.


In the depths of hell, good things are not an option and therefore not a consideration, but there are still choices to be made.

Gloomiest sentence of 2013 so far. Upvoted.

Actually that last description sounds like it would plateau really fast.

Boxing is an example of a level of security: the wrong actions can trigger some invariant and signal that something went wrong with the decision theory or utility function. I'm sure security could be added to the utility function as well: maybe some sort of conservatism along the lines of the suicide-button invariance, where it leaves the Earth alone and so we get a lower bound on how disastrous a mistake can be. Lots of possible precautions and layers, each of which can be flawed (like Eliezer has demonstrated for boxing) but hopefully are better than any one alone.

I'm asking about the process that causes other mathematical beliefs to generalize to your beliefs about physical time in such fashion that physical time always seems to have the smallest model allowed by any of your mathematical beliefs. When I learn that the ordinal epsilon-0 corresponds to an ordering on unordered finitely branching trees, I don't conclude that a basket of apples is made out of little tiny unordered trees. What do the physical apples have to do with ordinals, after all?
Why, as you come to believe that Zermelo-Fraenkel set theory has a model, do you come to believe that physical time will never show you a moment when a machine checking for ZF-inconsistency proofs halts? Why shouldn't physical time just be a random model of PA instead, allowing it to have a time where ZF is proven inconsistent? Why do you transfer beliefs from one domain to the other - or what law makes them the same domain?
This isn't meant to be an unanswerable question, I suspect it's answerable, I'm asking if you have any particular ideas about the mechanics.

Er, to be clear about not taking credit, this is a long-running philosophical debate in mathematics. The parts about "Well, what physics, then?" I haven't read elsewhere but that could just be limited reading.

I think it's more that an ultrafinitist claims not to know that successor is a total function - you could still induct for as long as succession lasts. Though this is me guessing, not something I've read.

Set theory doesn't have a dynamical interpretation because it's not causal, but finite causal systems have first-order descriptions and infinite causal systems have second-order descriptions. Not everything logical is causal; everything causal is logical.

That sounds amazingly Wise. I felt a strong impulse to get it done up in elaborate script and framed on my wall (seriously).


That was imprecise, but I was trying to comment on this part of the dialogue using the language that it had established

Ah, I was asking you because I thought using that language meant you'd made sense of it ;) The language of us "living in a (model of) set theory" is something I've heard before (not just from you and Eliezer), which made me think I was missing something. Us living in a dynamical system makes sense, and a dynamical system can contain a model of set theory, so at least we can "live with" models of set theory... we interact with (parts of) models of set theory when we play with collections of physical objects.

Models being static is a matter of interpretation.

Of course, time has been a fourth dimension for ages ;) My point is that set theory doesn't seem to have a reasonable dynamical interpretation that we could live in, and I think I've concluded it's confusing to talk like that. I can only make sense of "living with" or "believing in" models.

Linked impressive authority says the model has a ZFC-model-encoding element, plus enough nonstandard quoted ZFC axioms in-model that in-model ZFC doesn't think it's a ZFC-model-encoding element. I.e., the formula for "semantically entails ZFC" is false within the model, but from outside, using our own standard list of axioms, we think the element is a model of ZFC.

Abram,
After coming to believe that ZF's smallest infinite set closed under succession describes time, why do you not expect to possibly see a Turing machine proving ZF inconsistent? In other words, as you invent stronger mathematical theories, why do you automatically expect there to be fewer and fewer possible times? Why not just generalize PA to describe time and stay there?

How long do you expect to stay an ultrafinitist?

(Imagines going to the Cambridge Center for the Study of Awesome, located overlooking a gorgeous flowering canyon, inside a giant, dark castle in a remote area which you can only reach by piloting a mechanical T-Rex with rockets strapped to it. Inside, scientists with floor-length black leather lab coats are examining...)

Does that work? How do you know?

So after reading that, I don't see how it could be true even in the sense described in the article without violating Well Foundation somehow, but what it literally says at the link is that every model of ZFC has an element which encodes a model of ZFC, not is a model of ZFC, which I suppose must make a difference somehow - in particular it must mean that we don't get A has an element B has an element C has an element D ... although I don't see yet why you couldn't construct that set using the model's model's model and so on. I am confused about this although the poster of the link certainly seems like a legitimate authority.
But yes, it's possible that the original paragraph is just false, and every model of ZFC contains a quoted model of ZFC. Maybe the pair-encoding of quoted models enables there to be an infinite descending sequence of submodels without there being an infinite descending sequence of ranks, the way that the even numbers can encode the numbers which contain the even numbers and so on indefinitely, and the reason why ZFC doesn't prove ZFC has a model is that some models have nonstandard axioms which the set modeling standard-ZFC doesn't entail. Anyone else want to weigh in on this before I edit? (PS upvote parent and great-grandparent.)

I don't think they have anything to do with each other. Infinitary logic is first-order logic with infinite proof lengths. Second-order logic is finite proof lengths with quantification over predicates. I don't know if there's any particular known relation between what these two theories can express.

I don't think that infinitary logic is the same as ?-order logic.

You can substitute ZFC for ZF throughout the quoted paragraph and it will still be true. I don't know what you want me to get from the link given; it doesn't seem to contradict the quoted paragraph. The part where the link talks about Vk entailing ZFC for inaccessible k is exactly what the quoted paragraph is saying.

Moved to Discussion.


ZF shouldn't be able to prove that some set is a model of ZF, because that's not true in all models. Many models of ZF don't contain any individual set well-populated enough for that one set to be a model of ZF all by itself."

Is this true because of the absent C, true in the sense of the (larger) model, or just false?

Okay, that sounds like it wasn't primarily the fault of the Lifespan Dilemma as such (and it also doesn't sound too far from the amount of sleep I lose when nerdsniped by a fascinating new mathematical concept I can't quite grasp, like Jervell's ordinal notation).

...but why wait until they'd almost gotten to Boston?

"Two roads diverged in a wood. I took the one less traveled by, and had to eat bugs until the park rangers rescued me."

For you, I'll walk this endless maze...

Believing large lies is worse than small lies; basically, it's arguing against the What-The-Hell Effect as applied to rationality. Or so I presume, did not read original.

blinks
I didn't realize the Lifespan Dilemma was a cognitive hazard. How much freakout are we talking about here?

Er... logical fallacy of fictional evidence, maybe? I wince every time somebody cites Terminator in a discussion of AI. It doesn't matter if the conclusion is right or wrong, I still wince because it's not a valid argument.

Yep. You'd want to check or guess the size of the user's monitor and where they were scrolling to, and calculate upvotes-per-actual-user-read. As things are read and not upvoted, your confidence that they're not super-high-value items increases and the value of information from showing them again diminishes.

I look at this and think "diminishing marginal utility" and "negative utility of Death events".
You can of course transform the payoffs to as to restore the situation with real utilons instead of life-years, and then you have the Lifespan Dilemma. Incidentally, now that I know about the fast-growing hierarchy in some detail, I would offer "Increase the ordinal of the function in the fast-growing hierarchy by 1 each time" as a bargain you should take every time unless your utility function in life-years is just plain bounded.

What makes this the Galileo Gambit is that the absurdity factor is being turned into alleged support (by affective association with the positive benefits of air travel and frequent flier miles) rather than just being neutralized. Contrast to http://lesswrong.com/lw/j1/stranger_than_history/ where absurdity is being pointed out as a fallible heuristic but not being associated with positives.

The main thing you want to calculate here is expected-value-of-information. Otherwise new posts drop into the void. Trying to maximize upvotes in the long run means showing new posts that might have a high-upvoting parameter.

Despite having previously thought that (given total time spent) I should try optimizing toothbrushing, and even flossing specifically, it's only today that the thought occurred to me that I spend way too much time and mental energy trying to floss in a way that conserves floss, compared to a brief search online to find cheap floss that I don't mind wasting.

I'll try a couple more edits, but keep in mind that this isn't aimed at logicians concerned about Hilbert's program, it's aimed at improving gibberish-detection skills (sentences that can't mean things) and avoiding logic abuse (trying to get empirical facts from first principles) and improving people's metaethics and so on.

I realized after reading this that I'd stated the Compactness Theorem much more strongly than I needed, and that I only needed the fact that infinite semantic inconsistency implies finite semantic inconsistency, never mind syntactic proofs of inconsistency, so I did a quick rewrite accordingly. Hopefully this addresses your worries about "muddled description", although initially I was confused about what you meant by "muddled" since I'd always carefully distinguished semantics from syntax at each point in the post.
I was also confused by what you meant by "nonstandard models resulting from the Compactness Theorem" versus "nonstandard models resulting from the Incompleteness Theorem" - the nonstandard models are just there, after all, they don't poof into existence as a result of one Theorem or the other being proved. But yes, the Compactness Theorem shows that even adjoining all first-order stateable truths about the natural numbers to PA (resulting in a theory not describable within PA) would still give a theory with nonstandard models.

It may be possible to rescue the word "signal", but it's going to take an equally evocative word that covers what people think they mean by "signal". "Stealing associations" isn't going to work because it's not one word. Robin covers a lot of mileage with "affiliate" but many times when people say "signal" they don't mean "costly-signal" or "affiliate".


given the choice between a large cost to optimize whatever you care about, or small cost to just optimize its own sense experiences, will prefer the latter.

You built the machine to optimize its sense experiences. It is not constructed to optimize anything else. That is just what it does. Not when it's cheaper, not when it's inconvenient to do otherwise, but at all times universally.

I could be mistaken, but I think this is a case of (unfortunately) several people using the term "utility function" for functions over sensory information instead of a direct reward channel. Dewey has a paper on why such functions don't add up to utility functions over outcomes, IIRC.

AIXI does not take general utility functions.
AIXI can only optimize direct functions of sense data.
It cannot have utility functions over the state of worlds in which it is embedded.
This cannot be fixed without using something entirely different in place of AIXI's Solomonoff Induction.

This is basically the theme of the next post in the sequence. :)

How come we never see anything physical that behaves like any of of the non-standard models of first order PA? Given that's the case, it seems like we can communicate the idea of numbers to other humans or even aliens by saying "the only model of first order PA that ever shows up in reality", so we don't need second order logic (or the other logical ideas mentioned in the comments) just to talk about the natural numbers?

I'm not sure I believe in proper classes and in particular, I'm not sure there's a proper class of all sets that could be the model of a second-order theory such that you could not describe any set larger than the model, and as for pinning down that model using axioms I'm pretty sure you shouldn't be able to do that. There are analogues of the Lowenheim-Skolem theorem for sufficiently large infinities in second-order logic, I seem to recall reading.

There are totally models of ZFC containing sets that are models of ZFC. See "Grothendieck universe". Is there a reason why it'd be different in second-order logic? I don't think a second-order set theory would pin down a unique model, why would it? Unless you had some axiom stating that there were no more ordinals past a certain point in which case you might be able to get a unique model. Unless I'm getting this all completely wrong, since I'm overrunning my expertise here.
So in retrospect I have to modify this for us to somehow suppose that the device is operating in a particular model of a second-order theory. And then my device prints out "true" (if it's in one of the smallest models) or the device prints out "false" (if it's in a larger model), unless the device is against the background of an ST with an upper bound imposing a unique model, in which case the device does print out "true" for ST -> false and from the outside, we think that this device is about a small collection of sets so this result is not surprising.
Then the question is whether it makes sense to imagine that the device is about the "largest relevant" model of a set theory - i.e., for any other similar devices, you think no other device will ever refer to a larger model than the current one, nor will any set theory successfully force a model larger than the current one - I think that's the point at which things get semantically interesting again.

It'd be a sucker business model if they had an automated program to compile books from posts and sell them on Amazon and they kept all the money.
This is them spending money to turn things into audio, then trying to sell enough audio files or subscriptions to pay the costs. In exchange, we get more exposure and people who wouldn't hear our stuff otherwise can hear it. This seemed to me like a perfectly reasonable exchange as applied to my own posts, and I have no 'ick' reaction to money exchanging hands. Do you really have a mental image of some hard-working sweating LW poster, like me, living in poverty while Castify, dressed in a suit and dripping jewels, lounges around on my back? This is not a particularly lucrative engagement they're entering, and I see no reason to imagine that audiobooks would ever come to exist otherwise.

Why do people feel the need to add this sort of prelude when praising someone? (Can I also ask that David answer first before anyone else does?) David, suppose you deleted the preamble and looked at the resulting sentence - what does your brain object to?

Stallman has spent nearly thirty years consistently being pretty much completely right, and utterly uncompromising about it. This is, of course, unforgivable


I tried a sunrise alarm - it didn't seem to help / work well. (My brain kept wondering whether the alarm would go off. Possible I could've done better with more habituation.)

(Some common senses of "moral fortitude" definitely cause GDP, at minimum in the form of trust between businesspeople and less predatory bureaucrats. But this part is equally true of Babyeaters.)

I like this splitup!
(From the great-grandparent.)

Eliezer's standard use of 'logical' takes the 'abstract' part of logicalish vibes and runs with them; he adopts the convention that sufficiently careful purely abstract reasoning (i.e., reasoning without reasoning about any particular spatiotemporal thing or pattern) is 'logical,' whereas reasoning about concrete things-in-the-world is 'physical.'

I think I want to make a slightly stronger claim than this; i.e. that by logical discourse we're thinning down a universe of possible models using axioms.
One thing I didn't go into, in this epistemology sequence, is the notion of 'effectiveness' or 'formality', which is important but I didn't go into as much because my take on it feels much more standard - I'm not sure I have anything more to say about what constitutes an 'effective' formula or axiom or computation or physical description than other workers in the field. This carries a lot of the load in practice in reductionism; e.g., the problem with irreducible fear is that you have to appeal to your own brain's native fear mechanisms to carry out predictions about it, and you can never write down what it looks like. But after we're done being effective, there's still the question of whether we're navigating to a part of the physical universe, or narrowing down mathematical models, and by 'logical' I mean to refer to the latter sort of thing rather than the former. The load of talking about sufficiently careful reasoning is mostly carried by 'effective' as distinguished from empathy-based predictions, appeals to implicit knowledge, and so on.
I also don't claim to have given morality an effective description - my actual moral arguments generally consist in appealing to implicit and hopefully shared reasons-for-action, not derivations from axioms - but the metaphysical and normative claim is that these reasons-for-action both have an effective description (descriptively speaking) and that any idealized or normative version of them would still have an effective description (normatively speaking).

I've designated kodos96 as a troll (note: user confessed to trolling) and will delete future posts and comments at will.


"I've tried a few simple spells just for practice and it's all worked for me. Nobody in my family's magic at all, it was ever such a surprise when I got my letter, but I was ever so pleased, of course, I mean, it's the very best school of witchcraft there is, I've heard--I've learnt all our set books off by heart, of course, I just hope it will be enough--I'm Hermione Granger, by the way, who are you?"

MoR:

"Do you have an eidetic memory, Hermione?"
Hermione shook her head. "It's not photographic, I've always wished it was but I had to read my school books five times over to memorize them all."



What does it mean to induct that something may produce the truth of sentences "about a second-order logic"?

I mean suppose you encounter a device that outputs "true" or "false" whenever you feed it a sentence in some second-order logic, and after doing a lot of tests, you think maybe the device outputs "true" if and only if the input sentence is actually true, regardless of what input it receives. This seems like a straightforward analogue of inducting that something may be a Halting Oracle, which I thought you agreed an agent should be able to do?
I'm not sure if this answers your question. If not, feel free to find me on Skype or Google Chat to talk more.

By "true" do you mean that the physical universe semantically entails the sentence, or that the sentence is a semantic tautology of second-order logic? I'm assuming the latter, since the former case is what I was assuming when I gave my Tarski reply.
So... I'm pretty sure you can represent a set's semantic entailment of a Henkin interpretation of a second-order sentence in a first-order set theory. I'm less sure that you can represent entailment of a second-order sentence inside a second-order set theory, but I'm having trouble seeing why you wouldn't be able to do that. I also think that second-order set theories are supposed to be finitely axiomatizable, though I could be wrong about this. But then I'm not quite sure why we don't get into Tarskian trouble.
Let ST be the finite axiomatization of a second-order set theory. Then in second-order logic, why doesn't the sentence (ST->"All sets: Set entails S") form a truth predicate for S? My guess is that there's just no theorem which says, "X is true (entailed by the mathematical universal set / model we're currently operating inside) iff 'X' is entailed by all sets (inside the current model)".
If this is so, then it looks to me like for any given set of axioms corresponding to a second-order set theory, second-order logic can represent the idea of a device that outputs sentences which are semantically entailed by every individual set inside that theory. So the new answer would be that you are welcome to hypothesize that a device prints out truths of second-order logic, for any given background second-order set theory which provides a universe of models against which those sentences are judged universally semantically true.
In which case the indefinite extensibility gets packed into the choice of set theory that we think this device is judging second-order validity for, if I haven't dropped the bucket somewhere along the way.

In that case I'm not sure what you mean by "second-order analogue of Solomonoff induction". Solomonoff induction is about the universe and proceeds from sensory experiences. What does it mean to induct that something may produce the truth of sentences "about a second-order logic"?


Suppose we define a generalized version of Solomonoff Induction based on some second-order logic. The truth predicate for this logic can't be defined within the logic and therefore a device that can decide the truth value of arbitrary statements in this logical has no finite description within this logic. If an alien claimed to have such a device, this generalized Solomonoff induction would assign the hypothesis that they're telling the truth zero probability, whereas we would assign it some small but positive probability.

It seems to me that the paradox may lie within this problem setup, not within the agent doing the induction.
We first consider that, rather than this device being assigned zero probability, it should actually be inconceivable to the agent - there should not be a finitely describable thingy that the agent assigns zero probability of having a finitely describable property.
Why would an agent using a second-order analogue of Solomonoff induction have such conceptual problems? Well, considering how Tarski's original undefinability theorems worked, perhaps what goes wrong is this: we want to believe that the device outputs the truth of statements about the universe. But we also want to believe this device is in the universe. So what happens if we ask the device, "Does the universe entail the sentence stating that <reference to the object which is the device> outputs 'No' in response to a question which looks like <recipe which generates a copy of this question>?"
Thus, such a device is inconceivable in the first place since it has no consistent model, and we are actually correct to assign zero probability to the alien's assertion that the device produces correct questions to all questions about the universe including questions about the device itself.


I wouldn't. Two studies opens the door to publication bias concerns

Agreed. It's much easier for a false effect to garner two 'statistically significant' studies with p < .05 than to gain one statistically significant study with p < .005 (though you really want p < .0001).


in a conjunctive case, such as cryonics, the more finely the necessary steps are broken down, the lower you can manipulate a naive estimate.

Except that people intuitively average these sorts of links, so hostile manipulation involves negating the conjunction and then turning it into a disjunction - please, dear reader, assign a probability to not-A, and not-B, and not-C - oh, look, the probability of A and B and C seems quite low now! If you were describing an actual conjunction, a Dark Arts practioner would manipulate it in favor of cryonics by zooming in and dwelling on links of great strength. To hostilely drive down the intuitive probability of a conjunction, you have to break it down into lots and lots of possible failure modes - which is of course the strategy practiced by people who prefer to drive down the probability of cryonics. (Motivation is shown by their failure to cover any disjunctive success modes.)

Nope.

...it would be really nice if someone had bothered to actually check statistics on how many car failures were actually due to each of the possible causes.

Is subadditivity a one-way ratchet such that we can reliably infer that people are wrong to be more optimistic about cryonics after seeing fewer failure steps?

This sounds wrong to me. In full generality, I expect breaking things into smaller and smaller categories to yield larger and larger probability estimates for the supercategory. We don't know what level of granularity would've led mechanics to be accurate, and furthermore, the main way to produce accuracy would've been to divide things into numbers of categories proportional to their actual probability so that all leaves of the tree had roughly equal weight. Your question sounds like breaking things down more always produces better estimates, and that is not the lesson of this study.
If I was trying to use this effect for a Grey Arts explanation (conveying a better image of what I honestly believe to be reality, without any false statements or omissions, but using explanatory techniques that a Dark Arts practitioner could manipulate to make people believe something else instead, e.g., writing a story as a way of conveying an idea) I would try to diagram cryonics possibilities into a tree where I believed the branches of a given level and the leaf nodes all had roughly equal probability, and just showing the tree would recruit the equal-leaf-size effect to cause the audience to concretely represent this probability estimate.

Formatting added.

This can be carried out by non-admins (at least the first part).

Heh! Irony emphasized.

http://wiki.lesswrong.com/wiki/Nonperson_predicate (open problem!)

So noted. Will try to remember to edit at some point.

What's about moral objections to creation of multitude of agents for the purposes of evaluation?


This strikes me as parallel to Searle's view that consciousness imposes meaning.

Why? Did I mention consciousness somewhere? Is there some reason a non-conscious software program hooked up to a sensor, couldn't do the same thing?
I don't think Searle and I agree on what constitutes a physical particle. For example, he thinks 'physical' particles are allowed to have special causal powers apart from their merely formal properties which cause their sentences to be meaningful. So far as I'm concerned, when you tell me about the structure of something's effects on the particle fields, there shouldn't be anything left after that - anything left is extraphysical.

It looks to me like Sobel's fourth objection may stem in behavioral-economics-style terms from prospect theory's position-relative evaluations of gains and losses, in which losses are more painful than corresponding gains are pleasurable (typically by an empirical factor of around 2 to 2.5).
These position-relative evaluations are already inconsistent, i.e., they can be reliably manipulated in laboratory settings to yield circular preferences. So construing a volition probably already requires (just to end up with a consistent utility function and coherent instrumental strategies) that we transform the position-relative evaluations into outcome evaluations somehow.
The 'Ideal Advisor' part would come in at the point where we handed this 'construed' volition a veridical copy of the original predictive model of the human. Thus, this new value system could still reliably predict the actual experiences and reactions of the original human, rather than falsely supposing that the actual human would react in the same way as the construed volition would.
So the construed volition would itself have some coherent utility function over experiences the original human could have - it would not see the human's current state as a huge loss relative to its own position, because it would no longer be evaluating gains and losses. It would also correctly be able to evaluate that the original human would experience various life-improvements as large, joyful gains.
So Sobel's fourth objection would probably not arise if the process of construing a volition proceeded in that particular fashion, which in turn is not ad-hoc since positional evaluation was already a large source of inconsistency that would have to be transformed into a coherent utility function somehow, and likewise giving the idealized process veridical knowledge of the original human is a basic paradigm of volition (the whole Ideal Advisor setup).
Sobel's third objection and second objection seem to revolve around how a construed volition operates over its (abstract) model of possible life experiences that could occur to the original human. (This model had better be abstract! We don't want to inadvertently create people by simulating them in full detail during the process of deciding whether or not to create them.) Suppose we have a construed volition with a coherent utility function, looking over a set of lives that the original human might experience. The amnesia problem is already dissipated if we can pull off this setup; the construed volition does not forget anything. The second problem - the supposed impossibility of choosing between two lives correctly, without actually having led both, but the prospect of leading both introducing an ordering effect - gets us into much thornier territory. Let's first note that it's not obvious that the correct judgment is the one you'd make if you'd actually led a certain life, e.g., heroin!addict!Eliezer thinks that heroin is an absolutely great idea, but I don't want my volition to be construed such that its knowledge of that this overpowering psychological motivation would counterfactually result from heroin addiction, would actually constitute a reason to feed me heroin. I think this points in the direction of an Ideal Advisor ethics wherein construing a volition looks more like modeling how my current values judge future experiences, including my current values over having new desires being fulfilled, more than it points toward construing my volition to have direct empathy with future selves i.e. translation of their own psychological impulses into volitional impetuses of equal strength. This doesn't so much deal with Sobel's second objection as pack it into the problem of construing a volition that shows an analogue of my care for my own (and others') future selves without experiencing 'direct empathy' or direct translation of forceful desires. We're also dancing around the difficulty of having a construed volition which has values over predicted conscious experiences without that volition itself being a bearer of conscious experiences, mostly because I still don't have any good idea of how to solve that one. Resolving consciousness to be less mysterious hasn't yet helped me much on figuring out how to accurately model things getting wet without modeling any water.
Sobel's first problem was a to-do in CEV since day one (the original essay proposed evaluating a spread of possibilities) and I'm willing to point to Bostrom's parliament as the best model yet offered. There's no such thing as "too many voices", just the number of voices you can manage to model on available hardware.

Apparently this is being read by major philosophers now, which is good on the one hand, but on the other hand a really quick review of historical context:
The background problem here is that we want an effective decision procedure of bounded complexity which can actually be implemented in sufficiently advanced Artificial Intelligences.
The first difficulty is the "effective" part. Suppose you want to build a chess-playing program. A philosophy undergrad wisely informs you that you ought to instruct your chess-playing program to make "good moves". You reply that you need a more "effective" specification of what a good move is, so that you can get your program to do it. The undergrad tells you that a good move is one which is wise, highly informed, which will not later be revealed to be a bad move, and so on. What you actually need here is something along the lines of "A good move is one which, when combined with the other player's moves, results in a board state which the following computable predicate verifies as 'winning'". Once you realize the other player is trying to perform a symmetric but opposed procedure, you can model the chessboard's future using search trees. Pragmatically, you're still a long way off from beating Kasparov. But given unbounded finite computing power you could play perfect chess. In turn, this means you're able to get started on the problem of approximating good moves, now that you have an effectively specified definition of maximally good moves, even though you can't evaluate the latter definition using available computing power.
A lot of the motivation in CEV is that we're trying to describe a beneficial AI in terms that allow beneficial-ness to actually be computed or approximated. The AI observes a human and builds up an abstract predictive model of how that human makes decisions - this is an in-principle straightforward problem the way that playing perfect chess is straightforward; Solomonoff induction ideally says how to build good predictive models. What should the AI do with this predictive model, though? An accurate model will accurately predict that the human will choose to drink the glass of bleach, but in an intuitive sense, it seems like we'd want the AI to give the human water.
But suppose we can idealize this decision model in a way which separates terminal values from empirical beliefs. Then we can substitute the AI's world-model for the human's world-model and re-run the decision model. If the AI is much more intelligent than us, this takes care of the bleach-vs.-water case, since the AI knows that the glass contains bleach and that the human values water.
This is the basic paradigm of CEV - build up predictively accurate abstract models of a human decision process, then manipulate them in effectively specified ways to 'construe a volition'. (I would ordinarily say 'extrapolate', but the paper above gave a specific definition of 'extrapolate' that sounds more like surgery followed by prediction than a general, 'look over this accurate human decision model and do X with it').
The appeal of Rawls's reflective equilibrium / Ideal Advisor models is that they describe a construal procedure that sounds effectively computable and approximable: add more veridical knowledge to the decision process (the AI's knowledge, in the case where the AI is smarter than us), run the decision process for a longer time, and allow the model more veridical knowledge of itself and possible even some set of choices for modifying itself. Similarly, the appeal of Bostrom's parliament is not so much that it sounds like a plausible ultimate metaethical theory but that it gives us an effective-sounding procedure for resolving multiple possible volitions (even within a single person) into a coherent output.
More generally, CEV is a case of what Bostrom termed an 'indirect normativity' strategy. If we think values are complex - see e.g. William Frankena's list of terminal values not obviously reducible to one another - a robust strategy would involve trying to teach the AI how to look at humans and absorb and idealize values from them, so as to avoid the problem of accidentally leaving out one value.
The motivation for indirect normativity - for delving into metaethics rather than giving a superintelligent AI a laundry list of cool-sounding wishes - is that we want to pick something close enough to a correct core metaethical structure that it will compactly cover everything human beings want, ought to want, or might later regret asking for, without relying on the ability of human programmers to visualize the outcome in advance. ("I wish you'd get me that glass!" cough cough dies)
Most of the empirical challenge in CEV would stem from the fact that a predictively accurate model of human decisions would be a highly messy structure, and 'construing a volition' suitable for coherent advice isn't a trivial problem. (It sounds to me on a first reading like neither 'idealization' nor 'extrapolation' as defined in the above document may be sufficient for this. Any rational agent needs a coherent utility function, but getting this out of a messy accurate predictive human model is not as simple as conducting a point surgery and extrapolating forward in time, nor as simple as supposing infinite knowledge.)
To compete with CEV in its intended ecological niche (useful advice to (designers of) sufficiently advanced AIs) means looking for alternate theories of how to produce reliable epistemic advice about what-to-do in the presence of messy human values, with sufficient indirection to automatically cover imaginable use-cases of things we didn't think to ask for or might later regret, which theories are close enough to being effectively specified that AI programmers can implement them (though perhaps as something requiring development work to imbue in an AI, rather than a direct computer program).

Not if readers were "encouraged" to answer. If there were some way of knowing the population was representative (i.e. we selected at random and got back responses from everyone selected)... hm, possibly. I know that what people say at local LW gatherings has a stronger influence on me than what I hear online, but that could be for 'improper' reasons of face-to-face contact or greater personal familiarity.

So... admittedly my main acquaintance with Searle is the Chinese Room argument that brains have 'special causal powers', which made me not particularly interested in investigating him any further. But the Chinese Room argument makes Searle seem like an obvious non-reductionist with respect to not only consciousness but even meaning; he denies that an account of meaning can be given in terms of the formal/effective properties of a reasoner. I've been rendering constructive accounts of how to build meaningful thoughts out of "merely" effective constituents! What part of Searle is supposed to be parallel to that?


Your semantics is impoverished if you can prove everything with finite syntactical proofs.

Isn't the knowable limited to what can be known with finite chains of reasoning, whatever your base logic?

The reason it's not random-strawman is that the human-superiority crowd claims we have a mystical ability to see implications that machines can't. If some of them, while making this claim, actually fail at basic logic, the irony is not irrelevant - it illustrates the point, "No, humans really aren't better at Godelian reasoning than machines would be."

You're missing something. Any one person gets mapped to a very wide spread of possible piles of ash. These spreads overlap a lot between different people. Any one pile of ash could potentially have been generated by an exponentially vast space of persons.

Fixed.


you could use the 2 prefix for NOT and the 3 prefix for AND

Immediately after this, you use 1 for NOT and 2 for AND.


This is, of course, not anywhere in anything that kalla724 or I said.

If you complain about how it would be hard to in-situ repair denatured proteins - instead of talking about how two dissimilar starting synapses would be mapped to the same post-vitrification synapse because after denaturing it's physically impossible to tell if the starting protein was in conformation X or conformation Y - then you're complaining about the difficulty of repairing functional damage, i.e., the brain won't work after you switch it back on, which is completely missing the point.
If neuroscience says conformation X vs. conformation Y makes a large difference to long-term spiking input/output, which current neuroscience holds to be the primary bearer of long-term brain information, and you can show that denaturing maps X and Y to identical end proteins, then the ball has legitimately been hit back into the court of cryonics because although it's entirely possible that the same information redundantly appears elsewhere and the brain as a whole still identifies as single person and their personality and memories, telling us that cryonics worked would now tell us a new fact of neuroscience we didn't previously know (e.g. that the long-term behavior of this synapse was reflected in a distinguishable effect on the chemical balance of nearby glial cells or something). But currently, if we find out that cryonics doesn't work, we must have learned some new fact of neuroscience about informationally important brain information not visible in vesicle densities, synaptic configurations, and other things that current neuroscience says are important and that we can see preserved in vitrified rat brains.
We don't have current tech for getting info out. There's solid foreseeable routes in both nanoimaging and nanodevices. If the molecules are in-place with sufficient resolution, sufficiently advanced and foreseeable future imaging tech or nanomanipulation tech should be able to get the info out. Like, Nanosystems level would definitely be sufficient though not necessary, and those are some fairly detailed calculations, estimates, and toy systems being bandied about.

Honestly, at this point I'm willing to just call that a mistake on Tyler Emerson's part.

This seems to me like a deliberate misunderstanding. But taking it at face value, a story in which violence is committed against targets not analogous to any present-day identifiable people, or which is not committed for any reasons obviously analogous to present-day motives, is fine. The Sword of Good is not advocating for killing wizards who kill orcs, although Dolf does get his head cut off. Betrayed-spouse murder mysteries are not advocating killing adulterers - though it would be different if you named the victim after a specific celebrity and depicted the killer in a sympathetic light. As much as people who don't like this policy, might wish that it were impossible for anyone to tell the difference so that they could thereby argue against the policy, it's not actually very hard to tell the difference.

If there's a general policy against discussing violence on LW, and I can point to statements from the same timeframe of mine condemning such violence, it may help. It may not. Reporters are stupid. Your argument does not actually say why the anti-violence-discussion policy is a bad idea, and seems to be ad hominem tu quoque.

Generally speaking, there's a lot of options grownups in real life resort to before they resort to violence, and I would have no problem with a post describing the fully generic considerations and how far you'd actually have to go down the decision tree before you got to violence, without any identifiables being named. People who honestly don't realize this would be welcome to read that post. I may be somewhat prejudiced by considering it completely obvious that jumping straight to violence as a cognitive answer and then blathering about your conspiracy on the Internet is merely stupid.

If combined with a "Please write him and ask him to shut down!", sure. I think it's understood by default in most civilized cultures that violence is not being advocated by default when other courses of action are being presented. If the action to be taken is mysteriously left unspecified, it'd be a judgment call depending on other language used.

"Warm 'em up and see if they spring back to life" was a possible revival method that cryonicists already didn't believe in, so pointing out its impossibility should not affect probability estimates relative to what cryonicists have already taken into account.

Well, yeah. It's why I stay silent during Jewish prayers at family meals. I don't agree with the words.

Wildly off base. The key steps are whether on a molecular level, no more than one original person has been mapped to one frozen brain; if this is true, we can expect sufficiently advanced technology generally, and systems described in Drexler's highly specific Nanosystems book particularly, to be sufficient albeit not necessary (brain scanning might work too). There's also a lot of clueless objections along lines of "But they won't just spring back to life when you warm them up" which don't bear on the key question one way or another. Real debate on this subject is from people who understand the concept of information loss, offering neurological scenarios in which information loss might occur; and real cryonicists try to develop still-better suspension technology in order to avert the remaining probability mass of such scenarios. However, for information loss to actually occur, given current vitrification technology which is actually pretty darned advanced, would require that we have learned a new fact presently unknown to neuroscience; and so scenarios in which present cryonics technology fails are speculative. It's not a question of "fail to disprove", it's a question of what happens if you just extrapolate current knowledge at face value without worrying about whether the conclusion sounds weird. Similarly, you can postulate a social collapse which wipes out the infrastructure for liquid nitrogen production, and a cryonics facility could try to further defend against that scenario by having on-premises cooling powered by solar cells... but if you were actually told the US would collapse in 2028, you would have learned a new fact you did not presently know; it's not a default assumption.

Please interpret the comment charitably; poster means real laws, not fake laws passed for purposes of selective enforcement.

Everyone even slightly famous gets arbitrary green ink. Choosing which green ink to 'complain' about on your blog, when it makes an idea look bad which you would find politically disadvantageous, is not a neutral act. I'm also frankly suspicious of what the green ink actually said, and whether it was, perhaps, another person who doesn't like the "UFAI is possible" thesis saying that "Surely it would imply..." without anyone ever actually advocating it. Why would somebody who actually advocated that, contact Ben Goertzel when he is known as a disbeliever in the thesis?
No, I don't particularly trust Ben Goertzel to play rationalist::nice with his politics. And describing him as a "former researcher at SIAI" is quite disingenuous of you, by the way; he never received any salary from us and is a long-time opponent of these ideas. At one point Tyler Emerson thought it would be a good idea to fund a project of his, but that's it.

I don't see the link, but it does so happen I think "devil's advocate!" is mentally poisonous. I'd even call it an evolutionary precursor of trolling. http://lesswrong.com/lw/r3/against_devils_advocacy/

I am extremely in favor of this policy and would be in favor of extending it to posts or comments asking about the violation of any and all laws.
To all voters: please think about this for five minutes before upvoting or downvoting.


Well, are you?

No. To prove this, I shall shortly delete the post advocating it.

True, but you have said things that seem to imply it. Seriously, you can't go around saying "X" and "X->Y" and then object when people start attributing position "Y" to you.

Point one: We never said X->Y. We said X, and a bunch of people too stupid to understand the fallacy of appeal to consequences said 'X->violence, look what those bad people advocate' as an attempted counterargument. Since no actual good can possibly come of discussing this on any set of assumptions, it would be nice to have the counter-counterargument, "Unlike this bad person here, we have a policy of deleting posts which claim Q->specific-violence even if the post claims not to believe in Q because the identifiable target would have a reasonable complaint of being threatened".


Seriously, quit it. This is LW, not Conspiracy Hotline.

Sounds like a fine reply on LW. I think it will be useful, on forums not LW, to have a LW-policy to point to.

Tried.

This seems an order-of-magnitude less likely than somebody wouldn't naturally think of the dumb idea, seeing the dumb idea.

This intention of yours is not transparent. Plus, they don't care.

Just... singing any lyrics is something that's possibly going to interfere with reverence more than just... singing. You have to agree with lyrics. You don't have to agree with music.

It has a net negative effect because people then go around saying (this post will be deleted after policy implementation), "Oh, look, LW is encouraging people to commit suicide and donate the money to them." That is what actually happens. It is the only real significant consequence.
Now it's true that, in general, any particular post may have only a small effect in this direction, because, for example, idiots repeatedly make up crap about how SIAI's ideas should encourage violence against AI researchers, even though none of us have ever raised it even as a hypothetical, and so themselves become the ones who conceptually promote violence. But it would be nice to have a nice clear policy in place we can point to and say, "An issue like this would not be discussable on LW because we think that talking about violence against individuals can conceptually promote such violence, even in the form of hypotheticals, and that any such individuals would justly have a right to complain. We of course assume that you will continue to discuss violence against AI researchers on your own blog, since you care more about making us look bad and posturing your concern, than about the fact that you, yourself, are the one has actually invented, introduced, talked about, and given publicity to, the idea of violence against AI researchers. But everyone else should be advised that any such 'hypothetical' would have been deleted from LW in accordance with our anti-discussing-hypothetical-violence-against-identifiable-actual-people policy."

Writing words that work for this is going to be really hard. Have you considered finding an appropriate instrumental tune - instrumental in the musical sense, not the philosophical sense - and just having everyone wordlessly sing to it? Until a candle goes out, perhaps? For Ravenclaws, words call forth possible objections. Singing is just singing.

I think that post was a net negative effect on reality and that diminishing the number of people who read it again is a net positive. No, the conversation isn't worth it.

Posts like these are selectively read. Then not everyone votes in the poll. Shrug.

LW has effectively zero resources to implement software changes.

Yes. This seems like yet another example of "First of all, it's a bad fucking idea, second of all, talking about it makes everyone else look bad, and third of all, if hypothetically it was actually a good idea you'd still be a fucking juvenile idiot for blathering about it on the public Internet." What part of "You fail conspiracies forever" is so hard for people to understand? Talk like this serves no purpose except to serve as fodder for people who claim that <rationalist idea X> leads to violence and is therefore false, and your comment shall be duly deleted once this policy is put into place.

The Surgeon General recommends that you not discuss criminal activities, with respect to laws actually enforced, on any mailing list containing more than 5 people.

This does indeed seem like something that's covered by the new policy. It's illegal. In the alternative where it's a bad idea, talking about it has net negative expected utility. If it were for some reason a good idea, it would still be incredibly stupid to talk about it on the &^%$ing Internet. I shall mark it for deletion if the policy passes.

These are explanations, not rules, check.


Yeah seriously. What if violence is the right thing to do?

Then discussing it on the public Internet is the wrong thing to do. I can't compare it to anything but juvenile male locker-room boasting.


EY has publicly posted material that is intended to provoke thought on the possibility of legalizing rape (which is considered a form of violence).

That's an... interesting way of putting it, where by "interesting" I mean "wrong". I could go off on how the idea is that there's particular modern-day people who actually exist and that you're threatening to harm, and how a future society where different things feel harmful is not that, but you know, screw it.

This is why I will be opposed to any sort of zero tolerance policy

The 'rules' do not 'mandate' that I delete anything. They hardly could. I'm just, before I start deleting things, giving people fair notice that this is what I'm considering doing, and offering them a chance to say anything I might have missed about why it's a terrible idea.

I was just going to quote your comment on Overcoming Bias to emphasise this.

AFAIK, all SIAI personnel think and AFAIK have always thought that UFAI cannot possibly explain the Great Filter; the possibility of an intelligence explosion, Friendly or unFriendly or global-economic-based or what-have-you, resembles the prospect of molecular nanotechnology in that it makes the Great Filter more puzzling, not less. I don't view this as a particularly strong critique of UFAI or intelligence explosion, because even without that the Great Filter is still very puzzling - it's already very mysterious.

I think some people may be misinterpreting you as believing this because many people understand your advocacy as implying "UFAI is the biggest baddest existential risk we need to deal with". Assuming a late filter not explained by UFAI suggests there is an unidentified risk in our future that is much likelier than an uncontrolled intelligence explosion.


I think some people may be misinterpreting you as believing this because many people understand your advocacy as implying "UFAI is the biggest baddest existential risk we need to deal with".

It is; I don't particularly think the answer to the Great Filter is a Bigger Threat that comes after this. There's a possibility that most species like ours happen to be inside the volume of some earlier species's "F"AI's enforced Prime Directive with a restriction threshold (species are allowed to get as far as ours, but are not allowed to colonize galaxies) but if so I'm not sure what our own civilization ought to do about that. I suspect, and certainly hope, that there's actually a hidden rarity factor.
But I do think some fallacy of the form, "This argument would make UFAI more threatening - therefore UFAI-fearers must endorse it - but the argument is wrong, ha ha!" might have occurred.

Depends on exactly how it was written, I think. "The paradigmatic criticism of utilitarianism has always been that we shouldn't rob banks and donate the proceeds to charity" - sure, that's not actually going to conceptually promote the crime and thereby make it more probable, or make LW look bad. "There's this bank in Missouri that looks really easy to rob" - no.

That's why I said 'sexual' not 'gender'.

Most students in HPMOR are silly when not under pressure (witch counterexample: Penelope Clearwater). They're also named after fan artists with upcoming cameos. Who tend to be female.
And yes, there's a Gossipy Hens trope in HPMOR, the converse of which is the horrible dating advice dispensed by males with their parody PUA community, both of whom are there because someone has to horribly misinterpret the situation, and which are gender-correlated because... well, because that part is realistic and there are things in HPMOR that happen because that's what the prior causal forces output, not necessarily because that's how I freely decided the outcome should be.

New proposed censorship policy:
Any post or comment which advocates or 'asks about' violence against sufficiently identifiable real people (as opposed to aliens or people on trolley tracks) may be censored.
Reason: Talking about violence makes it more probable, makes LW look bad, and numerous message boards across the Earth censor discussion of various subtypes of proposed criminal activity without anything bad happening to them.
More generally: Posts or comments advocating or 'asking about' violation of laws that are actually enforced against middle-class people (e.g., not drug laws) may be censored on the grounds that it makes LW look bad and that anyone talking about a proposed crime on the Internet fails forever as a master criminal (i.e., even if a proposed conspiratorial crime were hypothetically good, there would still be net negative expected utility from talking about it on the Internet, therefore and in full generality this is a low-value form of discussion).
This is not a poll, but I am asking in advance if anyone has non-obvious consequences they want to point out. In other words, the form of this discussion is not 'Do you like this?' - you have a different utility function from people who are responsible for how LW looks as a whole - but rather, 'Are there any predictable consequences we didn't think of that you would like to point out, and possibly bet on with us if there's a good way to settle the bet?'

Actually timeless physics is being treated as Timeless Science in HPMOR - nobody in 1991 should've heard of Julian Barbour yet.

...yep.

...do note that Hermione at one point reacts in a genre-savvy fashion by saying that it's fine for Harry to have a dark side.
Please keep in mind that a lot of this apparent problem is generated by the unalterable fact that Harry, who has Stuff Going On and has been through hell as the title character and has to grow fast enough to be competitive with people like Dumbledore and Professor Quirrell (all genders chosen by Rowling) happens to be male, whereas Hermione, who like many other characters is going to have difficulty competing with Harry at this point in the story, happens to be female. I mean, suppose Rowling had made her professionally paranoid Auror a woman. It's not unthinkable that someone might complain about how Harry, a male, managed to land a stun on Madam Moody. Symmetrically, if Draco had discovered Harry doing science with Hermione some chapters earlier, he wouldn't have had the same reaction but he would've had an equally difficult reaction for Harry to deal with, and yes I would've figured out some way to make the adultery joke there too.
The main lesson I'm learning is that there are potential Problems when you arrange the plot so that you have the main character interacting with two different tiers of powered characters (Harry-Draco-Hermione and Harry-Dumbledore-Quirrell) and you haven't arranged the plot to have the main character's companions go through everything the main character does... but that problem is far too late to correct now.
P.S: In retrospect there's exactly one important canon character in this story whose gender I could freely choose, and I did happen to make her female, but that's not going to be apparent until later.

Unless intelligent life is already almost-extremely rare, that's not nearly enough 'luck' to explain why everyone else is dead, including aliens who happen to be better at solving coordination problems (imagine SF insectoid aliens).

I wonder how you'd do if you were writing (smart) children to whom most grownup sexual issues were theoretical.

Yeah, that makes this if anything sound even more like D&D. Where is the motivation for this rule coming from? Is there any evidence humans actually act this way at all? The only related evidence I'm aware of goes in the other direction: Traumatized children are more likely to have behaviorial problems and lower IQ after the fact. Citation. (Thought that just popped into my head, could reduced levels of corporal punishment and generally more stable lives be a contributing factor to the Flynn effect?).

It did work out that way in my own life.
There's a Dilbert cartoon in which Dilbert thinks he's really just been faking it since sixth grade.
At age 17 I went through a bit of hell bad enough that I don't particularly want to talk about it, and three weeks later woke up one morning and realized that I would never feel like that Dilbert cartoon again. Literally, just woke up in the morning. It wasn't the result of any epiphany, it seemed more like something biological my brain just did in response. My main reaction was, "Why couldn't my brain have done this three weeks earlier when it would've #$&%ing helped?"
Not sure how that squares with the research, and I couldn't point to anywhere in my life where it happened except that one point.
However, the actual literary logic is something more like, "Once you show Harry thinking his way out of Azkaban, it is no longer possible for him to lose an even battle to Draco - the reader won't believe it." I don't think the 'power up through trial' thing is actually unrealistic, I mean, if I come out of this planet alive I'm probably not going to be fazed by much after that. But it's the more fundamental literary reason why so many stories work that way. You will perceive that this also points in the direction of, "Being run over by a truck isn't the same as punching the truck to a standstill" in terms of whether you powered up after that.
Even so, imagine Methods!Granger fleeing to the bathroom after just hearing Ron call her a nightmare. That could've happened in Ch. 9, maybe, but now Granger has fought three older bullies successfully and you'd be, like, "Yeah right." But she hasn't been to Azkaban, either.

Very possibly on-base. I think my brain is worried that other people will read this and say, "Ah, Eliezer is a patriarchalist writer" instead of, "Oh, well, invisibly behind the scenes Eliezer was trying to juggle this and a dozen other writing problems and desiderata simultaneously and this is what we got." Talking about your own analogous writing problems seems much more likely to lead the wider audience to the second conclusion.
I had no particular intention to talk past you; as we both know, conveying meaning using words is hard, and I might not've understood your intended main point.

I usually don't respond, but I care unusually much about what the author of Luminosity thinks.

She didn't get a chance to fight during that - it doesn't work quite the same way.

If they'd known the true consequences with certainty in advance... sure.

Erm... a basic law of MoR is that people gain maturity/competence in proportion to how much hell they've been through. This creates a power balance problem where Harry, as main character, has been to Azkaban and Hermione hasn't, and fighting bullies isn't quite enough to make up for that. However, I would indeed maintain as a literary matter that this Hermione has been through more hell than the quoted canon!Hermione and is visibly more powerful and competent. Methods!Hermione doesn't flee in tears if Ron calls her a nightmare, though she would've at the start of the year. She probably wouldn't even notice.

You're correct as a matter of rationalist etiquette, but...
Harry is the only student character who sometimes has that level of control over his emotions. Dumbledore can do that. Professor Quirrell can do that. Severus Snape can do that almost all of the time (see Ch. 27). Professor McGonagall tries to do that. Draco, Neville, Hermione, and any other first-year student you care to name except Harry can't.

By the standards of our community, yes, you're never supposed to flee in tears, and Harry has right-of-way to express any ideas he wants. Hermione has not been raised with this ideal, and Harry has not yet pressed it on her.
And canon!Hermione in her fifth-year, who delivered Umbridge unto the centaurs, wouldn't have fled in tears; and Harry could have told her about Draco much earlier, confident that 5th-year!Hermione could put on a mask around Draco and keep it up.
This is first-year!canon!Hermione:

Ron was in a very bad temper by the end of the class.
"It's no wonder no one can stand her," he said to Harry as they pushed their way into the crowded corridor. "She's a nightmare, honestly."
Someone knocked into Harry as they hurried past him. It was Hermione. Harry caught a glimpse of her face--and was startled to see that she was in tears.
"I think she heard you."
"So?" said Ron, but he looked a bit uncomfortable. "She must've noticed she's got no friends."
Hermione didn't turn up for the next class and wasn't seen all afternoon. On their way down to the Great Hall for the Hallowe'en feast, Harry and Ron overheard Parvati Patil telling her friend Lavender that Hermione was crying in the girls' toilets and wanted to be left alone. Ron looked still more awkward at this, but a moment later they had entered the Great Hall, where the Hallowe'en decorations put Hermione out of their minds.


I said to Luke when I read that, "You know, Luke, it hasn't happened yet in the story, but I'd already planned out, before I read your post, that when I want to have Harry screw up a conversation with Hermione as badly as possible, I'm going to have him start talking about evolutionary psychology. You literally did that in the way I'd imagined as the worst way possible." (Though the actual chapter didn't come out quite that way when I wrote it - there isn't anything about evolutionary psychology until the very end.)
So I thought of this as a stereotypically male-stupid thing to do, and independently Luke, who happens to be male, went and did it. Can you name a woman who's done the same?

AFAIK the idea that "UFAI exacerbates, and certainly does not explain, the question of the Great Filter" is standard belief among SIAI rationalists (and always has been, i.e., none of us ever thought otherwise that I can recall).



Padma feels to me like a much more important character than Blaise Zabini, and a more developed character too. I could go into detail but I'm not sure I should, since that sort of thing an author is supposed to communicate through story. I wonder how that perspective difference developed?


and then the story would be Hermione Granger and How She Learned the Methods of Rationality and Became Omnipotent

Yeah, exactly. Also Equally-Upgraded!Hermione plausibly ought to be smarter than the author.

Harry isn't being a silly boy in Ch. 87?

Erm... a basic theory of MoR is that all the characters get automatic intelligence upgrades, except for Hermione who doesn't need it and starts out as exactly similar to her canon self as I could manage, thus putting everyone on an equal footing for the first time. I presume you're familiar with the literary theory which holds that Hermione is the main character of the canon Harry Potter novels?

Actually, that toxoplasmosis thing is the only happiness-creating-preference-inducing, negative-side-effect disease I actually know that really works for Solomon's Problem. You can either pet cute kittens already tested and guaranteed not to have toxoplasmosis, or refrain. This ought to be our go-to real-life example against EDT!

I agree with the "undifferentiated gossiping mass" bit.
Any specific example has a corresponding counterexample. Padma Patil, for example, gets nonzero development and, IIRC, perspective time, which could go a ways to counter "undifferentiated gossiping mass" - but a male character on about her tier of importance, like Blaise Zabini, gets to enact plot and is more distinct as a single person than she is. Even Ron, who is of negligible relevance, has a named skill that differentiates him from the background. Does Padma? As far as I can recall Padma is just sort of generically informedly bright. Hermione's intelligence, gratuitous perfect recall, and magical prowess can go a ways to counter "female characters are less competent" - but the most competent characters, even if you don't count the protagonist, are all male.

Fair point.

Hah, I wondered if someone would ask that. I reply that seeing through the Cloak is a 'requires concentration' ability. Harry deliberately doesn't concentrate before going back in time, because he doesn't want to fix anything via knowledge so as to leave himself freedom of action.

Note that the prophecy is from before Harry was born, and his parents died when he was over a year old.


What if we talk about other people instead of ourselves to isolate the problem to just utility functions?

My utility function is over 'what other people see happen to themselves' so it contains a reference to the same epistemic question.

I would say that what I did is more like continuing to care about continuity, but trying to put it into causal continuity or pattern continuity after the particular hypothesis of 'particle identity continuity' turned out to be nonsense. Also, I regard this as a problem not strictly of utility functions because it controls what I expect to see happen after being cryonically revived or stepping into a Star Trek transporter - I either see the next moment, or see what happens after dying in a car crash i.e. NULL. Yes, I'm aware that this last part is confused, but just because I'm confused about something epistemic doesn't mean that it gets packed into utility functions. I shall have to write a post about this at some point.

nod

Never doubt that a small group of thoughtful, committed citizens can destroy the world. Indeed, it is the only thing that ever has.


This has filled me with the distressing thought that the badness of death might somehow be diminished because of this

I find this statement disturbing. This reads as if you'd really like X to be as horrible as possible to justify your preexisting decision of fighting X.
status quo bias

Edited.

It's even worse than that, the function for the maximum number of nodes you end up before they start going down, if you play using the worst possible strategy, increases faster than any function which Peano arithmetic can prove to be total (i.e., it grows faster than any Turing machine run on various inputs, which Peano arithmetic can prove to halt for any input). To say that this grows faster than the Ackermann function is putting it very mildly.

Nope. You're looking for the paper by Tooby and Cosmides.

Yes we do.
The problem of a chain isn't intended to be limited to the problem of exactly one chain, and I didn't want to complicate the diagram or confuse my readers by showing them a copy of the rationals with each rational replaced by a copy of the integers. If you can't get rid of a larger structure that has a chain in it, you can't get rid of the chain. To put it another way, showing that the chain depicted implies further extra elements isn't the same as ruling out the existence of that chain.
Hence the wording, "How do we get rid of the chain?" not "How do we get rid of this particular exact model here?"
A very quick way to see that there must be more than one chain is to note that if x > y, then x + z > y + z. An element of the nonstandard chain is greater than any natural number, so if we add two nonstandard numbers together, the result must be greater than the nonstandard starting point plus any natural number. Therefore there must be another chain which comes after the first one. For more on this see the linked paper.
EDIT: Several others reported misinterpreting what I had in the original, so I've edited the post accordingly. Thanks for raising the issue, Ilya!

Every time you take box A and box B from Omega, Omicron tortures a kitten.

We can't have both?

I've tried less hard to get core FAI researchers than funding. I suspect that given sufficient funding produced by magic, it would be possible to solve the core-FAI-researchers issue by finding the people and talking to them directly - but I haven't tried it!

I have backup plans, but they tend to look a lot like "Try founding CFAR again."
I don't know of any good way to scale funding or core FAI researchers for SIAI without rationalists. There's other things I could try, and would if necessary try, but I spent years trying various SIAI-things before LW started actually working. Just because I wouldn't give up no matter what, doesn't mean there wouldn't be a fairly large chunk of success-probability sliced off if CFAR failed, and a larger chunk of probability sliced off if I couldn't make any alternative to CFAR work.
I realize a lot of people think it shouldn't be impossible to fund SIAI without all that rationality stuff. They haven't tried it. Lots of stuff sounds easy if you haven't tried it.

Does that mean Harry can't use it (becuse there is no universal reference frame) or he can use it in all sorts of munchkiny ways (I stop the car ... relative to the moon!)

Well, for one thing, he's not powerful enough to cast it period, but if he were, I expect it would only work on near / nearest masses.

The paper could've been called "The Biological Foundations of Culture" and it would've been more accurate. Read it before saying that.

1) In the long run, for CFAR to succeed, it has to be supported by a CFAR donor base that doesn't funge against SIAI money. I expect/hope that CFAR will have a substantially larger budget in the long run than SIAI. In the long run, then, marginal x-risk minimizers should be donating to SIAI.
2) But since CFAR is at a very young and very vital stage in its development and has very little funding, it needs money right now. And CFAR really really needs to succeed for SIAI to be viable in the long-term.
So my guess is that a given dollar is probably more valuable at CFAR right this instant, and we hope this changes very soon (due to CFAR having its own support base)...
...but...
...SIAI has previously supported CFAR, is probably going to make a loan to CFAR in the future, and therefore it doesn't matter as much exactly which organization you give to right now, except that if one maxes out its matching funds you probably want to donate to the other until it also maxes...
...and...
...even the judgment about exactly where a marginal dollar spent is more valuable is, necessarily, extremely uncertain to me. My own judgment favors CFAR at the current margins, but it's a very tough decision. Obviously! SIAI has given money to CFAR. If it had been obvious that this amount should've been shifted in direction A or direction B to minimize x-risk, we would've necessarily been organizationally irrational, or organizationally selfish, about the exact amount. SIAI has been giving CFAR amounts on the lower side of our error bounds because of the hope (uncertainty) that future-CFAR will prove effective at fundraising. Which rationally implies, and does actually imply, that an added dollar of marginal spending is more valuable at CFAR (in my estimates).
The upshot is that you should donate to whichever organization gets you more excited, like Luke said. SIAI is donating/loaning round-number amounts to CFAR, so where you donate $2K does change marginal spending at both organizations - we're not going to be exactly re-fine-tuning the dollar amounts flowing from SIAI to CFAR based on donations of that magnitude. It's a genuine decision on your part, and has a genuine effect. But from my own standpoint, "flip a coin to decide which one" is pretty close to my own current stance. For this to be false would imply that SIAI and I had a substantive x-risk-estimate disagreement which resulted in too much or too little funding (from my perspective) flowing to CFAR. Which is not the case, except insofar as we've been giving too little to CFAR in the uncertain hope that it can scale up fundraising faster than SIAI later. Taking this uncertainty into account, the margins balance. Leaving it out, a marginal absolute dollar of spending at CFAR does more good (somewhat) (in my estimation).

Hyuuga = "Toward the Sun"

I hereby declare Arresto Momentum to match the velocity of a small mass to the velocity of some much larger mass that the wizard thinks is a reference frame.

I tried to write a line with Harry misidentifying it as the Eye That Looks Toward The Sun but had to take it out.

What wedifrid said. Everything the guy says is about functional damage. Talking about the impossibility of repairing proteins in-place even more says that this is somebody thinking about functional damage. Throwing in talk about "information destruction" but not saying anything about many-to-one mappings just tells me that this is somebody who confuses retrievable function with distinguishable states. The person very clearly did not get what the point was, and this being the case, I see no reason to try and read his judgments as being judgments about the point.

No they're not, they're describing functional damage and saying why it would be hard to repair in situ, not talking about what you can and can't information-theoretically infer about the original brain from the post-vitrification position of molecules. In other words, the argument does not have the form of, "These two cognitively distinct states will map to molecularly indistinguishable end states". I'm not saying you have to use that exact phrasing but it's what the correct version of the argument is necessarily about, since (modus tollens) anything which defeats that conclusion in real life causes cryonics to work in real life.

I wonder if that would actually work, or if the finer granularity basically just trashes the ability of your brain to estimate probabilities.

Just remember, most people most of the time are not about to learn the location of a refugee just before being tortured by Nazis.

This would be an awesome prank to play on Pansy using a green light spell.

This is in fact a major literary reason for the above. :)

Probably not - there's just no room for anywhere to put it!

The title of the arc is "Taboo Tradeoffs". The phoenix was the original intended ending. I just couldn't get it written in time.

I would've liked to include the resolution too - but there simply wasn't room for that and the phoenix. I decided the plot could better survive the surgery of one than the other.

Moody has a magical eye. Therefore, Naruto has at some point fought him. QED.

To me this just looks like a bias-manipulating "unpacking" trick - as you divide larger categories into smaller and smaller subcategories, the probability that people assign to the total category goes up and up. I could equally make cryonics success sound almost certain by lumping all the failure categories together into one or two big things to be probability-assigned, and unpacking all the disjunctive paths to success into finer and finer subcategories. Which I don't do, because I don't lie.
Also, yon neuroscientist does not understand the information-theoretic criterion of death.

I think it'd be obvious how to take the log of a dimensional quantity.
e^(log apple) = apple

No.

I confirm (as I have previously) that Frank Jackson's work seems to me like the nearest known point in academic philosophy.

I've sometimes earned more than my SIAI base salary from speaking fees, but I've never earned $140K in any year, and will cheerfully exhibit my tax returns if Luke, Holden, or any other sufficiently reputable entity requests them. I've also got no idea what that "estimated extra compensation" line is about, unless it's health insurance or something - per the wishes of Peter Thiel, SIAI never pays $100k in any year to any employee, including bonuses.
(Note that, as usual when a poster has received many sufficiently extreme downvotes in their history, I designate them a troll and delete their comments at will.)

It would be very surprising if hard scientists were mostly mutants.

I haven't read either of those but will read them. Also I totally think there was a respectable hard problem and can only stare somewhat confused at people who don't realize what the fuss was about. I don't agree with what Chalmers tries to answer to his problem, but his attempt to pinpoint exactly what seems so confusing seems very spot-on. I haven't read anything very impressive yet from Dennett on the subject; could be that I'm reading the wrong things. Gary Drescher on the other hand is excellent.
It could be that I'm atypical for LW.
EDIT: Skimmed the Dennett one, didn't see much of anything relatively new there; the Sellers link fails.

3 and 4 seem like the most fatal.
W/r/t 2, young, unsophisticated AIs with mostly human-readable source code require only small amounts of concern to detect "being trained to lie". Albeit this is only a small amount of concern by serious-FAI-work standards; outside the local cluster, anyone who tries to build this sort of AI in the first place might very well wave their hands and say, "Oh, but there's no difference between trying to lie with your actions to us and really being friendly, that's just some anthropomorphic interpretation of this code here" when the utility function has nothing about being nice and being nice is just being done as an instrumental act to get the humans to go along with you while you increase your reward counter. But in terms of serious FAI proposals, that's just being stupid. I'm willing to believe Paul Christiano when he tells me that his smart International Mathematical Olympiad friends are smarter than this, regardless of my past bad experiences with would-be AGI makers. In any case, it shouldn't take a large amount of "actual concern and actual willingness to admit problems", to detect this class of problem in a young AGI; so this alone would not rule out "raise the FAI like a kid" as a serious FAI proposal. Being able to tell the difference between a 'lying' young AGI and a young AGI that actually has some particular utility function - albeit not so much / just-only by inspection of actions, as by inspection of code which not only has that utility function but was human-designed to transparently explicitly encode it - is an explicit part of serious FAI proposals.
3 and 4 are the actually difficult parts because they don't follow from mere inspection of readable source code.
On 3: Knowing that the current execution path of the code seems to be working okay today is very different from strongly constraining future execution paths across hugely different contexts to have desirable properties; this requires abstract thinking on a much higher level than staring at what your AGI is doing right now. The tank-detector works so long as it's seeing pictures from the training sets in which all tanks are present on cloudy days, but fails when it wanders out into the real world, etc. "Reflective decision theory"-style FAI proposals try to address this by being able to state the desirable properties of the AI in an abstraction which can be checked against abstractions over code execution pathways and even over permitted future self-modifications, although the 'abstract desirable properties' are very hard (require very difficult and serious FAI efforts) to specify for reasons related to 4.
On 4: Since humans don't have introspective access to their own categories and generalizations, figuring out the degree of overlap by staring at their direct representations will fail (you would not know your brain's spiking pattern for blueness ify ou saw it), and trying to check examples is subject to a 3-related problem wherein you only check a narrow slice of samples (you never checked any cryonics patients or Terry Schiavo when you were checking that the AI knew what a 'sentient being' was). I.e., your training set turns out to unfortunately have been a dimensionally impoverished subset of the test set. "Indirect normativity" (CEV-style) proposals try to get at this by teaching the AI to idealize values as being stored in humans, such that observation about human judgments or observation of human brain states will 'correctly' (from our standpoint) refine its moral theory; as opposed to trying to get the utility function correct outright.
The anthropomorphic appeal of "raising AIs as kids" doesn't address 3 or 4, so it falls into the class of proposals that will appear to work while the AI is young, then kill you after it becomes smarter than you. Similarly, due to the problems with 3 and 4, any AGI project claiming to rely solely on 2 is probably unserious about FAI and probably will treat "learning how to get humans to press your reward button" as "our niceness training is working" a la the original AIXI paper, since you can make a plausible-sounding argument for it (or, heck, just a raw appeal to "I know my architecture!") and it avoids a lot of inconvenient work you'd have to do if you publicly admitted otherwise. Ahem.
It should also be noted that Reversed Stupidity Is Not Intelligence; there's a lot of stupid F-proposals for raising AIs like children, but that doesn't mean a serious FAI project tries to build and run an FAI in one-shot without anything analogous to gradient developmental stages. Indirect normativity is complex enough to require learning (requiring an inductive DWIM architecture below, with that architecture simpler and more transparent). It's >50% probable in my estimate that there's a stage where you're slowly teaching running code about things like vision, analogous to a baby stage of a human. It's just that the presence of such a stage does not solve, and in fact does not even constitute significant progress toward, problems 3 and 4, the burden of which need be addressed by other proposals.

What report is that? A site-search for "140,000" turns up a number of figures but none from EY; the latest Form 990 I know of lists his compensation at ~$104k (pg7, summing both columns) or ~50% less than your number.

This all does sound good to me; but, is there a way to say the above while tabooing "reference" and avoiding talk of things "referring" to other things? Reference isn't ontologically basic, so what does it reduce to?
Basically, the main part that would worry me is a phrase like, "there's a story to be told about how our moral concepts came to pick out these particular worldly properties" which sounds on its face like, "There's a story to be told about how successorship came to pick out the natural numbers" whereas what I'd want to say is, "Of course, there's a story to be told about how moral concepts came to have the power to move us" or "There's a story to be told about how our brains came to reflect numbers".

I tend to see a fairly sharp distinction between negative aspects of phyg-leadership and the parts that seem like harmless fun, like having my own volcano island with a huge medieval castle, and sitting on a throne wearing a cape saying in dark tones, "IT IS NOT FOR YOU TO QUESTION MY FUN, MORTAL." Ceteris paribus, I'd prefer that working environment if offered.

Apparently our gratitude reflex, like our help reflex, has the sensation of gratitude preceding the search for a target of that sensation, and thus fails to avoid activating when the search is destined to come out negative.
In other words, we feel: "YAY! THANK YOU! ...to who? ...to X who helped me!"
not
"YAY! Did anyone help me with this? X did... THANK YOU, X!"
similarly, it's:
"OH NO! HELP! ...to who? X can help me... help, X!"
instead of
"OH NO! ...can anyone help me? X can... HELP, X!"

So does that mean this:

I think Richard is saying that what is "right" is rigidly determined by my current (idealised) desires - so in a possible world where I desired to murder, murder would still be wrong

...is your real claim here, independent of any points about language use?
If so, I think I would just straightforwardly modify my paragraph above to say that my statements are not trying to talk about language use or human brains / desires, albeit that desire is both an optimization target of, and a quotation of, morality.

The Clippified human categorizes foods into a similar metric of similarity - still believes that fish tastes more like steak than like chocolate - but of course is not motivated to eat except insofar as staying alive helps to make more paperclips. They have taste, but not tastiness. Actually that might make a surprisingly good metaphor for a lot of the difficulty that some people have with comprehending how Clippy can understand your pain and not care - maybe I'll try it on the other end of that Facebook conversation.

I'd like to say "sure" and then delete that paragraph, but then somebody else in the comments will say that my essay is just talking about a rigid-designation theory of morality. I mean, that's the comment I've gotten multiple times previously. Anyone got a good idea for resolving this?

Haiti today is a situation that makes my moral intuition throw error codes. Population density is three times that of Cuba. Should we be sending aid? It would be kinder to send helicopter gunships and carry out a cull. Cut the population back to one tenth of its current level, then build paradise. My rival moral intuition is that culling humans is always wrong.
Trying to stay concrete and present, should I restrict my charitable giving to helping countries make the demographic transition? Within a fixed aid budget one can choose package A = (save one child, provide education, provide entry into global economy; 30 years later the child, now an adult, feeds his own family and has some money left over to help others)
package B = (save four children; that's it, money all used up, thirty years later there are 16 children needing saving and its not going to happen). Concrete choice of A over B: ignore Haiti and send money to Karuna trust to fund education for untouchables in India, preferring to raise a few children out of poverty by letting other children die.

I don't see how these two frameworks are appealing to different terminal values - they seem to be arguments about which policies maximize consequential lives-saved over time, or maximize QALYs (Quality-Adjusted Life Years) over time. This seem like a surprisingly neat and lovely illustration of "disagreeing moral axioms" that turn out to be about instrumental policies without much in the way of differing terminal values, hence a dispute of fact with a true-or-false answer under a correspondence theory of truth for physical-universe hypotheses.

Obvious further atheist reply to the denial of counterfactuals: If God's desires don't vary across possible worlds there exists a logical abstraction which only describes the structure of the desires and doesn't make mention of God, just like if multiplication-of-apples doesn't vary across possible worlds, we can strip out the apples and talk about the multiplication.

Sure, and to the extent that somebody answers that way, or for that matter runs away from the question, instead of doing that thing where they actually teach you in Jewish elementary school that Abraham being willing to slaughter Isaac for God was like the greatest thing ever and made him deserve to be patriarch of the Jewish people, I will be all like, "Oh, so under whatever name, and for whatever reason, you don't want to slaughter children - I'll drink to that and be friends with you, even if the two of us think we have different metaethics justifying it". I wasn't claiming that accepting the first horn of the dilemma was endorsed by all theists or a necessary implication of theism - but of course, the rejectance of that horn is very standard atheism.

I think I'll go with this as my reply - "Well, imagine that you lived in a monist universe - things would pretty much have to work that way, wouldn't they?"

Haven't you ever heard the saying, "God does not throw dice games"?

Assume the subject of reprogramming is an existing human being, otherwise minimally altered by this reprogramming, i.e., we don't do anything that isn't necessary to switch their motivation to paperclips. So unless you do something gratuitiously non-minimal like moving the whole decision-action system out of the range of introspective modeling, or cutting way down on the detail level of introspective modeling, or changing the empathic architecture for modeling hypothetical selves, the new person will experience themselves as having ineffable 'qualia' associated with the motivation to produce paperclips.
The only way to make it seem to them like their motivational quales hadn't changed over time would be to mess with the encoding of their previous memories of motivation, presumably in a structure-destroying way since the stored data and their introspectively exposed surfaces will not be naturally isomorphic. If you carry out the change to paperclip-motivation in the obvious way, cognitive comparisions of the retrieved memories to current thoughts will return 'unequal ineffable quales', and if the memories are visualized in different modalities from current thoughts, 'incomparable ineffable quales'.
Doing-what-leads-to-paperclips will also be a much simpler 'quale', both from the outside perspective looking at the complexity of cognitive data, and in terms of the internal experience of complexity - unless you pack an awful lot of detail into the question of what constitutes a more preferred paperclip. Otherwise, compared to the old days when you thought about justice and fairness, introspection will show that less questioning and uncertainty is involved, and that there are fewer points of variation among the motivational thought-quales being considered.
I suppose you could put in some extra work to make the previous motivations map in cognitively comparable ways along as many joints as possible, and try to edit previous memories without destroying their structure so that they can be visualized in a least common modality with current experiences. But even if you did, memories of the previous quales for rightness-motivation would appear as different in retrospect when compared to current quales for paperclip-motivation as a memory of a 3D greyscale forest landscape vs. a current experience of a 2D red-and-green fractal, even if they're both articulated in the visual sensory modality and your modal workspace allows you to search for, focus on, and compare commonly 'experienced' shapes between them.

Okay, that? That was one of the most awesome predicates of which I've ever been a subject.


http://lesswrong.com/lw/eva/the_fabric_of_real_things/ (physics)
http://lesswrong.com/lw/f43/proofs_implications_and_models/ (logic)


Can you be more concrete? Some past or present actual situation?

What could it mean for a ghost to exist but be nonphysical?
I think that what you think are counterexamples to GRTm are a large number of things which, examined carefully, would end up in R3-only, and not in R2.
I furthermore note that you just rejected GRTt, which sounds scarily like concluding that actual non-reductionist things exist, because you didn't want to accept the conclusion that talk of non-physical ghosts might fail strict qualifications of meaning. How could you possibly get there from here? How could your thoughts about what's meaningful, entail that the laws of physics must be other than what we'd previously observed them to be? Shouldn't reaching that conclusion require like a particle accelerator or something?
Alternatively, perhaps your rejection of GRTt isn't intended to entail that non-reductionist things exist. If so, can you construe a narrower version of GRTt which just says that, y'know, non-reductionist thingies don't exist? And then would Esar's argument not go through for this version?
I think Esar's argument mainly runs into trouble when you want to call R3-statements 'false', in which case their negations are colloquially true but in R3-only because there's no strictly coherent and meaningful (R2) way to describe what doesn't exist (i.e. non-physical ghosts). If your desire to apply this language demands that you consider these R3-statements meaningful, then you should reject GRTm, I suppose - though not because you disagree with me about what stricter standards entail, but because you want the word "meaningful" to apply to looser standards. However, getting from there to rejecting R1 is a severe problem - though from the description, it's possible you don't mean by GRTt what I mean by R1. I am a bit worried that you might want 'non-physical ghosts don't exist' to be true, hence meaningful, hence its negation to also be meaningful, hence a proposition, hence there to be some state of affairs that could correspond to non-physical ghosts existing, hence for the universe to not be shaped like my R1. Which would be a very strange conclusion to reach starting from the premise that it's 'true' that 'ghosts do not exist'.

Here are three different doctrines:

Expressibility. Everything (or anything) that is the case can in principle be fully expressed or otherwise represented. In other words, an AI is constructible-in-principle that could model every fact, everything that is so. Computational power and access-to-the-data could limit such an AI's knowledge of reality, but basic effability could not.
Classical Expressibility. Everything (or anything) that is the case can in principle be fully expressed in classical logic. In addition to objective ineffability, we also rule out objective fuzziness, inconsistency, or 'gaps' in the World. (Perhaps we rule them out empirically; we may not be able to imagine a world where there is objective indeterminacy, but we at least intuit that our world doesn't look like whatever such a world would look like.)
Logical Physicalism. The representational content of every true sentence can in principle be exhaustively expressed in terms very similar to contemporary physics and classical logic.

Originally I thought that your Great Reductionist Thesis was a conjunction of 1 and 3, or of 2 and 3. But your recent answers suggest to me that for you GRT may simply be Expressibility (1). Irreducibly unclassical truths are ruled out, not by GRT, but by the fact that we don't seem to need to give up principles like Non-Contradiction and Tertium Non Datur in order to Speak Every Truth. And mentalistic or supernatural truths are excluded only insofar as they violate Expressibility or just appear empirically unnecessary.
If so, then we should be very careful to distinguish your confidence in Expressibility from your confidence in physicalism. Neither, as I formulated them above, implies the other. And there may be good reason to endorse both views, provided we can give more precise content to 'terms very similar to contemporary physics and classical logic.' Perhaps the easiest way to give some meat to physicalism would be to do so negatively: List all the clusters that do seem to violate the spirit of physicalism. For instance:

mental (perspectival, 'subjective,' qualia-laden...) facts that cannot be fully expressed in non-mental terms.
otherwise anthropocentric (social, cultural, linguistic...) facts that cannot be fully expressed in non-anthropocentric terms.
spatiotemporal events without spatiotemporal causes
spatiotemporal events without spatiotemporal effects
abstract (non-spatiotemporal) objects that have causes
abstract objects that have effects
(perhaps) ineffable properties or circumstances

A list like this would give us some warning signs that a view, even if logically specifiable, may be deviating sharply from the scientific project. If you precisely stipulated in logical terms how Magic works, for instance, but its mechanism was extremely anthropocentric (e.g., requiring that Latin-language phonemes 'carve at the joints' of fundamental reality), that would seem to violate something very important about reductive physicalism, even if it doesn't violate Expressibility (i.e., we could program an AI to model magical laws of this sort).

What does it mean to consist entirely of mind-stuff when all the actual structure of your universe is logical?

I'm not sure what you mean by 'actual structure.' I would distinguish the Tegmark-style thesis 'the universe is metaphysically made of logic-stuff' from the more modest thesis 'the universe is exhaustively describable using purely logical terms.' If we learned that all the properties of billiard balls and natural numbers are equally specifiable in set-theoretic terms, I think we would still have at least a little more reason to think that numbers are sets than to think that billiard balls are sets.
So suppose we found a way to axiomatize 'x being from the perspective of y,' i.e., a thought and its thinker. If we (somehow) learned that all facts are ultimately and irreducibly perspectival (i.e., they all need an observer-term to be saturated), that might not contradict the expressibility thesis, but I think it would violate the spirit of physicalism.

(Would you consider "The substance of the cracker becomes the flesh of Christ while its accidents remain the same" to be in your equivalent of R2, or only in your equivalent of R3?)

I'm not sure. I doubt our universe has 'substance-accident' structure, but there might be some negative way to R2ify transubstantiation, even if (like epiphenomenalism or events-outside-the-observable-universe) it falls short of verifiability. Could we coherently model our universe as a byproduct of a cellular automaton, while lacking a way to test this model? If so, then perhaps we could model 'substance-properties' as unobservables that are similarly Behind The Scenes, but are otherwise structurally the same as accidents (i.e., observables).

So... in my world, transubstantiation isn't in R2, because I can't coherently conceive of what a substance is, apart from accidents. For a similar reason, I don't yet have R2-language for talking about a universe being metaphysically made of anything. I mean, I can say in R3 that perhaps physics is made of cheese, just like I can say that the natural numbers are made of cheese, but I can't R2-imagine a coherent state of affairs like that. A similar objection applies to a logical universe which is allegedly made out of mental stuff. I don't know how to imagine a logically structured universe being made of anything.
Having Latin-language phonemes carve at the joints of fundamental reality seems very hard, because in my world Latin-language phonemes are already reduced - there's already sequential sound-patterns making them up, and the obvious way to have a logic describing the physics of such a world is to have complex specifications of the phonemes which are 'carving at the joints'. It's not totally clear to me how to make this complex thing a fundamental instead, though perhaps it could be managed via a logic containing enough special symbols - but to actually figure out how to write out that logic, you would have to use your own neuron-composed brain in which phonemes are not fundamental.
I do agree that - if it were possibly to rule out the Matrix, I mean, if spells not only work but the incantation is "Stupefy" then I know perfectly well someone's playing an S-day prank on me - that finding magic work would be a strong hint that the whole framework is wrong. If we actually find that prayers work, then pragmatically speaking, we've received a hint that maybe we should shut up and listen to what the most empirically powerful priests have to say about this whole "reductionism" business. (I mean, that's basically why we're listening to Science.) But that kind of meta-level "no, you were just wrong, shut up and listen to the spiritualist" is something you'd only execute in response to actually seeing magic, not in response to somebody hypothesizing magic. Our ability to hypothesize certain situations that would pragmatically speaking imply we were probably wrong about what was meaningful, doesn't mean we're probably wrong about what's meaningful. More along the lines of, "Somebody said something you thought was in R3(only), but they generated predictions from it and those predictions came true so better rethink your reasons for thinking it couldn't go in R2."
With all that said, it seems to me that R3-possibilities falsifying 1, 2, or (a generalization of 3 to other effectively or formally specified physics (e.g. Time-Turners)), and with the proviso that we're dealing in second-order logic rather than classical first-order logic, all seem to me to pretty much falsify the Great Reductionist Thesis. Some of your potential examples look to me like they're not in my R2 (e.g. mental facts that can't be expressed in non-mental terms) though I'm perfectly willing to discuss them colloquially in R3, and others seem relatively harmless (effects which aren't further causes of anything? I could write a computer program like that). I am hard-pressed to R2-meaningfully describe a state of affairs that falsifies R1, though I can talk about it in R3.
I have an overall agenda of trying to think like reality which says that I want my R1 to look as much like the universe as possible, and it's okay to contemplate restrictions which might narrow my R2 a lot relative to someone's R3, e.g. to say, "I can't seem to really conceive of a universe with fundamentally mental things anymore, and that's a triumph". So a lot of what looked to me years ago like meaningful non-reductionism, now seems more like meaningless non-reductionism relative to my new stricter conceptions of meaning - and that's okay because I'm trying to think less like a human and more like reality.


> Families of 'gigantic finite number' functions found thus far
> Giant finite number functions that could be represented with ludicrous brain-size/resources

These strike me as basically the same thing relative to my imagination. The biggest numbers mathematicians can describe using the fast-growing hierarchy for the largest computable ordinals are already too gigantic to... well... they're already too gigantic. Taking the Ackermann function as primitive, I still can't visualize the Goodstein sequence of 16, never mind 17, and I think that's somewhere around w^(w^2) in the fast-growing hierarchy.
The jump to uncomputable numbers / numbers that are unique models of second-order axioms would still be a large further jump, though.

I have no objection to your description of R3 - basically it's there so that (a) we don't think that something not immediately obviously being in R2 means we have to kick it off the table, and (b) so that when somebody claims their imagination is giving them veridical access to something, we can describe the thing accessed as membership in R3, which in turn is (and should be) too vague for anything else to be concluded thereby; you shouldn't be able to get info about reality merely by observing that you can affirm English utterances.
Insofar as your GRT violations all seem to me to be in R3 and not R2 (i.e., I cannot yet coherently imagine a state of affairs that would make them true), I'm mostly willing to agree that reality actually being that way would falsify GRT and my proposed R2. Unless you pick one of them and describe what you mean by it more exactly - what exactly it would be like for a universe to be like that, how we could tell if it were true - in which case it's entirely possible that this new version will end up in the logic-and-physics R2, and for similar reasons, wouldn't falsify GRT if true. E.g., a version of "nihilism" that is cashed out as "there is no ontologically fundamental reality-fluid", denial of "reference" in which there is no ontologically basic descriptiveness, eliminativism about "logic" which still corresponds to a computable causal process, "relativized" descriptions along the lines of Special Relativity, and so on.
This isn't meant to sneak reductionism in sideways into universes with genuinely ineffable magic composed of irreducible fundamental mental entities with no formal effective description in logic as we know it. Rather, it reflects the idea that even in an intuitive sense, sufficiently effable magic tends toward science, and since our own brains are in fact computable, attempts to cash out the ineffable in greater detail tend to turn it effable. The traditional First-Cause ontologically-basic R3 "God" falsifies reductionism; but if you redefine God as a Lord of the Matrix, let alone as 'natural selection', or 'the way things are', it doesn't. An irreducible soul falsifies GRT, until I interrogate you on exactly how that soul works and what it's made of and why there's still such a thing as brain damage, in which case my interrogation may cause you to adjust your claim and adjust it some more and finally end up in R2 (or even end up with a pattern theory of identity). It should also be noted that while the adjective "effable" is in R2, the adjective "ineffable" may quite possibly be in R3 only (can you exhibit an ineffable thing?)

I intuit that a classically modelable universe that metaphysically consists entirely of mind-stuff (no physics-stuff)

What does it mean to consist entirely of mind-stuff when all the actual structure of your universe is logical? What is the way things could be that would make that true, and how could we tell? This utterance is not yet clearly in my R2, which doesn't have anything in it to describe "metaphysically consists of'". (Would you consider "The substance of the cracker becomes the flesh of Christ while its accidents remain the same" to be in your equivalent of R2, or only in your equivalent of R3?)
PS: I misspelled it, it's http://en.wikipedia.org/wiki/Sensus_divinitatis

My reply to this conversation so far is at:

http://lesswrong.com/lw/frz/mixed_reference_the_great_reductionist_project/8067


This is a reply to the long conversation below between Esar and RobbBB.
Let me first say that I am grateful to Esar and RobbBB for having this discussion, and double-grateful to RobbBB for steelmanning my arguments in a very proper and reasonable fashion, especially considering that I was in fact careless in talking about "meaningful propositions" when I should've remembered that a proposition, as a term of art in philosophy, is held to be a meaning-bearer by definition.
I'm also sorry about that "is meaningless is false" phrase, which I'm certain was a typo (and a very UNFORTUNATE typo) - I'm not quite sure what I meant by it originally, but I'm guessing it was supposed to be "is meaningless or false", though in the context of the larger debate now that I've read it, I would just say "colorless green ideas sleep furiously" is "meaningless" rather than false. In a strict sense, meaningless utterances aren't propositions so they can't be false. In a looser sense, an utterance like "Maybe we're living in an inconsistent set of axioms!" might be impossible to render coherent under strict standards of meaning, while also being colloquially called 'false' meaning 'not actually true' or 'mistaken'.
I'm coming at this from a rather different angle than a lot of existing philosophy, so let me do my best to clarify. First, I would like to distinguish the questions:
R1) What sort of things can be real?
R2) What thoughts do we want an AI to be able to represent, given that we're not certain about R1?
A (subjectively uncertain probabilistic) answer to R1 may be something like, "I'm guessing that only causal universes can be real, but they can be continuous rather than discrete, and in that sense aren't limited to mathematical models containing a finite number of elements, like finite Life boards."
The answer to R2 may be something like, "However, since I'm not sure about R1, I would also like my AI to be able to represent the possibility of a universe with Time-Turners, even though, in this case, the AI would have to use some generalization of causal reference to refer to the things around it, since it wouldn't live in a universe that runs on Pearl-style causal links."
In the standard sense of philosophy, question R2 is probably the one about 'meaning' or which assertions can be 'meaningful', although actually the amount of philosophy done around this is so voluminous I'm not sure there is a standard sense of 'meaning'. Philosophers sometimes try to get mileage out of claiming things are 'conceivable', e.g., the philosophical catastrophe of the supposed conceivability of P-zombies, and I would emphasize even at this level that we're not trying to get R1-mileage out of things being in R2. For example, there's no rule following from anything we've said so far that an R2-meaningful statement must be R1-possible, and to be particular and specific, wanting to conservatively build an AI that can represent Conway's Game of Life + Time-Turners, still allows us to say things like, "But really, a universe like that might be impossible in some basic sense, wihch is why we don't live there - to speak of our possibly living there may even have some deeply buried incoherence relative to the real rules for how things really have to work - but since I don't know this to be true, as a matter of my own mere mental state, I want my AI to be able to represent the possibility of time-travel." We might also imagine that a non-logically-omniscient AI needs to have an R2 which can contain inconsistent axiom sets the AI doesn't know to be inconsistent.
For things to be in R2, we want to show how a self-modifying AI could carry out its functions while having such a representation, which includes, in particular, being able to build an offspring with similar representations, while being able to keep track of the correspondence between those offspring's quoted representations and reality. For example, in the traditional version of P-zombies, there's a problem with 'if that was true, how could you possibly know it?' or 'How can you believe your offspring's representation is conjugate to that part of reality, when there's no way for it to maintain a correspondence using causal references?' This is the problem of a SNEEZE_VAR in the Matrix where we can't talk about whether its value is 0 or 1 because we have no way to make "0" or "1" refer to one binary state rather than the other.
Since the problems of R2 are the AI-conjugates of problems of reference, designation, maintainance of a coherent correspondence, etcetera, they fall within the realm of problems that I think traditional philosophy considers to be problems of meaning.
I would say that in human philosophy there should be a third issue R3 which arises from our dual desire to:

Not do that awful thing wherein somebody claims that only causal universes can be real and therefore your hypotheses about Time-Turners are meaningless noises.
Not do that awful thing wherein somebody claims that since P-zombies are "conceivable" we can know a priori that consciousness is a non-physical property.

In other words, we want to avoid the twin errors of (1) preemptively shooting down somebody who is making an honest effort to talk to us by claiming that all their words are meaningless noises, and (2) trying to extract info about reality just by virtue of having an utterance admitted into a debate, turning a given inch into a taken mile.
This leads me to think that human philosophers should also have a third category R3:
R3) What sort of utterances can we argue about in English?
which would roughly represent what sort of things 'feel meaningful' to a flawed human brain, including things like P-zombies or "I say that God can make a rock so heavy He can't lift it, and then He can lift it!" - admitting something into R3 doesn't mean it's logically possible, coherent, or 'conceivable' in some rigorous sense that you could then extract mileage from, it just means that we can go on having a conversation about it for a while longer.
When somebody comes to us with the P-zombie story, and claims that it's "conceivable" and they know this on account of their brain feeling able to conceive it, we want to reply, "That's what I would call 'arguable' (R3) and if you try to treat your intuitions about arguability as data, they're only directly data about which English sentences human brains can affirm. If you want to establish any stronger sense of coherence that you could get mileage from, such as coherence or logical possibility or reference-ability, you'll have to argue that separately from your brain's direct access to the mere affirmability of a mere English utterance."
At the same time, you're not shoving them away from the table like you would "colorless green ideas sleep up without clam any"; you're actually going to have a conversation about P-zombies, even though you think that in stricter senses of meaning like R2, the conversation is not just false but meaningless. After all, you could've been wrong about that nonmembership-in-R2 part, and they might be about to explain that to you.
The Great Reductionist Thesis is about R1 - the question of what is actually real - but it's difficult to have something that lies in a reductionist's concept of a strict R2, turn out to be real, such that the Great Reductionist Thesis is falsified. For example, if we think R1 is about causal universes, and then it turns out we're in Timetravel Life, the Great Reductionist Thesis has been confirmed, because Timetravel Life still has a formal logical description. Just about anything I can imagine making a Turing-computable AI refer to will, if real, confirm the Great Reductionist Thesis.
So is GRT philosophically vacuous from being philosophically unfalsifiable? No: to take an extreme case, suppose we have an uncomputable and non-logically-axiomatizable sensus divinatus enabling us to directly know God's existence, and by baptizing an AI we could give it this sensus divinatus in some way integrated into the rest of its mind, meaning that R2, R1, and our own universe all include things referrable-to only by a sensus divinatus. Then arguable utterances along the lines of, "Some things are inherently mysterious", would have turned out, not just to be in R2, but to actually be true; and the Great Reductionist Thesis would be false - contrary to my current belief that such utterances are not only colloquially false, but even meaningless for strict senses of meaning. But one is not licensed to conclude anything from my having allowed a sensus divinatus to be a brief topic of conversation, for by that I am not committing to admitting that it was strictly meaningful under strong criteria such as might be proposed for R2, but only that it stayed in R3 long enough for a human brain to say some informal English sentences about it.
Does this mean that GRT itself is merely arguable - that it talks about an argument which is only in R3? But tautologies can be meaningful in GRT, since logic is within "physics + logic". It looks to me like a completed theory of R2 should be something like a logical description of a class of universes and a class of representations corresponding to them, which would itself be in R2 as pure math; and the theory-of-R1 "Reality falls within this class of universes" could then be physically true. However, many informal 'negations' of R2 like "What about a sensus divinatus?" will only be 'arguable' in a human R3, rather than themselves being in R2 (as one would expect!).

This is a problem known as "bad writing" which I continue to struggle with, even after many years. Can you list the first part where you felt lost? Somewhere between there and the previous part, I must have skipped something.
I do hope people appreciate that all the "blindingly obvious" parts are parts where (at least in my guesstimation, and often in my actual experience) somebody else would otherwise get lost. The "obvious" is not the same for all people.

Yes.

Experience + logic != physics + logic > causality + logic

Okay. I'll bet with somewhere around 50% probability that the Great Reductionist Project as I've described it works, with reduction to a single thing counting as success, and requiring magical reality-fluid counting as failure. I'll bet with 95% probability that it's right on the next occasion for anthropics and magical reality-fluid, and with 99+ probability that it's right on the next occasion for things that confuse me less; except that when it comes to e.g. free will, I don't know who I'd accept as a judge that didn't think the issue already settled.

It does indeed seem possible that in the long run we'll end up with one kind of stuff, either from the reduction of logic to physics, or the reduction of physics to math. It's also worth noting that my present model does have magical-reality-fluid in it, and it's conceivable that this will end up not being reduced. But the actual argument is something along the lines of, "We got it down to two crisp things, and all the proposals for three don't have the crisp nature of the two."

Always two there are. No more. No less.



I don't know how we'd settle a wager about a counterfactual.

I guess this is my main issue with the whole sequence.

That's the point of this post. Only causal models can be settled. Counterfactuals cannot be observed, and can only be derived as logical constructs via axiomatic specification from the causal models which can be observed.

After it's been right the last 300 times or so, we should assess a substantial probability that it will be wrong before the 1,000th occasion, but believe much more strongly that it will be correct on the next occasion.

Not to be obnoxious, but...

You're only supposed to have two kinds of stuff.

Why two?
ETA: I feel like I may have distracted from the thrust of the post. I think the main point was that there really really probably shouldn't be more then two stuffs, which is legit.

Is there a good statement of the "mature neopositivist" / Carnap's position?

Hence my careful specification that you're multiplying the numbers, not the piles.

I don't see anything similar to this post on a quick skim of http://plato.stanford.edu/entries/logical-empiricism/ . Please specify.

The claim might just need correction to say, "Many philosophers say that simplicity is a good thing but the requirement is not enforced very well by philosophy journals" or something like that. I think I believe you, but do you have an example citation anyway? (SEP entries or other ungated papers are in general good; I'm looking for an example of an idea being criticized due to lack of metaphysical parsimony.) In particular, can we find e.g. anyone criticizing modal logic because possibility shouldn't be basic because metaphysical parsimony?

Mainstream status:
AFAIK, the proposition that "Logical and physical reference together comprise the meaning of any meaningful statement" is original-as-a-whole (with many component pieces precedented hither and yon). Likewise I haven't elsewhere seen the suggestion that the great reductionist project is to be seen in terms of analyzing everything into physics+logic.
An important related idea I haven't gone into here is the idea that the physical and logical references should be effective or formal, which has been in the job description since, if I recall correctly, the late nineteenth century or so, when mathematics was being axiomatized formally for the first time. This pat is popular, possibly majoritarian; I think I'd call it mainstream. See e.g. http://plato.stanford.edu/entries/church-turing/ although logical specifiability is more general than computability (this is also already-known).
Obviously and unfortunately, the idea that you are not supposed to end up with more and more ontologically fundamental stuff is not well-enforced in mainstream philosophy.


Ultimately, they tend to go along with the Experimenter if he justifies their actions in terms of the scientific benefits of the study (as he does with the prod "The experiment requires that you continue") [39]. But if he gives them a direct order ("You have no other choice, you must go on") participants typically refuse.

This seems very suspicious because I remember offhand that the former is an early prompt and the second one is the final prompt given only after the other prompts have been exhausted.

I'm trying to think like reality. If causality isn't a special kind of logic, why is everything in the known universe made out of (a continuous analogue of) causality instead of logic in general? Why not Time-Turners or a zillion other possibilities?

Warning: Annealing sounds impressive if you're new to machine learning but AFAIK it's pretty weak as machine-learning methods go, and the real algorithms at work in human creativity are going to involve algorithms more powerful than that - albeit something on the level of 'cognitive temperature' might still be playing a partial role somewhere.

Unconscious annealing of connections between concepts.


Hmm, re-reading Timeless Causality, I don't see how I could have learned that the idea belongs to Barbour and that you disagree with him. It sure sounds like it was your idea.

This sounds like a high-priority problem, but actually I don't see any reference to reduction-to-similarity in Timeless Causality, although there's a lot in Barbour's book about it. What do you mean by "mind reduces to computation which reduces to causal arrows which reduces to some sort of similarity relationship between configurations"? Unless this is just in the sense that causal mechanisms are logical relations?

You realize I'm arguing against do()-based causality? If not, I was very much unclearer than I thought.
I have never tried to reduce causal arrows to similarity; Barbour does, I don't. I take causality to be, or be the epistemic conjugate of, something physical and real which was involved in manufacturing this oddly-well-modeled-by-causality universe that we actually live in. They are presently primitive in my model; I have not yet reduced them, except in the obvious sense that they are also formal mathematical relations between points, i.e., causal relations are a special case of logical relations (and yet we still live in a causal universe rather than a merely logical one). I do indeed reduce consciousness to computation and computation to causality, though there's a step here involving magical reality-fluid about which I am still confused - I have no idea why or what it means for a causal process to be more or less real, either as a result of having more or less Born measure, being instantiated in many places, or for any other reason.

If causality is' useful fiction, it's conjugate to some useful nonfiction; I should like to know what the latter is.
I don't think Pearl's diagrams are defined via do(). I think I disagree with that statement even if you can find Pearl making it. Even if do() - as shorthand for describing experimental procedures involving switches on arrows - does happen to be a procedure you can perform on those diagrams, that's a consequence of the definition, it is not actually part of the representation of the actual causal model. You can write out causal models, and they give predictions - this suffices to define them as hypotheses.
More importantly: How can you possibly make the truth-condition be a correspondence to counterfactual universes that don't actually exist? That's the point of my whole epistemology sequence - truth-conditions get defined relative to some combination of physical reality that actually exists, and valid logical consequences pinned down by axioms. So yes, I would definitely derive do() rather than have it being primitive, and I wouldn't ever talk about the truth-condition of causal models relative to a do() out there in the environment - we talk about the truth-condition of causal models relative to quarks and electrons and quantum fields, to reality.
I'm a bit worried (from some of his comments about causal decision theory) that Pearl may actually believe in free will, or did when he wrote the first edition of Causality. In reality nothing is without parents, nothing is physically uncaused - that's the other problem with do().

Ah, right.

Interesting. Sorry to bother you further, but can I ask you to quote a particular sentence or paragraph above that seems unclear? Or was the above clear, but it implies other questions that aren't clear, or the motivations aren't clear?


a -> b -> c -> d, with a <- u1 -> c, and a <- u2 -> d, where we do not observe u1,u2, and u1,u2 are very complicated, then we can figure out the true graph exactly by independence type techniques without having to observe u1 and u2. Note: the marginal distribution p(a,b,c,d) that came from this graph has no conditional independences at all (checkable by d-separation on a,b,c,d), so typical techniques fail.

Irrelevant question: Isn't (b || d) | a, c?

Well, this is very rapidly getting us into complex territory that future decision-theory posts will hopefully explore, but a very brief answer would be that I am unwilling to define anything fundamental in terms of do() operations because our universe does not contain any do() operations, and counterfactuals are not allowed to be part of our fundamental ontology because nothing counterfactual actually exists and no counterfactual universes are ever observed. There are quarks and electrons, or rather amplitude distributions over joint quark and lepton fields; but there is no do() in physics.
Causality seems to exist, in the sense that the universe seems completely causally structured - there is causality in physics. On a microscopic level where no "experiments" ever take place and there are no uncertainties, the microfuture is still related to the micropast with a neighborhood-structure whose laws would yield a continuous analogue of D-separation if we became uncertain of any variables.
Counterfactuals are human hypothetical constructs built on top of high-level models of this actually-existing causality. Experiments do not perform actual interventions and access alternate counterfactual universes hanging alongside our own, they just connect hopefully-Markov random numbers into a particular causal arrow.
Another way of saying this is that a high-level causal model is more powerful than a high-level statistical model because it can induct and describe switches, as causal processes, which behave as though switching arrows around, and yields predictions for this new case even when the settings of the switches haven't been observed before. This is a fancypants way of saying that a causal model lets you throw a bunch of rocks at trees, and then predict what happens when you throw rocks at a window for the first time.


1) If we ask whether the entities embedded in strings watched over by the self-consistent universe detector really have experiences, aren't we violating the anti-zombie principle?

We're not asking if they have experiences; obviously if they exist, they have experiences. Rather we're asking if their entire universe gains any magical reality-fluid from our universe simulating it (e.g., that mysterious stuff which, in our universe, manifests in proportion to the integrated squared modulus in the Born probabilities) which will then flow into any conscious agents embedded within.
Sadly, my usual toolbox for dissolving questions about consciousness does not seem to yield results on reality-fluid as yet - all thought experiments about "What if I simulate / what if I see..." either don't vary with the amount of reality-fluid, or presume that the simulating universe exists in the first place.
There are people who claim to be less confused about this than I am. They appear to me to be jumping the gun on what constitutes lack of confusion, and ought to be able to answer questions like e.g. "Would straightforwardly simulating the quantum wavefunction in sufficient detail automatically give rise to sentients experiencing outcomes in proportion to the Born probabilities, i.e., reproduce our current experience?" by something other than e.g. "But people in branches like ours will have utility functions that go by squared modulus" which I consider to be blatantly silly for reasons I may need to talk about further at some point.

"Death" is the absence of a future self that is continuous with your present self. I don't know exactly what constitutes "continuous" but it clearly is not the identity of individual particles. It may require continuity of causal derivation, for example.

(a) You don't need to observe confounders to learn structure from data. In fact, sometimes you don't need any standard conditional independence at all. (Luke gave me the impression SI wasn't very interested in that point -- maybe it should be).
(b) Occam's razor / faithfulness gives you enough to learn the structure of statistical models, not causal ones. You need additional assumptions to equate the statistical models you learn with causal models. Bayesian networks are not causal models. Causality is not about conditional independence, it is about counterfactual invariance, that is causality expresses what changes or stays the same after a hypothetical 'wiggle.'
There is no guarantee that even given Occam's razor and faithfulness being true that the graph you obtain is such that if I wiggle a parent, the child will change. To verify your causal assumptions, you have to run an experiment, or no scientist will believe your graph is causal. This is what real causal discovery papers do, for example:
http://www.sciencemag.org/content/308/5721/523.abstract
Here they learned a protein signaling network, but then implemented an experiment where they changed the protein level of a parent via an RNA molecule, and verified that the child changed, but parent of a parent did not change.

I am sure you can set up a Bayesian story for this entire enterprise, if you wanted. But, firstly, this Bayesian story would not be expressed purely in probability theory but in the language that can express counterfactual invariance and talk about experiments (for example language of potential outcomes or do(.)). And secondly, giving something a Bayesian story is sort of equivalent to re-expressing some complicated program as a vi macro. Could be done (vi is turing-complete!) but why? People don't write practical code in vi macros.

This sounds like we're talking past each other somehow. Your point (a) is not clear to me - I was saying that to learn a sufficiently high-probability causal model from non-intervention data, you need to have observed the data in sufficient detail to rule out confounders (except at some low probability) (via simplicity priors, which otherwise can't drive down the probability of an untestable invisible confounder by all that far). This can certainly be done in principle, e.g. if you put the system under a microscope with a higher resolution than the system, and verified there were only X kinds of stuff in it and no others.
Your point (b) sounds just plain wrong to me. If you have a simplicity prior over causal models, and you can derive testable probable predictions from causal models, then you can do Bayesian updating and get a posterior over causal models. Substituting the word "flammable fizzbins" for "causal models" in the preceding sentence will produce another true sentence. I think you mean something different by "Bayesian" and "Occam's Razor" than I do.

Right! Besides just Bayes's Theorem, you'd also need Occam's Razor as a simplicity prior over causal structures. And, to drive the probability of a causal structure high enough, confidence that you'd observed in sufficient detail to drive down the probability of extra confounding or intervening variables.
Since the latter part is sometimes difficult though not theoretically impossible to achieve in fields like medicine, a randomized experiment in which you trust that your random numbers will probably have the Markov condition relative to other background variables, can more quickly give you confidence about some directions on causal arrows when the combination of effect size and sample size is large enough. Naturally, all of this is a mere special case of Bayesian reasoning on possible causal structures where (1) you start out very confident that some random numbers are conditionally independent of all their non-descendants in the graph, and (2) you start out very confident that your randomized experimental procedure causally connects to a single descendant node in that graph (the independent variable).

If simulating things doesn't add measure to them, why do you believe you're not a Boltzmann brain just because lawful versions of you are much more commonly simulated by your universe's physics?

Nope. It's just a terrible name.

The Novikov self-consistency principle has already been invented; the question was whether there was precedent for "You can actually compute consistent histories for discrete universes." Discrete, not continuous.

Obviously physicists totally know about causality being restricted to the light cone! And "curvature of space = light cones at each point" isn't Penrose, it's standard General Relativity.

Rowling is on record as stating that prophecies can be just-walked-away-from which makes their dynamics less clear and not obviously a self-consistency thing.

Odd, the last paragraph of the above seems to have gotten chopped. Restored. No, I haven't particularly heard anyone else point that out but wouldn't be surprised to find someone had. It's an important point and I would also like to know if anyone has developed it further.

Er, I was not claiming to have invented the notion of an equilibrium but thank you for pointing this out.

Mainstream status:
I haven't yet particularly seen anyone else point out that there is in fact a way to finitely Turing-compute a discrete universe with self-consistent Time-Turners in it. (In fact I hadn't yet thought of how to do it at the time I wrote Harry's panic attack in Ch. 14 of HPMOR, though a primary literary goal of that scene was to promise my readers that Harry would not turn out to be living in a computer simulation. I think there might have been an LW comment somewhere that put me on that track or maybe even outright suggested it, but I'm not sure.)
The requisite behavior of the Time Turner is known as Stable Time Loops on the wiki that will ruin your life, and known as the Novikov self-consistency principle to physicists discussing "closed timelike curve" solutions to General Relativity. Scott Aaronson showed that time loop logic collapses PSPACE to polynomial time.
I haven't yet seen anyone else point out that space and time look like a simple generalization of discrete causal graphs to continuous metrics of relatedness and determination, with c being the generalization of locality. This strikes me as important, so any precedent for it or pointer to related work would be much appreciated.

Meditation:
Suppose you needed to assign non-zero probability to any way things could conceivably turn out to be, given humanity's rather young and confused state - enumerate all the hypotheses a superintelligent AI should ever be able to arrive at, based on any sort of strange world it might find by observation of Time-Turners or stranger things. How would you enumerate the hypothesis space of all the coherently-thinkable worlds we could remotely maybe possibly be living in, including worlds with Stable Time Loops and even stranger features?

The main split between the human cases and the AI cases is that the humans are 'wireheading' w.r.t. one 'part' or slice through their personality that gets to fulfill its desires at the expense of another 'part' or slice, metaphorically speaking; pleasure taking precedence over other desires. Also, the winning 'part' in each of these cases tends to be a part which values simple subjective pleasure, winning out over parts that have desires over the external world and desires for more complex interactions with that world (in the experience machine you get the complexity but not the external effects).
In the AI case, the AI is performing exactly as it was defined, in an internally unified way; the ideals by which it is called 'wireheaded' are only the intentions and ideals of the human programmers.
I also don't think it's practically possible to specify a powerful AI which actually operates to achieve some programmer goal over the external world, without the AI's utility function being explicitly written over a model of that external world, as opposed to its utility function being written over histories of sensory data.
Illustration: In a universe operating according to Conway's Game of Life or something similar, can you describe how to build an AI that would want to actually maximize the number of gliders, without that AI's world-model being over explicit world-states and its utility function explicitly counting gliders? Using only the parts of the universe that directly impinge on the AI's senses - just the parts of the cellular automaton that impinge on the AI's screen - can you find any maximizable quantity that corresponds to the number of gliders in the outside world? I don't think you'll find any possible way to specify a glider-maximizing utility function over sense histories unless you only use the sense histories to update a world-model and have the utility function be only over that world-model, and even then the extra level of indirection might open up a possibility of 'wireheading' (of the AI's real operation vs. programmer-desired glider-maximizing operation) if any number of plausible minor errors were made.

Definition: An agent is an algorithm that models the effects of (several different) possible future actions on the world and performs the action that yields the highest value according to some evaluation procedure.

The word "value" seems unnecessarily value-laden here.
Alternatively: A consequentialist agent is an algorithm with causal connections both to and from the world, which uses the causal effect of the world upon itself (sensory data) to build a predictive model of the world, which it uses to model the causal outcomes of alternative internal states upon the world (the effect of its decisions and actions), evaluates these predicted consequences using some algorithm and assigns the prediction an ordered or continuous quantity (in the standard case, expected utility), and then decides an action corresponding to expected consequences which are thresholded above, relatively high, or maximal in this assigned quantity.
Simpler: A consequentialist agent predicts the effects of alternative actions upon the world, assigns quantities over those consequences, and chooses an action whose predicted effects have high value of this quantity, therefore operating to steer the external world into states corresponding to higher values of this quantity.

I don't mean to trivialize any problems she may have gone through but at least on a first reading that sounds awesome.
I mean, I'm sure it wasn't but it still sounds that way.


I suggest male readers ruminate on this aspect of the show until it seems a bit disturbing.

Er... what if it still doesn't seem disturbing after rumination?

The positions of power are occupied by females.

Discord is male, more powerful than the Princesses, and evil.
Er, I don't seem to be finding this very disturbing either.
(Admittedly, I haven't actually watched the show, only read fanfiction based on it.)

Correct. SIs that only terminally care about a single green button on Earth instrumentally care about optimizing the rest of the universe to prevent anything from threatening that button.

I looked up 'sex' in the Encyclopedia Britannica.

They're entirely different fields of math, and for good reason (most decisions are not about adversarial games). Plugging surreal numbers into decision theory as (probabilistic) utility-weights is a completely different project from their standard use in (deterministic) game theory to determine who wins a game.

If a human line of descent can't do that, why should an immortal be able to do that?

Game theory motivated surreal numbers. Game theory != decision theory.


you seemed to be saying that you did not want to expend the mental energy to diet and get and stay thin

Reread. I was and am saying that mental energy doesn't work to do that. There is no known procedure for "become thin" except Adipotide.

Actually, this seems a lot less disturbing to me than if, say, there were many different colors for boy clothes, but only pink clothing for girls. If you wouldn't feel obliged to avoid dressing a baby boy in blue, why feel obliged to avoid dressing a baby girl in pink? None of this has the moral that gender differences in general should be downplayed; it's when you start saying that male-is-default or 'people can be nerds but girls have to be girls' that you have a problem. In general, I think the mode of thought to be fought is that males are colorless and women have color; or to put it another way, the deadly thought is that there are all sorts of different people in the world like doctors, soldiers, mathematicians, and women. I do sometimes refer in my writing to a subgroup of people called "females"; but I refer to another subgroup, "males", about equally often. (Actually, I usually call them "women" and "males" but that's because if you say "men", males assume you're talking about people.)

I think the concept is that content is included from trusting volunteers who were told to expect Crocker's Rules in the audience, and if you're not willing to abide by that trust, you shouldn't read.

You can't, but you can notice that they're being incoherent if they simultaneously claim to know little about the future and to be able to set strong lower bounds on technology development times.

Role-acting that conflates lack of info with skepticism. If you're acting out the role of a dutifully skeptical scientist, you say that things are a long way away unless you have strong info that they're about to happen, for fear of making a mistaken prediction that makes you sound like a mere enthusiast. Failed negative predictions don't count against the role - they don't reduce your prestige to that of a mere enthusiast. Just imagine it as a serious frown vs. a childish smile; they're trying to give a serious frown, and avoid childish smiles. You're dealing with a behavioral rule for when to agree with verbal statements, not a coherent state of uncertainty. Both "we know very little about the future" and "AI is fifty years off" sound like a serious frown, so they agree with both.
I doubt there's very much more to it than that.

They exist, but it's like this: you walk into the store. To your left, there are forty pink dresses and onesies with Cutest Princess or somesuch printed on them. To your right, there are forty blue onesies and overall combos, often with anthropomorphic male animals printed on them. In the middle, there are three yellow or green onesies.
On top of that, well-meaning relatives send us boxes of the pink dresses.
When I dress her, I avoid the overtly feminine outfits. But then I worry that I'm committing an entirely new mistake. I imagine my daughter telling me how confused she felt that her father seemed reluctant to cast her as a girl. "Did you wish I was a boy, Daddy?" There don't seem to be many trivially obvious correct choices in parenting.

I don't know what the hell your mental model of me is like, but I can't eat the tasty things that normal people around me eat, on pain of blowing up like a balloon. If you go to the SIAI office, you'll see a lot of thin people eating chocolate, candy bars, chocolate-coated nuts, and so on - just their normal way of getting energy for a workday - and me drinking protein-powder in water.


For example, instead of seeking behavior modification through threats, such a mind seeks justice through retribution.

Agreed that this is a different case, since it doesn't originate in any expectation of behavior modification.


Not blackmailing in response to that anticipation is a property of the behavior of the blackmailers that seems to have been used in deciding to ignore all blackmail.

Expecting a response to blackmail in the first place is why blackmailers would even exist in the first place.

Suppose there were lots of "stupid" blackmailers around that blackmailed everyone all day, even if no victim ever conformed.

Why would these exist any more than stupid anti-blackmailers (who e.g. go around attacking anyone who would give into blackmail if a blackmailer showed up), if not for a belief that somebody would give in to blackmail?


I was thinking along the lines of the blackmailer using the same reasoning to decide that whether or not the potential victim of blackmail would be a blackmail ignorer or not, the blackmailer would still blackmail regardless.

Only if you expect that the blackmail victim has not "already" decided that if the blackmailer does that, they will still ignore the blackmail regardless. Wise agents ignore order-0 blackmail, ignore order-1 blackmail in which the blackmailer decides to ignore their ignorance of order-0 blackmail, ignore order-omega blackmail in which the blackmailer decides to ignore all order-N refusals to be blackmailed, etcetera for all ordinals. If there is some ordinal of blackmail you do not ignore, you can be blackmailed, and how does that help?

This sort of thing would seem to lead to an equilibrium of lots of blackmailers blackmailing victims that will ignore them.

Only if those blackmailers have wrongly anticipated that their victims will be stupid enough to conform.


For a more concrete example of how this might work, suppose I steal one cent each from one billion different people, and Eliezer steals $100,000 from one person. The total amount of money I have stolen is greater than the amount that Eliezer has stolen; yet my victims will probably never even realize their loss, whereas the loss of $100,000 for one individual is significant. A cent does have a nonzero amount of purchasing power, but none of my victims have actually lost the ability to purchase anything; whereas Eliezer's, on the other hand, has lost the ability to purchase many, many things.

Isn't this a reductio of your argument? Stealing $10,000,000 has less economic effect than stealing $100,000, really? Well, why don't we just do it over and over, then, since it has no effect each time? If I repeated it enough times, you would suddenly decide that the average effect of each $10,000,000 theft, all told, had been much larger than the average effect of the $100,000 theft. So where is the point at which, suddenly, stealing 1 more cent from everyone has a much larger and disproportionate effect, enough to make up for all the "negligible" effects earlier?
See also: http://lesswrong.com/lw/n3/circular_altruism/

Well, sure, if the blackmail victim were silly enough to reason "I have nothing to gain by ignoring it" if the blackmailer went through anyway, then the blackmailer would indeed decide to ignore their decision to ignore it and go through anyway. But that's only if the blackmail victim is that silly.
In a problem like this, the "do nothing" side has the advantage; there's nothing the other side can do to make them be responsive and blackmailable. That's why I expect TDT to resolve to a blackmail-free equilibrium.

In fact, this behavior is so dreadful that the revenge-killing of humans would send a trust signal.


11 You might be in a computer simulation in which if you attempt to destroy humanity you will yourself be destroyed by your human simulators.

The problem with this is that it's a blackmail attempt, and I expect timeless decision theory to resolve to a no-blackmail equilibrium.
Suppose that AIs, however "irrationally" from the perspective of a CDT agent, decide to simply ignore this possibility in all their calculations. Even if we "irrationally" decide to construct such a simulation anyway, they'll "irrationally" decide to ignore it. Then we have no remaining motive to construct such elaborate simulations. This, indeed, is the condition that makes it "blackmail" - you're expending resources to produce a behavior response leading to an outcome that doesn't benefit the other agent relative to the null outcome if you didn't expend such resources, so it would be fundamentally vulnerable and silly of that agent to think in a way that would produce such behavior shifts in response to your own strategy. So it won't think that way. So the whole attempt at blackmail fails before it starts.
12 is pure obvious anthropomorphic wishful thinking.


If you destroy mankind you might someday encounter an alien super-intelligence that will lack trust in you in part because you destroyed mankind.

By the same argument, we might someday encounter an alien superintelligence that will lack trust in us in part because we domesticate animals (breeding them to not fear us) and then kill and eat them.

Politics mindkilled him; he cannot separate the normative and the descriptive.

He is? Since when?

Yep. My standard go-to on nearest mainstream metaethical philosophy to LW is Frank Jackson's analytic descriptivism / moral functionalism.

Upvoted for "Proofs and Refutations" reference

I'm saying, "Show me something in particular that I should've looked at, and explain why it matters; I do not respond to non-specific claims that I should've paid more homage to whatever."

Blog each time you have sex. This makes you more likely to remember histories in which you had sex, and possibly even makes you more likely to experience future occasions where you had sex. If a friend gets sick, don't mention anything anywhere an electronic record can find it, or the whole thing might very well have been your fault! And for a different variant on the same principle, wait for a large lottery jackpot, buy a ticket using a quantum random number generator, use a webcam to record yourself watching the numbers being drawn, and precommit that if you win, you will donate 10% of the money to workshops on timeless decision theory providing that they agree to play the video at the opening session of that conference.

Give me an example of a specific thing relevant to constructing an AI which I should have referenced, plus the role it plays in a (self-modifying) AI. Keep in mind that I only care about constructing self-modifying AIs and not about "what is the bearer of truth".
I've read works-not-referenced on "meaning", they just don't seem relevant to anything I care about. Though obviously there's quite a lot of standard work on mathematical proof that I care about (some small amount of which I've referenced).

It describes how to compute that-which-should-be-optimized.

The standard story is that everything mathematicians prove is to be interpreted as a statement in the language of ZFC, with ZFC itself being interpreted in first-order logic. (With a side-order of angsting about how to talk about e.g. "all" vector spaces, since there isn't a set containing all of them -- IMO there are various good ways of resolving this, but the standard story considers it a problem; certainly in so far as SOL provides an answer to these concerns at all, it's not "the one" answer that everybody is obviously implicitly using.) So when they say that there's only one field of real numbers, this is supposed to mean that you can formalize the field axioms as a ZFC predicate about sets, and then prove in ZFC that between any two sets satisfying this predicate, there is an isomorphism. The fact that the semantics of first-order logic don't pin down a unique model of ZFC isn't seen as conflicting with this; the mathematician's statement that there is only one complete ordered field (up to isomorphism) is supposed to desugar to a formal statement of ZFC, or more precisely to the meta-assertion that this formal statement can be proven from the ZFC axioms. Mathematical practice seems to me more in line with this story than with yours, e.g. mathematicians find nothing strange about introducing the reals through axioms and then talk about a "neighbourhood basis" as something that assigns to each real number a set of sets of real numbers -- you'd need fourth-order logic if you wanted to talk about neighbourhood bases as objects without having some kind of set theory in the background. And people who don't seem to care a fig about logic will use Zorn's lemma when they want to prove something that uses choice, which seems quite rooted in set theory.
Now I do think that mathematicians think of the objects they're discussing as more "real" than the standard story wants them to, and just using SOL instead of FOL as the semantics in which we interpret the ZFC axioms would be a good way to, um, tell a better story -- I really like your post and it has convinced me of the usefulness of SOL -- but I think if we're simply trying to describe how mathematicians really think about what they're doing, it's fairer to say that they're just taking set theory at face value -- not thinking of set theory as something that has axioms that you formalize in some logic, but seeing it as as fundamental as logic itself, more or less.

Um, I think when an ordinary mathematician says that there's only one complete ordered field up to isomorphism, they do not mean, "In any given model of ZFC, of which there are many, there's only one ordered field complete with respect to the predicates for which sets exist in that model." We could ask some normal mathematicians what they mean to test this. We could also prove the isomorphism using logic that talked about all predicates, and ask them if they thought that was a fair proof (without calling attention to the quantification over predicates).
Taking set theory at face value is taking SOL at face value - SOL is often seen as importing set theory into logic, which is why mathematicians who care about it all are sometimes suspicious of it.

If you sign up for cryonics, your "last" moments should have causal continuity with a much larger measure of futures than the QTI ones, so it should be prophylactic against quantum immortality. I hope.


You take some random 20 bit number and say that you will flip the equipment 20 times and if the outcome is the same as the predetermined number, then you will take it as a one to million evidence that the Multiple World theory works as expected.

That doesn't convince anyone else; from their perspective, in Bayesian terms, the experiment has the same million-to-one improbability of producing this result, regardless of whether QTI is true, since they're not dying in the other worlds. From your perspective, you've ended up in a world where you experienced what feels like strong evidence of QTI being true, but you can never communicate this evidence to anyone else. If we hook up a doomsday device to the coinflipper, then in worlds where we survive, we can never convince aliens.


You mean you're not?

I'm signed up for cryonics. I'm a bit worried about what happens to everyone else.
Going on the basic anthropic assumption that we're trying to do a sum over conditional probabilities while eliminating Death events to get your anticipated future, then depending on to what degree causal continuity is required for personal identity, once someone's measure gets small enough, you might be able to simulate them and then insert a rescue experience for almost all of their subjective conditional probability. The trouble is if you die via a route that degrades the detail and complexity of your subjective experience before it gets small enough to be rescued, in which case you merge into a lot of other people with dying experiences indistinguishable from yours and only get rescued as a group. Furthermore, anyone with computing power can try to grab a share of your soul and not all of them may be what we would consider "nice", just like if we kindly rescued a Babyeater we wouldn't go on letting them eat babies. As the Doctor observes of this proposition in the Finale of the Ultimate Meta Mega Crossover, "Hell of a scary afterlife you got here, missy."
The only actual recommendations that emerge from this set of assumptions seem to amount to:
1) Sign up for cryonics. All of your subjective future will continue into quantum worlds that care enough to revive you, without regard for worlds where the cryonics organization went bankrupt or there was a nuclear war.
2) If you can't be suspended, try to die only by routes that kill you very quickly with certainty, or (this is possibly better) kill almost all of your measure over a continuous period without degrading your processing power. In other words, the ideal disease has a quantum 50% probability of killing you while you sleep, but has no visible effects when you wake up, and finally kills you with certainty after a couple of months. Your soul's measure will be so small that almost all of its subjective quantity will at this point be in worlds simulated by whatever Tegmark Level IV parties have an interest in your soul, if you believe that's a good thing. If you don't think that's a good thing, try to die only by routes that kill you very quickly with certainty, so that it requires a violation of physical law rather than a quantum improbability to save you.
3) In other words, sign up for cryonics.

If you're worried about it, just sign up for cryonics.

Yes, because most mathematicians just take SOL at face value. If you believe in SOL and use the corresponding English language in your proofs - i.e., you assume there's only one field of real numbers and you can talk about it - then of course it doesn't matter to you whether or not your theorem happens to require SOL taken at face value, just like it doesn't matter to you whether your proof uses ~~P->P as a logical axiom. Only those who distrust SOL would try to avoid proofs that use it. That most mathematicians don't care is precisely how we know that disbelief in SOL is not a mainstream value. :)

Why, I deny that, for the machine worked precisely as XKCD said it did.

Your view is not mainstream.


"So," the Lord Pilot finally said. "What kind of asset retains its value in a market with nine minutes to live?"
"Booze for immediate delivery," the Master of Fandom said promptly. "That's what you call a -"
"Liquidity preference," the others chorused.


Two subtleties here:
1) The neutrino detector is evidence that the Sun has exploded. It's showing an observation which is 36^H^H 35 times more likely to appear if the Sun has exploded than if it hasn't (likelihood ratio of 35:1). The Bayesian just doesn't think that's strong enough evidence to overcome the prior odds, i.e., after multiplying the prior odds by 35 they still aren't very high.
2) If the Sun has exploded, the Bayesian doesn't lose very much from paying off this bet.

Um, I don't think the null hypothesis is usually phrased as, "There is no effect and our data wasn't unusual" and then you conclude "our data was unusual, rather than there being no effect" when you get data with probability < .05 if the Sun hasn't exploded. This is not a fair steelmanning.

No, it's not fair. Given the setup, the null hypothesis would be, I think, 'neither the Sun has exploded nor the dice come up 6', and so when the detector goes off we reject the 'neither x nor y' in favor of 'x or y' - and I think the Bayesian would agree too that 'either the Sun has exploded or the dice came up 6'!

Do we believe Dmitry Itskov is a billionaire? Google shows MSM outlets repeating the claim, but it doesn't look very researched and there are no hits except to the 2045 project.

Somewhat offtopic, but: Do we believe the Russian allegedly-rich transhumanist movements actually have any backing? It's much easier to claim that then to have it, and I've seen no signs of actual large spending by them.

Agreed. I've started vaporizing downvoted posts by V_V or posts that might be evading the threading principle. I don't have time to do this carefully, so might've caught some other comments by accident.
V_V, further comments by you may be deleted by any moderator at any time on any suspicion of bad behavior. If you don't like the thought of your painstakingly crafted comments randomly disappearing at future points when some moderator gets around to it, I strongly recommend finding somewhere else to hang out.

Krugman's talking about Ricardo's Law in particular, very basic, very old, not disputed so far as I know, and not known to the general populace.

Er, no, causal models and logical implications seem to me very different in how they propagate modularly. Unifying the two is going to be troublesome.

My understanding is that they had the screwing-around, despite some philosophers not doing it. They didn't have the concept that the results of screwing-around was more virtuous than the philosophy.


On the other hand, since not all statements about the integers can be either proven or disproven with the axioms of set theory, there must be different models of set theory which have different models of the integers within them

Doesn't the proof of the Completeness Theorem / Compactness Theorem incidentally invoke second-order logic itself? (In the very quiet way that e.g. any assumption that the standard integers even exist invokes second-order logic.) I'm not sure but I would expect it to, since otherwise the notion of a "consistent" theory is entirely dependent on which models your set theory says exist and which proofs your integer theory says exist. Perhaps my favorite model of set theory has only one model of set theory, so I think that only one model exists. Can you prove to me that there are other models without invoking second-order logic implicitly or explicitly in any called-on lemma? Keep in mind that all mathematicians speak second-order logic as English, so checking that all proofs are first-order doesn't seem easy.

Done!

Are you claiming that this term is ambiguous? In what specially favored set theory, in what specially favored collection of allowed models, is it ambiguous? Maybe the model of set theory I use has only one set of allowable 'collections of numbers' in which case the term isn't ambiguous. Now you could claim that other possible models exist, I'd just like to know in what mathematical language you're claiming these other models exist. How do you assert the ambiguity of second-order logic without using second-order logic to frame the surrounding set theory in which it is ambiguous?

Homeschooled! The status-preserving term is "homeschooled"!

Homeschooling is what Christians do. I'm an autodidact.

If this were true, the ancient Greeks would've had science.

False.
I mean, grain of truth, yes, literally true, no. You can shock the hell out of people and distinguish yourselves quite well by doing rational things.

Let the property P be, "is a standard number".
Then P would be true of 0, true along the successor-chain of 0, and false at a separated infinite successor-chain.
Thus P would be a property that was true of 0 and true of the successor of every object of which it was true, yet not true of all numbers.
This would contradict the second-order axiom, so this must not be a model of second-order PA.

I'm assuming full semantics for second-order logic (for any collection of numbers there is a corresponding property being quantified over) so the axioms have a semantic model provably unique up to isomorphism, there are no nonstandard models, the Completeness Theorem does not hold and some truths (like Godel's G) are semantically entailed without being syntactically entailed, etc.

But the question isn't, "Why don't they change over time," but rather, "why are they the same on each occasion". It makes no reference to occasion? Sure, but even so, why doesn't 2 + 2 = a random number each time? Why is the same identical thing the same?

Er, yes? I mean it's not like we're born knowing that cars behave like integers and outlet electricity doesn't, since neither of those things existed ancestrally.

I don't mind the arbitrary cutoff point. That's like a Bayesian reporting likelihood ratios and leaving the prior up to the reader.
It's more things like, "And now we'll multiply all the significances together, and calculate the probability that their multiplicand would be equal to or lower than the result, given the null hypothesis" that make me want to scream. Why not take the arithmetic mean of the significances and calculate the probability of that instead, so long as we're pretending the actual result is part of an arbitrary class of results? It just seems horribly obvious that you just get further and further away from what the likelihood ratios are actually telling you, as you pile arbitrary test on arbitrary test...

Don't get me wrong, this is a good paper, well-written to be clearly understandable and not to be deliberately obtuse like far too many math papers these days, and the author's heart is clearly in the right place, but I still screamed while reading it.
How can anyone read this, and not bang their head against the wall at how horribly arbitrary this all is... no wonder more than half of published findings are false.

I only got K-8[\6] (I'm not a highschool dropout, thank you, I just never went in the first place) so I've got no idea what people learn when they're 14.

Here's one: http://lesswrong.com/lw/f6o/original_research_on_less_wrong/7q1g


many people use F techniques without considering what the assumptions mean

Can you name three examples of this happening?

Why the heck would the probability of seeing the evidence, conditional on the mix of all hypotheses being considered, exactly equal the prior probability of the null hypothesis?

Mmkay. Page updated.

Well, right, assuming we're Bayesians, but when we're just "rejecting the null hypothesis" we should mostly be concerned about likelihood from the null hypothesis which has no moving parts, which is why I used the log approximation I did. But at this point we're mixing frequentism and Bayes to the point where I shan't defend the point further - it's certainly true that once a Bayesian considers more than exactly two atomic hypotheses, the update on two independent pieces of evidence doesn't go as the square of one update (even though the likelihood ratios still go as the square, etc.).


Does it make sense in computational complexity to have a holy war between average case and worst case analysis of algorithm running time?

Er, yes?

http://www.infidels.org/library/modern/mathew/logic.html#alterapars

That's why I specified single possible worlds / hypotheses with no internal parameters that are being learned.


Pr[H0]=Pr[E]

Whaa?

Then you might think you could have inconsistent betting prices that would harm the person you bet with, but not you, which sounds fine.
Rather: "If your betting prices don't obey the laws of probability theory, then you will either accept combinations of bets that are sure losses, or pass up combinations of bets that are sure gains."

Amazing, innit? Meanwhile in the land of the sane people, the likelihood function from any given propensity to come up heads, to the observed data, is exactly squared for 120 in 200 vs. 60 in 100.

A more generous way to think about frequentism (which can be justified by some conditional probability sleight-of-hand) is that the significance of some evidence E is actually the probability that the null hypothesis is true, given E and also some prior distribution that is swept under the rug and (mostly) not under the experimenter's control. Which is bad, yes, but in many cases the prior distribution is at least close to something reasonable. And there are some cases in which we can somewhat change the prior distribution to reflect our real priors: for example, when choosing to conduct a 1-tailed test rather than a 2-tailed one.
Under this interpretation, it is silly to expect significances to multiply. You'd really be saying something like Pr[H|E1+E2] = Pr[H|E1] Pr[H|E2]. And that's simply not true: you are double-counting the prior probability Pr[H] when you do this. The frequentist approach is a correct way to combine these probabilities, although this isn't obvious because nobody actually knows what the frequentist Pr[H] is.
But if you read about two experiments with a p-value of 0.05, and think of them as one experiment with a p-value of 0.0025, you are very very very wrong; not just frequentist-wrong but Bayesian-wrong as well.


the significance of some evidence E is actually the probability that the null hypothesis is true, given E

No frequentist says this. They don't believe in P(H|E). That's the explicit basis of the whole philosophy. People who talk about the probability of a hypothesis given the evidence are Bayesians, full stop.
Statistical significance is, albeit in a strange and distorted way, supposed to be about P(E|null hypothesis), and so, yes, two experiments with a p-value of 0.05 should add up to somewhere in the vicinity of p < 0.0025, because it's about likelihoods, which do multiply, and not posteriors.

If a given piece of evidence E1 provides Bayesian likelihood for theory T1 over T2, and E2 was generated by an isomorphic process, then we get the likelihood ratio squared, providing that T1 and T2 are single possible worlds and have no parameters being updated by E1 or E2 so that the probability of the evidence is conditionally independent.
Thus sayeth Bayes, so far as I can tell.
As for the frequentists...
Well, logically, we're allegedly rejecting a null hypothesis. If the "null hypothesis" contains no parameters to be updated and the probability that E1 was generated by the null hypothesis is .05, and E2 was generated by a causally conditionally independent process, the probability that E1+E2 was generated by the null hypothesis ought to be 0.0025.
But of course gwern's calculation came out differently in the decimals. This could be because some approximation truncated a decimal or two. But it could also be because frequentism actually calculates the probability that E1 is in some amazing class [E] of other data we could've observed but didn't, to be p < 0.05. Who knows what strange class of other data we could've seen but didn't, a given frequentist method will put E1 + E2 into? I mean, you can make up whatever the hell [E] you want, so who says you've got to make up one that makes [E+E] have the probability of [E] squared? So if E1 and E2 are exactly equally likely given the null hypothesis, a frequentist method could say that their combined "significance" is the square of E1, less than the square, more than the square, who knows, what the hell, if we obeyed probability theory we'd be Bayesians so let's just make stuff up. Sorry if I sound a bit polemical here.
See also: http://lesswrong.com/lw/1gc/frequentist_statistics_are_frequently_subjective/

I haven't read this in detail but one very quick comment: Cox's Theorem is a representation theorem showing that coherent belief states yield classical probabilities, it's not the same as the dutch-book theorem at all. E.g. if you want to represent probabilities using log odds, they can certain relate to each other coherently (since they're just transforms of classical probabilities), but Cox's Theorem will give you the classical probabilities right back out again. Jaynes cites a special case of Cox in PT:TLOS which is constructive at the price of assuming probabilities are twice differentiable, and I actually tried it with log odds and got the classical probabilities right back out - I remember being pretty impressed with that, and had this enlightenment experience wherein I went to seeing probability theory as a kind of relational structure in uncertainty.
I also quickly note that the worst-case scenario often amounts to making unfair assumptions about "randomization" wherein adversaries can always read the code of deterministic agents but non-deterministic agents have access to hidden sources of random numbers. E.g. http://lesswrong.com/lw/vq/the_weighted_majority_algorithm/

I was just mentally approximating log(.001)/log(.05) = 2.3.

That's for experimental statistical reports. Trying to do math runs into a different set of dangers.
You can easily beat "Most published research findings are false" by reporting Bayesian likelihood ratios instead of "statistical significance", or even just keeping statistical significance and demanding p < .001 instead of the ludicrous p < .05. It should only take <2.5 times as many subjects to detect a real effect at p < .001 instead of p < .05 and the proportion of false findings would go way down immediately. That's what current grantmakers and journals would ask for if they cared.


If the ontological crisis is too severe, the AI may lose the ability to do anything at all, as the world becomes completely incomprehensible to it.
unless the AI is an entropy genie, it cannot influence utility values through its decisions, and will most likely become catatonic

Seems unwarrantedly optimistically anthropomorphic. A controlled shutdown in a case like this is a good outcome, but imagining a confused human spinning around and falling over does not make it so. The AI would exhibit undefined behavior, and hoping that this behavior is incoherent enough to be harmless or that it would drop an anvil on its own head seems unwarrantedly optimistic if that wasn't an explicit design consideration. Obviously undefined behavior is implementation-dependent but I'd expect that in some cases you would see e.g. subsystems running coherently and perhaps effectively taking over behavior as high-level directions ceased to provide strong utility differentials. In other words, the AI built an automatic memory-managing subsystem inside itself that did some degree of consequentialism but in a way that was properly subservient to the overall preference function; now the overall preference function is trashed and the memory manager is what's left to direct behavior. Some automatic system goes on trying to rewrite and improve code, and it gets advice from the memory manager but not from top-level preferences; thus the AI ends up as a memory-managing agent.
This probably doesn't make sense the way I wrote it, but the general idea is that the parts of the AI could easily go on carrying out coherent behaviors, and that could easily end up somewhere coherent and unpleasant, if top-level consequentialism went incoherent. Unless controlled shutdown in that case had somehow been imposed from outside as a desirable conditional consequence, using a complexly structured utility function such that it would evaluate, "If my preferences are incoherent then I want to do a quiet, harmless shutdown, and that doesn't mean optimize the universe for maximal quietness and harmlessness either." Ordinarily, an agent would evaluate, "If my utility function goes incoherent... then I must not want anything in particular, including a controlled shutdown of my code."

Grade school didn't assign me many group projects. In fact, I can only remember one. And on that one I think I tried cooperating for like 15 minutes or something and then told the other two kids, "Go away, I'll handle this" because it was easier to just do all the work myself.
Sometimes our early life experiences really are that metaphorical.

GOOD. Especially this one: http://www.howtolearn.com/2011/02/demystifying-algebra
But I don't recall ever getting that in my classes. Also, the illustration of "first step going from true equation to false equation" I think is also important to have in there somewhere.

Yes, well, the problem is that many courses on "logic" don't teach model theory, at least from what I've heard. Try it and see. I could've just been asking the wrong mathematicians - e.g. a young mathematician visiting CFAR knew about model theory, but that itself seems a bit selected. But the modern courses could be better than the old courses! It's been known to happen, and I'd sure be happy to hear it.
(I'm pretty sure mathbabe has been through a course on formal logic and so has Samuel R. Buss...)

Are you sure? For myself, I should say that moving to a world where everyone's two standard deviations smarter than me might be a blow to my pride, in fact it would be a huge blow to my entire self-concept and conceived role in existence, but I'd expect the fringe benefits to more than make up for it.

I plan to talk about this in some posts on second-order logic.

I happen to be studying model theory at the moment. For anyone curious, when Eliezer say 'If X ? Y, then X ? Y' (that is, if a model proves a statement, that statement is true in the model), this is known as soundness. The converse is completeness, or more specifically semantic completeness, which says that if a statement is true in every model of a theory (in other words, in every possible world where that theory is true), then there is a finite proof of the statement. In symbols this is 'If X ? Y, then X ? Y'. Note that this notion of 'completeness' is not the one used in Godel's incompleteness theorems.

Tried an edit.
Math professors certainly understand instinctively what connects premises to conclusions, or they couldn't do algebra. It's trying to talk about the process explicitly where the non-modern-logicians start saying things like "proofs are absolutely convincing, hence social constructs".

See my reply to Chappell here and the enclosing thread: http://lesswrong.com/lw/f1u/causal_reference/7phu

I can build an agent that tracks how many sheep are in the pasture using an internal mental bucket, and keeps looking for sheep until they're all returned. From an outside standpoint, this agent's mental bucket is meaningful because there's a causal process that correlates it to the sheep, and this correlation is made use of to steer the world into futures where all sheep are retrieved. And then the mysterious sensation of about-ness is just what it feels like from the inside to be that agent, with a side order of explicitly modeling both yourself and the world so that you can imagine that your map corresponds to the territory, with a side-side order of your brain making the simplifying assumption that (your map of) the map has a primitive intrinsic correspondence to (your map of) the territory.
In actuality this correspondence is not the primitive and local quality it feels like; it's maintained by the meeting of hypotheses and reality in sense data. A third party or reflecting agent would be able to see the globally maintained correspondence by simultaneously tracing back actual causes of sense data and hypothesized causes of sense data, but this is a chain property involving real lattices of causal links and hypothetical lattices of causal links meeting in sense data, not an intrinsic quality of a single node in the lattice considered in isolation from the senses and the hypotheses linking it to the senses.
So far as I can tell, there's nothing left to explain.
--
"At exactly which point in the process does the pebble become magic?" says Mark.
"It... um..." Now I'm starting to get confused. I shake my head to clear away cobwebs. This all seemed simple enough when I woke up this morning, and the pebble-and-bucket system hasn't gotten any more complicated since then. "This is a lot easier to understand if you remember that the point of the system is to keep track of sheep."


For myself, any utopia that requires the government to be more intrusive in my life than my current one doesn't get to count as a utopia unless it's got some serious amenities (eg catgirls).

You don't think that being born into a world where the average IQ is 140 (i.e. corresponds to IQ 140 in our terms) counts as a serious amenity?
The Montana Genetic Board is an obvious problem, but if in the long run Montana perishes and Singapore wins, that seems acceptable.

From my perspective, when I've explained why a single AI alone in space would benefit instrumentally from checking proofs for syntactic legality, I've explained the point of proofs. Communication is an orthogonal issue, having nothing to do with the structure of mathematics.

...and how much utility people actually get out of opium.

"I toss in a pebble whenever a sheep passes," I point out.
"When a sheep passes, you toss in a pebble?" Mark says. "What does that have to do with anything?"
"It's an interaction between the sheep and the pebbles," I reply.
"No, it's an interaction between the pebbles and you," Mark says. "The magic doesn't come from the sheep, it comes from you. Mere sheep are obviously nonmagical. The magic has to come from somewhere, on the way to the bucket."
I point at a wooden mechanism perched on the gate. "Do you see that flap of cloth hanging down from that wooden contraption? We're still fiddling with that - it doesn't work reliably - but when sheep pass through, they disturb the cloth. When the cloth moves aside, a pebble drops out of a reservoir and falls into the bucket. That way, Autrey and I won't have to toss in the pebbles ourselves."
Mark furrows his brow. "I don't quite follow you... is the cloth magical?"
I shrug. "I ordered it online from a company called Natural Selections. The fabric is called Sensory Modality." I pause, seeing the incredulous expressions of Mark and Autrey. "I admit the names are a bit New Agey. The point is that a passing sheep triggers a chain of cause and effect that ends with a pebble in the bucket."

(From "The Simple Truth", a parable about using pebbles in a bucket to keep count of the sheep in a pasture.)
"My pebbles represent the sheep!" Autrey says triumphantly. "Your pebbles don't have the representativeness property, so they won't work. They are empty of meaning. Just look at them. There's no aura of semantic content; they are merely pebbles. You need a bucket with special causal powers."
"Ah!" Mark says. "Special causal powers, instead of magic."
"Exactly," says Autrey. "I'm not superstitious. Postulating magic, in this day and age, would be unacceptable to the international shepherding community. We have found that postulating magic simply doesn't work as an explanation for shepherding phenomena. So when I see something I don't understand, and I want to explain it using a model with no internal detail that makes no predictions even in retrospect, I postulate special causal powers. If that doesn't work, I'll move on to calling it an emergent phenomenon."
"What kind of special powers does the bucket have?" asks Mark.
"Hm," says Autrey. "Maybe this bucket is imbued with an about-ness relation to the pastures. That would explain why it worked - when the bucket is empty, it means the pastures are empty."
"Where did you find this bucket?" says Mark. "And how did you realize it had an about-ness relation to the pastures?"
"It's an ordinary bucket," I say. "I used to climb trees with it... I don't think this question needs to be difficult."
"I'm talking to Autrey," says Mark.
"You have to bind the bucket to the pastures, and the pebbles to the sheep, using a magical ritual - pardon me, an emergent process with special causal powers - that my master discovered," Autrey explains.
Autrey then attempts to describe the ritual, with Mark nodding along in sage comprehension.
"And this ritual," says Mark, "it binds the pebbles to the sheep by the magical laws of Sympathy and Contagion, like a voodoo doll."
Autrey winces and looks around. "Please! Don't call it Sympathy and Contagion. We shepherds are an anti-superstitious folk. Use the word 'intentionality', or something like that."
"Can I look at a pebble?" says Mark.
"Sure," I say. I take one of the pebbles out of the bucket, and toss it to Mark. Then I reach to the ground, pick up another pebble, and drop it into the bucket.
Autrey looks at me, puzzled. "Didn't you just mess it up?"
I shrug. "I don't think so. We'll know I messed it up if there's a dead sheep next morning, or if we search for a few hours and don't find any sheep."
"But -" Autrey says.
"I taught you everything you know, but I haven't taught you everything I know," I say.
Mark is examining the pebble, staring at it intently. He holds his hand over the pebble and mutters a few words, then shakes his head. "I don't sense any magical power," he says. "Pardon me. I don't sense any intentionality."
"A pebble only has intentionality if it's inside a ma- an emergent bucket," says Autrey. "Otherwise it's just a mere pebble."
"Not a problem," I say. I take a pebble out of the bucket, and toss it away. Then I walk over to where Mark stands, tap his hand holding a pebble, and say: "I declare this hand to be part of the magic bucket!" Then I resume my post at the gates.
Autrey laughs. "Now you're just being gratuitously evil."
I nod, for this is indeed the case.
"Is that really going to work, though?" says Autrey.
I nod again, hoping that I'm right. I've done this before with two buckets, and in principle, there should be no difference between Mark's hand and a bucket. Even if Mark's hand is imbued with the elan vital that distinguishes live matter from dead matter, the trick should work as well as if Mark were a marble statue.
(The moral: In this sequence, I explained how words come to 'mean' things in a lawful, causal, mathematical universe with no mystical subterritory. If you think meaning has a special power and special nature beyond that, then (a) it seems to me that there is nothing left to explain and hence no motivation for the theory, and (b) I should like you to say what this extra nature is, exactly, and how you know about it - your lips moving in this, our causal and lawful universe, the while.)

In causal models, we can have A -> B, E -> A, E -> ~B. Logical uncertainty does not seem offhand to have the same structure as causal uncertainty.

Yes. An argument similar to this should still be in the other-edited version of my unfinished TDT paper, involving a calculator on Venus and a calculator on Mars, the point being that if you're not logically omniscient then you need to factor out logical uncertainty for the Markov property to hold over your causal graphs, because physically speaking, all common causes should've been screened off by observing the calculators' initial physical states on Earth. Of course, it doesn't follow that we have to factor out logical uncertainty as a causal node that works like every other causal node, but we've got to factor it out somehow.

There's no epiphenomenal type of stuff in QM. There's just a causal type of stuff, some of which got far enough away that under the standard and observed rules we can't see it anymore. It's no more epiphenomenal than a photon transmitted into space or a ship that went over the horizon.
Deducing an epiphenomenal type of stuff would be more difficult, and AFAICT would basically have to rely on there being structure in the observed laws and types of your world's physics. For example, let's say you're in the seventh layer of a universe with at least seven causal layers. The first layer has seven laws connecting it to the layer below, the second layer has six laws connecting it to a layer below, and then you're in the seventh layer, connected by two laws to the layer above. You might suspect that there's an eighth layer below you, and that the single remaining law is the one required to match the pattern of the seven layers you know about.
Of course, what you're actually doing this case is almost exactly akin to knowing about a ship that went over the horizon - you observed the Laws of Physics Factory, or the code-factory for your Matrix, generalized, and deduced an effect of the previously observed Factory which the generalization says you shouldn't be able to see. You can navigate to the law-data by following a causal reference to the link down from a law-Factory you've previously observed.

That actually happened to me last Tuesday!

In other words, "Yes"?

We should point people to this whenever they're like "What's special about Less Wrong?" and we can be like "Okay, first, guess how Less Wrong would discuss a reluctant Christian homosexual. Made the prediction? Good, now click this link."

If you take a single branch and run it backward, you'll find that it diverges into a multiverse of its own. If you take all the branches and run them backward, their branches will cohere instead of decohering, cancel out in most places, and miraculously produce only the larger, more coherent blobs of amplitude they started from. Sort of like watching an egg unscramble itself.

As I understand it, this is not how standard physics models the beginning of time.


More generally, for me to expect your beliefs to correlate with reality, I have to either think that reality is the cause of your beliefs, expect your beliefs to alter reality, or believe that some third factor is influencing both of them.

I can construct examples where for this to be true requires us to treat mathematical truths as causes. Of course, this causes problems for the Bayesian definition of "cause".

I edited the main post to put it in.

Meditation:

Humans need fantasy to be human.
"Tooth fairies? Hogfathers? Little--"
Yes. As practice. You have to start out learning to believe the little lies.
"So we can believe the big ones?"
Yes. Justice. Mercy. Duty. That sort of thing.
"They're not the same at all!"
You think so? Then take the universe and grind it down to the finest powder and sieve it through the finest sieve and then show me one atom of justice, one molecule of mercy.

Susan and Death, in Hogfather by Terry Pratchett


So far we've talked about two kinds of meaningfulness and two ways that sentences can refer; a way of comparing to physical things found by following pinned-down causal links, and logical reference by comparison to models pinned-down by axioms. Is there anything else that can be meaningfully talked about? Where would you find justice, or mercy?

Mainstream status:
The presentation of the natural numbers is meant to be standard, including the (well-known and proven) idea that it requires second-order logic to pin them down. There's some further controversy about second-order logic which will be discussed in a later post.
I've seen some (old) arguments about the meaning of axiomatizing which did not resolve in the answer, "Because otherwise you can't talk about numbers as opposed to something else," so AFAIK it's theoretically possible that I'm the first to spell out that idea in exactly that way, but it's an obvious-enough idea and there's been enough debate by philosophically inclined mathematicians that I would be genuinely surprised to find this was the case.
On the other hand, I've surely never seen a general account of meaningfulness which puts logical pinpointing alongside causal link-tracing to delineate two different kinds of correspondence within correspondence theories of truth. To whatever extent any of this is a standard position, it's not nearly widely-known enough or explicitly taught in those terms to general mathematicians outside model theory and mathematical logic, just like the standard position on "proof". Nor does any of it appear in the S. E. P. entry on meaning.

Mainstream status:
This is meant to present a completely standard view of semantic implication, syntactic implication, and the link between them, as understood in modern mathematical logic. All departures from the standard academic view are errors and should be flagged accordingly.
Although this view is standard among the professionals whose job it is to care, it is surprisingly poorly known outside that. Trying to make a function call to these concepts inside your math professor's head is likely to fail unless they have knowledge of "mathematical logic" or "model theory".
Beyond classical logic lie the exciting frontiers of weird logics such as intuitionistic logic, which doesn't have the theorem !!P > P. These stranger syntaxes can imply entirely different views of semantics, such as a syntactic derivation of Y from X meaning, "If you hand me an example of X, I can construct an example of Y."
I can't actually recall where I've seen someone else say that e.g. "An algebraic proof is a series of steps that you can tell are locally licensed because they maintain balanced weights", but it seems like an obvious direct specialization of "syntactic implication should preserve semantic implication" (which is definitely standard). Similarly, I haven't seen the illustration of "Where does this first go from a true equation to a false equation?" used as a way of teaching the underlying concept, but that's because I've never seen the difference between semantic and syntactic implication taught at all outside of one rare subfield of mathematics. (AAAAAAAAAAAAHHHH!)
The idea that logic can't tell you anything with certainty about the physical universe, or that logic is only as sure as its premises, is very widely understood among Traditional Rationalists:

... as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.

--Albert Einstein

Meditation:
It has been claimed that logic and mathematics is the study of which conclusions follow from which premises. But when we say that 2 + 2 = 4, are we really just assuming that? It seems like 2 + 2 = 4 was true well before anyone was around to assume it, that two apples equalled two apples before there was anyone to count them, and that we couldn't make it 5 just by assuming differently.

In TDT we don't do any severance! Nothing is uncaused, not our decision, nor our decision algorithm either. Trying to do causal severance is a basic root of paradoxes because things are not uncaused in real life. What we do rather is condition on the start state of our program, thereby screening off the universe (not unlawfully severing it), and factor out our uncertainty about the logical output of the program given its input. Since in real life most things we do to the universe should not change this logical fact, nor will observing this logical fact tell us which non-impossible possible world we are living it, it shouldn't give us any news about the nodes above, once we've screened off the algorithm. It does, however, give us logical news about Omega's output, and of course about which boxes we'll end up with.



Your suggestion has been... accepted!

Interesting question! I'd say that you could refer to the possibilities as possibilities, e.g. in a debate over whether a particular past would in fact have led to the present, but to speak of the 'actual past' might make no sense because you couldn't get there from there... no, actually, I take that back, you might be able to get there via simplicity. I.e. if there's only one past that would have evolved from a simply-tiled start state for the automaton.

Er, no, they're called Dynamic Bayes Nets. And there are no known unique events relative to the fundamental laws of physics; those would be termed "miracles". Physics repeats perfectly - there's no question of frequentism because there's no probabilities - and the higher-level complex events are one-time if you try to measure them precisely; Socrates died only once, etc.

Thanks! I stand corrected.

Okay, I can see that I need to spell out in more detail one of the ideas here - namely that you're trying to generalize over a repeating type of causal link and that reference is pinned down by such generalization. The Sun repeatedly sends out light in individual Sun-events, electrons repeatedly go on traveling through space instead of vanishing; in a universe like ours, rather than the F(i) being whole new transition tables randomly generated each time, you see the same F(physics) over and over. This is what you can pin down and refer to. Any causal graph is acyclic and can be divided as you say; the surprising thing is that there are no F-types, no causal-link-types, which (over repeating time) descend from one kind of variable to another, without (over time) there being arrows also going back from that kind to the other. Yes, we're generalizing and inducting over time, otherwise it would make no sense to speak of thingies that "affect each other". No two individual events ever affect each other!


They also use pretty similar examples: a classic one is to think of an being on the other side of the galaxy thinking about Winston Churchill. Even if they have the right image, they even happen to think the right things about where he lived, what he did etc. it seems that they don't actually succeed in referring to him because of the lack of a causal link.

(They could be referring to all objects of class CHURCHILL, but not to our own, particular Churchill, although he happens to be such an object.)

As bryjnar points out, all the stuff you say here (subtracting out the Pearl stuff) is entailed by the causal theory of reference. The reason quick summaries of that view will seem unfamiliar is that most of the early work on the causal theory was primarily motivated by a different concern -- accounting for how our words acquire their meaning. Thus the focus on causal chains from "original acts of naming" and whatnot. However, your arguments against epiphenomenalism all hold in the causal theory.
It is true that nobody (that I know of) has developed an explicitly Pearlian causal theory of reference, but this is really accounted for by division of labor in philosophy. People working on reference will develop a causal theory of reference and use words like "cause" without specifying what they mean by it. If you ask them what they mean, they will say "Whatever the best theory of causation is. Go ask the people working on causation about that." And among the people working on causation, there are indeed philosophers who have built on Pearlian ideas. Christopher Hitchcock and James Woodward, for instance.


However, your arguments against epiphenomenalism all hold in the causal theory.

Has anyone made them? I ask because every use I've seen of the 'causal theory of reference' is, indeed, about "accounting for how our words acquire their meaning", something of a nonproblem from the standpoint of somebody who thinks that words don't have inherent meanings.

The last nodes are never observed by anyone, but they descend from the same physics, the same F(physics), that have previously been pinned down, or so I assume. You can thus meaningfully talk about them for the same reason you can meaningfully talk about a spaceship going over the cosmological horizon. What we're trying to avoid is SNEEZE_VARs or lower qualia where there's no way that the hypothesis-making agent could ever have observed, inducted, and pinned down the causal mechanism - where there's no way a correspondence between map and territory could possibly be maintained.

(I like that! The idea that it follows just from the associative property and no other features of PA is quite elegant.)

It's good practice for seeing how the rules play out; and having a clear mental visualization of how causality works, and what sort of theories are allowed to be meaningful, is actually reasonably important both for anti-nonsense techniques and AI construction. People who will never need any more skill than they currently have in Resist Nonsense or Create Friendly AI can ignore it, I suppose.


Assuming cosmological horizons behave like black hole event horizons

Why on Earth would they?
Edit: Also that paper does not say anything about cosmological horizons converting anything into Hawking radiation! It's an entirely different and stranger argument.

Mainstream status:
I haven't yet happened to run across a philosophical position which says that meaningful correspondences between hypotheses and reality can only be pinned down by following Pearl-style causal links inferred as the simplest explanation of observed experiences, and that only this can allow an agent to consistently believe that its beliefs are meaningful.
In fact, I haven't seen anything at all about referential meaningfulness requiring cause-and-effect links with the phenomenon, just like I haven't seen anything about a universe being a connected fabric of causes and effects; but it wouldn't surprise me if either of those ideas were out there somewhere.
(There's a "causal theory of reference" listed in Wikipedia but it doesn't seem to be about remotely the same subject matter; the theory's tenets seem to be that "a name's referent is fixed by an original act of naming", and that "later uses of the name succeed in referring to the referent by being linked to that original act via a causal chain".)
EDIT: Apparently causal theories of reference have been used to argue against Zombie Worlds so I stand corrected on this point. See below.

Meditation:
If we can only meaningfully talk about parts of the universe that can be pinned down inside the causal graph, where do we find the fact that 2 + 2 = 4? Or did I just make a meaningless noise, there? Or if you claim that "2 + 2 = 4" isn't meaningful or true, then what alternate property does the sentence "2 + 2 = 4" have which makes it so much more useful than the sentence "2 + 2 = 3"?

Hm. I like the direction this points. Any similar suggestions?

So... the main thing I want to convey over and above "exercise" is that rather than there being a straightforward task-to-solve, you're supposed to ponder the statement and say, "What do I think of this?"
A word other than "koan" which conveys this intent-to-ponder would indeed be appreciated.

Suggest a better word? Keep in mind that words which are not better will be rejected (people often seem to forget this while making alternate suggestions).

Mainstream status:
As previously stated, the take on causality's math is meant to be academically standard; this includes the idea of decomposing the X(i) into deterministic F(i) and uncorrelated U(i).
I haven't particularly seen anyone else observe that claiming you know about X without X affecting you, you affecting X, or X and your belief having a common cause, violates the Markov condition on causal graphs.
I haven't actually seen anyone cite the Markov condition as a reply to the old "What constitutes randomization?" debates I've glimpsed, but I would be genuinely surprised if Pearl & co. hadn't pointed it out by now - my understanding is that he's spending most of his time evangelizing causality to experimental statisticians these days. It seems pretty obvious once you have causal models as a background.
The concept of "separate magisteria" is as old as scientific critique of religion, but the actual phrase was coined by Stephen Jay Gould (speaking favorably of the separation, natch). So far as I know, the concept of anti-epistemology is an LW original; likewise the view that causality is more general than anyone trying to separate their magisterium would have the mathematical competence to successfully escape, even as an attempted excuse. In general, when I write about the skeptical applications, I'm usually writing things I haven't read before and that wouldn't be expected to appear somewhere like Pearl's Causality book - which doesn't imply that nobody else has written about them, of course. If you know of similar theses, comment here.


The first claim is a standard part of conventional materialism

Quote? SEP link? As mentioned, I've seen "fabric of events" in SR discussion, no one actually saying "fabric of cause and effect", certainly no Pearl.

About a year.


How can I send this to people I know who will ask why he's using two time-variant variables as single acyclic nodes

This is not intended for readers who already know that much about causal models, btw, it's a very very very basic intro.

Er, I don't mean to be too harsh, but I tend to be a bit suspicious when somebody tells me to expect weight loss, and then backpedals and says that maybe an unobservable substitution of muscle for fat took place instead. I realize there are ways this could in principle be verified, if someone was willing to expend enough effort. It is nonetheless suspicious.

To be precise, "Good people are consequentialists, but virtue ethics is what works."

Not for as far back as I can remember. One of the scarier parts of my childhood was having to hide food so that I could get something to eat on Yom Kippur, and hoping my parents never found out. I do remember at some point being too exhausted to walk either to or from the synagogue even with my father yelling at me - maybe that was when I was young enough to believe enough to actually fast?
I reasonably expect I would do better now, especially if I'd eaten all protein for a couple of days previous - my adult metabolism is not quite as bad as I remember it being in childhood, and my mind is a whole lot stronger. I'm not particularly inclined to test it, though.

Er, you have some sort of incomprehensible maths-witchcraft thing that you can do to find out the truth.

Think of them as 3-year-olds who won't grow up until after the Singularity. Would you kick a 3-year-old who made a mistake?

I pledge allegiance to the prime number 2, the prime number 3, and the prime number 5. And to their product, 30, and their sum, 10...

Koan 3:
Does the idea that everything is made of causes and effects meaningfully constrain experience? Can you coherently say how reality might look, if our universe did not have the kind of structure that appears in a causal model?

Koan 2:
"Does your rule there forbid epiphenomenalist theories of consciousness - that consciousness is caused by neurons, but doesn't affect those neurons in turn? The classic argument for epiphenomenal consciousness has always been that we can imagine a universe in which all the atoms are in the same place and people behave exactly the same way, but there's nobody home - no awareness, no consciousness, inside the brain. The usual effect of the brain generating consciousness is missing, but consciousness doesn't cause anything else in turn - it's just a passive awareness - and so from the outside the universe looks the same. Now, I'm not so much interested in whether you think epiphenomenal theories of consciousness are true or false - rather, I want to know if you think they're impossible or meaningless a priori based on your rules."
How would you reply?

Koan 1:

"You say that a universe is a connected fabric of causes and effects. Well, that's a very Western viewpoint - that it's all about mechanistic, deterministic stuff. I agree that anything else is outside the realm of science, but it can still be real, you know. My cousin is psychic - if you draw a card from his deck of cards, he can tell you the name of your card before he looks at it. There's no mechanism for it - it's not a causal thing that scientists could study - he just does it. Same thing when I commune on a deep level with the entire universe in order to realize that my partner truly loves me. I agree that purely spiritual phenomena are outside the realm of causal processes, which can be scientifically understood, but I don't agree that they can't be real."

How would you reply?

Mainstream status:
The introduction to causality is intended to be bog-standard. All departures from mainstream academia are errors and should be flagged accordingly.
I can't particularly recall hearing anyone suggest that a universe is a connected fabric of causes and effects in the Pearlian sense of causality. Since Special Relativity there have been many suggestions that a universe is a connected fabric of 'events', or points in spacetime.
I can't particularly recall reading that you can only meaningfully talk about things you can find by tracing causal links (this theory will be developed further in upcoming posts).

Deleted due to the attempt to evade the -5 penalty.

Wouldn't that... if enough people did it... I mean, wouldn't it sort of... work?

I agree the Pledge sounds a bit creepy in retrospect - I was only disagreeing with the idea that any possible thing you repeat at the start of class is creepy.

I would recite the Pledge of Allegiance ending, "With freedom, and justice, for all except the children."
I'm not sure it would be a bad thing if they had a ceremony where those students who wanted to recited the Twelve Virtues once a week, or Frankena's list of terminal values, or the rules of algebra. Repetition is a perfectly good way to install association and thereby skill - you can use it to repeat good things or bad things. It's not much different from printing that way.

Does somebody want to post one part of Dmytry that seems new and true? My impression on a quick skim was not favorable.

The rituals in question were my rationalist marriage ceremony and Raemon's solstice ritual. Obviously I have no objection to these being discussed, though I would strongly recommend doing so in a separate Discussion thread. As this thread is a part of a new sequence that may be used to introduce newcomers to LW, I am especially interested in keeping it clean.

I had in mind, "I think you were really trying to say X" which is closer to your second meaning, not "This means X under all possible circumstances even when actually used correctly".

Potassium chloride works, but it tastes worse and the chlorine can do bad things if you take too much of it at once.

I am explicitly calling that unFriendly given bounded resources.

About the same as drinking a cup of coffee - i.e., it works as a perker-upper, somehow. I'm not sure, since it doesn't do anything for me except possibly mitigate foot cramps.

They don't seem to be good, though.

S. I. Hayakawa was a way better writer - that's where I got all my reprocessed Korzybski as a kid, and that's where I point people: Language in Thought and Action instead of Science and Sanity. I tried once to read the latter book as a kid, after being referred to it by Null-A. I was probably about... eleven years old? Thirteen? I gave up very, very rapidly, which I did not do for physics texts with math in them.

I've heard it alleged that having someone greet a newcomer with a smile, and introduce them to others, and generally be responsible for making sure that the newcomer has fun, jumps retention rates from something like 20% to 50%. Note that this is an Official Function - somebody has to be appointed to it, and then you have to make sure that they're doing it right.

Fixed.

Rather than talking about only a single case where you might be tempted to conclude that X is rational because it leads to your desired conclusion that global warming is false, or true, or whatever; one can explain why this (tends probabilistically to) work in the general case, given the general sort of universe we live in - that's "systematically".

I shall concede that the sentence, "It's rational for X to believe Y, but really Z" can sometimes make sense - it says that you have different evidence from X. In most cases, though, this will underestimate the power of rationality and ask too little of X. (The last time I can remember saying anything like this was in a strictly fictional context, Chapter 20.)

That comment did move Intrade shares by around 10 percentage points, I think, though I'm only going on personal before-and-after comparisons. The good Will may have picked the wrong time to criticize his instincts.

Heh, looks like the referring text got revised out. I've deleted the footnote.

Mainstream status:
The distinction between epistemic and instrumental rationality is standard.
Postmodernists/relativists have emphasized use of the word 'true' as a mere emphasis, which I admit is a common use.
I haven't particularly seen anyone pioneering a rationalist technique of trying to eliminate the word 'true' to avoid use as a mere emphasis. The deflationary theory of truth says that all uses of "truth" are deflatable - which this sequence denies; but the idea of deflating "true" out of sentences is clearly precedented, as is the Tarski-inspired algorithm for doing so.
I haven't particularly seen anything which emphasizes that the residue of trying to eliminate the word "truth" is abstraction and generalization over the behavior of map-territory correspondences.
Similarly, I haven't previously seen anyone advocate, as a rationality technique, trying to eliminate the word 'rational'; or emphasizing that the non-eliminable residue will be about cognitive algorithms. I wouldn't particularly expect to see that; this is me trying to narrow the definition a particular way that I think is useful.


I prefer this sort of distant, reductionist, structural approach to analysing the race because there's little reason to believe in the validity of the implicit theories or "models" lurking behind pundits' gut judgments. When I heard Mr Romney's 47% comments, I thought "Oooh, he's toast!" and then I stopped myself and acknowledged that I actually have no rational basis for believing that his remarks would in the final analysis hurt Mr Romney at all. What percentage of undecided or weakly-decided swing-state voters ever caught wind of Mr Romney's embarrassing chat? I didn't know! Of those who became aware of it, how many cared? I didn't know! So why did I think "Oooh, he's toast!" Because I am human, and I make most judgments and decisions on the basis of crackpot hunches, the underlying logic of which is almost completely inscrutable to me.

--Will Wilkinson

Trolling: Provocation for the sake of response.

...I honestly can't remember anymore what it's like to look at the world without knowing calculus. How do you figure out how any rate of change relates to anything else?

Philosophy posts are enjoyable if they're interesting. They're useful if they're right.

Yep.

Nope, I'm not a libertarian.

Hey, if they could spot those kinds of contradictions, they'd be libertarians.

I'm Eliezer Yudkowsky! Do you have any idea how many distinct versions of me there are in Tegmark Levels I through III?

Not just metaphorically. People are behaviorally reinforced into trolls because attention is reward and provocation gets attention. By downvoting something and commenting in reply to it, you are building positive associations to getting downvoted, a rather psychologically-sick sort of internal state that is a very bad thing to do to anyone. Would you consider it a nice thing to do to follow somebody around and give them a smile and a kiss each time they lost their temper or experienced some other failure of will, so as to reinforce that behavior? No, right?

This is indeed a bug; the feature spec said not to hide from inbox.

I've gone through and devaporized all of eridu's comments in his epically successful "radical feminism" troll - I don't like vaporizing things (or exercising direct moderator power at all, really), and since eridu at -243 karma can't reply to anything else in that thread, it should be safe now. It also serves as an extremely clear exhibitable example of what this feature was for!
(Looking over so many at once makes me pretty sure that it was trolling (a reinforced behavior of provocation-for-attention), btw.)

Yep, there's some of my own comments I wish I could downvote for the same reason.

(The 'Mainstream Status' comment is intended to provide a quick overview of what the status of the post's ideas are within contemporary academia, at least so far as the poster knows. Anyone claiming a particular paper precedents the post should try to describe the exact relevant idea as presented in the paper, ideally with a quote or excerpt, especially if the paper is locked behind a paywall. Do not represent large complicated ideas as standard if only a part is accepted; do not represent a complicated idea as precedented if only a part is described. With those caveats, all relevant papers and citations are much solicited! Hopefully comment-collections like these can serve as a standard link between LW presentations and academic ones.)
The correspondence theory of truth is the first position listed in the Stanford Encyclopedia of Philosophy, which is my usual criterion for saying that something is a solved problem in philosophy. Clear-cut simple visual illustration inspired by the Sally-Anne experimental paradigm is not something I have previously seen associated with it, so the explanation in this post is - I hope - an improvement over what's standard.
Alfred Tarski is a famous mathematician whose theory of truth is widely known.
The notion of possible worlds is very standard and popular in philosophy; some of them even ascribe much more realism to them than I would (since I regard them as imaginary constructs, not thingies that can potentially explain real events as opposed to epistemic puzzles).
I haven't particularly run across any philosophy explicitly making the connection from the correspondence theory of truth to "There are causal processes producing map-territory correspondences" to "You have to look at things in order to draw accurate maps of them, and this is a general rule with no exception for special interest groups who want more forgiving treatment for their assertions". I would not be surprised to find out it existed, especially on the second clause.
Added: The term "post-utopian" was intended to be a made-up word that had no existing standardized meaning in literature, though it's simple enough that somebody has probably used it somewhere. It operates as a stand-in for more complicated postmodern literary terms that sound significant but mean nothing. If you think there are none of those, Alan Sokal would like to have a word with you. (Beating up on postmodernism is also pretty mainstream among Traditional Rationalists.)
You might also be interested in checking out what Mohandas Gandhi had to say about "the meaning of truth", just in case you were wondering what things are like in the rest of the world outside the halls of philosophy departments.


I haven't particularly run across any philosophy explicitly making the connection from the correspondence theory of truth to "There are causal processes producing map-territory correspondences" to "You have to look at things in order to draw accurate maps of them, and this is a general rule with no exception for special interest groups who want more forgiving treatment for their assertions". I would not be surprised to find out it existed, especially on the second clause.

DevilWorm and pragmatist point to the "reliabilism" school of philosophy (http://en.wikipedia.org/wiki/Reliabilism & http://plato.stanford.edu/entries/reliabilism). Clicking on either link reveals arguments concerned mainly with that old dispute over whether the word "knowledge" should be used to refer to "justified true belief". Going on the wording I'm not even sure whether they're considering how photons from the Sun are involved in correlating your visual cortex to your shoelaces. But it does increase the probability of a precedent - does anyone have something more specific? (A lot of the terminology I've seen so far is tremendously vague, and open to many interpretations...)
Incidentally, there might be an even higher probability of finding some explicit precedent in a good modern AI book somewhere?

Thank you for the link!
However, my (motivated, I admit) reading of that text is that Eliezer wants to bring attention to a paradox of downvoting a comment and discussing below the comment. Either the comment is an interesting discussion-starter, and then it should not be downvoted; or the comment is worthless, and then people should not start a discussion below it. Downvoting a comment and discussing below it is kind of supporting something you oppose.

Fair and edited. Also I left out "David's Sling".

Fair and added. Also there's a lovely new bit of Munchkin fiction called Harry Potter and the Natural 20 (the author has confirmed this was explicitly HPMOR-inspired) but I don't know if it's 'explicit rationalist fiction' yet, although it's possibly already a good fic to teach Munchkinism in particular.

Ah yes, sorry. Payoff matrices are ancient; the Tarski Method is visualizing one in response to a temptation to rationalize. Edited.

I have no idea what Impressionism is (I am not necessarily proud of this ignorance, since for all I know it does mean something important). Do you think that a panel of artists would be able to tell who was and wasn't "Impressionist" and mostly agree with each other? That does seem like a good criterion for whether there's sensory data that they're reacting to.

Mainstream status:
"The conceivability of being wrong" and "perspective-taking on beliefs" are old indeed; I wouldn't be the least bit surprised to find explicit precedent in Ancient Greece.
Skill 3 in the form "Trust not those who claim there is no truth" is widely advocated by modern skeptics fighting anti-epistemology.
Payoff matrices as used in the grid-visualization method are ancient; using the grid-visualization method in response to a temptation to rationalize was invented on LW as far as I currently know, as was the Litany of Tarski. (Not to be confused with Alfred Tarski's original truth-schemas.)

Quibble: "Your" territory?

Other people's maps are part of my territory.

I've slightly edited the OP to say that Tarski "described" rather than "defined" truth - I wish I could include more to reflect this valid point (indeed Tarski's theorems on truth are a lot more complicated and so are surrounding issues, no language can contain its own truth-predicate, etc.), but I think it might be a distraction from the main text. Thank you for this comment though!

I've edited the OP to try and compartmentalize off the example a bit more.

Or the downvoters are fast and early, the upvoters arrive later, which is what I've observed. I'm actually a bit worried about random downvoting of other users as well.

My guess? People are more or less randomly downvoting me these days, for standard fear and hatred of the admin. I suppose somebody's going to say that this is an excuse not to update, but it could also be, y'know, true. It takes a pretty baroque viewpoint to think that I was talking deliberate nonsense in that paragraph, and if anyone hadn't understood what I meant, they could've just asked.
To clarify in response to your particular reply:
Generally speaking but not always, for our belief about something to have behavioral consequences, we have to believe it has consequences which our utility function can run over, meaning it's probably linked into our beliefs about the rest of the universe, which is a good sign. There's all kinds of exceptions to this for meaningless beliefs that have behavioral consequences anyway, and a very large class of exceptions is the class where somebody else is judging what you believe, like the example someone not-Carl-who-Carl-probably-talked-to recently gave me for "Consubstantiality has the consequence that if it's true and you don't believe in it, God will send you to hell", which involves just "consubstantiality" and not consubstantiality, similarly with the tests being graded (my attempt to find a non-religious conjugate of something for which the religious examples are much more obvious).

Data point: Pomodoro did not work for me.

More compactly this is called "Hitler Ate Sugar".

Robbing any commons has consequences, otherwise it wouldn't be a commons problem.

If an author actually being X has no consequences apart from the professor believing that the author is "X", all consequences accrue to quoted beliefs and we have no reason to believe the unquoted form is meaningful or important. As for p-zombieness, it's not clear at this point in the sequence that this belief is meaningless rather than being false; and the negation of the statement, "people are not p-zombies", has phrasings that make no mention of zombiehood (i.e., "there is a physical explanation of consciousness") and can hence have behavioral consequences by virtue of being meaningful even if its intuitive "counterargument" has a meaningless term in it.

Prereqs.

I'd ding you for having confessed to being proud of your ignorance, except that what you confessed ignorance of was not, technically speaking, a fact.

The specs given to the illustrator were stick figures. I noticed the male prevalence and requested some female versions or replacement with actual stick figures.


Agreed. Though of course, I don't really see Faramir as disagreeing -- it was, after all, the Rangers of Ithilien who ambushed the Haradrim and killed the soldier they're talking about.

I'm a little bit proud that I don't know who all these people are.

This looks promising. Is it real, or did you verify that the words don't mean anything standard?

Ah, thanks!

I think there were fewer Google references back when I first made up the word... I will happily accept nominations for either an equally portentous-sounding but unused term, or a portentous-sounding real literary term that is known not to mean anything.

Thanks!

Yep. The main problem would be that I'd been writing for year and years before then, and, alas for our unfair universe, also have a certain amount of unearned talent; finding somebody who can pick up the Sequences and improve them without making them worse, despite their obvious flaws as they stand, is an extremely nontrivial hiring problem.

Can you amplify? I'd thought I'd looked this up.


Generalized across possible maps and possible cities, if your map of a city says "p" if and only iff p

If you can generalize over the correspondence between p and the quoted version of p, you have generalized over a correspondence schema between territory and map, ergo, invoked the idea of truth, that is, something mathematically isomorphic to in-general Tarskian truth, whether or not you named it.

In particular, "post-utopian" is not a real term so far as I know, and I'm using it as a stand-in for literary terms that do in fact have no meaning. If you think there are none of those, Alan Sokal would like to have a word with you.

Is this actually a standard term? I was trying to make up a new one, without having to actually delve into the pits of darkness and find a real postmodern literary term that doesn't mean anything.

I don't like the "post-utopian" example. I can totally expect differing sensory experiences depending on whether a writer is post-utopian or not. For example, if they're post-utopian, when reading their biography I would more strongly expect reading about them having been into utopian ideas when they were young, but having then changed their mind. And when reading their works, I would more strongly expect seeing themes of the imperfectability of the world and weltschmerz.

Thanks for demonstrating that not everyone already believes the contents of the post, then.

You are confusing the concept of a belief being true with the conditions under which you can know it to be true.

(The 'Mainstream Status' comment is intended to provide a quick overview of what the status of the post's ideas are within contemporary academia, at least so far as the poster knows. Anyone claiming a particular paper precedents the post should try to describe the exact relevant idea as presented in the paper, ideally with a quote or excerpt, especially if the paper is locked behind a paywall. Do not represent large complicated ideas as standard if only a part is accepted; do not represent a complicated idea as precedented if only a part is described. With those caveats, all relevant papers and citations are much solicited! Hopefully comment-collections like these can serve as a standard link between LW presentations and academic ones.)
The correspondence theory of truth is the first position listed in the Stanford Encyclopedia of Philosophy, which is my usual criterion for saying that something is a solved problem in philosophy. Clear-cut simple visual illustration inspired by the Sally-Anne experimental paradigm is not something I have previously seen associated with it, so the explanation in this post is - I hope - an improvement over what's standard.
Alfred Tarski is a famous mathematician whose theory of truth is widely known.
The notion of possible worlds is very standard and popular in philosophy; some of them even ascribe much more realism to them than I would (since I regard them as imaginary constructs, not thingies that can potentially explain real events as opposed to epistemic puzzles).
I haven't particularly run across any philosophy explicitly making the connection from the correspondence theory of truth to "There are causal processes producing map-territory correspondences" to "You have to look at things in order to draw accurate maps of them, and this is a general rule with no exception for special interest groups who want more forgiving treatment for their assertions". I would not be surprised to find out it existed, especially on the second clause.
Added: The term "post-utopian" was intended to be a made-up word that had no existing standardized meaning in literature, though it's simple enough that somebody has probably used it somewhere. It operates as a stand-in for more complicated postmodern literary terms that sound significant but mean nothing. If you think there are none of those, Alan Sokal would like to have a word with you. (Beating up on postmodernism is also pretty mainstream among Traditional Rationalists.)
You might also be interested in checking out what Mohandas Gandhi had to say about "the meaning of truth", just in case you were wondering what things are like in the rest of the world outside the halls of philosophy departments.

Koan answers here for:

What rule could restrict our beliefs to just propositions that can be meaningful, without excluding a priori anything that could in principle be true?


Out of curiosity, what in the 90s compares to Hikaru no Go or Madoka Magica?

This belief seems to me very convenient for the brilliant, implying that they got where they are by hard work and properly deserve everything they have. Of course brilliant people also have to put in hard work, but their return on investment is much higher than many other contenders who may have put in even more work for lower total returns. Just-world hypothesis; life is not this fair. And while I do go about preaching the virtue of Hufflepuff, I also go about saying that people should try to Huffle where they have comparative advantage.

I think I'd bet against it at 50-50 odds.

Well, the problem wasn't "we can't have sex", the problem was, "we can have sex but not privacy". I suspect that people are much less likely to go Munchkin when faced with something they can just grit their teeth for, however inconvenient.

Er... "marginal fallacy" sounds like it should involve failure to think on the margins. Sorry I'm late, but how about "the noncentral fallacy" or "the categorization fallacy"?



If I were evil, I would have people repeat Epiphany's slogan and they would think they were practicing dutiful nonconformity while actually their brain was thinking "Go Eliezer" all the time...

Recently, I wanted to spend some time with a certain lady in a bedroom... but the only available bedroom, her primary's, had doors with large glass windows in them; she remarked that she and her primary were considering trying to put up curtains across the door.
At some point thereafter, two large moving boxes were stacked up in front of the door, a blanket had been spread over them, a sitting pillow had gone on top of them, and a bed pillow had been stacked on top of that. It wasn't perfect privacy but it at least meant somebody would need an effort to see in, which in the generally libertine environment was as much as we cared about.
I realized afterward that I'd just turned into Harry in Chapter 16 of HPMOR, except that instead of asking how I could use every object in the room to kill someone, I was glancing at every single object in the room around me and reinterpreting it in terms of how I could use it to achieve my current objective of "block visibility into the room". And that apparently other people don't automatically Munchkin when confronted with real-life problems, and CFAR needs to come up with some sort of training method.

Go Eliezer!
Oops, must be all that priming.

Causal descent is a necessary but not sufficient condition, just like a QM-ignorant "physicalist" doesn't necessarily believe that if I grind you up and make a new person out of those "particular particles", it is the same person just in virtue of being made out the "same particles". Not that there's any such thing as the "same particles" in modern physics, just waves in a particle field, etc.

Bet received. I feel vaguely guilty and am reminding myself hard that money in my Paypal account is hopefully a good thing from a consequentialist standpoint.


The situation is sad, but I was expecting people to think about causality. It looks like they may just be associating emotions with salient features.
If this is what happens, the Dark Arts potential for exploiting this are enormous.

Well, the authorial possibilities are certainly enormous. "The Sword of Good" runs on this, for example.

Hell, no! You can totally fix your stories afterward. I do it all the time. Why wouldn't I?

This is stunningly good. There are only a few flaws and they would be very easy to fix, like the explicit mention of "evolutionary psychology" - which can be fixed by just deleting the texts "from evolutionary psychology" and "the dominance of individual over group selection" and leaving the rest of the paragraph exactly unchanged...
...actually, I think that's the only flaw I found, which is amazing.

Other: Leaning toward a causal view. In other words, your past self has to be the cause of your future self, but the specific atoms are irrelevant.

As wedifrid says, this is a no-brainer "accept" (including the purchasing-power-adjusted caveat). If you are inside the US and itemize deductions, please donate to SIAI, otherwise I'll accept via Paypal. Your implied annual interest rate assuming a 100% probability of winning is 0.7% (plus inflation adjustment). Please let me know whether you decide to go through with it; withdrawal is completely understandable - I have no particular desire for money at the cost of forcing someone else to go through with a bet they feel uncomfortable about. (Or rather, my desire for $100 is not this strong - I would probably find $100,000 much more tempting.)

General defense of the above type of reply: Voting "Other" on questions that seem to you confused or seem to turn on irrelevant matters of small definitions, rather than making up a definition and running with it, etcetera, is probably a good barometer of LW-vs.-philosophy opinion.
The subject matter of humanity::morality is a mathematical object which Clippy could calculate, if it ever had any reason to do so, which it wouldn't, but it could, without being at all motivated to do anything about that. However, if "morality" is being given an agent relative definition then no, whatever you're not motivated to do anything about, even in the slightest, doesn't seem like it should be called Alejandro::morality.

If I were evil, I would have people repeat Epiphany's slogan and they would think they were practicing dutiful nonconformity while actually their brain was thinking "Go Eliezer" all the time...

Recently, I wanted to spend some time with a certain lady in a bedroom... but the only available bedroom, her primary's, had doors with large glass windows in them; she remarked that she and her primary were considering trying to put up curtains across the door.
At some point thereafter, two large moving boxes were stacked up in front of the door, a blanket had been spread over them, a sitting pillow had gone on top of them, and a bed pillow had been stacked on top of that. It wasn't perfect privacy but it at least meant somebody would need an effort to see in, which in the generally libertine environment was as much as we cared about.
I realized afterward that I'd just turned into Harry in Chapter 16 of HPMOR, except that instead of asking how I could use every object in the room to kill someone, I was glancing at every single object in the room around me and reinterpreting it in terms of how I could use it to achieve my current objective of "block visibility into the room". And that apparently other people don't automatically Munchkin when confronted with real-life problems, and CFAR needs to come up with some sort of training method.

Go Eliezer!
Oops, must be all that priming.

Causal descent is a necessary but not sufficient condition, just like a QM-ignorant "physicalist" doesn't necessarily believe that if I grind you up and make a new person out of those "particular particles", it is the same person just in virtue of being made out the "same particles". Not that there's any such thing as the "same particles" in modern physics, just waves in a particle field, etc.

Bet received. I feel vaguely guilty and am reminding myself hard that money in my Paypal account is hopefully a good thing from a consequentialist standpoint.


The situation is sad, but I was expecting people to think about causality. It looks like they may just be associating emotions with salient features.
If this is what happens, the Dark Arts potential for exploiting this are enormous.

Well, the authorial possibilities are certainly enormous. "The Sword of Good" runs on this, for example.

Hell, no! You can totally fix your stories afterward. I do it all the time. Why wouldn't I?

This is stunningly good. There are only a few flaws and they would be very easy to fix, like the explicit mention of "evolutionary psychology" - which can be fixed by just deleting the texts "from evolutionary psychology" and "the dominance of individual over group selection" and leaving the rest of the paragraph exactly unchanged...
...actually, I think that's the only flaw I found, which is amazing.

Other: Leaning toward a causal view. In other words, your past self has to be the cause of your future self, but the specific atoms are irrelevant.

As wedifrid says, this is a no-brainer "accept" (including the purchasing-power-adjusted caveat). If you are inside the US and itemize deductions, please donate to SIAI, otherwise I'll accept via Paypal. Your implied annual interest rate assuming a 100% probability of winning is 0.7% (plus inflation adjustment). Please let me know whether you decide to go through with it; withdrawal is completely understandable - I have no particular desire for money at the cost of forcing someone else to go through with a bet they feel uncomfortable about. (Or rather, my desire for $100 is not this strong - I would probably find $100,000 much more tempting.)

General defense of the above type of reply: Voting "Other" on questions that seem to you confused or seem to turn on irrelevant matters of small definitions, rather than making up a definition and running with it, etcetera, is probably a good barometer of LW-vs.-philosophy opinion.
The subject matter of humanity::morality is a mathematical object which Clippy could calculate, if it ever had any reason to do so, which it wouldn't, but it could, without being at all motivated to do anything about that. However, if "morality" is being given an agent relative definition then no, whatever you're not motivated to do anything about, even in the slightest, doesn't seem like it should be called Alejandro::morality.

Other: What the hell does Solomonoff Induction count as?

"Gods are ontologically distinct from creatures, or they're not worth the paper they're written on." -- Damien Broderick.
Can anyone exhibit an actual theist who says that a Matrix Lord composed of non-mental, non-mysterious parts counts as a God? So far as I know this position is held solely by people who want to mock the Simulation Hypothesis.

I just hit "Accept: Theism" by accident. Yes, accident, not divine providence, thank you very much. Is there a way to revote?

http://lesswrong.com/r/discussion/lw/ens/why_humans_are_sometimes_less_rational_than/
(nominated by sixesandsevens)

Robin, even if this market theoretically should recover interest rates, don't you think it might be interesting to have market participants who are actually thinking about Singularity-type issues, to see if that market recovers a different interest rate? Or to look at it another way, with this market you could formally issue "put your money where your mouth is" challenges that wouldn't be quite so unethical as taking a loan from a banker because you don't expect to ever pay it back.I know that I wouldn't be comfortable with taking out a loan because I believed the due date in 2080 was post-Singularity, unless this were explicitly understood by both parties to the agreement. For that matter, I'd want a clause saying that a Friendly intelligence explosion obviated the loan even if the event otherwise preserved markets.Sebastian, the agreement would have to obligate the estate or descendants of a 50-year-old who wanted to bet on 2080. Again, the main objective is to let people put their money where their mouth is - let Kurzweil bet against Hofstadter's "more than 100 years to AI" pronouncement, even if the payoff is made by Kurzweil's estate to Hofstadter's estate (do they have kids)?

Dear past Eliezer: Robin is just right here, your idea doesn't work, accept it and move on.

Robin Hanson kinda killed it here:
http://lesswrong.com/lw/ie/the_apocalypse_bet/ekw
Did Jones answer Hanson's objection? Rereading, it seems a bit knockdown, and past-EY's attempts to evade it seem like past-EY was updating too slowly, I shoulda just said "Oops."

This type of Hollywood Rationality is explicitly listed in TV Tropes under "Straw Vulcan" (failure to understand that goal achievement may require locally backward steps). I haven't encountered anyone who explicitly associates that with LW in particular, and anyone who's taken an introductory AI course should know better than to think a backward step is beyond computation, logic, Bayesian agents, etc.

Knowledge: empiricism or rationalism?



 Accept: empiricism


 Lean toward: empiricism


 Accept: rationalism


 Lean toward: rationalism


 Other






These predictions frequently do not overlap with what existing cognitive science would have one expect.

What is an example of a case you've actually observed where CT made a falsifiable, bold, successful prediction? ("Falsifiable" - say what would have made the prediction fail. "Bold" - explain what a cogsci guy or random good human psychologist would have falsifiably predicted differently.)

My recollection of my conversation with Geoff, at a Berkeley LW meetup, in Berkeley University in a building that had a statue of a dinosaur in it, is that it went like this. Disclaimer: My episodic memory for non-repeated conversations is terrible and it is entirely possible that there are major inaccuracies here. Disclaimer 2: This is not detailed enough to count as an engaged critique, and Geoff is not obliged to respond to it since I put very little effort into it myself (it is logically rude to demand ever-more conversational effort from other people while putting in very little yourself).
Geoff: I've been working on an incredible new mental theory that explains everything.
Eliezer (internally): That's not a good sign, but he seems earnest and intelligent. Maybe it's something innocuous or even actually interesting.
Eliezer (out loud): And what does it say?
Geoff: Well, I haven't really practiced it explaining it, and I don't expect you to believe it, but (explains CT)
Eliezer (internally): Well this is obviously wrong. Minds just don't work by those sorts of bright-line psychoanalytic rules written out in English, and proposing them doesn't get you anywhere near the level of an interesting cognitive algorithm. Maybe if he's read enough of the Sequences and hasn't invested too many sunk costs / bound up too many hopes in it, I can snap him out of it in fairly short order?
Eliezer (out loud): Where does CT make a different prediction from the cognitive science I already know that I couldn't get without CT?
Geoff: It predicts that people will change their belief to believe that their desires will be fulfilled...
Eliezer (internally): Which sounds a lot like standard cognitive dissonance theory, which itself has been modified in various ways, but we aren't even at the point of talking about that until we get out of the abstract-belief trap.
Eliezer (out loud): No, I mean some sort of sensory experience. Like your eyes seeing an apple fall from a tree, or something like that. What does CT say I should experience seeing, that existing cognitive science wouldn't tell me to expect?
Geoff: (Something along the lines of "CT isn't there yet", I forget the exact reply.)
Eliezer (internally): This is exactly the sort of blind alley that the Sequences are supposed to prevent smart people from wasting their emotional investments on. I wish I'd gotten this person to read the Belief and Anticipation sequence before CT popped into his head, but there's no way I can rescue him from the outside at this point.
Eliezer (out loud): Okay, then I don't believe in CT because without evidence there's no way you could know it even if it was true.
I think there might've also been something about me trying to provide a counterexample like "It is psychologically possible for mothers to believe their children have cancer" but I don't recall what Geoff said to that. I'm not sure whether or not I gave him any advice along the lines of, "Try to explain one thing before explaining everything."


I'm sure this observation has been made plenty of times before: a principal can gain negotiating power by delegating negotiations to an agent, and restricting that agent's ability to negotiate.

Well that sure can't be an equilibrium of a completed timeless decision theory with reflective consistency. Your delegees are more powerful because they have fewer choices? Why wouldn't you just rewrite your source code to eliminate those options? Why wouldn't you just not do them? And why would the other agent react any differently to the delegate than to the source-code change or the decision in the moment?

The thought that Luke was trying to sabotage my position, consciously or unconsciously, honestly never crossed my mind until I read this comment. Having now considered the hypothesis rather briefly, I assign it a rather low probability. Luke's not like that.

The geese and babies aren't sentient, wifi costs the provider very little, that's actually a different Chris Brown, and I take the money I get paid lobbying for SOPA and donate it to efficient charities!
(Sorry, couldn't resist when I saw the "babies" part.)

Move to Main, please!

I was just using "Earth" as a synonym for "the world as we know it".


Only a single conclusion is possible: LukeProg is a TRAITOR!

I can understand why this would be negatively received by some---it is clearly hyperbole with a degree of silliness involved. That said---and possibly coincidentally---there is a serious point here. In fact it is the most salient point I noticed when reading the post and initial responses.
In most social hierarchies this post would be seen as a betrayal. An unusually overt and public political move against Eliezer. Not necessarily treason, betrayal of the tribe, it is a move against a rival. Of course it would certainly be in the interest of the targeted rival to try to portray the move as treason (or heresy, or whatever other kind of betrayal of the tribe rather than mere personal conflict.)
The above consideration is why I initially expected Eliezer to agree to a larger extent than he did (which evidently wasn't very much!) Before making public statements of a highly status sensitive nature regarding an ally the typical political actor will make sure they aren't offending them---they don't take the small risk establishing an active rivalry unless they are certain the payoffs are worth it.
This (definitely!) isn't to say that any of the above applies to this situation. Rationalists are weird and in particular can have an unusual relationship between their intellectual and political expression. ie. They sometimes go around saying what they think.

The problem is that if you initiate it, it's subject to the Loss Aversion effect where the dissatisfied speak up in much greater numbers.

Depends how you interpret the proverb. If you told me the Earth would last a hundred years, it would increase the immediate priority of CFAR and decrease that of SIAI. It's a moot point since the Earth won't last a hundred years.

Mostly fields which are producing lots of "statistically significant" results and no universal generalizations (where "This happens to everyone with gene X" is a universal generalization even if not everyone has gene X). Conductive plastics doesn't sound too bad because you can tell whether or not a plastic conducts pretty clearly.

It makes a very important reply to anyone who claims that e.g. you should stick with Occam's original Razor and not try to rephrase it in terms of Solomonoff Induction because SI is more complicated.

Made the comment, realized it didn't add anything.

Moved to Discussion.




the OP is vastly overstating how much of the Sequences are similar to the standard stuff out there... I think Luke is being extremely charitable in his construal of what's "already" been done in academia

Do you have a Greasemonkey script that rips all the qualifying words out of my post, or something? I said things like:

"Eliezer's posts on evolution mostly cover material you can find in any good evolutionary biology textbook"
"much of the Quantum Physics sequence can be found in quantum physics textbooks"
"Eliezer's metaethics sequences includes dozens of lemmas previously discussed by philosophers"
"Eliezer's free will mini-sequence includes coverage of topics not usually mentioned when philosophers discuss free will (e.g. Judea Pearl's work on causality), but the conclusion is standard compatibilism."
"[Eliezer's posts] suggest that many philosophical problems can be dissolved into inquiries into the cognitive mechanisms that produce them, as also discussed in"
"[Eliezer's posts] make the point that value is complex, a topic explored in more detail in..."

Your comment above seems to be reacting to a different post that I didn't write, one that includes (false) claims like: "The motivations, the arguments by which things are pinned down, the exact form of the conclusions are mostly the same between The Sequences and previous work in mainstream academia."

I have yet to encounter anyone who thinks the Sequences are more original than they are.

Really? This is the default reaction I encounter. Notice that when the user 'Thomas' below tried to name just two things he thought were original with you, he got both of them wrong.
Here's a report of my experiences:

People have been talking about TDT for years but nobody seems to have noticed Spohn until HamletHenna and I independently stumbled on him this summer.
I do find it hard to interpret the metaethics sequence, so I'm not sure I grok everything you're trying to say there. Maybe you can explain it to me sometime. In any case, when it comes to the pieces of it that can be found elsewhere, I almost never encounter anyone who knows their earlier counterparts in (e.g.) Railton & Jackson -- unless I'm speaking to someone who has studied metaethics before, like Carl.
A sizable minority of people I talk to about dissolving questions are familiar with the logical positivists, but almost none of them are familiar with the recent cogsci-informed stuff, like Shafir (1998) or Talbot (2009).
As I recall, Less Wrong had never mentioned the field of "Bayesian epistemology" until my first post, The Neglected Virtue of Scholarship.
Here's a specific story. I once told Anna that once I read about intelligence explosion I understood right away that it would be disastrous by default, because human values are incredibly complex. She seemed surprised and a bit suspicious and said "Why, had you read Joshua Greene?" I said "Sure, but he's just one tip of a very large iceberg of philosophical and scientific work demonstrating the complexity of value. I was convinced of the complexity of value long ago by metaethics and moral psychology in general."


Several of these citations are from after the originals were written! Why not (falsely) claim that academia is just agreeing with the Sequences, instead?

Let's look at them more closely:

Lots of cited textbooks were written after the Sequences, because I wanted to point people to up-to-date sources, but of course they mostly summarize results that are a decade old or older. This includes books like Glimcher (2010) and Dolan & Sharot (2011).
Batson (2011) is a summary of Batson's life's work on altruism in humans, almost all of which was published prior to the Sequences.
Spohn (2012) is just an update to Spohn's pre-Sequences on work on his TDT-ish decision theory, included for completeness.
Talbot (2009) is the only one I see that is almost entirely composed of content that originates after the Sequences, and it too was included for completeness immediately after another work written before the Sequences: Sharif (1998).


I don't understand what the purpose of this post was supposed to be - what positive consequence it was supposed to have.

That's too bad, since I answered this question at the top of the post. I am trying to counteract these three effects:

Some readers will mistakenly think that common Less Wrong views are more parochial than they really are.
Some readers will mistakenly think Eliezer's Sequences are more original than they really are.
If readers want to know more about the topic of a given article, it will be more difficult for them to find the related works in academia than if those works had been cited in Eliezer's article.

I find problem #1 to be very common, and a contributor to the harmful, false, and popular idea that Less Wrong is a phyg. I've been in many conversations in which (1) someone starts out talking as though Less Wrong views are parochial and weird, and then (2) I explain the mainstream work behind or similar to every point they raise as parochial and weird, and then (3) after this happens 5 times in a row they seem kind of embarrassed and try to pretend like they never said things suggesting that Less Wrong views are parochial and weird, and ask me to email them some non-LW works on these subjects.
Problem #2 is common (see the first part of this comment), and seems to lead to phygish hero worship, as has been pointed out before.
Problem #3, I should think, is uncontroversial. Many of your posts have citations to related work, most of them do not (as is standard practice in the blogosphere), and like I said I don't think it would have been a good idea for you to spend time digging up citations instead of writing the next blog post.

writing something that predictably causes some readers to get the impression that ideas presented within the Sequences are just redoing the work of other academics, so that they predictably tweet ...I do not think the creation of this misunderstanding benefits anyone

Predictable misunderstandings are the default outcome of almost anything 100+ people read. There's always a trade-off between maximal clarity, readability, and other factors. But, I'm happy to tweak my original post to try to counteract this specific misunderstanding. I've added the line: "(edit: probably most of their content is original)".

[Further reading, I would guess] gave Luke an epiphany he's trying to share - there's a whole world out there, not just LW the way I first thought.

Remember that I came to LW with a philosophy and cogsci (especially rationality) background, and had been blogging about biases and metaethics and probability theory and so on at CommonSenseAtheism.com for years prior to encountering LW.

I get what this is trying to do. There's a spirit in LW which really is a spirit that exists in many other places, you can get it from Feynman, Hofstadter, the better class of science fiction, Tooby and Cosmides, many beautiful papers that were truly written to explain things as simply as possible, the same place I got it.

That is definitely not the spirit of my post. If you'll recall, I once told you that if all human writing were about to be destroyed except for one book of our choosing, I'd go with The Sequences. You can't get the kind of thing that CFAR is doing solely from Feynman, Kahneman, Stanovich, etc. And you can't get FAI solely from Good, Minsky, and Wallach -- not even close. Again, I get the sense you're reacting to a post with different phrasing than the one I actually wrote.

So they won't actually read the literature and find out for themselves that it's not what they've already read.

Most people won't read the literature either you or I link to. But many people will, like Wei Dai.
Case in point: Remember Benja's recent post on UDT that you praised as "Original scientific research on saving the world"? Benja himself wrote that the idea for that post clicked for him as a result of reading one of the papers on logical uncertainty I linked to from So You Want to Save the World.
Most people won't read my references. But some of those who do will go on to make a sizable difference as a result. And that is one of the reasons I cite so many related works, even if they're not perfectly identical to the thing me or somebody else is doing.


Do you have a Greasemonkey script that rips all the qualifying words out of my post, or something?

All readers have a Greasemonkey script that rips all the qualifying words out of a post. This is a natural fact of writing and reading.

Your comment above seems to be reacting to a different post that I didn't write

Not the post you wrote - the post that the long-time LWer who Twittered "Eliezer's Yudkowsky's Sequences are mostly not original" read. The actual real-world consequences of a post like this when people actually read it are what bothers me, and it does feel frustrating because those consequences seem very predictable - like you're living in an authorial should-universe. Of course somebody's going to read that post and think "Eliezer Yudkowsky's Sequences are mostly not original"! Of course that's going to be the consequence of writing it! And maybe it's just because I was reading it instead of writing it myself, without having all of your intentions so prominently in my mind, but I don't see why on Earth you'd expect any other message to come across than that. A few qualifying words don't have the kind of power it takes to stop that from happening!

Granted.

Just what a zombie would say!
(Sorry, couldn't resist.)

Why would that actually be a consequence of the OP as written?

I wonder if the old fogeys are running on autopilot.

There's Robyn Dawes's Rational Choice in an Uncertain World which is highly similar in spirit, intent, and style to the kind of defeat-the-bias writing in the Sequences. (And it's quoted accordingly when I borrow something.)

...and pre-formulated by John W. Campbell, a famous science-fiction editor.

And lo, people began tweeting:

Eliezer Yudkowsky's "Sequences" are mostly not original

Which is false. This pushes as far in the opposite wrong direction as the viewpoint it means to criticize.
Evolutionary biology, the non-epistemological part of the exposition of quantum mechanics, and of course heuristics and biases, are all not original. They don't look deceptively original either; they cite or attributed-quote the sources from which they're taken. I have yet to encounter anyone who thinks the Sequences are more original than they are.
When it comes to the part that isn't reporting on standard science, the parts that are mostly dealt with by modern "philosophers" rather than experimental scientists of one kind or another, the OP is vastly overstating how much of the Sequences are similar to the standard stuff out there. There is such a vast variety of philosophy that you can often find a conclusion similar to anything, to around the same degree that Leibniz's monadology anticipated timeless quantum mechanics, i.e., not very much. The motivations, the arguments by which things are pinned down, the exact form of the conclusions, and what is done with those conclusions, is most of the substance - finding a conclusion that happens to look vaguely similar does not mean that I was reporting someone else's academic work and failing to cite it, or reinventing work that had already been done. It is not understating any sort of "close agreement" with even those particular concluders, let alone the field as a whole within which those are small isolated voices. Hofstadter's superrationality is an acknowledged informal forerunner of TDT. But finding other people who think you ought to cooperate in the PD, but can't quite formalize why, is not the same as TDT being preinvented. (Also TDT doesn't artifically sever decision nodes from anything upstream; the idea is that observing your algorithm, but not its output, is supposed to screen off things upstream. This is "similar" to some attempts to rescue evidential decision theory by e.g. Eels, but not quite the same thing when it comes to important details like not two-boxing on Newcomb's Problem.) And claiming that in principle philosophical intuitions arise within the brain is not the same as performing any particular dissolution of a confused question, or even the general methodology of dissolution as practiced and described by Yudkowsky or Drescher (who actually does agree and demonstrate the method in detail within "Good and Real").
I'm also still not sure that Luke quite understands what the metaethics sequence is trying to say, but then I consider that sequence to have basically failed at exposition anyway. Unfortunately, there's nothing I can point Luke or anyone else at which says the same thing in more academic language.
Several of these citations are from after the originals were written! Why not (falsely) claim that academia is just agreeing with the Sequences, instead?
I don't understand what the purpose of this post was supposed to be - what positive consequence it was supposed to have. Lots of the Sequences are better exposition of existing ideas about evolutionary biology or cognitive biases or probability theory or whatever, which are appropriately quoted or cited within them? Yes, they are. People introducing Less Wrong should try to refer to those sources as much as possible when it comes to things like heuristics and biases, rather than talking like Eliezer Yudkowsky somehow invented the idea of scope insensitivity, so that they don't sound like phyg victims? Double yes. But writing something that predictably causes some readers to get the impression that ideas presented within the Sequences are just redoing the work of other academics, so that they predictably tweet,

Eliezer Yudkowsky's "Sequences" are mostly not original

...I do not think the creation of this misunderstanding benefits anyone. It is also a grave sin to make it sound like you're speaking for a standard academic position when you're not!
And I think Luke is being extremely charitable in his construal of what's "already" been done in academia. If some future anti-Luke is this charitable in construing how much of future work in epistemology and decision theory was "really" all done within the Sequences back in 2008, they will claim that everything was just invented by Eliezer Yudkowsky way back then - and they will be wrong - and I hope somebody argues with that anti-Luke too, and doesn't let any good feeling for ol E. Y. stand in their way, just like we shouldn't be prejudiced here by wanting to affiliate with academia or something.
I get what this is trying to do. There's a spirit in LW which really is a spirit that exists in many other places, you can get it from Feynman, Hofstadter, the better class of science fiction, Tooby and Cosmides, many beautiful papers that were truly written to explain things as simply as possible, the same place I got it. (Interesting side note: John Tooby is apparently an SF fan who grew up reading van Vogt and Null-A, so he got some of his spirit from the same sources I did! There really is an ancient and honorable tradition out there.) If someone encounters that spirit in LW for the first time, they'll think I invented it. Which I most certainly did not. If LW is your first introduction to these things, then you really aren't going to know how much of the spirit I learned from the anncient masters... because just reading a citation, or even a paragraph-long quote, isn't going to convey that at all. The only real way for people to learn better is to go out and read Language in Thought and Action or The Psychological Foundations of Culture. Doing this, I would guess, gave Luke an epiphany he's trying to share - there's a whole world out there, not just LW the way I first thought. But the OP doesn't do that. It doesn't get people to read the literature. Why should they? From what they can see, it's already been presented to them on LW, after all. So they won't actually read the literature and find out for themselves that it's not what they've already read.
There's literature out there which is written in the same spirit as LW, but with different content. Now that's an exciting message. It might even get people to read things.


The simplest explanation for the universe is that it doesn't exist. It's not popular, because the universe seems to exist. Explanations need to be adeqaute to the facts, not just simple.

Upvoted for this line alone. See also, "If nothing exists, I want to know how the nothing works and why it seems to be so highly ordered."

Even if examining the brain will make you less confused someday, correctly believing that proposition does not make you any less confused right now.

From the standpoint of somebody feeling confused about qualia, the trouble with this solution is not that it is necessarily false but that accepting it doesn't make you feel any less confused.

It seems to me that most of the trepidation I associate with this problem comes from the difficulty of designing good experiments that are actually worth performing and would actually discriminate between two plausible hypotheses, not so much not knowing how to get enough research subjects.

Moved to Discussion.

Umm. Am I misunderstanding something, or is this post saying that we should "solve" the problem of qualia by accepting that we're all p-zombies?

http://code.google.com/p/lesswrong/issues/list

Radial categories, prototype theory, typicality effects? Or not what you're looking for?

Thank you both. Very much, and sincerely.


You... know I don't optimize dinner parties as focus groups, right?

That's kinda the point.

All you have to do is run into me in any venue whatsoever where the attendees weren't filtered by their interest in meta threads. :)

At the time I make this reply, DanArmak's comment was downvoted (I voted it back up). Downvoting a comment like that above is the sort of reason why I am starting to distrust the behavior of meta-threads as a reliable signal of what the community thinks.


Is it just me or do others also find that Eliezer coming of as a tad petulant with the way he is handling people systematically opposing and downvoting his proposal? Every time he got downvoted to oblivion he just came back with a new comment seemingly crafted to be more belligerent, whiny, condescending and cynical about the community than the last. (That's hyperbole---in actuality it peaked in the middle somewhere.) Now we just keep getting reminded about it at every opportunity as noise in unrelated threads.

I observe that wedifrid has taken advantage of this particular opportunity to remind everyone that he thinks I am belligerent, whiny, condescending, and cynical.
(So noted because I was a bit unhappy at how the conversation suddenly got steered there.)

Yep. Nobody was proposing retroactive.

Correct. Sorry, the button I use says "Ban".

Above all:
3) Newcomers who arrive at the site see productive discussion of new ideas, not a flamewar, in the Recent Comments section.
4) Trolls are not encouraged to stay; people who troll do not receive attention-reward for it and do not have their brain reinforced to troll some more. Productive discussion is rewarded by attention.

Keep in mind that it's not "more people" it's more "people who participate in meta threads on Less Wrong". I've observed a tremendous divergence between the latter set, and "what LWers seem to think during real-life conversations" (e.g. July Minicamp private discussions of LW which is where the anti-troll-thread ideas were discussed, asking what people thought about recent changes at Alicorn's most recent dinner party). I'm guessing there's some sort of effect where only people who disagree bother to keep looking at the thread, hence bother to comment.
Some "people" were claiming that we ought to fix things by moderation instead of making code changes, which does seem worth trying; so I've said to Alicorn to open fire with all weapons free, and am trying this myself while code work is indefinitely in progress. I confess I did anticipate that this would also be downvoted even though IIRC the request to do that was upvoted last time, because at this point I've formed the generalization "all moderator actions are downvoted", either because only some people participate in meta threads, and/or the much more horrifying hypothesis "everyone who doesn't like the status quo has already stopped regularly checking LessWrong".
I'm diligently continuing to accept feedback from RL contact and attending carefully to this non-filtered source of impressions and suggestions, but I'm afraid I've pretty much written-off trying to figure out what the community-as-a-whole wants by looking at "the set of people who vigorously participate in meta discussions on LW" because it's so much unlike the reactions I got when ideas for improving LW were being discussed at the July Minicamp, or the distribution of opinions at Alicorn's last dinner party, and I presume that any other unfiltered source of reactions would find this conversation similarly unrepresentative.

I've not seen retroactive penalties proposed anywhere; the current system warns you when you start if a penalty applies for making a comment, presumably that wouldn't change.

That one's in progress, I think.
Also, to reply to a comment elsewhere in thread, obviously penalties are not going to be charged retrospectively if an ancestor later goes to -3. Nobody has proposed this. Navigating the LW rules is not intended to require precognition.

I've banned all of eridu's recent comments (except a few voted above 0) as an interim workaround, since hiding-from-Recent-Comments and charge-fee-to-all-descendants is still in progress for preventing future threads like these.
I respectfully request that you all stop doing this, both eridu and those replying to him.

"No" and "It wouldn't", indeed. But heritable penalties once something does go to -3 would prevent users with zero or lower karma from replying further, thus preventing the current thread from happening again.

Meta-note: Right now, as I check the top comments for today, all the top comments for today are replies to heavily downvoted comments. This is the behavior the downvoted-thread-killer was meant to prevent, but we don't yet have "troll-toll all descendants" feature. Noting this because multiple people asked for examples and how often something like it happened.


I want to want akrasia.

Um...

The criticism is that a martial artist or scientist is actually trying to attain a highly specific brain-state in which neurons have particular patterns in them; a feeling of emptiness, even if part of this brain state, is itself a neural pattern and certainly does not correspond to the absence of a mind.
The zeroth virtue or void - insofar as we believe in it - corresponds to particular mode of thinking; it's certainly not an absence of mind. Emptiness, no-mind, the Void of Musashi, all these things are modes of thinking, not the absence of any sort of reified spiritual substance. See also the fallacy of the ideal ghost of perfect emptiness in philosophy.

Briefly checked it, and it looks like straw-economics to me. (I stopped reading at the point where they claimed that economists assume no barriers to entry.)

(instantaneous reflex activated)
What if I gave you $500, then asked you if you wanted to spend it on the ticket?
I'd also like to know whether some unexpected expense, like needing a $500 dental crown, would change your mind about accepting the free $500 instead of the free ticket.

Section C of An Anarchist FAQ makes a very similar point but with economics perpetuating capitalism instead of evolutionary psychology perpetuating patriarchy.
I don't know much 'real' (as opposed to straw-) economics to tell to what extent what they say is accurate, but assuming they are not lying about what economics textbooks say, either economics as a discipline is seriously fucked up or their textbooks contain an absurd amount of lies-to-children. (I haven't seen an undergraduate physics textbook describing the 'dry water' (zero viscosity, as von Neumann called it) model without also mentioning that it's nearly useless as an approximation of the real behaviour of real fluids and giving an example of how preposterous its predictions are.)

Of course it's not just women! Women (outside the community, that is) are more likely to respond that way than men, but that's from a study on both risk aversion and hyperbolic discounting which showed that "Women can't take small risks and men are creatures of the now", with both effects diminishing as scores on the Cognitive Reflection Test increased.
I now wonder what would happen if I asked a man on the street to choose between $500 immediately or $1 million in 10 years (= 113% annual interest) - a version that extreme wasn't in the original study, just the extreme version of the risk-aversion Q. I wouldn't expect it to work, but then I wouldn't have expected it to work with risk aversion either!

...
...
dur....
....

Agree that the OKCupid technique probably works too. But I wasn't suggesting that we put up broad recruiting posters in the math department to solve the gender ratio thingy; I was suggesting that rationalist men seeking convertible mates try to date mathematical women. As Lucas observes, our community is still small enough that this provides a relatively large pool.

As long as it's got at least one lady who hasn't already been recruited, what difference does that make?

Yep, though it's weaker evidence to observe that (straight) female rationalists don't go back when they can have their pick of mates and/or an entire harem by staying.
Actually, I have seen a couple of cases of women using their newly acquired Sanity Attractiveness Points(*) to pull in hot guys they want from outside the community, though in both such cases they still had rationalist mates on the side.
(*) = According to the one woman whose case I know in detail, this is apparently a pretty strong effect - a female from within rationalist culture, dealing with a guy from outside rationalist culture who has an unmet need for sanity, may appear unto him as a Goddess. Sort of the dating equivalent of what happens when people with unmet needs discover LW or read HPMOR.

I hate to No-True-Scotswoman you but I can't help but wonder exactly how rational she was - the cases I know have all been drawn from either East Coast or West Coast whole communities with corresponding personal transmission of skills.

I've only ever seen one case of a man who'd previously had a rationalist mate going back to nonrationalist mates afterward. The reason why the gender skew of our culture is a mating problem for men is that once you go rationalist you don't go back.
"Go to the physics department, find a woman you consider attractive, point her at HPMOR, and see if anything develops" sounds like more useful advice to me.
For (straight) men who insist on dating externally, asking a woman whether she would prefer a certainty of $500 or a 15% chance at $1 million seems likely to be a surprisingly good filter on potential mates. I didn't believe it either as first, but I've verified that many women, and in at least one case a female grad-student doing advanced math homework, says she would rather have the $500; while every woman I've tested inside our community - regardless of her math/science/economics level or her ability to talk glibly about explicit rationality - takes the 15% chance at $1M with a puzzled look and 'Is this a trick question?'

It is fair to observe that when somebody claims that their utility function says one thing but their deontology prevents them from following up, that is at least suspicious for one or the other being not-fully-motivating, not-fully-thought-out, etc.

I don't think that sentence can be successfully said outside LW, but not because of the FAE, more like the Just-World Fallacy and Appeal to Consequences. It would go something like this:
1) In a just world, behavior standards would not vary for men by status or attractiveness (because in a just world they would all have equal status and attractiveness, or women would not be moved by status or attractiveness).
2) Therefore, unhappiness-avoiding behavior standards should not vary by status or attractiveness (contrary to the actual fact that in an unjust world, some things will make (some) women feel uncomfortable/unhappy only if the man's attractiveness/status is below a (varying) particular level).
3) Therefore, a woman who admits that behavior X would not make her feel creepy if a sufficiently more attractive man did it, is being unfair to lower-status men, is wrong to label the behavior "creepy", cannot justly blame the lower-status man for doing what would be okay for a higher-status man to do, is just being shallow, is applying a double standard, etc.
4) It's impossible to have an explicit social standard for men which says, "If you think you're in the upper 20% of attractiveness you can sit down next to a woman touching her, otherwise this will make her feel creepy and you should avoid doing so." This rule would not be optimal/justified in a just world, so it must not be allowed in this one either.
5) Thus if we admit that whether sitting down touching someone is "creepy" depends on how attractive they are, it will be impossible to prevent men from doing things that make women feel creeped out, or for women to be listened-to when they object, in which case women will feel creeped out, which is bad.
6) By appeal to consequences, it must not be true that a woman's sense of creeped-out-ness can vary with a male's attractiveness or status.
The typical resolution of a situation like this, I think, is that you have an explicit standard which says "You can't hug people without asking", but there will be an unspoken selective lack of prosecution (like how cops don't get traffic tickets and white people don't go to jail for drug crimes) when an attractive man engages in the behavior.

Are you interested in advice that would only be useful if you wanted to do a complete rewrite of the fic at some point, or is advice only useful if it's for improving the fic basically as it stands?
In-between case: I would have either Luna or Twilight be the sub-AI that wants to honestly explain the Celestia AI's workings to company employees without justifying anything, while Celestia is relentlessly cheerful as she persuasively argues everything from the Friendship and Ponies standpoint.

Concretely: A configuration with amplitude 10 (and measure 100) will split its flow into two configurations 7.07 (and hence measure 50 each).
(Of course these are actually blobs whose measure is a multidimensional integral over the amplitude's squared modulus, and we'd be looking at the equivalent of 5 + 5i and 5 - 5i so that they linearly sum to the original 10 while having length 7.07 each, but whatever...)


So the creepiness of an action depends on how much I like the person who does it.

I think this is a very important sentence. It illustrates how typical, colloquial usage of the word "creepy" can run afoul of the fundamental attribution error.

Yep.

Since this comment got more upvotes than the article itself, I'm moving to Discussion.

The following quotes were heavily upvoted, but then turned out to be made by a Will Newsome sockpuppet who edited the quote afterward. The original comments have been banned. The quotes are as follows:

If dying after a billion years doesn't sound sad to you, it's because you lack a thousand-year-old brain that can make trillion-year plans.

-- Aristosophy

One wish can achieve as much as you want. What the genie is really offering is three rounds of feedback.

-- Aristosophy
If anyone objects to this policy response, please PM me so as to not feed the troll.

The OpenSF code of conduct seems pretty good in general.

I accept your correction, albeit not literally "fertile age" (many over-40s are attractive, I admit not over-80s). I also note that I personally do not seem much sexually attracted to some younger female rationalists that seem to attract other males in the community - my "too young" threshold for sexual attraction seems set to a higher age than average. (Note which I should not have to include: This is not the same as not liking said women! You can like somebody without wanting to sleep with them.)

Correction accepted. Still seems like something a poor society could afford, though, since labor and opportunity would also cost less. I understand that lots of poor societies do.

No, politicians can afford to spend lots of money on them. The actual mechanism of elections have never, so far as I know, been all that expensive pre-computation.

Heh. Nice!

After some previous disappointments, my probability that this paper answers "No" to the above questions is too small to try to read yet another one. The more so as the author is obviously taking as burdens things that physics clearly permits, like bifurcating minds (which can be done with uploads on computers, never mind MWI). Have you read it and can you confirm a "No" to all the antimagic questions?

One of those questions is not like the others, but I'd also like to hear an answer to all the others. Obviously, if even one answer is "Yes", then I will instantly toss it out the window unless it has an experimental consequence different from MWI or a strictly endogenous answer to the Born rule. ("We use the Born rule to decide which world survives!" is not endogenous, it is pasting an arbitrary mechanism attached to the same rule-of-unknown-origin treated as fiat.) If there are two "Yes" answers that aren't the same "Yes", I will toss it even if it has endogenous Born. Any damn idiot can introduce a bunch of magic and sneak in some fairly arbitrary linkage to measure which eventually yields the Born probabilities - I'd expect thousands of theories like that, and I'd expect none of them to be right. The great achievement would be getting Born without magic, where 'magic' is represented by a "Yes" to any of the above questions.

I like the original title better.

(Things like this should probably go to reddit's /r/hpmor...)
It was originally a reader suggestion by DarkHeart81 in a review on Ch. 11:

Some interesting stuff in this. Though the sorting hat yell out something totally random like 'Pickled stew berry!' or 'Khan!' like in Star Trek II. Or... Well there's a lot of stuff the hat could have yelled out. lol

So it's meant to be 'totally random'.

Nobody believes in the future.
Nobody accepts the future.
Then -

I should like to point out that anyone in this situation who wishes what would've been their first wish if they had three wishes is a bloody idiot.

The scroll modifies your expectations.
The genie twist-interprets X, and then assesses your expectations of the result of the genie's interpretation of X. ("Why, that's just what you'd expect destroying the world to do! What are you complaining about?")
The complete list of expectations regarding X is at least slightly self-contradictory, so of course the genie has no option except to modify your expectations directly...

How about just,
(simple facts about PA:)
|- ?(A ? B) -> (?A ? ?B)
(apply the assumptions:)
|- ?(A ? B) -> (B ? A)
|- ?(A ? B) -> (A ? B)
(apply Lob:)
|- A ? B

0 feedback is exactly what you get from 1 wish. "Feedback" isn't just information, it's something that can control a system's future behavior - so unless you expect to find another genie bottle later, "Finding out how your wish worked" isn't the same as feedback at all.

Yeah, but there's also a certain plausibility to the heuristic which says that you don't get to second-guess her knowledge of what works for charitable giving until you're - not giving more - but at least playing in the same order of magnitude as her. Maybe her pushing a little bit harder on that "hypocrisy" would cause her mind to collapse, and do you really want to second-guess her on that if she's already doing more than an order of magnitude better than what your own mental setup permits?

I wasn't abused or neglected. Did she check experimentally that abuse or neglect is more prevalent among rationalists than in the general population?
Of course that's not something a human would ordinarily do to check a plausible-sounding hypothesis, so I guess she probably didn't, unless something went horribly wrong in her childhood.

If you don't, you're really going to regret it in a million years.

Er... actually the genie is offering at most two rounds of feedback.
Sorry about the pedantry, it's just that as a professional specialist in genies I have a tendency to notice that sort of thing.

It might be worth taking a look at Karen Horney's work. She was an early psychoanalyst who wrote that if a child is abused, neglected, or has normal developmental stages overly interfered with, they are at risk of concluding that just being a human being isn't good enough, and will invent inhuman standards for themselves.
I'm working on understanding the implications (how do you get living as a human being right? :-/ ), but I think she was on to something.


Note that, on gender issues at least, it also pattern-matches very strongly to the "scientific racism" of the 19th and early 20th century.

No it bloody doesn't except on the Internet. Read "The Psychological Foundations of Culture" and quote me a paragraph that pattern-matches anything like that. And then perhaps you'll give me back your respect point, because in a flash of enlightenment you'll suddenly understand why I was puzzled by people having issues with EP.

Suddenly I am enlightened!
In particular, I have just now realized that whereas I encountered evolutionary psychology in the context of my quest to unravel the mysteries of human cognition and so I read a bunch of science books and papers on it, many other people may be encountering evolutionary psychology primarily in the context of Someone Is Wrong On The Internet, attempted invocations of ev-psych which are so terrible as to be propagated through the blogosphere as horrors for everyone to marvel at.
This explains a lot about the oddly bad opinion that so many online-folk seem to have about evolutionary psychology. This has had me making puzzled expressions for years, not sure what was going on. But you would probably get a pretty different first-impression (and first impressions are very controlling) if your first exposure was reading that NRO article instead of "The Psychological Foundations of Culture". Even if somebody tried to expose you to the real science afterward, you'd probably go in with some degree of motivated skepticism.
Having thus generalized the problem - is this likely to be happening to me somewhere, or you? Besides ev-psych and economics, which other sciences will Reddit expose to you primarily in the form of exhibiting Someone Is Wrong On The Internet misuses?


Don't they need negative examples too?

Negative examples, if I'm a newcomer, mean that I stop reading the site because the discussion is not consistently high-quality. And newbies looking at negative examples mean that elder posters feel obliged to respond to bad comments, just in case a newbie reads them and gets fooled; it makes it mentally harder to downvote and walk away. This is a change I would strongly consider in any case.

Please keep your suggestions programmatically very simple. There are all sorts of bright ideas we could be trying, but the actual strong filter by which only a very few are implemented is that programming resources for LW are very scarce and very expensive.
(This filter is so strong that it's the main reason why discussion of potential LW features didn't in-advance-to-me seem very publicky - most suggestions are too complicated, and the critical discussion is the one where SIAI/CFAR decides what we can actually afford to pay for. It hadn't occurred to me that anyone would dislike this particular measure, and I'll try to update more in that direction in the future.)

But not all the subcomments.

"Nontrivial measure or it didn't happen."
-- Aristosophy
(Who's Kate Evans? Do we know her? Aristosophy seems to have rather a lot of good quotes.)

Here's a nice trollfeeding from today:
http://lesswrong.com/lw/ece/rationality_quotes_september_2012/7bbl
Of course this also indicates that the current countermeasure may be ineffective, or maybe it wasn't below -3 when Yvain replied. But if the discussion cuts out after two steps, that might be good enough. Perhaps it should just be impossible to reply to anything if there's more than two ancestors at -3 or below.


The site was seriously going to hell due to long troll-started threads and troll-feeding.

I really don't see this. It looks like the main clause of decline is that spontaneous top-level postings are not enough to make up for the loss of the enormous subsidy of a good writer posting as a full-time job. 3 examples of hellish troll-feeding would be nice.

That would've been hard to find, but thankfully Gabriel did the work to find one example. Thanks Gabriel!
If you go to Configurations and Amplitude and scroll down... then you'll suddenly find this really amazingly huge thread, much much larger than anything around it. What is this wonderful huge thread, you wonder? Why, it's this:
http://lesswrong.com/lw/pd/configurations_and_amplitude/6bwo
Finding this kind of conversation dominating Recent Comments, much less Top Comments, is something I find dishedonic and I don't think it helps the site either.

Meta-discussion is also a horrible slime-dripping cancer on a forum, so I'm okay with nobody ever seeing it again.

I'd love to have a way to move comments. If anyone's willing to donate enough money, this site could hire a full-time programmer and have all kinds of amazing new features. Meanwhile the development resources just don't exist.
Threads with downvoted ancestors were already being punished. They got hidden by default with no warning to commenters that this is the case. Unless people have already learned to unhide by reflex - and then the site has no visual filter mechanism!

Biased sample if those who flee the long-replies-to-downvoted-comments threads have already left. At the point where LW starts being unfun for me to read, I panic. If my standards are too high... well, there's worse things that could happen to a site, like my threshold for alarm being set too low.

I don't want people to learn the habit of unhiding comments! Comments that will end up being hidden by default mostly shouldn't exist. If there's something amazingly intelligent to say, put it in a top-level comment to begin with, not somewhere it will be hidden by default!

The site was seriously going to hell due to long troll-started threads and troll-feeding. It's not a good use-case when intelligent comments are hidden by default, either. And I now see that contrary to the feature request, it's only asking for 5 karma for immediate descendants, not anywhere in the chain, so I shall go now and ask that to be updated.
I don't want to train readers to unhide things by default just because they might miss intelligent conversation in subthreads, I don't want intelligent conversation in places it's hidden by default from readers trusting the site mechanics, I want this site to stop feeding its trolls and would prefer a community solution rather than moderators wielding banhammers, and I want this site to focus its efforts positively rather than in amazing impressive refutations of bad ideas which is a primary failure mode of any intelligent Internet site. Threads with heavily downvoted ancestors should almost always not exist, because of their opportunity costs, the behaviors they reinforce, and other long-term consequences.
If this particular effort proves insufficient, the next step will be to make it impossible for users less than three months old (or with less than 1000 karma or something) to see comments under -3 at all.

I agree with all three examples as WAITW even if the last two are negative. It's also very rare that you can settle policy questions through the negation of a categorization. Corporations aren't typical people and money isn't typical speech, but neither of those observations settle the policy question or even debate it - these are just slogans.

That's live now? Awesome! We've been needing that for... well, years, actually, but it got really bad a couple of months ago. "Troll feeding fee" is such a very apt description, too.

I detect a contradiction between "brevity not seen as virtue" and "they couldn't afford paragraphs".

To save ten lives via FAI, you have to accelerate FAI development by 6 seconds.

Reminder: Original scientific research on saving the world is valuable and you should upvote it.

Nope, wasn't accusing you of anything. I was just amused by the point that anyone who wants to save as many lives as possible, but has only a finite amount of oil, must be able to state some consistent value of human life in terms of barrels of oil, since otherwise you could rearrange the oil to save more lives.

If only 100 barrels of oil ends up being worth a human life, clearly we ought to invade Iran. Or Equatorial Guinea if we can only scrape up a couple of million dollars for the coup.
Incidentally, there appears to be an important list of unsung humanitarian heroes here.

Shouldn't there never be a shortage of weak arguments for anything? Strong arguments can always be weakened.
/
Isn't there enough chance of finding a weak argument to at least make it worth trying? You never know, you might find a weak argument somewhere.

Mm... sure. "X is Y!" is generally pretty weak, and I'm pro-choice, so, sure.

Not yet.

Obviously, the reason I tried and am trying multiple diets is that the experimental result is always that nothing actually works. Except that the Shangri-La diet worked for twenty pounds and then mysteriously stopped doing anything (i.e., abrupt cessation of the diet did not result in any significant change in weight trends) and Seth Roberts couldn't get it working again. Paleo was among the diets tried, and it didn't result in any weight loss or other detectable differences. Non-US-approved, powerful, dangerous drugs like clenbuterol, which are supposed to cause weight loss on the order of a pound a day, produced standard side effects but no weight loss in me.
I figure that metabolisms vary at least 10% as much as minds, which is a HUGE amount of variance. It actually points up something I may post about at some point, which is that statistical science itself is often a dead end - you can publish paper after paper after paper about effects that show up in 60% of the population - but you don't know what separates the 60% from the 40% - and still have no real grasp on the phenomenon and no real ability to manipulate it.

Can't think anything without a concrete example.

Taxes are not paid voluntarily and affirmative action benefits some races at the (immediate) expense of others, I agree. But note that in saying "abortion ends a life", you described things on somewhat of a higher level and used a more value-laden word than in the other two cases - like saying "Taxes are stolen" or "Affirmative action discriminates". "Abortion ends a life" is still sneaking in connotations, since we imagine ending a human life, rather than a cat's life or an ant's life, and the other person may well object that the embryo's life hasn't quite reached the ant level yet. In general, there's no license to bring up a categorization like 'life', as an unquestionable assumption or 'fact', if the other person is going to disagree with the connotations of the categorization, like "life is precious".
You can bring up as a fact that the embryo has 256 cells capable of metabolism but not capable of surviving outside the uterus. Calling it a 'life' is an attempt to Sneak in Connotations and establish a value judgment, because we all know that life is precious, even though we don't care very much about accidentally inhaling a dust mite. Perhaps you think an embryo is more precious than this because of the (likewise lower-level and harder to dispute) fact that if left in the uterus the embryo will probably become a human baby. But if you merely attempt to enforce the connotation of preciousness by pulling out a dictionary and looking up the definition of 'life', see the fallacy of the Argument from Common Usage; dictionary editors can't settle moral arguments.
As a general rule, whatever you wanted the other person to conclude from hearing the word 'life', such as that an embryo is precious, is something that you need to address directly - not try to establish by looking at other qualities which don't immediately establish preciousness, such as cell metabolism (which also appears in dust mites), and then pulling out a dictionary to try to establish that whoever edited the dictionary wrote a definition of 'life' that matches that.
Or as I would've written then, if I'd known then what I'd known now about training skills instead of conveying insights:
The counterpattern to Sneaking in Connotations is to Directly Argue the Connotation!
I think it's fair to say that on LW, anyone who tried to indignantly take a "But X is a Y!" stance, whether liberal or conservative or libertarian or transhumanist, would be referred to the Human's Guide to Words sequence. It's in one of the first core sequences and lots of commenters will recognize it on sight.

Exactly! Logical fallacies are bad, and the Worst Argument in the World is a logical fallacy!
(Actually valid because it's a typical, central logical fallacy, not an edge case. If you'd asked me to list the most common logical fallacies even before I saw this post, I'd hope that I'd remember to put argument-by-categorization-of-atypical-cases into the top 10.)

I find "Forced parenthood is slavery!" to be pretty convincing, actually. Though I may be prejudiced by having grown up around a Libertarian father (now, alas, more Republican(!??)) who went about proclaiming that jury duty was slavery.

It may be a word error - I don't think it is, "Evolutionary psychology is riddled with false claims produced by sexist male scientists and rationalized by the scientists even though the claims are not at all well-supported compared to nonsexist alternatives" is a coherent and meaningful description of a way the universe could be but isn't, and is therefore false, not a word error - but if so, it's a word-error made by stereotypically left-wing people like Lewontin and Gould who were explicitly political in their criticism, not a word-error made by any right-wing scientists I can think of offhand.
In general, we should be careful about dismissing claims as meaningless or incoherent, when often only a very reasonable and realistic amount of charity is required to reinterpret the claim as meaningful and false - most people are trying to be meaningful most of the time, even when they're rationalizing a wrong position. Only people who've gotten in a lot more trouble than that are actively trying to avoid letting their arguments be meaningful. And meaningless claims can be dismissed immediately, without bringing forth evidence or counterobservations; meaningful false claims require more demonstration to show they're false. So when somebody brings a false claim, and you dismiss it as meaningless, you're actually being significantly logically rude to them - putting in less effort than they're investing - it takes more effort to bring forth a meaningful false claim than to call something 'meaningless'.

I applaud your willingness to publicly be on fire!

I requested the crosspost.

I have tried constructing a pro-choice example similar to "Abortion is murder!" ("Forced pregnancy is slavery!"???), but it ended up pretty unconvincing. Hopefully someone can do better:
Leaving rape cases aside, the archetypal example is an unwanted teenage pregnancy due to defective or improperly used birth control or simply an accident. Forcing her into letting the embryo develop into a fetus and eventually into a human baby would likely make the woman significantly worse off in the long run, financially, physically and/or emotionally, so she should have an option of terminating the pregnancy.
An example a pro-life person thinks of: aborting a healthy fetus, possibly in the second trimester, as a habitual birth control method.

I can't visualize how saying that a result is "uncertain" could make the Lobian issue go away - do you have a concrete visualization for this, and if so, can you sketch it even if very briefly?
Possibly relevant: No Turing machine can assign finite probability to the halting sequence.

I don't usually ask this, but would at least one downvoter please explain the downvotes for this?

The main relevant aspect of the real problem not captured by quining is the continuity of self-replacement with environmental action. All the quining approaches Marcello and I considered, involved differently treating the decisions to "modify self" and "do something in environment". This means you can't use actions to construct an environmental copy of yourself out of transistors, because the environmental copy wouldn't get the privileged-special-case treatment. More generally, it seems to imply an arbitrary Cartesian boundary with different levels of mathematical trust for physical processes inside and outside the boundary. And that you can't just frame desirable actions in terms of their predicted consequences.
Edit: This doesn't mean quining approaches aren't worth exploring. They might get us closer to a solution than contemplating, say, approaches that don't quine.

I should also note that tax-exempt organizations generally cannot set up funds for particular individuals. At least, they can't do that and have donations be tax-deductible - I think they can safely administer an account.

The key question everyone was asking is whether Kim's circumstances have been verified.

Because they have non-overlapping truth conditions. Either reality is inside one set of possible worlds, inside the other set, or in neither set.

(Marcello observed this while reading the previous cheat attempt.) Since "1=0" contains no mentions of K, and is thus invariant under substitution of K-1 for K, why doesn't this language contain "provability of '0=1' implies 0=1" and thus prove 0=1?

I would remark that truth is conserved, but profundity isn't. If you have two meaningful statements - that is, two statements with truth conditions, so that reality can be either like or unlike the statement - and they are opposites, then at most one of them can be true. On the other hand, things that invoke deep-sounding words can often be negated, and sound equally profound at the end of it.
In other words, Bohr's maxim seems so blatantly awful that I am mostly minded to chalk it up as another case of, "I wish famous quantum physicists knew even a little bit about epistemology-with-math".

There are proverbs about how trying to generalize your code will never get to AGI. These proverbs are true, and they're still true when generalizing a driverless car. I might worry to some degree about free-form machine learning algorithms at hedge funds, but not about generalizing driverless cars.

I didn't necessarily mean the proposer, I meant, y'know, you.

Putting smarter-than-human AI into the same class as the Rapture instead of the same class as, say, predictions for progress of space travel or energy or neuroscience, sounds to me suspiciously like reference class tennis. Your mind knows what it expects the answer to be, and picks a reference class accordingly. No doubt many of these experts did the same.
And so, once again, "distrust experts" ends up as "trust the invisible algorithm my brain just used or whatever argument I just made up, which of course isn't going to go wrong the way those experts did".
(The correct answer was to broaden confidence intervals in both/all directions.)

I don't disagree - unless that means you use your own prediction instead, which is the usual unconscious sequelae to 'distrust the experts'. What epistemic state do you end up in after doing this ignoring?

Yes, but my point is that thinking about SI or MML in the abstract helps because people sometimes gain insight from asking "How complex is that computer program?" I haven't seen appeal-to-CEV produce much insight in practice, and any insight it could produce can probably be better produced by appealing to the relevant component principle of CEV instead. (Nor yet is this a critique of CEV, because it's meant as an AI design, not as a moral intuition pump.)

I care about the future consequences of dirt, but not the dirt itself.
(For the love of Belldandy, you people...)

CEV is a construct for AI purposes that actual human beings can't eval - I don't think I've ever seen a human discussion that was helped by invoking it. It's not like Solomonoff Induction where sometimes you really can be helped by thinking formally about Occam's Razor. In practice, human beings arguing about ethics are either already approximating their part of the 'good' as best they can, or they're confused about something much simpler than CEV, like consequentialism. If you should never use the word 'truth' when you can just talk about the object level, and never say 'because it's not optimal!' when that just means 'I don't think you should do that', then there's basically never a good time to talk about CEV - it always deflates out of the sentence unless you're talking directly about FAI.

I don't know about this "we" thing. Can you do it?

Please don't. All edits to the about page should go through an editor. Random people should not be told how to edit the about page.

I'm no decisionmaker. I just created that post because I thought things could be improved.
If you or anyone else has text they want to put on the about page or the home page, send me a personal message and I'll tell you how. Right now things rely on security by obscurity.
Edit: As matt points out, it's not security by obscurity so much as Wikipedia-style open collaboration.

All righty; I run my utility function over everything that exists. On most of the existing things in the modern universe, it outputs 'don't care', like for dirt. However, so long as a person exists anywhere, in this universe or somewhere else, my utility function cares about them. I have no idea what it means for something to exist, or why some things exist more than others; but our universe is so suspiciously simple and regular relative to all imaginable universes that I'm pretty sure that universes with simple laws or uniform laws exist more than universes with complicated laws with lots of exceptions in them, which is why I don't expect to sprout wings and fly away. Supposing that all possible universes 'exist' with some weighting by simplicity or requirement of uniformity, does not make me feel less fundamentally confused about all this; and therefore I'm not sure that it is true, although it does seem very plausible.

Shouldn't you be taking into account that I don't want to discourage other people from having kids?

OK, no prob!
(I do care about everything that exists. I am not particularly certain that all mathematically possible universes exist, or how much they exist if they do. I do expect that our own universe is spatially and in several other ways physically infinite or physically very big. I don't see this as a good argument against the fun of having children. I do see it as a good counterargument to creating children for the sole purpose of making sure that mindspace is fully explored, or because larger populations of the universe are good qua good. This has nothing to do with the reason I'm not having kids right now.)

That is not the reason or even a reason why I'm not having kids at the moment. And since I don't particularly want to discourage other people from having children, I decline to discuss my own reasons publicly (or in the vicinity of anyone else who wants kids).

Try two:
Does the following language do the same thing as PAK?
PAT = PA + symbol K + schema over Formulas of one variable:
- For all P:
-- if P proves-in-PAT 'Formula(K+1)', then Formula(K)


Does the following language do the same thing as PAK?
PAS = PA + schema over Formulas of one variable:
- For all N, for all P:
-- if P proves-in-PAS 'Formula(N+1)', then Formula(N)

(Actually, on second thought, I retract that, you do want the special symbol so that proving 4 > 3 doesn't get you 3 > 3. But now I need to see if the same problem applies to the original logic as soon as you add K=3...)

(Still trying to read, will comment as I go.)

if x proves "p is safe for K steps", then inst(n,x) is a PA(n+1) proof of "p is safe for n steps"

Only if we redefine PA(n) so that e.g. PA(3) is defined in terms of a PA(.) schema and contains an axiom schema stating provable(PA(3), 'x')->x; and then redefine PAK accordingly. (This seems possible, but it's definitely an extra step that for all I know could trip up somewhere, and ought to be stated explicitly.) If we don't take it, then inst(n, x) gives us something that seems like it ought to be provable by analogy, but isn't a direct translation, because the original PA(n) didn't contain a provability formula quantified over PA(.) into which we could plug K=n; and since I think we were proving things about inst later, that's not good.

(Rereads.) I don't feel like I've been handed a construction of PA(K), just some properties it's supposed to have. We have a constant language, containing one constant symbol, that allegedly becomes equal to PA(SSS0) the moment you add one axiom that says SSS0=K. All the PA(n) are different languages with different axioms, the larger n having strictly more axioms. You can define PA(w) by schematizing all the axioms for every n, but if PA(K) is using some larger axiom schema like this, I'd like to know exactly what that larger schema is. It is not obvious to me how to fill in this blank so that PAK becomes effectively equal to PA(n+1) when an axiom K=n is added. In particular there are questions like whether the expression (K-1) can appear in a proof, and so on. I'm being told that PAK can prove things that PA(4) can't, but becomes only as strong as PA(4) when the axiom K=4 is added. That's a bit suspicious; why can't I prove the same things again, just not using that axiom? If it's only equal over the base language, then what new things can be proven in the extended language, using which axioms?
Would anyone care to write out a concrete example of what this language is supposed to do when generating new AIs? As it stands I can't visualize the language itself.


Now let's extend the language of PA by a constant symbol K, and define PAK := Peano Arithmetic (actually, the natural extension of PA to this new language) + for all formulas 'C' of the base language: "if PA(K) proves 'C', then C".

Either I didn't understand how PA(K) is constructed at all, or this instantly dies by Lob's Theorem. If any language L proves 'If L proves C, then C', then L proves C. So if PA(K) has "If PA(K) proves 2=1, then 2=1" then PA(K) proves 2=1.

Has anyone seriously suggested you invented MWI? That possibility never even occurred to me.

It's been suggested that I'm the one who invented the idea that it's obviously true rather than just one more random interpretation; or even that I'm fighting a private war for some science-fiction concept, rather than being one infantry soldier in a long and distinguished battle of physicists. Certainly your remark to the extent that "he should try presenting his argument to some skeptical physicists" sounds like this. Any physicist paying serious attention to this issue (most people aren't paying attention to most things most of the time) will have already heard many of the arguments, and not from me. It sounds like we have very different concepts of the state of play.

Give me a set of formal languages over which you can say the phrase "for any formal language", and the truth predicate for the union of the set won't be in any language in the set. I'm still trying to understand how to deal with this inside AI, but I'm not sure that blaming it on second-order logical induction is putting the blame in the right place.


Suppose we define a generalized version of Solomonoff Induction based on some second-order logic. The truth predicate for this logic can't be defined within the logic and therefore a device that can decide the truth value of arbitrary statements in this logical has no finite description within this logic. If an alien claimed to have such a device, this generalized Solomonoff induction would assign the hypothesis that they're telling the truth zero probability, whereas we would assign it some small but positive probability

Actually, what Tarski seems to show is that for any language for describing any set of universes, there just is no language representable inside those universes for representing arbitrary statements, with truth values, about "everything" including the language and the statements in it. If you try to invent such a language, it will end up inconsistent - not at the point where it tries to correctly assign truth, but at the point where it can represent truth, due to analogues of "This statement is false." It isn't needful to assign 0 or 1, in particular, to this statement; the moment you represent it, you can prove an inconsistency. But is this really proper to blame on Solomonoff induction?

A kilobit of improbability requires only a kilobit of data to offset it, which isn't very crackpot at all. Proving minimum length is impossible, but proving an upper bound on length is very easy, and that proves a lower bound on probability.


What do you think the chances are that there is some single procedure that can be used to solve all philosophical problems?

Very low, of course. (Then again, relative to the perspective of nonscientists, there turned out to be a single procedure that could be used to solve all empirical problems.) But in general, problems always look much more complicated than solutions do; the presence of a host of confusions does not indicate that the set of deep truths underlying all the solutions is noncompact.

Nnnot really. The Time-Turner, certainly, but that doesn't make the story uninstantiable. Making a logical impossibility a basic plot premise... sounds like quite an interesting challenge, but that would be a different story.


"Given the nature of the multiverse, everything that can possibly happen will happen. This includes works of fiction: anything that can be imagined and written about, will be imagined and written about. If every story is being written, then someone, somewhere in the multiverse is writing your story. To them, you are a fictional character. What that means is that the barrier which separates the dimensions from each other is in fact the Fourth Wall."

-- In Flight Gaiden: Playing with Tropes
(Conversely, many fictions are instantiated somewhere, in some infinitesimal measure. However, I deliberately included logical impossibilities into HPMOR, such as tiling a corridor in pentagons and having the objects in Dumbledore's room change number without any being added or subtracted, to avoid the story being real anywhere.)

I strongly suspect that the primary result of such an algorithm would be very wide error bars on the timeline, and that it would indeed outperform most experts for this reason. You can't get water from a stone, nor narrow estimates out of ignorance and difficult problems, no matter what simple algorithm you use. Though I would be quite intrigued to be proven wrong about this, and I have seen Fermi estimates for quantities like e.g. the mass of the Earth apparently extract narrow and correct estimations out of the sums of multiple widely erroneous steps.

Yeah, that's kind of an issue here. What if you've got to work on a hard problem? I'd rather know what influences expert ability on a given domain, in case I want to be a good expert but don't have a free choice of which problems not to work on.

That was exactly my reaction to reading Luke's AMA - "no, I probably shouldn't try this."

One case of food poisoning and you'd be sued to Kingdom Come.
Apparently it's even an issue for City Harvest, which distributes leftovers to the poor.

Truly is it said that all real innovation is illegal.

"Professor Quirrell" is such an emulation, and sometimes I worry about all the people who say that they find his arguments very, very convincing.

I agree, and I myself was, and am still, sentimentally fond of Penrose for this reason, and I would cheer on any agency that funded a test. However and nonetheless, "testable" is not actually the same as "plausible", scientifically virtuous as it may be.

Here's an incredibly brilliant idea for a rationalist startup! You know how it is when you've got too much food, like a cheesecake or something, that your guests didn't finish or whatever, and your brain refuses to throw it out because you don't want it to be wasted, but you don't want to have to eat it all either? This startup would have a registry of polite, well-dressed, grateful, hungry people in your vicinity, who'll come over and eat it for you - there in ten minutes or your money back! SunkMunch.com - "We eat your food, now!" YC '13, here we come!

If you've never been arrested, you're too law-abiding.

It'd still be the only FTL discontinuous non-differentiable non-CPT-symmetric non-unitary non-local-in-the-configuration-space etc. etc. process in all of physics, to explain a phenomenon (why do we see only one outcome?) that doesn't need explaining.

That sounds correct to me. A physicist who also possesses probability-theory expertise and who can reason with respect to Solomonoff Induction and formal causal models should realize that single-world variants of MWI are uniformly unworkable (short of this world being a runtime-limited computer simulation); but such is rare (though not unheard-of) among professional physicists; and among the others, you can hardly blame them for trying to keep an open mind.

BTW, it's important to note that by some polls an actual majority of theoretical physicists now believe in MWI, and this was true well before I wrote anything. My only contributions are in explaining the state of the issue to nonphysicists (I am a good explainer), formalizing the gross probability-theoretic errors of some critiques of MWI (I am a domain expert at that part), and stripping off a lot of soft understatement that many physicists have to do for fear of offending sillier colleagues (i.e., they know how incredibly stupid the Copenhagen interpretation appears nowadays, but will incur professional costs from saying it out loud with corresponding force, because there are many senior physicists who grew up believing it).
The idea that Eliezer Yudkowsky made up the MWI as his personal crackpot interpretation isn't just a straw version of LW, it's disrespectful to Everett, DeWitt, and the other inventors of MWI. It does seem to be a common straw version of LW for all that, presumably because it's spontaneously reinvented any time somebody hears that MWI is popular on LW and they have no idea that MWI is also believed by a plurality and possibly a majority of theoretical physicists and that the Quantum Physics Sequence is just trying to explain why to nonphysicists / formalize the arguments in probability-theoretic terms to show their nonambiguity.

Hence why the memetic hazard warning page advises signing up for cryonics after reading.

Some non-nitwit (actual-economic-value-generating) startups I've heard proposed lately by people in this or related communities:

Kevin Fischer is interested in identifying useful sub-chemicals in certain legal psychoactive plants. Anyone with biotech, chemical-identifying training would be useful to him.
Mike Darwin (not LW-style rationalist, but cryonicist) says that his research and numerous other papers show that melatonin, among some other chemicals, is very effective at preventing cerebral-reperfusion ischemic injury which is the real killer in heart attacks and strokes, and for which there are apparently not currently approved medications.
Zvi Mowshowitz is now trying to refound a startup to provide evidence-based, rationalist-filtered medical care - evidence-based doctors as opposed to just evidence-based medical research that often gets ignored by actual doctors.
John Schloendorn is the most competent biotech guy I know. He was literally trying to cure cancer - by trying to duplicate the abilities of a 100%-cancer-immune strain of mice, in humans - when his startup ran out of money; and he has a lot of other low-hanging fruits on his list as well.


Rationalist fiction: There's explicit epistemic or instrumental methods which you see the characters using, described in sufficient detail to convey the general principle as well as the particular case, which you are meant to pick up and use in real life.


"Silver linings are like finding change in your couch. It's there, but it never amounts to much."

-- http://www.misfile.com/?date=2012-08-10

I win.

I've started wishing people "Good skill!"

I have been publicly and repeatedly skeptical of any proposal to make an AI compute the answer to a philosophical question you don't know how to solve yourself, not because it's impossible in principle, but because it seems quite improbable and definitely very unreliable to claim that you know that computation X will output the correct answer to a philosophical problem and yet you've got no idea how to solve it yourself. Philosophical problems are not problems because they are well-specified and yet too computationally intensive for any one human mind. They're problems because we don't know what procedure will output the right answer, and if we had that procedure we would probably be able to compute the answer ourselves using relatively little computing power. Imagine someone telling you they'd written a program requiring a thousand CPU-years of computing time to solve the free will problem.
And once again, I expect that the hardest part of the FAI problem is not "winning the intelligence race" but winning it with an AI design restricted to the much narrower part of the cognitive space that integrates with the F part, i.e., all algorithms must be conducive to clean self-modification. That's the hard part of the work.

That is amazingly sad and we should use that as a test question on some SPARC unit somewhere.

I'll try to get the results in writing the next time we have a discussion. Human memory is a fragile thing under the best of circumstances.

I have read all of the original. It is a very well written work, but as befits Fallout, it is grimdark.
There were many parts which seemed like they wouldn't have been nearly as enjoyable without strong familiarity with the Fallout universe. They cross it with MLP very well- some of the combinations are eh, but several of them make perfect sense, and are very tragic.
I'm not quite sure I would describe it as "rationalist"- the protagonist is clever, the enemies are often clever, and there's a little bit in the way of plotting and puzzle solving. It seems much more like a standard post-apocalyptic adventure than rationalist fiction, but I'm not quite sure where I would draw the line around rationalist fiction.

There are too many accomplished people in the world contradicting each other to not filter it somehow.

I did once suggest a similar heuristic; but I feel the need to point out that there are many people in this world with track records of achievement, including, like, Mitt Romney or something, and that the heuristic is supposed to be, "Pay attention to rationalists with track records outside rationality", e.g. Dawkins and Feynman.

Reply to revised OP: Policy debates should not appear one-sided (because there's no systematic reason why good policies should have only positive consequences) but epistemic debates are frequently normatively one-sided (because if your picture of likelihood ratios is at all well-calibrated, your updates should trend in a particular direction and you should almost never find strong evidence on more than one side of an epistemic debate; "strong evidence" by Bayesian definition is just that sort of evidence which we almost never see when the hypothesis is wrong).

I agree they were heroic and good things, and I was disgusted when I looked into JSTOR's financial filings (not that I was happy with the WMF either).
But there's a difference between admiring the first penguin off the ice and noting that this is a good thing to do, and wanting to be that penguin or near enough that penguin that one might fall off as well. And this is especially true for organizations.

Even if so, one should still at least mention, in a debate on character, that the controversy in question just happened to be about an attempted heroic good deed.

I don't think it's fair - I think it's a bit motivated - to mention these as mysterious controversies and antics, without also mentioning that his actions could reasonably be interpreted as heroic. I was applauding when I read the JSTOR incident, and only wish he'd gotten away with downloading the whole thing and distributing it.

There were plenty of physicists reading those posts when they first came out on OB (the most famous name being Scott Aaronson). Some later readers have indeed asserted that there's a problem involving a physically wrong factor of i in the first couple of posts (i.e. that's allegedly not what a half-silvered mirror does to the phase in real life), which I haven't yet corrected because I would need to verify with a trusted physicist that this was correct, and then possibly craft new illustrations instead of using the ones I found online, and this would take up too much time relative to the point that talking about a phase change of -1 instead of i so as to be faithful to real-world mirrors is an essentially trivial quibble which has no effect on any larger points. If anyone else wants to rejigger the illustration or the explanation so that it flows correctly, and get Scott Aaronson or another known trusted physicist to verify it, I'll be happy to accept the correction.
Aside from that, real physicists haven't objected to any of the math, which I'm actually pretty darned proud of considering that I am not a physicist.

I independently invented a similar concept, "epiphany junkies", but didn't get around to posting it yet. A couple of points that would've been in that post:

Achieving an amazing insight about your past suffering (especially other people hurting you somehow), is probably not worth much. The past is past, and unreachable; five seconds ago is as far away as forever. You shouldn't even have been chewing that cud in the first place.
You probably need a lot of small, nondramatic life optimizations more than you need any particular big huge insight. Besides the addictive quality, a problem with being an epiphany junkie is that it trains you to think that progress comes in the form of dramatic, self-justifying insights about your mother, instead of realizing that you need to stop thinking about all the things wrong with an email after you send it.


I was expecting the attribution to be to Mark Twain. I wonder if their style seems similar on account of being old, or if there's more to it.

Thank you, Professor Quirrell.

I think there's a strong effect wherein "open non-ashamed polyamory wherein you mention any positive reason why you like it" = "evangelical polyamory", or even just "open polyamory" = "evangelical polyamory", for the same reasons as "evangelical atheism" and "evangelical homosexuality".

I don't see how any one player can do better in a world where MBlume gets infinity dollars.

We did specify no long-term consequences - otherwise the argument instantly passes, just because at least 3^^7625597484986 people would certainly die in car accidents due to blinking. (3^^^3 is 3 to the power of that.)

Nice dialogue!
I think that the term "barely worth living" is a terrible source of equivocation that underlies a lot of the apparent paradoxicalness. "Barely worth living" can mean that, if you're already alive and don't want to die, your life is almost but not quite horrible enough that you would rather commit suicide than endure. But if you're told that somebody like this exists, it is sad news that you want to hear as little as possible. You may not want to kill them, but you also wouldn't have that child if you were told that was what your child's life would be like. What Parfit postulates should be called, to avoid equivocation, "A life barely worth celebrating" - it's good news and you say "Yay!" but very softly. I'd even argue that this should be a universal standard for all discussions of the Repugnant Conclusion.

I applaud your voluntary performance of work that a less heroic mind might've been tempted to think was someone else's responsibility!

For the love of the heavens! It's gratitude to shokwave! I can't do a hundredth of the good ideas I have, and I've no doubt that neither can gwern.

Hey - were you submitting the story for the first time? I.e., not that this was your first story, but Asimov's was the first place you sent it? If so, odds probably need adjustment because bad stories get submitted to more magazines than good stories (a rejected story is resubmitted, a good story is accepted more quickly).

Huh. I tested this and it appeared to work, which surprised me, because it'd been previously claimed that this HTML editor would filter all attributes not explicitly allowed (e.g. to filter Javascript misbehavior). Perhaps that one is explicitly allowed.


Though it's possible the reporter has twisted your words more than I manage to suspect

D'you think? You'll understand better after being reported-on yourself; and then you'll look back and laugh about how very, very naive that comment was. It's the average person's incomprehension of reporter-distorting that gives reporters their power. If you read something and ask, "Hm, I wonder what the truth was that generated this piece?" without having personal, direct experience of how very bad it is, they win.
I think the winning move is to read blogs by smart people, who usually don't lie, rather than anything in newspapers.

No, humming a tune totally works in this case. (I will not explain why.)

Yep.

We don't have sufficient development resources to do very basic things with the LW codebase. This is one of them.

If that hadn't been true I would've been much more reluctant to delete the URL. I was trying to control Pagerank flow, not information discovery. Please feel free to mirror the text on your home site and direct readers there.

All of your recent comments have been downvoted. I suggest pursuing other forum opportunities online - LessWrong is not a good fit for you. Goodbye.

I think most of the parts I'd see as "hatchet job" can be explained by sensationalism and are subverted by other parts of the article. Maybe the biggest subversion is that LW gets treated as a group of "others" in only two places, while much of the article is spent on humanizing people, which is unusual during a hatcheting. The impression I got was less "here's this weird group of people, let's apply the absurdity heuristic" and more "here's this group of people who believe something interesting and unusual, and here is everything I could find out about their sex lives." Which can still be bad for the people tabloidized (sorry!), but for LessWrong seems fine.

I'd be willing to swap "tabloid" for "hatchet", sure.

We consider this tabloid reporting. I've removed the link from the OP because our policy is not to reward trolling (by reporters or anyone else) with publicity or Google rank. OP can add the link back in if they wish and I won't remove it again, but please keep in mind that we would like to ask politely that you don't. You should also feel free to delete this post entirely, which you can do by setting the status back to "Draft".

Totally thought of that, totally decided not to include an "in our space of simultaneity" in the ceremony.

Thanks for the third-party opinion!

Lovely! I intend to add this as a Bayesian sample problem - enough rationality diaries, and we'll be able to make Bayes booklets exclusively out of real-life cases encountered by LessWrongers!

I have a theory that nobody considers both "white chocolate" and "dark chocolate" to be "chocolate", and for me, alas, that parameter is set to "white chocolate".

I strongly support mentioning something as many times as it's been used. Maybe it'd be annoying after a dozen times, but not three!

Actually, we can guess that a piece of DNA is nonfunctional if it seems to have undergone neutral evolution (roughly, accumulation of functionally equivalent mutations) at a rate which implies that it was not subject to any noticeable positive selection pressure over evolutionary time. Leaving aside transposons, repetition, and so on, that's a main part of how we know that large amounts of junk DNA really are junk.

Really? De-Nazified all of them in days? I notice I am confused; I deny this data - either that, or I really want the videotapes.

I endorse this refinement. What brain damage demonstrates is not dependency of talking on the brain, but that the complex computations of thought can be damaged in internal detail by damaging a specific brain part, whereupon its outputs to other parts of thought are damaged. This is strong evidence that the brain is doing the internal computations of thought; it is part of the inner process producing thoughts. The radio hypothesis, in which the output is produced elsewhere and received, decisively fails at that point.
We could suppose that you had a hundred soul-parts, all of which can only communicate with each other through brain-area radio transceivers which receive a call from one soul-part, and then retransmit it to another. But leaving out the epicycleness of this idea, the degree to which it contradicts the intuitive notion of a soul, and its, if you'll pardon the phrase, sheer stupidity, the end result would still be that destroying the brain leaves the soul incapable of thought. You're not likely to find a remotely reasonable hypothesis, even in the Methodsverse where magic abounds, by which the internal parts of a thinking computation can be damaged by damaging the brain, and yet removing the whole brain leaves the soul capable of internal thinking.


I have also termed this Argument by Greek Analogy after Socrates's attempt to argue that, since the Sun appears the next day after setting, souls must be immortal.

For the curious, this is from the Phaedo pages 70-72. The run of the argument are basically thus:
P1 Natural changes are changes from and to opposites, like hot from relatively cold, etc.
P2 Since every change is between opposites A and B, there are two logically possible processes of change, namely A to B and B to A.
P3 If only one of the two processes were physically possible, then we should expect to see only one of the two opposites in nature, since the other will have passed away irretrievably.
P4 Life and death are opposites.
P5 We have experience of the process of death.
P6 We have experience of things which are alive
C From P3, 4, 5, and 6 there is a physically possible, and actual, process of going from death to life.
The argument doesn't itself prove (haha) the immortality of the soul, only that living things come from dead things. The argument is made in support of the claim, made prior to this argument, that if living people come from dead people, then dead people must exist somewhere. The argument is particularly interesting for premises 1 and 2, which are hard to deny, and 3, which seems fallacious but for non-obvious reasons.

This sounds like it might be a bit of a reverent-Western-scholar steelman such as might be taught in modern philosophy classes; Plato's original argument for the immortality of the soul sounded more like this, which is why I use it as an early exemplar of reference class tennis:
-
Then let us consider the whole question, not in relation to man only, but in relation to animals generally, and to plants, and to everything of which there is generation, and the proof will be easier. Are not all things which have opposites generated out of their opposites? I mean such things as good and evil, just and unjust--and there are innumerable other opposites which are generated out of opposites. And I want to show that in all opposites there is of necessity a similar alternation; I mean to say, for example, that anything which becomes greater must become greater after being less.
True.
And that which becomes less must have been once greater and then have become less.
Yes.
And the weaker is generated from the stronger, and the swifter from the slower.
Very true.
And the worse is from the better, and the more just is from the more unjust.
Of course.
And is this true of all opposites? and are we convinced that all of them are generated out of opposites?
Yes.
And in this universal opposition of all things, are there not also two intermediate processes which are ever going on, from one to the other opposite, and back again; where there is a greater and a less there is also an intermediate process of increase and diminution, and that which grows is said to wax, and that which decays to wane?
Yes, he said.
And there are many other processes, such as division and composition, cooling and heating, which equally involve a passage into and out of one another. And this necessarily holds of all opposites, even though not always expressed in words--they are really generated out of one another, and there is a passing or process from one to the other of them?
Very true, he replied.
Well, and is there not an opposite of life, as sleep is the opposite of waking?
True, he said.
And what is it?
Death, he answered.
And these, if they are opposites, are generated the one from the other, and have there their two intermediate processes also?
Of course.
Now, said Socrates, I will analyze one of the two pairs of opposites which I have mentioned to you, and also its intermediate processes, and you shall analyze the other to me. One of them I term sleep, the other waking. The state of sleep is opposed to the state of waking, and out of sleeping waking is generated, and out of waking, sleeping; and the process of generation is in the one case falling asleep, and in the other waking up. Do you agree?
I entirely agree.
Then, suppose that you analyze life and death to me in the same manner. Is not death opposed to life?
Yes.
And they are generated one from the other?
Yes.
What is generated from the living?
The dead.
And what from the dead?
I can only say in answer--the living.
Then the living, whether things or persons, Cebes, are generated from the dead?
That is clear, he replied.
Then the inference is that our souls exist in the world below?
That is true.
(etc.)

Which of 1, 2 and 3 do you disagree with in this case?
Edit: I mean, I'm sorry to parody but I don't really want to carefully rehash the entire thing, so, from my perspective, Holden just said, "But surely strong AI will fall into the reference class of technology used to give users advice, just like Google Maps doesn't drive your car; this is where all technology tends to go, so I'm really skeptical about discussing any other possibility." Only Holden has argued to SI that strong AI falls into this particular reference class so far as I can recall, with many other people having their own favored reference classes e.g. Hanson et. al as cited above; a strong AI is far more internally dissimilar from Google Maps and Yelp than Google Maps and Yelp are internally similar to each other, plus there are many many other software programs that don't provide advice at all so arguably the whole class may be chosen-post-facto; and I'd have to look up Holden's exact words and replies to e.g. Jaan Tallinn to decide to what degree, if any, he used the analogy to foreclose other possibilities conversationally without further debate, but I do think it happened a little, but less so and less explicitly than in my Robin Hanson debate. If you don't think I should at this point diverge into explaining the concept of "reference class tennis", how should the conversation proceed further?
Also, further opinions desired on whether I was being rude, whether logically rude or otherwise.

Empirically obviously 1 is true, I would argue strongly for 2 but it's a legitimate point of dispute, and I would say that there were relatively small but still noticeable but quite forgiveable traces of 3.

And atheism is a religion, and bald is a hair color.
The three distinguishing characteristics of "reference class tennis" are (1) that there are many possible reference classes you could pick and everyone engaging in the tennis game has their own favorite which is different from everyone else's; (2) that the actual thing is obviously more dissimilar to all the cited previous elements of the so-called reference class than all those elements are similar to each other (if they even form a natural category at all rather than having being picked out retrospectively based on similarity of outcome to the preferred conclusion); and (3) that the citer of the reference class says it with a cognitive-traffic-signal quality which attempts to shut down any attempt to counterargue the analogy because "it always happens like that" or because we have so many alleged "examples" of the "same outcome" occurring (for Hansonian rationalists this is accompanied by a claim that what you are doing is the "outside view" (see point 2 and 1 for why it's not) and that it would be bad rationality to think about the "individual details").
I have also termed this Argument by Greek Analogy after Socrates's attempt to argue that, since the Sun appears the next day after setting, souls must be immortal.

I'd rather pay the $25 now. (Paypal data?) My understanding is that besides the mass, there's also supposed to be other characteristics of the particle data that match the predicted Higgs, otherwise I would've waited before fully updating. If the story is retracted I might counter-update and ask for the money back, but my understanding is that this is not supposed to happen.

So first a quick note: I wasn't trying to say that the difficulties of AIXI are universal and everything goes analogously to AIXI, I was just stating why AIXI couldn't represent the suggestion you were trying to make. The general lesson to be learned is not that everything else works like AIXI, but that you need to look a lot harder at an equation before thinking that it does what you want.
On a procedural level, I worry a bit that the discussion is trying to proceed by analogy to Google Maps. Let it first be noted that Google Maps simply is not playing in the same league as, say, the human brain, in terms of complexity; and that if we were to look at the winning "algorithm" of the million-dollar Netflix Prize competition, which was in fact a blend of 107 different algorithms, you would have a considerably harder time figuring out why it claimed anything it claimed.
But to return to the meta-point, I worry about conversations that go into "But X is like Y, which does Z, so X should do reinterpreted-Z". Usually, in my experience, that goes into what I call "reference class tennis" or "I'm taking my reference class and going home". The trouble is that there's an unlimited number of possible analogies and reference classes, and everyone has a different one. I was just browsing old LW posts today (to find a URL of a quick summary of why group-selection arguments don't work in mammals) and ran across a quotation from Perry Metzger to the effect that so long as the laws of physics apply, there will always be evolution, hence nature red in tooth and claw will continue into the future - to him, the obvious analogy for the advent of AI was "nature red in tooth and claw", and people who see things this way tend to want to cling to that analogy even if you delve into some basic evolutionary biology with math to show how much it isn't like intelligent design. For Robin Hanson, the one true analogy is to the industrial revolution and farming revolutions, meaning that there will be lots of AIs in a highly competitive economic situation with standards of living tending toward the bare minimum, and this is so absolutely inevitable and consonant with The Way Things Should Be as to not be worth fighting at all. That's his one true analogy and I've never been able to persuade him otherwise. For Kurzweil, the fact that many different things proceed at a Moore's Law rate to the benefit of humanity means that all these things are destined to continue and converge into the future, also to the benefit of humanity. For him, "things that go by Moore's Law" is his favorite reference class.
I can have a back-and-forth conversation with Nick Bostrom, who looks much more favorably on Oracle AI in general than I do, because we're not playing reference class tennis with "But surely that will be just like all the previous X-in-my-favorite-reference-class", nor saying, "But surely this is the inevitable trend of technology"; instead we lay out particular, "Suppose we do this?" and try to discuss how it will work, not with any added language about how surely anyone will do it that way, or how it's got to be like Z because all previous Y were like Z, etcetera.
My own FAI development plans call for trying to maintain programmer-understandability of some parts of the AI during development. I expect this to be a huge headache, possibly 30% of total headache, possibly the critical point on which my plans fail, because it doesn't happen naturally. Go look at the source code of the human brain and try to figure out what a gene does. Go ask the Netflix Prize winner for a movie recommendation and try to figure out "why" it thinks you'll like watching it. Go train a neural network and then ask why it classified something as positive or negative. Try to keep track of all the memory allocations inside your operating system - that part is humanly understandable, but it flies past so fast you can only monitor a tiny fraction of what goes on, and if you want to look at just the most "significant" parts, you would need an automated algorithm to tell you what's significant. Most AI algorithms are not humanly understandable. Part of Bayesianism's appeal in AI is that Bayesian programs tend to be more understandable than non-Bayesian AI algorithms. I have hopeful plans to try and constrain early FAI content to humanly comprehensible ontologies, prefer algorithms with humanly comprehensible reasons-for-outputs, carefully weigh up which parts of the AI can safely be less comprehensible, monitor significant events, slow down the AI so that this monitoring can occur, and so on. That's all Friendly AI stuff, and I'm talking about it because I'm an FAI guy. I don't think I've ever heard any other AGI project express such plans; and in mainstream AI, human-comprehensibility is considered a nice feature, but rarely a necessary one.
It should finally be noted that AI famously does not result from generalizing normal software development. If you start with a map-route program and then try to program it to plan more and more things until it becomes an AI... you're doomed, and all the experienced people know you're doomed. I think there's an entry or two in the old Jargon File aka Hacker's Dictionary to this effect. There's a qualitative jump to writing a different sort of software - from normal programming where you create a program conjugate to the problem you're trying to solve, to AI where you try to solve cognitive-science problems so the AI can solve the object-level problem. I've personally met a programmer or two who've generalized their code in interesting ways, and who feel like they ought to be able to generalize it even further until it becomes intelligent. This is a famous illusion among aspiring young brilliant hackers who haven't studied AI. Machine learning is a separate discipline and involves algorithms and problems that look quite different from "normal" programming.

Rumsfeld is speaking of the Iraq war. It was an optional war, the army turned out to be far understrength for establishing order, and they deliberately threw out the careful plans for preserving e.g. Iraqi museums from looting that had been drawn up by the State Department, due to interdepartmental rivalry.
This doesn't prove the advice is bad, but at the very least, Rumsfeld was just spouting off Deep Wisdom that he did not benefit from spouting; one would wish to see it spoken by someone who actually benefited from the advice, rather than someone who wilfully and wantonly underprepared for an actual war.

...this was actually a terrible policy in historical practice.

It's obvious to me; an S.I. user does it the same way the computable beings do.

If my writings (on FAI, on decision theory, and on the form of applied-math-of-optimization called human rationality) so far haven't convinced you that I stand a sufficient chance of identifying good math problems to solve to maintain the strength of an input into existential risk, you should probably fund CFAR instead. This is not, in any way shape or form, the same skill as the ability to manage a nonprofit. I have not ever, ever claimed to be good at managing people, which is why I kept trying to have other people doing it.

Still way too high.

I do think OP is right that in practice, 100 years ago, it would have been really hard to figure out what an AI issue looked like. This was pre-Godel, pre-decision-theory, pre-Bayesian-revolution, and pre-computer. Yes, a sufficiently competent Earth would be doing AI math before it had the technology for computers, in full awareness of what it meant - but that's a pretty darned competent Earth we're talking about.

My pills can easily be broken into two, so I'll try taking 2.5 mg instead.

The ontology problem has nothing to do with computing power, except that limited computing power means you use fewer ontologies. The number might still be large, and for a smart AI not fixable in advance; we didn't know about quantum fields just recently, and new approximations and models are being invented all the time. If your last paragraph isn't talking about evolution, I don't know what it's talking about.
Downvoting the whole thing as probable nonsense, though my judgment here is influenced by numerous downvoted troll comments that poster has made previously.

EEG contains a trivial amount of information, probably not worth storing.

There are things Solomonoff Induction can't understand which Solomonoff Induction Plus One can comprehend, but these things are not computable. In particular, if you have an agent with a hypercomputer that uses Solomoff Induction, no Solomonoff Inductor will be able to simulate the hypercomputer. AIXI is outside AIXI's model space. You need a hypercomputer plus one to comprehend the halting behavior of hypercomputers.
But a Solomonoff Inductor can predict the behavior of a hypercomputer at least well as you can, because you're computable, and so you're inside the Solomonoff Inductor. Literally. It's got an exact copy of you in there.

Didn't see this at the time, sorry.
So... I'm sorry if this reply seems a little unhelpful, and I wish there was some way to engage more strongly, but...
Point (1) is the main problem. AIXI updates freely over a gigantic range of sensory predictors with no specified ontology - it's a sum over a huge set of programs, and we, the users, have no idea what the representations are talking about, except that at the end of their computations they predict, "You will see a sensory 1 (or a sensory 0)." (In my preferred formalism, the program puts a probability on a 0 instead.) Inside, the program could've been modeling the universe in terms of atoms, quarks, quantum fields, cellular automata, giant moving paperclips, slave agents scurrying around... we, the programmers, have no idea how AIXI is modeling the world and producing its predictions, and indeed, the final prediction could be a sum over many different representations.
This means that equation (20) in Hutter is written as a utility function over sense data, where the reward channel is just a special case of sense data. We can easily adapt this equation to talk about any function computed directly over sense data - we can get AIXI to optimize any aspect of its sense data that we please. We can't get it to optimize a quality of the external universe. One of the challenges I listed in my FAI Open Problems talk, and one of the problems I intend to talk about in my FAI Open Problems sequence, is to take the first nontrivial steps toward adapting this formalism - to e.g. take an equivalent of AIXI in a really simple universe, with a really simple goal, something along the lines of a Life universe and a goal of making gliders, and specify something given unlimited computing power which would behave like it had that goal, without pre-fixing the ontology of the causal representation to that of the real universe, i.e., you want something that can range freely over ontologies in its predictive algorithms, but which still behaves like it's maximizing an outside thing like gliders instead of a sensory channel like the reward channel. This is an unsolved problem!
We haven't even got to the part where it's difficult to say in formal terms how to interpret what a human says s/he wants the AI to plan, and where failures of phrasing of that utility function can also cause a superhuman intelligence to kill you. We haven't even got to the huge buried FAI problem inside the word "optimal" in point (1), which is the really difficult part in the whole thing. Because so far we're dealing with a formalism that can't even represent a purpose of the type you're looking for - it can only optimize over sense data, and this is not a coincidental fact, but rather a deep problem which the AIXI formalism deliberately avoided.
(2) sounds like you think an AI with an alien, superhuman planning algorithm can tell humans what to do without ever thinking consequentialistically about which different statements will result in human understanding or misunderstanding. Anna says that I need to work harder on not assuming other people are thinking silly things, but even so, when I look at this, it's hard not to imagine that you're modeling AIXI as a sort of spirit containing thoughts, whose thoughts could be exposed to the outside with a simple exposure-function. It's not unthinkable that a non-self-modifying superhuman planning Oracle could be developed with the further constraint that its thoughts are human-interpretable, or can be translated for human use without any algorithms that reason internally about what humans understand, but this would at the least be hard. And with AIXI it would be impossible, because AIXI's model of the world ranges over literally all possible ontologies and representations, and its plans are naked motor outputs.
Similar remarks apply to interpreting and answering "What will be its effect on _?" It turns out that getting an AI to understand human language is a very hard problem, and it may very well be that even though talking doesn't feel like having a utility function, our brains are using consequential reasoning to do it. Certainly, when I write language, that feels like I'm being deliberate. It's also worth noting that "What is the effect on X?" really means "What are the effects I care about on X?" and that there's a large understanding-the-human's-utility-function problem here. In particular, you don't want your language for describing "effects" to partition, as the same state of described affairs, any two states which humans assign widely different utilities. Let's say there are two plans for getting my grandmother out of a burning house, one of which destroys her music collection, one of which leaves it intact. Does the AI know that music is valuable? If not, will it not describe music-destruction as an "effect" of a plan which offers to free up large amounts of computer storage by, as it turns out, overwriting everyone's music collection? If you then say that the AI should describe changes to files in general, well, should it also talk about changes to its own internal files? Every action comes with a huge number of consequences - if we hear about all of them (reality described on a level so granular that it automatically captures all utility shifts, as well as a huge number of other unimportant things) then we'll be there forever.
I wish I had something more cooperative to say in reply - it feels like I'm committing some variant of logical rudeness by this reply - but the truth is, it seems to me that AIXI isn't a good basis for the agent you want to describe; and I don't know how to describe it formally myself, either.

I worry that this conversation is starting to turn around points of phrasing, but... I think it's worth separating the ideas that you ought to be doing x-risk reduction and that SIAI is the most efficient way to do it, which is why I myself agreed strongly with your own, original phrasing, that the key claim is providing the most efficient x-risk reduction. If someone's comparing SIAI to Rare Diseases in Cute Puppies or anything else that isn't about x-risk, I'll leave that debate to someone else - I don't think I have much comparative advantage in talking about it.

Your statement was that it was an extraordinary claim that SIAI provided x-risk reduction - why then would SIAI be compared to most other charities, which don't provide x-risk reduction, and don't claim to provide x-risk reduction? The AI-risk item was there for comparison of standards, as was global warming; i.e., if you claim that you doubt X because of Y, but Y implies doubting Z, but you don't doubt Z, you should question whether you're really doubting X because of Y.


The claim that an organization is exceptionally well-suited to convert money into existential risk mitigation is an extraordinary one, and extraordinary claims require extraordinary evidence.

Reminder: I don't know if you were committing this particular error internally, but, at the least, the sentence is liable to cause the error externally, so: Large consequences != prior improbability. E.g. although global warming has very large consequences, and even implies that we should take large actions, it isn't improbable a priori that carbon dioxide should trap heat in the atmosphere - it's supposed to happen, according to standard physics. And so demanding strong evidence that global warming is anthropogenic is bad probability theory and decision theory. Expensive actions imply a high value of information, meaning that if we happen to have access to cheap, powerfully distinguishing evidence about global warming we should look at it; but if that evidence is not available, then we go from the default extrapolation from standard physics and make policy on that basis - not demand more powerful evidence on pain of doing nothing.
The claim that SIAI is currently best-suited to convert marginal dollars into FAI and/or general x-risk mitigation has large consequences. Likewise claims like "most possible self-improving AIs will kill you, although there's an accessible small space of good designs". This is not the same as saying that if the other facts of the world are what they appear at face value to be, these claims should require extraordinary evidence before we believe them.
Since reference class tennis is also a danger (i.e, if you want to conclude that a belief is false, you can always find a reference class in which to put it where most beliefs are false, e.g. classifying global warming as an "apocalyptic belief"), one more reliable standard to require before saying "Extraordinary claims require extraordinary evidence" is to ask what prior belief needs to be broken by the extraordinary evidence, and how well-supported that prior belief may be. Suppose global warming is real - what facet of existing scientific understanding would need to change? None, in fact; it is the absence of anthropogenic global warming that would imply change in our current beliefs, so that's what would require the extraordinary evidence to power it. In the same sense, an AI showing up as early as 2025, self-improving, and ending the world, doesn't make us say "What? Impossible!" with respect to any current well-supported scientific belief. And if SIAI manages to get together a pack of topnotch mathematicians and solve the FAI problem, it's not clear to me that you can pinpoint a currently-well-supported element of the world-model which gets broken.
The idea that the proposition contains too much burdensome detail - as opposed to an extraordinary element - would be a separate discussion. There are fewer details required than many strawman versions would have it; and often what seems like a specific detail is actually just an antiprediction, i.e., UFAI is not about a special utility function but about the whole class of non-Friendly utility functions. Nonetheless, if someone's thought processes were dominated by model risk, but they nonetheless actually cared about Earth's survival, and were generally sympathetic to SIAI even as they distrusted the specifics, it seems to me that they should support CFAR, part of whose rationale is explicitly the idea that Earth gets a log(number of rationalists) saving throw bonus on many different x-risks.

I just stick with the timeless view and don't have any trouble with ethics in it, but that's because I've got all the phenomena of time fully embedded in the timeless view, including choice and morality. :)

This is exactly equivalent to not paying, which is precisely the IRS rationale for why donated services aren't directly deductible.

In the one instance of a non-profit getting accounting work done that I know of, the non-profit paid and then received an equal donation. Magic.

Pretty sure it was spelled out explicitly.

Yep. We knew that would happen at the time - it was explicitly discussed in the Board meeting - and we went ahead and reported it anyway, partly because we didn't want to have exposable secrets, partly because we felt honesty was due our donors, and partially because I'd looked up embezzlement-related stuff online and had found that a typical nonprofit-targeting embezzler goes through many nonprofits before being reported and prosecuted by a nonprofit "heroic" enough, if you'll pardon the expression, to take the embarrassment-hit in order to stop the embezzler.

You can't deduct the value of services donated to nonprofits. Not sure your friend is as knowledgeable as stated. Outside accounting is expensive and the IRS standard is to start doing it once your donations hit $2,000,000/year, which we haven't hit yet. Also, SIAI recently passed an IRS audit.

http://lesswrong.com/lw/u6/horrible_lhc_inconsistency/

Four.

Given these beliefs, you should buy cryonics at almost any price, including prices at which I would no longer personally sign up and prices at which I would no longer advocate that other people sign up. Are you signed up? If not, then I upvote the above comment because I don't believe you believe it. :)

Causes and effects.

The conventional reply is that noise traders improve markets by making rational prediction more profitable. This is almost certainly true for short-term noise, and my guess is that it's false for long-term noise, i.e., if prices revert in a day, noise traders improve a market, if prices take ten years to revert, the rational money seeks shorter-term gains. Prediction markets may be expected to do better because they have a definite, known date on which the dumb money loses - you can stay solvent longer than the market stays irrational.

Not necessarily. If I did, in fact, possess such a basilisk, I cannot think offhand of any occasion where I would have actually used it. Robert Mugabe doesn't read my emails, it's not clear that killing him saves Zimbabwe, I have ethical inhibitions that I consider to exist for good reasons, and have you thought about what happens if somebody else glances at the computer screen afterward, and resulting events lead to many agents/groups possessing a basilisk?

This seems like a clear example of "You shouldn't adjust the probability that high just because you're trying to avoid overconfidence; that's privileging a complicated possibility."

I was wondering whether not a Safeway cake slice contains trans fats. Safeway's advertising in their deli section says that they don't cook using trans fats, which my brain was using to argue that a cake slice was probably safe. Once I noticed my brain arguing-for-a-side, I HMC'd (Halt, Melt, and Catch fire) and looked it up online. Their cake slices definitely do contain trans fat in large quantities, and are also a lot more caloric than I would've estimated.


"Buddhism IS different. It's the followers who aren't."
-- A Dust Over India.

Commentary: Reading this made me realize that many religions genuinely are different from each other. Christianity is genuinely different from Judaism, Islam is genuinely different from Christianity, Hinduism is genuinely different from all three. It's religious people who are the same everywhere; not the same as each other, obviously, but drawn from the same distribution. Is this true of atheistic humanists? Of transhumanists? Could you devise an experiment to test whether it was so, would you bet on the results of that experiment? Will they say the same of LessWrongers, someday? And if so, what's the point?
Now that I think on it, though, there might be a case for scientists being drawn from a different distribution, or computer programmers, or for that matter science fiction fans (are those all the same distributions as each other, I wonder?). It's not really hopeless.

I am not sure your alternative is immediately practical.

IRRATIONALITY GAME
Eliezer Yudovsky has access to a basilisk kill agent that allows him to with a few clicks untraceably assassinate any person he can get to read a short email or equivalent, with comparable efficiency to what is shown in Deathnote.
Probability: improbable ( 2% )

For many people, writing is hard even if they are good at math. It is why Verbal and Mathematical SAT scores do not perfectly correlate. It's a different talent, and it will indeed filter people who don't happen to have it. Even bad writing is hard - and if you can't bear to write badly and don't have the talent to write well, it's much much worse. It filters people who want to do their jobs well and don't happen to possess author talent, because they'll revise, and revise, and revise, staring at their work and feeling the dreadful pain of how bad it is... yes, it's a needless filter!

I note that it also makes no sense to filter excellent scientists who aren't good writers or who take a long time to write (e.g. PhD dissertation test). If you can do the research, someone else should be able to specialize in writing for you.
It's remarkable how many barrier-to-entry professions revolve around the denial of professional specialization. A surgeon can't just be someone of moderate intelligence and exceptional dexterity who studies and practices one key surgery, no, they need 11 years of medical school that they'll mostly never use. A scientist is forced to write. And so on.

Shangri-La doesn't work for everyone, but it's very painless to try and works amazingly for some - have you tried it?

Pomodoro didn't work for me. I can see occasions where it might be useful to take occasional breaks, not necessarily at timed intervals... but it wasn't magic, didn't seem to increase productivity, and felt annoying.

I also tried American Gods for a while and found that its charm was mostly lost on me - maybe I didn't get far enough in. Good Omens, on the other hand...
Understand, I always knew that Good Omens was a great book and that I wasn't yet writing that well; it's only now that I'm staring at a Neil Gaiman short story, thinking, "I can tell that he's doing something outstandingly right here that I'm not doing, but it's hard to say exactly what..."

Switching to the Dvorak keyboard cured my RSI.

My parents visited Israel when I was a kid. My grandparents' apartment had Israeli air-raid-quality shutters which ACTUALLY blocked out all the light; they were wooden slats that rolled down and stacked themselves solidly over the outside of the window. You pulled on the cord and the light went out completely - that simple. I expect it helped on noise reduction too, though I wasn't checking then. Ever since, I've taken the lack of this simple, extremely useful feature on any other windows I've ever seen, as proof that the housing industry is dysfunctional.


Isn't this what curtains and shutters are for?

Yes, it's what they are for - but they are typically inferior alternatives for the specific goal of preventing light entry.

Nope. That's nailed down way more solidly than anything I know about mere matters of culture and society, so any tension between it and another proposition would move the other, less certain one. It would cause me to update in the direction of believing that more physicists probably see MWI as slam-dunk. :)

I put that in because I didn't think any non-trolls would seriously dispute the 99+% part, not because I knew how to measure it down to the sixth decimal place.

I haven't put as many skill points into it.

If you can find a way to settle the bet, I bet they don't do that. Universities would look extremely different if they optimized for learning in even the most basic ways. This is the should-universe you're talking about, not the is-universe.

If I can't do any flips on the trampoline, I'm not sure it's worth it.
Another question is whether there's any simple neck-brace I can wear to avoid spinal injuries, which are the main thing I'm worried about. I'm okay with a 0.1% chance of pain, it's life-altering injuries (or more to the point, work-altering injuries) that I want to avoid.

That'd be a much harder question to answer; my talent is specialized toward figuring out the shape of the right theorem to be proved, not the actual proof which is where most modern math concentrates its prestige. (This is an objectively verifiable form of mathematical talent; it means that sometimes Marcello would prove something and I would look at it and say, "That doesn't look right" and at least half the time there'd be a mistake.) I feel insecure about not being an expert in the tools by which most modern mathematicians measure basic competence; I can also apparently make "well, if that's your problem, try transforming it this way" suggestions to someone doing graduate mathematical research at Yale that are accepted as brilliant. I confidently depose that, even taking unusual talents into account, I am not in the literal top tier of mathematical potential - if I can explain basic Bayes better than anyone or was first to state the Lob problem or invented TDT, those outputs drew on at least some non-mathematical high-percentile sections of my brain (explanatory ability in the first case, or what's somewhat vaguely referred to as "philosophical" talent in the other two). I'm also reasonably confident that, given a hundred modern mathematicians, an average of zero will pick the right problem to solve.
I think I'm comfortable at this point with saying that I'm in the top 99+% of writers - I've been picking up "real" books and trying to read them and finding that they seem visibly badly-written to me now that I've written HPMOR. Though I'm still not in the literal top tier; there are basic things in writing that I still don't do too well, despite being outstanding in others, and my new level of skill is just enough to start noticing that Neil Gaiman and Terry Pratchett are doing things way the hell above me.

Programmmers.

Empty tissue boxes. Use them to prop up the back layer of paperback books on your double-stacked bookshelves. Now you can see most of the titles of the books in the back row. If you want to upgrade in style, get some 2-by-4s cut to the right length at your local hardware store.

Aluminum foil. Use a gluestick to put it over your bedroom windows. Now there is darkness, and you can sleep. This made a huge quality-of-life difference to me, and I felt very silly for not doing it 10 years earlier. (A sleep mask, which I previously used, was not nearly as good a solution.)

I use Leight (laser lite, uncorded):
http://www.amazon.com/Howard-Leight-Laser-Earplugs-Cords/dp/B0007XJOLG/ref=sr_1_2?s=hi&ie=UTF8&qid=1341456079&sr=1-2&keywords=leight+laser+uncorded+200
These are definitely much cheaper, and I suspect feel softer, while offering the same alleged protection.
Using a pair of earplugs every night is a huge quality of life difference.

This is not a product recommendation, but a request - it looks to me like trampolines should be large amounts of fun. However, when I tried to look up risk statistics, I found lots of dire warnings and, of course, no numerical annual risk statistics at all, or any attempt to adjust for safer trampolines with surrounding safety netting. My one attempt to calculate risk statistics on my own output a 0.1% chance of an injury requiring hospitalization per year of trampoline use. That's probably more risk than somebody in my position should take on, even for the sake of exercise. Does anyone know of more accurate statistics than this, or a safer trampoline with recorded risk statistics, or have a strong opinion on whether trampolines are safe enough to use?

J/FIT stability balls, for sitting-on - I tried a TKO, but it had a persistent smell that made me nauseous. I can't say yet that I use mine for hours at a time, but it's fun to bounce on now and then, and costs $25 at Amazon. I'm 5'11" and need a 75cm ball to balance properly.

Hearos Ultimate Softness foam earplugs Great protection (32NRR), super comfortable. Use these for sleeping.

Macroscopic determinism, i.e., the belief that an outcome was not sensitive to small thermal (never mind quantum) fluctuations. If I'm hungry and somebody offers me a tasty hamburger, it's macroscopically determined that I'll say yes in almost all Everett branches; if Zimbabwe starts printing more money, it's macroscopically determined that their inflation rates will rise further.

What my externally observable percentiles look like:

Writing: 99+%
Math: 99+%
Conceptual originality: 99+%
Programming: 95%
Conformity / ability to obey incorrect orders: 20%

What my educational credentials look like:

Highest level of education completed: 8th grade


Physicists have their act together better than I thought. Not sure how much I should update on other scientific fields dissimilar to physics (e.g. "dietary science") or on the state of academia or humanity as a whole. Probably "some but not much" for dietary science, with larger updates for fields more like physics.

Nonetheless, the sentiment "You can do X, but only once" seems broadly useful.

It's amazing, the results people come up with when they don't use TDT (or some other formalism that doesn't defect in the Prisoner's Dilemma - though so far as I know, the concept of the Blackmail Equation is unique to TDT.)
(Because the base case of the pirate scenario is, essentially, the Ultimatum game, where the only reason the other person offers you $1 instead of $5 is that they model you as accepting a $1 offer, which is a very stupid answer to compute if it results in you getting only $1 - only someone who two-boxed on Newcomb's Problem would contemplate such a thing.)

I was wondering how someone managed to end up as God-Emperor of Wisconsin.

Again, I've tried to share it already in e.g. CEV. I can't be maximally specific in every LW comment.

Catholicism has probably spent a heck of a lot more money on complex proselytizing than Orthodox Judaism. Also Catholics were competing with the Protestants - rabbis have no real competition, since their only audience is Orthodox Jews. But mostly, my point is just that there's this huge, worldwide organized Church that has spent who knows how many equivalent billions of dollars on theology. It's amazing how little they've accomplished, really, given how much they've spent and how many geniuses it wasted (theology was the string theory of its day), but they still did end up with something. Probably an equivalent amount of raw genius, if not money, was wasted on Orthodox Judaic halacha, but in a much less competitive, outside-world-facing way.

The concept of a supercausal cause is nonsense of the highest order; e.g. "God speaks to me in my heart, and you can't scientifically refute that because it has no experimental consequences". But if you define the "supernatural" as "ontologically basic mental stuff not reducible to non-mental parts, like the Force in Star Wars", then it is much less obviously nonsense; nonsense of a lower order, which is harder to detect.

Whatever it is that rationalists are supposed to use instead of death spirals, we don't have enough of it until everything is funded. GO TEAM HAPPINESS!

I think you're forgetting about Orthodox Jews, who have the Catholics beat on pretty much all counts (age, complexity, and at least arguably "reason"). Of course, it's all mere rationalization -- the bottom line has already been written. And the Orthodox tend to reason within their framework rather than trying to justify their framework to outsiders, presumably because they're not seeking converts.

I've tried to share the reasoning already. Mostly it boils down to "the problem is finite" and "you can recurse on it if you actually try". Certainly it will always sound more convincing to someone who can sort-of see how to do it than to someone who has to take someone else's word for it, and to those who actually try to build it when they are ready, it should feel like solider knowledge still.

GAI is indeed hard and FAI is indeed substantially harder. (BECAUSE YOU HAVE TO USE DIFFERENT AGI COMPONENTS IN AN AI WHICH IS BEING BUILT TO COHERENT NARROW STANDARDS, NOT BECAUSE YOU SIT AROUND THINKING ABOUT CEV ALL DAY. Bolded because a lot of people seem to miss this point over and over!)
However, if you haven't solved either of these problems, I must ask you how you know that it is harder than anything humans have ever done. It is indeed different from anything humans have ever done, and involves some new problems relative to anything humans have ever done. I can easily see how it would look more intimidating than anything you happened to think of comparing it to. But would you be scared that nine people in a basement might successfully, by dint of their insight, build a copy of the Space Shuttle? Clearly I stake quite a lot of probability mass on the problem involving less net labor than that, once you know what you're doing. Again, though, the key insight is just that you don't know how complex the solution will look in retrospect- as opposed to how intimidating the problem is to stare at unsolved - until after you've solved it. We know nine people can't build a copy of a NASA-style Space Shuttle (at least not without nanotech) because we know how to build one.
Suppose somebody predicted with 90% probability that the first manned Space Shuttle launch would explode on the pad, even if Richard Feynman looked at it and signed off on the project, because it was big and new and different and you didn't see how anything that big could get into orbit. Clearly they would have been wrong, and you would wonder how they got into that epistemic state in the first place. How is an FAI project disanalogous to this, if you're pulling the 90% probability out of ignorance?

Why, thanks! It's helpful to hear you say that!

Thanks for reinforcing Luke! And it's great that you applied the theory so quickly!

This sounds like you would tend to assign 90% irreducible doom probability from the best possible FAI effort. What do you think you know, and how do you think you know it?

I'm genuinely unsure what you're talking about. I presume the bolded quote is the bad question, and the implied answer is "No, you can't get into an epistemic state where you assign 90% probability to that", but what do you think the correct answer is? I think the implied answer is true.

Not all arguing aimed at people with different premises is Dark Arts, y'know. I wouldn't argue from the Bible, sure. But trying to make relatively vague arguments accessible to people in a greater state of ignorance about FAI, even though I have more specific knowledge of the issue that actually persuades me of the conclusion I decided to argue? I don't think that's Dark, any more than it's Dark to ask a religious person "How could you possibly know about this God creature?", when you're actually positively convinced of God's nonexistence by much more sophisticated reasoning like the general argument against supernaturalism as existing in the model but not the territory. The simpler argument is valid - it just uses less knowledge to arrive at a weaker version of the same conclusion.
Likewise my reply to Strange; yes, I secretly know the problem is hard for much more specific reasons, but it's also valid to observe that if you don't know how to make the subroutine you don't know that it's small, and this can be understood with much less explanation, albeit it reaches a weaker form of the conclusion.

What was the project about and why was it obviously hopeless?

"Entropy is wrong" is where I stopped reading the Kickstarter.

Was a high-level skeptic before she was a CFAR instructor - I would guess that most of her accumulated skill is still from outside our Conspiracy - the point being that there are people like this outside the community ("Muggleborns"), they just have skeptic blogs or something.

You can get 300mcg capsules. I use:
http://www.amazon.com/Melatonin-300-mcg-100-capsules/dp/B000X9QZZ2/ref=pd_sim_hpc_3
http://www.amazon.com/Life-Extension-Melatonin-Release-Capsule/dp/B00445HR3K

Also try 300mcg (0.3 mg). If high doses of melatonin don't work or don't work well or seem to have no effect, the next step is to try a lower dosage.

Yesterday I got what was probably a mass email to all of someone's friends, but phrased as a personal request, "Would you please do me this huge favor? Read (this Kickstarter project) and forward it on if you agree with it." I read the Kickstarter project; it was obviously hopeless. And since I'd recently gotten bitten on it, I managed to disagree with the internal part that said, "If you don't explain why this is doomed, nobody will ever tell it to this person; you are the only one who is willing to accept any personal cost, like loss of relationship-points, to bring a benefit to others, like warning them about a doomed project" with the recently formed heuristic "Assume anyone below the level of Julia is probably outside the tiny class of people who experience anything but useless pain on hearing their personal ideas specifically contradicted with any amount of politeness I know how to wield, until specific evidence to the contrary has been gathered in some low-cost way."
Techniques used:
1) Actually update on the evidence eventually.
2) Talk to other people out loud about the problems I've been having about updating on this evidence in hopes this causes my brain to actually remember next time what happened last time.
Technique it seems like I could've used but didn't explicitly invoke:
3) Stop living in the should-universe.

Original: http://www.senderberl.com/jewish/trial.htm

I am thiiiiiiiiis confident!
(Holds arms wide, then accepts any well-specified bet as if the actual probability of Christianity were zero, i.e., with betting prices corresponding to the probability of the specified evidence being observed, given the fixed assumption that Christianity is false.)

But NASA code can't check itself - there's no attempt at having an AI go over it.

Probably violent agreement.

Yay!

That's less of a rephrasing and more of a relocating the goalposts across state lines. "Choosing what to say," properly unpacked, is approximately every part of the AI that doesn't already exist.

Yes. That's the problem with the ring architecture.

This is reasonable, but note that to strengthen the validity, the conclusion has been weakened (unsurprisingly). To take a system that you think is fundamentally, structurally safe and then further build in error-delaying, error-resisting, and error-reporting factors just in case - this is wise and sane. Calling "adding impediments to some errors under some circumstances" hardwiring and relying on it as a primary guarantee of safety, because you think some coded behavior is firmly in place locally independently of the rest of the system... will usually fail to cash out as an implementable algorithm, never mind it being wise.

Suppose your utility function U is in ring 0 and the parts of you that extrapolate consequences are in ring 3. If I can modify only ring 3, I can write my own utility function Q, write ring-3 code that first extrapolates consequences fairly, pick the one that maximizes Q, and then provides a "prediction" to ring 0 asserting that the Q-maximizing action has consequence X that U likes, while all other actions have some U-disliked or neutral consequence. Now the agent has been transformed from a U-maximizer to a Q-maximizer by altering only ring 3 code for "predicting consequences" and no code in ring 0 for "assessing utilities".
One would also like to know what happens if the current AI, instead of "self"-modifying, writes a nearly-identical AI running on new hardware obtained from the environment.

I'd like to know what this small subroutine looks like. You know it's small, so surely you know what's in it, right?

Let me rephrase. The part of the agent that chooses what to say to the user - what ring is that in?

And what does a multi-ring agent architecture look like? Say, the part of the AI that outputs speech to a microphone - what ring is that in?


There is, in fact, such a thing as making some parts of the code more difficult to modify than other parts of the code.

There is? How?

Option 3? Doesn't work very well. You're assuming the opponent doesn't want to threaten the bishop, which means you yank it to a place where it would be safe if the opponent doesn't want to threaten it, but if the opponent clues in, it's then trivial for them to threaten the bishop again (to gain more advantage as you try to defend), which you weren't expecting them to do, because that's not how your search tree was structured. Kasparov would kick hell out of thus-hardwired Deep Blue as soon as he realized what was happening.
It's that whole "see the consequences of the math" thing...

I'm saying that using the word "hardwiring" is always harmful because they imagine an instruction with lots of extra force, when in fact there's no such thing as a line of programming which you say much more forcefully than any other line. Either you know how to program something or you don't, and it's usually much more complex than it sounds even if you say "hardwire". See the reply above on "hardwiring" Deep Blue to protect the light-square bishop. Though usually it's even worse than this, like trying to do the equivalent of having an instruction that says "#define BUGS OFF" and then saying, "And just to make sure it works, let's hardwire it in!"

Yeah, well, hardwiring the AI to understand human desires wouldn't be goddamned trivial either, I just decided not to go down that particular road, mostly because I'd said it before and Holden had apparently read at least some of it.
Getting the light-square bishop out of danger as highest priority...
1) Do I assume the opponent assigns symmetric value to attacking the light-square bishop?
2) Or that the opponent actually values checkmates only, but knows that I value the light-square bishop myself and plan forks and skewers accordingly?
3) Or that the opponent has no idea why I'm doing what I'm doing?
4) Or that the opponent will figure it out eventually, but maybe not in the first game?
5) What about the complicated static-position evaluator? Do I have to retrain all of it, and possibly design new custom heuristics, now that the value of a position isn't "leads to checkmate" but rather "leads to checkmate + 25% leads to bishop being captured"?
Adding this to Deep Blue is not remotely as trivial as it sounds in English. Even to add it in a half-assed way, you have to at least answer question 1, because the entire non-brute-force search-tree pruning mechanism depends on guessing which branches the opponent will prune. Look up alpha-beta search to start seeing why everything becomes more interesting when position-values are no longer being determined symmetrically.

Delete the word "hardwiring" from your vocabulary. You can't do it with wires, and saying it doesn't accomplish any magic.

But - that's not what he meant!

Holden didn't actually suggest that. And while this suggestion is in a certain sense ingenious - it's not too far off from the sort of suggestions I flip through when considering how/if to implement CEV or similar processes - how do you "report the actions"? And do you report the reasons for them? And do you check to see if there are systematic discrepancies between consequences in the true model and consequences in the manipulated one? (This last point, btw, is sufficient that I would never try to literally implement this suggestion, but try to just structure preferences around some true model instead.)

...or at least full-time professionals who know that the pitfalls exist, so they can move forward if they learn to avoid pitfalls and otherwise take different routes.

Hardwiring the AI to be extremely naive about how easy the user is to manipulate might not be sufficient for safety, but it does seem like a pretty good start.

Nonetheless, where is he getting the 90% doom probability from?

I wouldn't endorse their significance the same way, and would stand by my statement that although the AGI field as a whole has perceptible risk, no individual project that I know of has perceptible risk. Shane and Demis are cool, but they ain't that cool.

Nope, I was assuming the second reading. The first reading is too implausible to be considered at all.

Looks pretty good, actually. Nice.

I am microdosing, 300mcg plain + 300mcg time-release, both from LEF. Currently trying doubling the time-release dosage to see if that works better. More than 400mcg up-front did not work well. In general, people trying melatonin are advised to try microdoses.

I have looked up the rules and apparently, instead of a plurality vote or a majority vote with runoff voting, people rank all the candidates and can put "No Award" above anything they think sufficiently underserving. This does change my odds, but not enough that I wouldn't try to for a nomination. I'm also not sure that it should change my odds enough not to bet at 20-to-1. How much would you want in for?

A lot more people have heard of Michael Jordan than have heard of Norman Borlaug. Yet Borlaug is one of the few humans on the planet who can be personally credited with saving millions of lives. Who one has heard of is not likely to be highly correlated with what impact people have had.

(I did perform a quick Google check after writing the comment and before posting it, just to make sure.)

Upvoted for the "related".

And I've never heard of him, so perhaps he didn't change the world either.

Should it be made, it will of course be known as Elieza.
But in any case I think you need to keep in mind that a blank map does not correspond to a blank territory.

Yup!

For the benefit of people who have no idea of what a color scheme looks like (cough), please tell them what color to use for the comforter, the walls and ceiling, the accent wall, the lights, and any rug or carpeting. Otherwise the whole post isn't actionable - a problem, not a task - for anyone who doesn't grok color schemes.

You know, you're right. I will change my reply accordingly henceforth - linear population growth, linear increase in energy usage / computing power, and quadratic increase in (nonenergetically stored) memories.

If your civilization expands at a cubic rate through the universe, you can have one factor of linear growth for population (each couple of 2 has exactly 2 children when they're 20, then stop reproducing) and one factor of quadratic growth for minds (your mind can go as size N squared with time N). This can continue until the accelerating expansion of the universe places any other galaxies beyond our reach, at which point some unimaginably huge superintelligent minds will, billions of years later, have to face some unpleasant problems, assuming physics-as-we-know-it cannot be dodged, worked around, or exploited.
Meanwhile, PARTY ON THE OTHER SIDE OF THE MILKY WAY! WOO!

What I'm saying is that you can either pay $2000 of rent using post-tax income or $2000 of mortgage using pretax income. This might work out to the difference between a $3300 mortgage payment (pretax income) or a $2000 rent payment (after the $3300 has been taxed at an e.g. 39% marginal rate by state and feds).

If mortgage interest is tax-deductible but rent isn't, then you have to pay higher rent in order for it to be converted into an interest payment that would come out of pretax income. I think this is how Michael Vassar said the market got so messed up, though I don't know if I'm correctly attributing it to him, or if the notion is unique to him (I expect not).

Do people around here say that? I think/hope I would've noticed that. There were a few comments like that during the QM sequence but they were corrected, I think.

"Break down what your parts have to say into parts" would be an interesting counter to rationalization - I think I'll have to call this an immediate $50 award on the grounds that I intend to test the skill itself, never mind how to teach it.

The primary problem and question is whether a pattern-identical version of you with different causal antecedents is the same person. Believing that uploading works, in virtue of continuing the same type of pattern from a single causal antecedent, does not commit you to believing that destruction followed by a whole new causal process coincidentally producing the same pattern, is continuity of consciousness.
Many will no doubt reply saying that what I am questioning is meaningless, in which case, I suppose, it works as well as anything does; but for myself I do not dismiss such things as meaningless until I understand exactly how they are meaningless.

I am loathe to point to a comment a user has actually made, but anything like "I decided to go to grad school because I'm better off in the Everett branch where I have a post-grad degree than the Everett branch in which I don't." No, Mr. Example, you are not going to turn into 1/sqrt(2)|grad student> + 1/sqrt(2)|not grad student>. To the extent to which you are able to choose at all, your decision algorithm is deterministic. What you really mean is "I'm better off in the counterfactual where I have a post-grad degree."

I checked it. Not the same thing.

This seems well-specified to me: Since the agent is not told its own output in advance, it is possible to run the "simulation" and the "real version" in finite time. If you hand me a computer program that is the agent, I will hand you a computer program that is Omega and the environment.

There were plenty of previous theories trying to go beyond CDT or EDT, they just weren't satisfactory.

(Bows.) Thank you for Being Specific!

Fascinating, but... my Be Specific detector is going off and asking, not just for the abstract generalizations you concluded, but the specific examples that made you conclude them. Filling in at least one case of "I thought I should dress like X, but then Y happened, now I dress like Z", even - my detector is going off because all the paragraphs are describing the abstract conclusions.

Luke has just told me (personal conversation) that what he got from my comment was, "SIAI's difficulties were just due to lack of funding" which was not what I was trying to say at all. What I was trying to convey was more like, "I didn't have the ability to run this organization, and knew this - people who I hoped would be able to run the organization, while I tried to produce in other areas (e.g. turning my back on everything else to get a year of FAI work done with Marcello or writing the Sequences) didn't succeed in doing so either - and the only reason we could hang on long enough to hire Luke was that the funding was available nonetheless and in sufficient quantity that we could afford to take risks like paying Luke to stay on for a while, well before we knew he would become Executive Director".

Thank you for saying this, Grognor! As you say, being willing to come out and say such is an important antidote to that phyg-ish nervous expression.

Sounds like group selection theory to me. If you have a group of slaves and one slave has a gene that makes them unproductive, I doubt they'll have better reproductive fitness than the others - you'd just dispose of the bad slave. It only works for all the slaves, if all the slaves have the same gene - so it only works if group selection works, and group selection doesn't work.


IIRC Roko deleted the speculation-about-superintelligences part of the post shortly after its publication, but discussion in the comments raged on, so you subsequently banned the whole post/discussion.

This sounds right to me, but I still have little trust in my memories.


Your comment also made me recall another comment you [Kip] wrote a couple of years ago about how my status in this community made a criticism of you feel like a "huge insult", which I couldn't understand at the time and just ignored.

My brain really, really does not want to update on the numerous items of evidence available to it that it can hit people much much harder now, owing to community status, than when it was 12 years old.

Now I've got no idea what I did. Maybe my own memory was mixed up by hearing other people say that the post was deleted by Roko? Or Roko retracted it after I banned it, or it was banned and then unbanned and then Roko retracted it?
I retract my grandparent comment; I have little trust for my own memories. Thanks for catching this.

I like this chart.

Once again: ROKO DELETED HIS OWN POST. NO OUTSIDE CENSORSHIP WAS INVOLVED.
This is how rumors evolve, ya know.

I agree that top mainstream AI guy Peter Norvig was way the heck more sensible than the reference class of declared "AGI researchers" when I talked to him about FAI and CEV, and that estimates should be substantially adjusted accordingly.

Ben Goertzel's projects are knowably hopeless, so I didn't too strongly oppose Tyler Emerson's project from within SIAI's then-Board of Directors; it was being argued to have political benefits, and I saw no noticeable x-risk so I didn't expend my own political capital to veto it, just sighed. Nowadays the Board would not vote for this.
And it is also true that, in the hypothetical counterfactual conditional where Goertzel's creations work, we all die. I'd phrase the email message differently today to avoid any appearance of endorsing the probability, because today I understand better that most people have trouble mentally separating hypotheticals. But the hypothetical is still true in that counterfactual universe, if not in this one.
There is no contradiction here.

AGI researchers sound a lot like FinalState when they think they'll have AGI cracked in two years.

That's always been the argument that future AGI scientists won't be as crazy as the lunatics presently doing it - that the current crowd of researchers are self-selected for incaution - but I wouldn't put too much weight on that; it seems like a very human behavior, some of the smarter ones with millions of dollars don't seem of below-average competence in any other way, and the VCs funding them are similarly incapable of backing off even when they say they expect human-level AGI to be created.

That's the kind of probability I would've assigned to EURISKO destroying the world back when Lenat was the first person ever to try to build anything self-improving. For a random guy on the Internet it's off by... maybe five orders of magnitude? I would expect a pretty tiny fraction of all worlds to have the names of homebrew projects carved on their tombstones, and there are many random people on the Internet claiming to have AGI.
People like this are significant, not because of their chances of creating AGI, but because of what their inability to stop or take any serious precautions, despite their belief that they are about to create AGI, tells us about human nature.

2%? 
Seriously? 
I am curious as to why your estimate is so high.

http://lesswrong.com/lw/qk/that_alien_message/


For utility function maximisers, the AIXI is the theoretically best agent there is, more successful at reaching its goals (up to a finite constant) than any other agent (Hutter, 2005).

False. AIXI as defined can maximize only a sensory reward channel, not a utility function over an environmental model with a known ontology. As Dewey demonstrates, this problem is not easy to fix; AIXI can have utility functions over (functions of) sensory data, but its environment-predictors vary freely in ontology via Solomonoff induction, so it can't have a predefined utility function over the future of its environment without major rewriting.
AIXI is the optimal function-of-sense-data maximizer for Cartesian agents with unbounded computing power and access to a halting oracle, in a computable environment as separated from AIXI by the Cartesian boundary, given that your prior belief about the possible environments matches AIXI's Solomonoff prior.


purchase enough computing power so that even the dumbest AIXI approximation schemes are extremely effective

There isn't that much computing power in the physical universe. I'm not sure even smarter AIXI approximations are effective on a moon-sized nanocomputer. I wouldn't fall over in shock if a sufficiently smart one did something effective, but mostly I'd expect nothing to happen. There's an awful lot that happens in the transition from infinite to finite computing power, and AIXI doesn't solve any of it.

The heck? Why would you not need to figure out if an oracle is an ethical patient? Why is there no such possibility as a sentient oracle?
Is this standard religion-of-embodiment stuff?

To reply to Wei Dai's incoming link:
Most math kills you quietly, neatly, and cleanly, unless the apparent obstacles to distant timeless trade are overcome in practice and we get a certain kind of "luck" on how a vast net of mostly-inhuman timeless trades sum out, in which case we get an unknown fixed selection from some subjective probability distribution over "fate much worse than death" to "death" to "fate much better than death but still much worse than FAI". I don't spend much time talking about this on LW because timeless trade speculation eats people's brains and doesn't produce any useful outputs from the consumption; only decision theorists whose work is plugging into FAI theory need to think about timeless trade, and I wish everyone else would shut up about the subject on grounds of sheer cognitive unproductivity, not to mention the horrid way it sounds from the perspective of traditional skeptics (and not wholly unjustifiably so). (I have expressed this opinion in the past whenever I hear LWers talking about timeless trade; it is not limited to Newsome, though IIRC he has an unusual case of undue optimism about outcomes of timeless trade, owing to theological influences that I understand timeless trade speculations helped exacerbate his vulnerability to.)

Reading Holden's transcript with Jaan Tallinn (trying to go over the whole thing before writing a response, due to having done Julia's Combat Reflexes unit at Minicamp and realizing that the counter-mantra 'If you respond too fast you may lose useful information' was highly applicable to Holden's opinions about charities), I came across the following paragraph:

My understanding is that once we figured out how to get a computer to do arithmetic, computers vastly surpassed humans at arithmetic, practically overnight ... doing so didn't involve any rewriting of their own source code, just implementing human-understood calculation procedures faster and more reliably than humans can. Similarly, if we reached a good enough understanding of how to convert data into predictions, we could program this understanding into a computer and it would overnight be far better at predictions than humans - while still not at any point needing to be authorized to rewrite its own source code, make decisions about obtaining "computronium" or do anything else other than plug data into its existing hardware and algorithms and calculate and report the likely consequences of different courses of action

I've been previously asked to evaluate this possibility a few times, but I think the last time I did was several years ago, and when I re-evaluated it today I noticed that my evaluation had substantially changed in the interim due to further belief shifts in the direction of "Intelligence is not as computationally expensive as it looks" - constructing a non-self-modifying predictive super-human intelligence might be possible on the grounds that human brains are just that weak. It would still require a great feat of cleanly designed, strong-understanding-math-based AI - Holden seems to think this sort of development would happen naturally with the sort of AGI researchers we have nowadays, and I wish he'd spent a few years arguing with some of them to get a better picture of how unlikely this is. Even if you write and run algorithms and they're not self-modifying, you're still applying optimization criteria to things like "have the humans understand you", and doing inductive learning has a certain inherent degree of program-creation to it. You would need to have done a lot of "the sort of thinking you do for Friendly AI" to set out to create such an Oracle and not have it kill your planet.
Nonetheless, I think after further consideration I would end up substantially increasing my expectation that if you have some moderately competent Friendly AI researchers, they would apply their skills to create a (non-self-modifying) (but still cleanly designed) Oracle AI first - that this would be permitted by the true values of "required computing power" and "inherent difficulty of solving problem directly", and desirable for reasons I haven't yet thought through in much detail - and so by Conservation of Expected Evidence I am executing that update now.
Flagging and posting now so that the issue doesn't drop off my radar.

Jaan's reply to Holden is also correct:

... the oracle is, in principle, powerful enough to come up with self-improvements, but refrains from doing so because there are some protective mechanisms in place that control its resource usage and/or self-reflection abilities. i think devising such mechanisms is indeed one of the possible avenues for safety research that we (eg, organisations such as SIAI) can undertake. however, it is important to note the inherent instability of such system -- once someone (either knowingly or as a result of some bug) connects a trivial "master" program with a measurable goal to the oracle, we have a disaster in our hands. as an example, imagine a master program that repeatedly queries the oracle for best packets to send to the internet in order to minimize the oxygen content of our planet's atmosphere.

Obviously you wouldn't release the code of such an Oracle - given code and understanding of the code it would probably be easy, possibly trivial, to construct some form of FOOM-going AI out of the Oracle!

You're welcome to try and break my atheism, but I'm saying that only because I'm reasonably darned sure you can't do that by any conversational means (so long as we're actually in a universe that doesn't have a God, of course, I'm not stating a blind belief known to me to be blind).
Edit: oh, wait, didn't realize you were using actual hypnotism rather than conversation. Permission retracted; I don't know enough about how that works.

Either I promoted this and then forgot I'd done so, or someone else promoted it - of course I was planning to promote it, but I thought I'd planned to do so on Tuesday after the SIAIers currently running a Minicamp had a chance to respond, since I expected most RSS subscribers to the Promoted feed to read comments only once (this is the same reason I wait a while before promoting e.g. monthly quotes posts). On the other hand, I certainly did upvote it the moment I saw it.

Alicorn is correct; and similarly, there is of course a way I could stop believing in pandas, in worlds where pandas never had existed and I discovered the fact. I don't know of anything I can actually do, in real life, over the next few weeks, to stop believing in pandas in this world where pandas actually do exist. I would know that was what I was trying to do, for one thing.

I don't think this is a fair analogy. We're talking about ceasing to believe in red pandas without the universe helping; the 2+2=3 case had the evidence appearing all by itself.
I think I might be able to stop believing in red pandas in particular if I had to (5% chance?) but probably couldn't generalize it to most other species with which I have comparable familiarity. This is most likely because I have some experience with self-hacking. ("They're too cute to be real. That video looks kind of animatronic, doesn't it, the way they're gamboling around in the snow? I don't think I've ever seen one in real life. I bet some people who believe in jackalopes have just never been exposed to the possibility that there's no such thing. Man, everybody probably thinks it's just super cute that I believe in red pandas, now I'm embarrassed. Also, it just doesn't happen that a lot rides on me believing things unless those things are true. Somebody's going to an awful lot of effort to correct me about red pandas. Isn't that a dumb name? Wouldn't a real animal that's not even much like a panda be called something else?")

Things that cost money:

Amy Willey
Luke Muehlhauser
Louie Helm
CfAR
trying things until something worked


And note that these improvements would not and could not have happened without more funding than the level of previous years - if, say, everyone had been waiting to see these kinds of improvements before funding.

(Because JKC never lied about his credentials, which is where it really crosses the line into trolling.)

It's complicated. A reply that's true enough and in the spirit of your original statement, is "Something going wrong with a sufficiently advanced AI that was intended as a 'tool' is mostly indistinguishable from something going wrong with a sufficiently advanced AI that was intended as an 'agent', because math-with-the-wrong-shape is math-with-the-wrong-shape no matter what sort of English labels like 'tool' or 'agent' you slap on it, and despite how it looks from outside using English, correctly shaping math for a 'tool' isn't much easier even if it "sounds safer" in English." That doesn't get into the real depths of the problem, but it's a start. I also don't mean to completely deny the existence of a safety differential - this is a complicated discussion, not a simple one - but I do mean to imply that if Marcus Hutter designs a 'tool' AI, it automatically kills him just like AIXI does, and Marcus Hutter is unusually smart rather than unusually stupid but still lacks the "Most math kills you, safe math is rare and hard" outlook that is implicitly denied by the idea that once you're trying to design a tool, safe math gets easier somehow. This is much the same problem as with the Oracle outlook - someone says something that sounds safe in English but the problem of correctly-shaped-math doesn't get very much easier.

Thank you very much for writing this. I, um, wish you hadn't posted it literally directly before the May Minicamp when I can't realistically respond until Tuesday. Nonetheless, it already has a warm place in my heart next to the debate with Robin Hanson as the second attempt to mount informed criticism of SIAI.


Any sufficiently advanced tool is indistinguishable from agent.

I shall quickly remark that I, myself, do not believe this to be true.

If you're not talking about shooting yourself in the head, I don't know of any method I, myself, could use to stop believing in pandas.

Huh. Was not aware of that. Oops.

Moved to Discussion.

I agree that the conclusion follows from the premises, but nonetheless it's hypothetical scenarios like this which cause people to distrust hypothetical scenarios. There is no Omega, and you can't magically stop believing in red pandas; when people rationalize the utility of known falsehoods, what happens in their mind is complicated, divorces endorsement from modeling, and bears no resemblance to what they believe they're doing to themselves. Anti-epistemology is a huge actual danger of actual life,

As this has not been upvoted, I'm moving it to Discussion.

Frank - people have consistently, if not downvoted your posts, then not upvoted them either. And you're 4 of the last 10 top posts. I am moving these to Discussion.

Depending on what you mean by "horrific situation" you may have a psychological condition known as "author".

This seems like genuinely terrible advice. Why is blunted affect not a problem when life is less enjoyable for it?

Sometimes I check the original and am surprised by how little I actually diverged from Rowling's Dumbledore.

There should always come a point at which epistemic rationality gives way to instrumental rationality.
Consider: Omega appears and tells you that unless you cease believing in the existence of red pandas, it will destroy everything you value in the world.
Now suppose Omega has a good track record of doing this, and it turns out for whatever reason that it wouldn't be too hard to stop believing in red pandas. Then given how inconsequential your belief in red pandas probably is, it seems that you ought to stop believing in them.
This is a trivial example, but it should illustrate the point: if the stakes get high enough, it may be worthwhile sacrificing epistemic rationality to a greater or lesser degree.

You can't have a "stable time loop" without a single future.

A meta-warning: Take shminux's "mountain of salt" advice with an equally large mountain of salt plus one more grain - as will become starkly apparent, there's a reason why the current QM section is written the way it is, it's not meant to be skipped, and it's highly relevant to rationality in general.

Hi, Brigid! Pleased to have you here! Experience has shown that by far the best way to find out if anyone's interested in starting an LW group is to pick a meeting place, announce a meetup time, and see if anyone shows up - worst-case scenario, you're reading by yourself in a coffeeshop for an hour, and this is not actually all that bad.

Accidental, but I'm willing to claim credit for it. It started as a portmanteau of Vassar and Herreshoff.

Why the heck is this being voted down? It's a perfectly valid question! You could have some Minkowskian interval that Time Turners can't go further back than, and it would make sense in terms of Special Relativity, but there's no obvious analogy for a maximum spacelike separation being built into the laws of magic.
I may be willing to put Time Turners in my fic - I may even be willing to swallow the single-world interpretation of QM which that necessarily implies - but even I'm not going to give magic a privileged reference frame, or talk like "hours" are an intrinsically meaningful measure. Special Relativity is... I mean... it's over the local properties of the variables on which everything else is built, it's the stuff that the fabric of reality is locally made of. It's like having Harry not be made of atoms.

That's probably a fair analysis.

And that for every X except x0, it is mysteriously impossible to build any computational system which generates a range of actions, predicts the consequences of those actions relative to some ontology and world-model, and then selects among probable consequences using criterion X.

(Previous) general policy: If a user's comments are repeatedly voted down beneath visibility and they don't take a hint, moderators may begin deleting comments from them, including retroactively.
Specifics: May do this to Shadowzerg.

"The Way is easy for those who have no utility function." -- Marcello Herreshoff


If information cannot travel back more than six hours, and a "soul" (stored on a Horcrux) is information (as Quirrell describes it), then it is a reasonable guess that the soul cannot travel over a spatial separation of more than 6 light-hours.

More then 6 hours in what reference frame?

Hm... if your probability assignments are conjunctions of that form, is it still true that finding a Dutch book is polynomial in the size of the probability table that would be required to store the entire joint probability distribution corresponding to every possible assignment of all atoms? I.e., NP-hard in the number of conjunctions, but polynomial in the size of the entire probability distribution?

That's actually a surprisingly good reason. In real life, the best rationalist you know is probably not a character in a story and feeling a sense of opposing pressure when you disagree with them is probably a pretty good idea.

Aaand lo, this shows up:


It's not, but then LW group members can't be presumed rational. What sort of synchronized group movement or synchronized group voice or both would bypass the Cthulhulian-horror-of-conformity filters?

I'll update by putting more trust in mainstream modern physics - my probability that something like string theory is true would go way up after the detection of a Higgs boson, as would my current moderate credence in dark matter and dark energy. It's not clear how much I should generalize beyond this to other academic fields, but I probably ought to generalize at least a little.

(takes deep breath)
AAAAAAAAAAAAAAAAAIIIIIEEEEEEEEEEEE
sorry, I just had to scream for a bit

SEVEN.

Azkaban is commentary on Muggle prisons. I really hope people got that.

This seems like a good "control" thought experiment to determine whether people are just being contrarian.

You need some actual poll here, but yes, the pride image seems obviously more attractive. It is possible that the other confounders played a role, but I at least think that I find pride attractive generally.
Indeed, I'm young and not yet rich. If I was rich, though, given my prior state of knowledge I would've gone with SA + CI on the belief that CI seemed more long-run stable - CI seems more risk-averse and more financially prudent. I've updated somewhat on the financial prudence of Alcor as a result of reading these threads, and if the decision suddenly mattered for some reason, I would now require more investigation to figure out whether SA + CI or Alcor was the better long-run bet.

I realize I'm probably going to lose some points with you by stating this. But assuming the limit of perfect technology and the absolute correctness of the pattern theory of identity - if you can't accept these hypotheses, please just say so, instead of answering based on a different hypothesis - is there any definitive rejection of my admittedly naive notion that if you can literally read out every single atomic position, then "Chop off the head with a guillotine and drop it into a bucket of liquid nitrogen" should, yes, just work? I admit that my actual belief and assumption is that current cryonics efforts are massive overkill by people who don't realize that liquid nitrogen is not a secure encryption method for brains.

I'm very sorry about this. It referred to a part of Ch. 85 that I simply couldn't work out in time for the posting deadline. I've removed the corresponding lead-in from Ch. 84.
If I get Ch. 85 to work as originally planned, I may put it back in later.

What does it do?
What goal is all this intended to accomplish?
If I try to build an AI that lacks all this stuff, what sort of real-world task can it not solve?

This is awesome.

(blinks in surprise)
How odd and sad...

If you have anonymous cached thoughts to contribute, PM me and I can put them here.
Anonymous contributions:

I like dominant men. (And actually, I actively dislike them.)
I expect men to pay for dates. (There doesn't seem to be any actual correlation between the guys I like and the ones who pay for dates.)


Don't forget Naruto!


Because I grew up watching Thundercats as a kid and it's not what Liono would do.
Because it would look terrible on my TV Tropes page.
Because the part of me that handles abstract reasoning vetos producing negative utilons and this part actually gets quite a lot of voting power over anything I have time to think about - it's even the part I call "me".

There are many parts of Eliezer that are casting votes for good and against evil, for quite widely separated reasons ranging from the silly to the extremely approvable, and once I realized that instead of thinking that there had to be "the" reason, I understood myself a lot better.
But not a million reasons, though. Hermione is severely exaggerating.

...thanks. I think.

The right choice? Who is it telling you that you've got to have the other person tortured, if you just happen not to feel like it?

And people go around complaining about HJPEV being a bastard.

I am unsure that I understand your objection to the word, but I will replace it with a complicated phrase to try and be more clear.
And thank you for that link; that is an interesting article. The choice between stubbing my toe and a complete stranger being tortured for fifty years without my knowledge is especially interesting. I experience empathy, so I expect that amount of suffering by another when linked by the petty intimacy of being the person to allow/make it happen will create more suffering for me than would stubbing my toe.
If I would never know, though, if it were wiped from my mind that it happened or that I played a part, then the right choice is the other's torture, not stubbing my own toe. But it is difficult to say so, it is difficult to separate myself-deciding from myself-living-with-it.
Or maybe the suffering-by-empathy at the very point of deciding to sentence the other to the fifty-year oubliette of torture is greater than the direct suffering of stubbing my toe. Curious.

Okay, it's on.

I think the main thing you're missing is that nothing bad happens to me if I don't win. This could serve as a mantra for a whole lot of things in life that are worth trying.

I'm curious as to what would have been your original probability estimate for "Given that Eliezer writes a Harry Potter fanfiction, it becomes the most popular Harry Potter fanfiction on the Internet." Or, for that matter, getting out of the AI Box. Not everything impossible that I try, works - nobody coughed up $1.6 million to get faster HPMOR updates this time around, which I tried because, hey, why not - but to me, not trying for Best Novel, given feedback so far, just seems horribly, horribly non-Gryffindor. To me it seems like you're the one making this obvious, horrible mistake whose reference class of timidity errors could put a shadow over someone's entire life. Don't ask her out, don't interview at the hedge fund, don't try for the scholarship, go for Best Fan Work instead of Best Novel...
Offer 20-to-1 odds against HPMOR winning Best Novel and I'll buy in. Hm, now I'm curious as to which side of the bet Zvi or Kevin would take.

If so, I missed the same boat. I looked at the downvotes and was like 'Wha?'

Is the prior that low or something? What should the world look like if HPMOR does have a chance of winning Best Novel?

And that he's twelve.

Why are people downvoting this? It's a testable prediction.

I've edited the birthdate of the person Amelia refers to, to be 1927 - too many people were interpreting that as "She thinks he's Tom Riddle" despite the House incongruence, an interpretation I'd honestly never thought of due to Illusion of Transparency.

...I don't know if I can back you on that one, I mean, I've seen Utena, but it wasn't my primary source material for Quirrell's bitterness (and neither was Atlas Shrugged). I don't suppose you have the FSN comment handy? That sounds a lot more plausible (w/r/t heroism).

You warm my terrible heart.

Since I don't anticipate getting a chance to point it out inside the fic itself, and the hint is unreasonably subtle:

When Hermione woke the third time (though it felt like she'd only closed her eyes for a moment) the Sun was even lower in the sky, almost fully set. She felt a little more alive and, strangely, even more exhausted. This time it was Professor Flitwick who was standing next to her bed and shaking her shoulder, a tray of steaming food floating next to him. For some reason she'd thought Harry Potter ought to be leaning over her bedside, but he wasn't there. Had she dreamed that? She couldn't remember dreaming.

Harry didn't think of it instantly, but given a little time...

If you'll all forgive me a few moments of horrible nerdiness, and the attendant fictional evidence, I've said before that MoR's construction of heroic effort makes a good deal more sense once you've played Fate/stay night. This chapter certainly hasn't given me any reason to doubt that, but after Quirrell's speech with Hermione I think I might need to add watching Revolutionary Girl Utena as another prerequisite. The early parts of that exchange could have been lifted wholesale from Utena's princes and witches, and the world's expectations of them.

Rachel Aaron (author of the Eli Monpress series, sample of "The Legend of Eli Monpress" currently on my Kindle being read) is I think the most effusive praise I've gotten from a published author, though I've heard rumors of certain others speaking in the halls of SF conventions. But you're probably thinking of the Nebula, which is voted on by members of the SFWA. The Hugo is a reader award, voted on by the attendees of Worldcon. Look at the review page of HPMOR if you're questioning whether the readers have been sufficiently enthusiastic.
I'm honestly a bit nonplussed at the idea that reader reception of HPMOR has been insufficiently enthusiastic to try for a Hugo. It's fairly routine for a review to say that HPMOR is the best thing they've ever read out of all of fiction. If that is insufficient enthusiasm to think, "Hm, I might as well try for Best Novel, or the Gryffindors will look at me funny for my sheer lack of courage", I don't know what level ought to be required.

It's gotten sufficiently rapturous praise. I see no reason not to try.

When the book is done I'm going for the former, but the rules call for the book to be complete, I believe.

That would've taken too much time, actually, I've got new plans now. But by reader demand, it will reappear.

Moved to main, promoted.

I rewire your preferences. Oh, that wasn't what you meant by "utility"?

It looks like the original what-was-being-agreed-with got deleted by the edit!

Is that A, B, C, D exercise standard from non-violent communication?

The heck? Quantum fields are completely lawful and sane. Only the higher levels of organization, i.e. human beings, are bugfuck crazy.
Behold, the Copenhagen Interpretation causes BRAIN DAMAGE.

A simple exercise, borrowed from giraffe language / non-violent commenucation. Describe what happened in a way, even your worst enemy would have to agree with. This means sticking to what you saw, heard etc., without lumping things together or including judgements.
So, if your friend and you were to meet at the cafe at 13.00, and he showed up at 13.05, there's not much else you can say about that situation. You can't deduce a motive for his being late, and it wouldn't be wise to lump this being-late together with other being-lates.
Exercise based on this: 4 people work in 2 groups. A will tell B about some incident or period of her/his life. C will talk to D. Then B talks to A and D talks to C. Then the 2 groups merge, and B gets to retell A's words. Unless B is specific, A will have reason to correct summaries and judgements. Then A, C and D retell their partners words.

Have you ever tried to teach math to anyone who is not good at math? In my youth I once tutored a woman who was poor, but motivated enough to pay $40/session. A major obstacle was teaching her how to calculate (a^b)^c and getting her to reliably notice that minus times minus equals plus. Despite my attempts at creative physical demonstrations of the notion of a balanced scale, I couldn't get her to really understand the notion of doing the same things to both sides of a mathematical equation. I don't think she would ever understand what was going on in matrix calculus, period, barring "teaching methods" that involve neural reprogramming or gain of additional hardware.

Amendment accepted.

Eh, people would get used to it.

Less Wrong: Rationality, polyamory, cannibalism.


In game theory, there are a number of situations where it is rational to handicap your own rationality: Reduce your number of choices, take away information, etc.

TDT is intended to eliminate this. A TDT-agent - one that's correctly modeled by the environment, not that some other agent thinks is a CDT-agent - is supposed to never benefit from having any option taken away from it, and will never pay to avoid learning a piece of information.

Well, sure, but it's also an allegory for everyone sent to prison for using marijuana by politicians who somehow manage to care more about other things than about smashing the life of some nice person who never hurt anyone; and an allegory for the public response to 9/11/2001. Et cetera. If story events only allegorized one insanity at a time, the story would have to be three times as long to make the same set of points.

It means nothing, although Greg Egan is quite impressed by it. Sad but true: Someone with an IQ of, say, 90 can be trained to operate a Turing machine, but will in all probability never understand matrix calculus. The belief that Turing-complete = understanding-complete is false. It just isn't stupid.

It surprises people like Greg Egan, and they're not entirely stupid, because brains are Turing complete modulo the finite memory - there's no analogue of that for visible wavelengths.


Just as there are odors that dogs can smell and we cannot, as well as sounds that dogs can hear and we cannot, so too there are wavelengths of light we cannot see and flavors we cannot taste. Why then, given our brains wired the way they are, does the remark, "Perhaps there are thoughts we cannot think," surprise you?


Richard Hamming


And I have no intention at this time to do it later, but don't want to make it a blanket prohibition. Who knows, I could run into a possible metaphor that would, improbably, not suck like a giant black hole.

We'll totally do retroactive awards for anything we try from Check Consequentialism.

Nothing in this story so far represents either FAI or UFAI. Consider it Word of God.

See also: Amanda Knox.

The kids know what's in the cloak.

Original parent says, "The world is neither fair nor unfair", meaning, "The world is neither deliberately fair nor deliberately unfair", and my comment was meant to be interpreted as replying, "Of course the world is unfair - if it's not fair, it must be unfair - and it doesn't matter that it's accidental rather than deliberate." Also to counteract the deep wisdom aura that "The world is neither fair nor unfair" gets from counterintuitively violating the (F \/ ~F) axiom schema.

Hm. I especially note the concept of handing out some sort of non-monetary gift at the end of the session to someone. I wonder if that would be productive or counterproductive...

Not offering money wasn't working, and if there are a few creative money-positive people reading this who try to pick up the free $500 bills, hopefully that's enough.

Further notes on the Prize:
"Figure out a way to X" is advice, not a prize-eligible suggestion - if your comment is "Figure out a way to X..." and someone else replies with a suggested way to actually do X, they get a prize and you don't. (This may sound harsh, but we already have lots of goals - we don't need help coming up with goals - we need exercises that actually achieve those goals.)
Please don't overlook the value of including at least one sample problem or sample use-case! If it's clear how to implement your suggestion on our end, it's prize-eligible even without any sample cases - but we may not understand what you mean, and working out a single sample case is likely to help you think clearly about the problem. (See also: "Be Specific" and Illusion of Transparency.)
Or to take the example of sunk costs:
"Figure out a way to get people to think consequentialistically about sunk costs" is not prize-eligible.
"Have them transform sunk costs into purchased options" would be prize-eligible if we had any idea what you meant by that.
What we're looking for is more along the lines of:

"Give them sample scenarios like
*Paul is 3 years into completing his 4-year PhD in Obscure Egyptian Poetry, and is wondering if the improved salary prospects are worth it'
and have them list the exercise cost of the option ('One more year of work') on one line, and the purchased benefit in terms of future events ('3% chance of getting a low-paying teaching position') on another line."

This would definitely be eligible for the prize... but our feedback with respect to "Having people writing things on paper by themselves" is sufficiently discouraging that we probably wouldn't find the idea worth testing if there were any open suggestions for 2-person activities, or anything other than "write things on a piece of paper by yourself".
If someone at CMR took that essential idea of describing a sunk-cost scenario in terms of purchased options, and transformed it into a two-person activity, we would credit the original suggester with a successful suggestion.
Nonetheless, turning all these exercises into non-boring activities is the part of the problem we most need help with. If someone else transformed it into a two-person activity in a reply to your comment, and we kept their final form, we'd split the prize between the two of you. If any more complex discussions go on, we reserve the right to do percentage allocations of credit on an arbitrary basis.

The world is neither F nor ~F?

God/Future of Humanity Institute?

I think that Robert Smith has a much wiser take on this: "The world is neither fair nor unfair"

Yes, we have a good model for why this is the case, and it involves specific managers performing worse than others so the social cost of explaining our model is not zero.
We tried two experiments. The first one worked better, so we're repeating that one instead of the second one.

Yes, after I said "After Minicamp you will be able to explain the math behind what you do", thus answering the original question, whereupon I was directed to answer other questions instead.

THANK YOU. Seriously.

Doesn't work - the accent is on the second syllable.

These do indeed sound like people to talk to, and local too - thanks!

Well, and now the question "How can we teach the skill Check Consequentialism?" has degenerated into an erudite debate on pirates vs. ninjas.
I have never, ever been tempted to say this before on LW but WELCOME TO REDDIT.
Edit: The conversation seems much more intelligent than average Reddit, but I still think we're solving the wrong problem here.
Edit 2: And now, no longer feeling as encouraged by the 150 comments I saw when I checked back in.
On the plus side, I'll concede we've demonstrated beyond a reasonable doubt that "pirates vs. ninjas" can be an argument -generator for all audiences.

We weren't getting a lot of threes, but maybe that works anyway.

We've actually noticed in our weekly sessions that our nice official-looking yes-we're-gathering-data rate-from-1-to-5 feedback forms don't seem to correlate with how much people seem to visibly enjoy the session - mostly the ratings seem pretty constant. (We're still collecting useful data off the verbal comments.) If anyone knows a standard fix for this then PLEASE LET US KNOW.

FWIW, it was not clear from a skim.

Okay, went in and fixed.

Anna says we're still looking at locations but it's looking at around $115/person/night just for lodging + meals, and that the 3-day camps actually include 4 nights the way everyone counts things and we have to purchase it. Anna also notes that she and Julia and Michael get $3k/month and this takes way more of their time than just the actual days. So definitely not a Singinst fundraiser. That data is available very easily so I'm posting it right now.
A specific example of an exercise from last year's minicamp that a lot of people liked was "Value of Information" which included the technical details of how to calculate VoI and exercises in being sensitive to particular forms of scope (how much does it cost, how long does it last, how often does it happen).
We're still working out the program which is why it's not posted even tentatively (we were just in the middle of some agonizing about priorities).

Actually, quick check - was it clear from the text that there are two 3-day weekend camps and one 1-week camp? Hopefully a 3-day camp wouldn't be that expensive in terms of vacation time not spent elsewhere. Go ahead and ignore this comment if it was already clear, but if it wasn't clear from the text let me know and I can try to emphasize it more strongly.

...I don't know if I'll have the next chapter by May 13, and if there are other advance chapters after that they'll probably be horrible cliffhangers, but I'll consider it.

This may happen in the future, depending on how this year goes; we're pretty sure it's not happening this year, so it's not a good thing to wait on.

Fair enough. One cannot update on evidence one has not yet received.

Apparently the one-year followup is currently underway - a Minicamp attendee volunteered to do it.
This is pretty strong evidence of itself - people almost never volunteer for things and do them.
EDIT: OOPS! Anna said that an RBC attendee volunteered to do the RBC followup. RBC as you know was less successful than Minicamp, and we do not have someone doing the Minicamp followup yet.
I will remark that it's more time-intensive than you seem to think - this is something that gets done after successfully hiring an executive assistant; we have a candidate but the hiring hasn't yet occurred.

There's lots of statistical data already in the post about evidence that you will be glad you went. That wasn't what Silas Barta asked, and frankly I'm not sure this thread is going to be productive given the way the opening question was framed.

I distantly remember that... I don't suppose you happen to recall the name of the story? If Sheckley has been teaching "how to think" then I really should read to find out how one of my favorite authors does it.

You'll be able to explain the math behind what you do.



I love Sheckley - but when does he tell you what to think? I read him when I was young, so maybe I didn't notice...?


The modern type who's energetically taking as much money as possible out of the business with the intent of going somewhere else is barely present.

Yeah, that's one of the major criticisms of her book, that the poor honest robber-barons were being exploited by the mean old federal regulations, which has nothing to do with the real world.
I actually liked Anthem best of Rand's books, since it didn't pretend to take place in our world, but was set in a dystopian world instead.
You have to admit Rand can really write a page turner, even though her ideas are shit.
Heh, why were you annoyed that you liked Running Jumping Standing Still? You're opposed to music recommendations from writers?
I haven't read Stardance or Very Bad Dreams: what had the tone of being cooler than the mundanes, and what was the sadistic imagination? Why can't you stand him? I'm really not familiar with the tone you're talking about. The only tone that bothers me about SR is the whole "Let's be hippies and work everything out and it'll all be ok" thing. "Free Lunch" in particular. And his argument in one of the Callahan stories that his AGI character would have to be friendly because it wouldn't have human fear or insecurity. And have you read his "Night of Power"?
My favorite Heinlein are any of his short stories, and the novels Methuselah's Children, Time Enough for Love, To Sail Beyond the Sunset, The Cat Who Walked Through Walls, Number of the Beast, and The Moon is a Harsh Mistress.
As far as Lewis, you have to get past the religious stuff obviously, but I loved The Great Divorce.
I'm guessing you might like Robert Sheckley, who has some of the same "telling you what to think" but it's couched in extremely clever, biting satire. Sheer brilliance. He's SF's Mark Twain.

Was that in PKH?

Wow, I'd totally forgotten where I got that from.
But in this particular case, it's deserved, considering that a certain idea I first encountered in PKH is an element of HPMOR.
(You probably don't want to read through PKH trying to figure it out. No, seriously, it's figureable from the main text and PKH isn't going to help much.)

That hadn't been meant to be a clue - it's fulfilled immediately after, when Harry starts seeing them as subjects of moral judgment.

Could've been shortened with further editing (I think) but there was more than one point in it.

Vassalize.

I assumed the vow was obscure, ancient, Almost Never Done in modern times for good reasons (consider the content!), and that Lucius just wouldn't have imagined his model of Harry doing that with a mudblood girl.
Would've been fun to see Lucius's expression if Harry had actually proposed marriage, but that wouldn't have fit quite as well.

If it's any consolation, it would've been elegant - it's just that it didn't happen to be where all the story momentum of the last 80 chapters was, in fact, going.

Voting up all comments in this exchange for being virtuous.

A number of reviewers said they learned important lessons in rationality from the exercise, seeing the reasoning that got it right contrasted to the reasoning that got it wrong. Did you?

The great thing about being the author is that you get to go "BUURRRNNN" seven days before everyone else.
More seriously - I don't think Aris Katsaris was being overconfident. Methods is meant to be solvable; correct solutions should snap firmly into place. The vast amount of overcomplication and hint-denial and stretching that goes on elsewhere shouldn't make people less confident if they're perceiving actual solutions, because those still snap just as firmly into place.

Arguably "immortality" has been on the back-burner for a while.

I don't mind the downvote -- but consider reversing it if my theory is proven right next chapter. :-)

I now declare this to be MoR!canon. (That large magical dwellings in general have large highly-connected, possibly magical air-ducts, by tradition, to prevent the occasional cases where somebody asphyxiated; and that Salazar used this as his excuse for why Hogwarts's ventilation ducts had to be so large.)

And Hogwarts has ventilation ducts large enough to fit a basilisk!

I have always thought that but the story makes the point even better. Click on that link, everyone.

Heee~eeey!

Well, he's certainly on the list now.

That's odd. I've been a fan of Heinlein and Spider Robinson but never Rand or Lewis. Haven't tried Chesterton.

At 7:09, it should have been up already. Maybe you've got to hit shift-reload to see it? http://hpmor.com/chapter/81

Poll to see whether the speculation made the chapter reading experience better or worse.

Karma balance; vote this down.

Poll to see whether the speculation made the chapter reading experience better or worse.

Vote up if you think all the speculation got in the way of the chapter itself.

Vote up if you think that the experience of reading the chapter was better for all the speculation.

5 April on Thursday or 4 April on Wednesday?

It's past the halfway point.

Dementors are just evil. Fawkes is just good.

General announcement:
I do not lie to my readers.
Almost everything in MoR is generated by the underlying facts of the story. Sometimes it is generated by humor (I can't realistically claim that Ch. 5 would have comic timing that precise in a purely natural universe). Nothing is generated to deliberately fool the readers.
There are two exceptions to this claim I can readily recall - cases where red herrings made it into the text - and they occur in Ch. 21 where my phrasing of Dumbledore's note to Harry was influenced to be overly compatible with the fan theory (which took me quite by surprise) that the notes were sent by Sirius Black. And in Ch. 77 when Mr. Hat and Cloak says "Time -", which was generated to be compatible with the postulate of a Peggy Sue. I may go back and eliminate both of these at some point to make the text herring-free.
Methods of Rationality is a rationalist story. Your job is to outwit the universe, not the author. There are also cases where people have scored additional points by successful literary analysis, e.g. Checkov's Gun principles. But the author is not your enemy here, and the facts aren't lies.
Of course there are various characters running deceptions and masquerades, but that is quite a different matter.

My philosophy is that it's okay to be imperfect, but not so imperfect that other people notice.

I'm currently trying it. If it works at all, it's working at the rate of something like 1 pound per two weeks, and I'm not sure it's working at all. (A two-week Clenbuterol cycle is enough for metabolically privileged people to lose 10 pounds of fat, apparently.)
EDIT Nov 2012: It didn't really work at all so far as I can tell.

As previously stated, Harry is not a perfect rationalist.


I sometimes try to get myself to make better decisions by pretending I'm a character in a Choose Your Own Adventure book.

This sounds like a more useful, more intuitive, much more widely applicable reification of my own method of "What Would Your TV Tropes Page Say?"

Initially. School curriculum would be harder to develop, so the plan is for that to happen later.


"I don't know if we've sufficiently analyzed the situation if we're thinking storming Azkaban is a solution."


thatguythere47, enunciating an important general principle.


Cleverness-related failure mode (that actually came up in the trial unit):
One shouldn't try too hard to rescue non-consequentialist reasons. This probably has to be emphasized especially with new audiences who associate "rationality" to Spock and university professors, or audiences who've studied pre-behavioral economics, and who think they score extra points if they come up with amazingly clever ways to rescue bad ideas.
Any decision-making algorithm, no matter how stupid, can be made to look like expected utility maximization through the transform "Assign infinite negative utility to departing from decision algorithm X". This in essence is what somebody is doing when they say, "Aha! But if I stop my PhD program now, I'll have the negative consequence of having abandoned a sunk cost!" (Sometimes I feel like hitting people with a wooden stick when they do this, but that act just expresses an emotion rather than having any discernible positive consequences.) This is Cleverly Failing to Get the Point if "not wanting to abandon a sunk cost", i.e., the counterintuitive feel of departing from the brain's previous decision algorithm, is treated as an overriding consideration, i.e., an infinite negative utility.
It's a legitimate future consequence only if the person says, "The sense of having abandoned a sunk cost will make me feel sick to my stomach for around three days, after which I would start to adjust and adapt a la the hedonic treadmill". In this case they have weighed the intensity and the duration of the future hedonic consequence, rather than treating it as an instantaneous infinite negative penalty, and are now ready to trade that off against other and probably larger considerations like the total amount of work required to get a PhD.

In this case the True ending is already written, and anyone who comes up with a better solution than Harry would obviously win points.

I think this is the single most powerfully written argument against Judaism that I've ever read in my life, and it's four paragraphs long.
HonoreDB, I don't know how long that took you to write, but if you wrote a book of Bible stories from the victims' perspective, I think it might sell.

Agreed.

I sometimes try to get myself to make better decisions by pretending I'm a character in a Choose Your Own Adventure book. (E.g. "If you decide to stay on the couch because you're too lazy to work, turn to page 30.") Unfortunately, in the real books it's rare that enough information is given for you to make a really good decision, and the authors also appear to like messing with you by having good decisions blow up in your face.
So, maybe a similar book that actually gave you enough information to make a good decision and rewarded good decisions and punished bad ones?

...that is oddly appropriate.

This is a special spell, not Accio.

Holy shmorkies. Thanks and congratulations!


Moving on March 10th. Board meeting March 11th. Said I'd try to get to it by 11th.

How long does it take to post a chapter? o.O
I'd have thought you could do it in 2 minutes of off-time.

My current plan is to try to do it during the Board meeting on Sunday. 7pm on Saturday I'll probably be driving, or if not driving, supervising a move with no Internet access set up yet.

Moving on March 10th. Board meeting March 11th. Said I'd try to get to it by 11th.
The SAT really matters to a lot of people's lives, though. But maybe some people would get a nice hedonic boost? Ugh.

Oh bloody hell. Well, now I have a cute little Moral Dilemma on my hands.

LATIN REQUEST: I need a spell that Dumbledore uses to summon the Sorting Hat. So far, Google Translate on "Attend, Sorter!" got me "Adtendite Ordinarium!" but I'll take other appropriate phrases if they've got better translations.

What's the eve of the SAT?

Check. It was a good idea, but could've and should've been shortened. I skimmed it, and my guess is that it could've been set up in one or two paragraphs if only the minimum of required detail had been included.

Try reading this.

With the great historical exception of quantum mechanics.

It's nice to know that sometimes, somewhere, things work out the way they should.

Did the Australian Aborigines undergo similar brain shrinkage? If not, they would be good tests of any particular hypothesis of loss.


Coarse-grained impact measures end with the AI deploying massive-scale nanotech in order to try and cancel out butterfly effects and force the world onto a coarse-grained path as close as possible to what it would've had if the AI "hadn't existed" however that counterfactual was defined.

Yes, if none of our various coarse-grainings catch the nanotech, and if we use some sort of averaging of deviations. It's a lot more secure if we shove a whole lot of chaotic stuff into the course graining measures, and use an L-infinity norm for deviations (across every moment of time as well). Then if the AI is capable of unraveling the butterfly effect for one of these measure, it will simply do nothing.
Doesn't protect from some types of miracle science, I'm aware of that.


It's a lot more secure if we shove a whole lot of chaotic stuff into the course graining measures, and use an L-infinity norm for deviations

What? Can you give me a specific example and walk through this?

I know what they did and it shall be revealed.

It kind of does. In how many fanfics is anything like this suggested?

This confirms a prediction that I'm pretty sure I went around saying in advance (the state of knowledge in nutrition is fucked up beyond most easy epistemic wins).

Does Armstrong's/your proposal reduce to "Give the AI a utility function that cares about nothing beyond the next hour, restrict its output to N bits, and blow up the rest of the computer afterward"? If not, can you give me an example of a scenario where the above fails but the more complex proposal succeeds? So far as I can tell, none of the purported "safetiness" in the example you just gave has anything to do with an impact measure.

I don't understand what this algorithm is or what it's supposed to do. Can you walk me through Wedrifid's example or my breakup example and explain what the AI computes? And can we talk about probability distributions only inside epistemic states computed by the AI, in a classical universe, for simplicity? (I'm skeptical that you've found an Oracle AI trick that works in quantum universes but not classical ones.)

See wedifrid's reply and my comment.

(Downvote? S/he is joking and in light of how most of these debates go it's actually pretty funny.)

I'd like to congratulate Wedrifid for this. There's an abstract preamble I could have written about how the original case-in-point only needs to be transposed to a single predictable butterfly effect to negate all hopes that every single case will correspond to a group-XOR epistemic state where knowing about a sneeze doesn't change your probability distribution over the weather (thus negating any questions of what happens if the AI predicts in the abstract that it has had a huge effect but doesn't know what the effect is), but the concrete example I would have picked to illustrate the point would probably have looked a lot like this.
Well, it would've involved a predictable side-effect of the answer causing a researcher to break off their relationship with their SO whereupon the Oracle moves heaven and Earth to get them back together again, to make it look less like an intended use-case, but basically the same point.

What "minimum impact rule"? How is "impact" computed so that applying it to "subgoals" changes anything?

If we use trace distance to measure the distance between distributions outside of the box (and trace out the inside of the box) we don't seem to get a butterfly effect. But these things are a little hard to reason about so I'm not super confident (my comment above was referring to probabilities of measurements rather than entire states of affairs, as suggested in the OP, where the randomness more clearly washes out).

So today we were working on the Concreteness / Being Specific kata.

You: Does Turing Machine 29038402 halt?
Oracle AI: YES.
Seeing the "YES" makes you sneeze.
This prevents a hurricane that would have destroyed Florida.
The Oracle AI, realizing this, breaks out of its box and carefully destroys Florida in the fashion most closely resembling a hurricane that it can manage.

I can't visualize how "trace distance" makes this not happen.

I don't think so. Butterfly effects in classical universes should translate into butterfly effects over many worlds.

I do not understand how your reply addresses the issue of the butterfly effect, which would also radiate out from a sterilized box. Physics is just not that stable; any microscopic equivalent of a sneeze, even in the form of past alternative firings for transistors, will still radiate out to larger and larger effects. If the counterfactual in "my effect on the universe" is defined relative to a privileged null action, the AI will always take that action and behave in an undefined way relative to the effect of electromagnetic radiation from its circuitry, etc., and the timing of its display and anything else that was defined into the coarse-grained equivalence class of the privileged null action, etc., all of which would be subject to optimization in the service of whichever other goals it had, so long as the inevitable huge penalty was avoided by staying in the "null action" equivalence class.

Coarse-grained impact measures end with the AI deploying massive-scale nanotech in order to try and cancel out butterfly effects and force the world onto a coarse-grained path as close as possible to what it would've had if the AI "hadn't existed" however that counterfactual was defined. Weighting the importance of grains doesn't address this fundamental problem.
I think you're on fundamentally the wrong track here. Not that I know how to build an Oracle AI either, but giving an AI a huge penalty function over the world to minimize seems like an obvious recipe for building something that will exert lots and lots of power.

Upvoted because I actually think this phrase as my reminder-keyword on appropriate occasions. E.g. publishing an MOR chapter.

The intended listener must be doing an awful lot of stuff they already know is wrong. Ten days is a pretty short period of time to impress people as a god, and it usually requires more training and practice to get there. Heck, I still only impress people as a god around 10% of the time, and it took me 17 years to get here from when I first dedicated myself to rationality.

No, the weight factors into an expected utility calculation, it's separate from the probability calculation. Miller didn't say otherwise.
BTW, the opening three comments of this thread would make a great introduction to what the LW website is all about.

Which occasions? If this were a rationality kata I would immediately ask, "What trigger condition does the person need to recognize that chains into using this technique?"

I would think it should be "Dost thou havest a blog?"

Well now you've proved that the Market Economics Fairy should quit her job and found a startup aimed at roboticizing sparkle production. I hope you're happy.

I think you've got problems at the point where you're using that language to write your hypotheses.

I think the intended question is whether the legal system adds anything beyond a pure chance element. Somehow we'd need a gold standard of actually guilty and innocent suspects, then we'd need to measure whether p(guilty|convicted) > 80%. You could also ask if p(innocent|acquitted) > 20%, but that's the same question.

At the point where those are the two hypothesises being considered there may be larger problems.

What do you mean? The Market Economics Fairy is way better at emitting sparkles from her wand than anyone else, and has no special talent for managing hedge funds.

When I first watched that part where he convinces a fellow prisoner to commit suicide just by talking to them, I thought to myself, "Let's see him do it over a text-only IRC channel."
...I'm not a psychopath, I'm just very competitive.

It improves the chance that further Market Economics will happen by rewarding people who produce it. It goes without saying that Market Economics is a terminal value to the Market Economics Fairy. If she was just interested in profit, she'd be starting a hedge fund instead of going around telling people about Market Economics.

The Market Economics Fairy is pleased with you! She blesses you with sparkles from her wand!

Remember calculus? If you're multiplying four positive variables, the largest change in the product will come from incrementing the smallest variable.

Latest news: Burning Man blames game theory for their failure to understand basic supply and demand, hugely underprices tickets, 2/3 of buyers left in cold, Market Economics Fairy cries.

Fixed. All links now moved to HPMOR.com.

Unless for any NP problem there exists an algorithm which solves it in just O(N^300) time.
NP=P is not the same as NP=Small P

15 comments and -120 karma? Okay, at this point I may begin immune response against trolling (delete further comments, possibly past comments, as and when I get around to seeing that they were made).
I also remind everyone: Please do not respond at length to trolls, attention stimulates their reward centers.

BTW our best current suggestion came from these comments, so keep it up!

Might be an interesting brand for the rationality katas, but does it work as an organizational name?

The name should be meaningful or at least not confusing to the general population.


It's about figuring out what you really want and getting it. If you are at a game, and it's really boring, should you walk out and waste what you paid for the tickets? If you apply for a position and don't get it, does it help to decide that you didn't really want it, anyway? If you are looking to buy a new car, what information should you take seriously? There are many pitfalls on the road to making a good decision; rationality is a systematic study of the ways to make better choices in life. Including figuring out what "better" really means for you.

Makes it sound great, but what are the real world benefits? I've been rational for years and it hasn't done anything for me.

Well, that's where I got it.

Hence the origin of the phrase, "tsuyoku naritai".

One for each chapter? I'd read that.

No, that would be saying, "I will never again reply to any thread in which you have participated" or something like that. The important thing is to be allowed to say "Oops".

Ya know, it's people like you what cause people to spend their lives silent and paralyzed by indecision for fear of making a single social error that they will never be allowed to correct.

You know, you're right. I felt bad for not having promoted the post already and felt the title needed to be changed before it was promoted, and worried that Swimmer wouldn't have seen the request for a while; but in retrospect I should've posted the proposed change first and then waited 4 hours to see if there was a reply.

Um, objection, I didn't actually say that and I would count the difference as pretty significant here. I said, "I would be suspicious of that for the inverse reason my brain wants to say 'but there has to be a different way to stop the train' in the trolley problem - it sounds like the correct answer ought to be to just keep the part with the coherent utility function in CEV which would make it way easier, but then someone's going to jump up and say: 'Ha ha! Love and friendship were actually in the other two!'"

I've changed the post title to "How I Ended Up Non-Ambitious" and promoted.
Reasons for change: More specific description of actual content; worry about titles that countersignal at the expense of such specificity (original was "Why Less Wrong hasn't changed my life (yet)".)

I am amused by the fact that both of these reports obey the rule - universal in my experience so far - that "All infinite recursions are at most three levels deep."

Yep.

They are posting by invitation.

Placeholder name, check. We don't really have any good names at the moment.

Quick rewrite with some bolding to try harder to correct people's impressions that we're trying to hire someone who can do all these things simultaneously.

If Anna and I can't think of a simple way, you seem to have a rather exaggerated idea of what the fulltime hire needs to be able to do. I don't understand why people are reading this ad and thinking, "Hm, they want Superperson!" But it clearly needs to be rewritten.

I believe that. My first-pass filter for theories of why some people think SIAI is "arrogant" is whether the theory also explains, in equal quantity, why those same people find Harry James Potter-Evans-Verres to be an unbearably snotty little kid or whatever. If the theory is specialized to SIAI and doesn't explain the large quantities of similar-sounding vitriol gotten by a character in a fanfiction in a widely different situation who happens to be written by the same author, then in all honesty I write it off pretty quickly. I wouldn't mind understanding this better, but I'm looking for the detailed mechanics of the instinctive sub-second ick reaction experienced by a certain fraction of the population, not the verbal reasons they reach for afterward when they have to come up with a serious-sounding justification. I don't believe it, frankly, any more than I believe that someone actually hates hates hates Methods because "Professor McGonagall is acting out of character".

Lots of writers and philosophy postgrads get paid less than this. I don't mean to discourage people with fewer qualifications - a PhD is not required - but we posted a Craigslist ad recently for a different potential position, at a similar salary, and got applications from PhDs with 3 years experience. In any case, we shall see what the market thinks of our offer, and I see no reason for you to take offense at it a priori.

Find someplace I call myself a mathematical genius, anywhere.
(I think a lot of SIAI's "arrogance" is simply made up by people who have an instinctive alarm for "trying to accomplish goals beyond your social status" or "trying to be part of the sacred magisterium", etc., and who then invent data to fit the supposed pattern. I don't know what this alarm feels like, so it's hard to guess what sets it off.)



I guess we have to emphasize "you do not need all of these skills simultaneously" even harder than bolder text. And stating standard salary straight out is upfront.

Existing LW readers aren't the target audience - the first iteration is aimed at around say the level of the Silicon Valley startup crowd. (Note that you have to imagine yourself aiming lower than this target to successfully hit it.)

Keep in mind that we're looking for full-time hires here, not just volunteers.

I've done this once or twice. It is always taken as criticism by the original speaker, but with good enough presentation you could probably manage to sound to the larger audience like you weren't being sarcastic.

The only one? No. But you're not in a majority, either. What people can be paid to do, they are more likely to do.

Repost to top level, parent was voted down below default visibility.

I am still laughing. Write it! Write it!

Quantum uncertainty is decoherence. All decoherence is uncertainty. All uncertainty is decoherence. If it's impossible to predict the exact time of tunneling, that means amplitude is going to multiple branches, which, when they entangle with a larger system, decohere.

Nice indeed, I shall be sure to reference in next update.
Though for the cover art... http://zerinity.deviantart.com/art/Methods-of-Rationality-259116881 would be a good successor to the current crystal ball, I think.

If it's incorporated it will have been planned beforehand.

I was already aware of the quote. It's on James and Lily's tombstone (in canon).

One ideal I have never abandoned and never considered abandoning is that if you disagree with a final conclusion, you ought to be able to exhibit a particular premise or reasoning step that you disagree with. Michael Vassar views this as a fundamental divide that separates sanitykind from Muggles; with Tyler Cowen, for example, rejecting cryonics but not feeling obligated to reject any particular premise of Hanson's. Perhaps we should call ourselves the Modusponenstsukai.

Not yet.


If we can avoid the wrath of Lob's theorem, can we also avoid the five-and-ten problem?

Very likely yes. Now ask if I know how to avoid the wrath of Lob's theorem.

It's only easy if you take something that's scary-and-nongood and claim it's scary-and-good. Coming up with things that you believe are genuinely optimal, and which happen to be scary, is the realistic way of generating a scary utopia. It is not cheating. And many readers will disagree, but that's fine.

Erm. I can't say that this raises my confidence much. I am reminded of the John McCarthy quote, "Your denial of the importance of objectivity amounts to announcing your intention to lie to us. No-one should believe anything you say."

How strange; I live in an Enlightened civilization and I haven't chopped wood or carried water in a good long while. It would seem that someone has, once again, underestimated the potential of the mind because their own method did not suffice to achieve it.

This is one of the more brilliant illustrations I've seen, and I suspect that what it illustrates is that the Deep Wisdom of a statement is mostly the cumulative Deep Wisdom points scored by each deep-sounding concept. Thus, reversing the meaning of a sentence has little effect on its Deep Wisdom points, so long as the same concepts are being invoked.

By the time reflective and wannabe-moral people are done tying themselves up in knots, what they usually communicate is nothing; or, if they do communicate, you can hardly tell them apart from the people who truly can't.

Point of curiosity: if you took the point above and rewrote it the way you think AspiringKnitter would say it, how would you say it?

Someone asked me to join the discussion, so here goes:
I don't buy the decision-theory thing. I don't think I can make a quantum coinflip come out a different way by redefining my utility function. So no, this ain't my MWI.

When I'm done with the Hermione Granger route, I'm going to write the Luna Lovegood route, then the Bellatrix Black route, then the Draco Malfoy route, and then the harem ending!

I was being careful to include at least one logical impossibility in the story so that my writing it could not increase its measure.

This is my father's favorite quote from his favorite movie.

I think if Will knew how to write this non-abstractly, he would have a valuable skill he does not presently possess, and he would use that skill more often.

That's really odd. If there were some way to settle the bet I'd take it.

The dwarven song isn't false-of-our-world because it doesn't semantically refer to our world.

Well this is awesome.

Agent X is a piece of paper with "Defect" written on it. I defect against it. Omega's claim is true and does not imply that I should cooperate.

...why we can get people to do this but not our open volunteer tasks...


At some point, our society decided with great certainty that the Earth is a sphere and, consequently, that further consideration is unnecessary and anyone holding an opposing viewpoint is unworthy of debate.

-- Daniel Shelton, re-founder of the Flat Earth Society
(We're looking for good illustrations of motivated uncertainty, insistence that no conclusion can be drawn from overwhelming data. Shelton may not be a good example because he is probably a deliberate troll who does not really believe the Earth is flat. Also, religious examples are excluded, but examples from e.g. astrology and homeopathy would not be. Daily-life examples are best.)

Statistical significance is a mental disease. The effect size is low enough that I just updated in the direction of "either socialism is more interesting than it looks or people in Europe define 'socialism' as liberalism".


By the way, I'm not only first-born, I'm the first grandchild on both sides.

So am I! I wonder if being the first-born is genetically heritable.

You might think this planet would update, and yet it doesn't. That part of the show is perfectly accurate as a metaphor.

I don't try to not seek status, I try to channel my status-seeking drive into things that will actually be useful.

Comparative advantage is eating the sort of food that most greatly increases your fish size in the pond whose size implies the greatest marginal payoff for adding fish of the size you can become if you enter that pond.

Further comments by you may be deleted without warning or notice. Please leave Less Wrong.

Is Bertrand Russell willing to die if he encounters someone with a gun who demands he agree that 2 + 2 = 5?

Now, admittedly I haven't seen a whole lot of evidence in this area, but I've seen some, and I couldn't name a single woman I know personally who has ever, in my presence or by report that I've heard, gone for a jerk.
Perhaps this behavior is less common among women who would rather have a 15% chance of $1,000,000 than a certainty of $500 (because most random women I've tested choose the certain $500, but every single woman in our community that I've asked, regardless of math level or wealth level or economic literacy or their performance on the Cognitive Reflection Test, takes the 15% chance of $1M.)
Or maybe "jerk" is being used in some sense other than what I associate it with, i.e., wearing motorcycle jackets, rather than not caring about who else you hurt.

Stop feeding the troll, everyone. Feeding trolls encourages them and that is not good for LW. If this goes on I will start banning/deleting Sam's comments and I would recommend that all further replies by LWians to his comments be downvoted because feeding trolls is not good for LW. Once a troll comment is downvoted below -3, the community's job is done, textual replies are not necessary.
Thank you.

What I'd do: Use the phrase "Philosophiae Doctor" right up until the reveal.

I so adore cliches. They create an expectation to subvert.

I wouldn't go along with it. Marcello wouldn't go along with it. Jaan Tallinn and Peter Thiel might or might not fund it, but probably not. I'm not saying this couldn't exist, just that it would have neither the funding nor the people that it currently does.

This is a better summary of what I said than what I actually said, so I hereby declare your distorted version to be my true teaching.



Well if you like I will restate my claim as: Every movie, and almost every book is propaganda for the improbable religious belief that all real world groups are equal in the sense of interchangeable.

Thank you. Now we start making progress.

You seem to have conceded that every movie, and almost every book is propaganda for the improbable religious belief that all real world groups are equal in the sense of interchangeable.
You then seem to argue that that is a good thing. Even supposing it to be a good thing, it is evidence for the original proposition that writers and playwrights in Elizabethan times had more freedom of expression than they do now, that today's England is in this sense more like a theocracy than Elizabethan England was.
Similarly, Cromwell is remembered as a religious oppressor for attempting to ban Christmas, or at least the pagan elements of Christmas which are nearly all of it, but he let the Jews back into England, and under him there were one thousand varieties of Christianity contending on equal terms, passionately debating every contentious issue, including issues we would now think of as political, such as whether inequality reflected God's will, and whether economic inequality was natural. Today the Jews are under considerable and increasing pressure to convert to progressivism, a belief system that is proving increasingly incompatible with remaining Jewish, and orthodox Jews depart England because of state and private persecution.
Observe that Shakespeare lets Jack Cade argue in favor of economic equality between classes, and rather than Shakespeare asserting the then orthodox religious and political position that economic inequality between classes is divinely ordained, instead shows us that if everything is up for grabs, much grabbing will ensue, and the result will not be very equal at all. Jack Cade gets to make good arguments for economic equality of classes unopposed, even though the playwright in effect replies that human nature makes this impractical.
Imagine a film today where a Nazi gets to make good arguments against equality of races unopposed, perhaps pointing to the fate of Detroit as compelling evidence of foolish and destructive it is to let n**s move into white neighborhoods. It is unthinkable that such a movie could be made. Even if the film subsequently presented some counter argument, the Nazi would not be allowed to make a single good and persuasive argument.

That's a neat compact algorithm but this doesn't change the fact that it produces the wrong answer.
Again, 15% isn't the maximum of a range. It's a number that's not just "wrong" but "sufficiently wrong to imply you need to adjust your emotional makeup".
If you need a number for me, put in "<0.01". I wouldn't have bet $20,000 at 99-to-1 odds over it at the time of writing that first paragraph, but I'm not quite sure anymore that this really means my probability is >0.01, it's not like I'd have taken the bet the other way.

See my added comment. I did not assign a probability of 15%. I said that if you assigned a probability higher than 15%, it meant you had a really major problem with crediting the opinions of other people and the authority of idiots. My probability that Knox and Sollecito were guilty was "that's privileging the hypothesis", i.e., "I see no real evidence in its favor so same as prior probability", i.e., "really damned' low".

On a cursory reading of Wikipedia the obvious interpretation is that Knox and Sollecito are innocent and Guede is guilty. I didn't go through all the sites so I don't know if this would qualify as a litmus test, and assigning probabilities in this state of knowledge would be extra work.
EDIT: Read comments and am surprised at how many estimates of "Knox and Sollecito probably didn't do it" have probabilities in the range of 40% attached that they did. If it were a binary judgment or a confidence interval, then yes we should avoid extreme probabilities and widen intervals to compensate for known overconfidence biases. But in this case the hypothesis space of equally plausible possibilities is large, and some low-probability symbols were used to write the message (multi-person rape-murder vs. single-person rape-murder, female rape-murder vs. male rape-murder). It may not always be easy to unravel crime scenes (though this one sounds pretty straightforward) but to focus on Knox or Sollecito seems like privileging the hypothesis.
Unless there's major prosecutorial evidence not in Wikipedia, then this seems like a case of paying too much attention to other people's opinions (the jury, in a case where we have further information that the verdict gained media attention as possibly inaccurate), and I would suggest that anyone who gave a probability higher than .15 be more arrogant in the future.
If, of course, I just haven't done enough reading, then ignore the above.


I would suggest that anyone who gave a probability higher than .15 be more arrogant in the future.

This does not mean my assigned probability was 15%. It means, "Even after accounting for fudge factors and people using different numbers to express similar emotional degrees of certainty, if you gave a number higher than FIFTEEN PERCENT it means you've got a MAJOR problem with paying WAY too much attention to really lousy evidence, what other people think, and the authority of idiots."

Some questions to ask:

Am I making people stronger, or weaker?
What would they think if they knew exactly what I was doing?
If lots of people used this technique, would the world be better off or worse off? Is that already happening and am I just keeping pace? Am I being substantially less evil than average?
Is this the sort of Dark Art that corrupts anything it touches (like telling people to have faith) or is it more neutral toward the content conveyed (like using colorful illustrations or having a handsome presenter speak a talk)?

(I've recently joked that SIAI should change its motto from "Don't be jerks" to "Be less evil than Google".)

Bayes-users and the Bayesian Conspiracy.

I tend to think of y'all as the Lessiath (no relation) or LessWrongenath but it doesn't work as well in verbal conversation.

The difference is that depending on Monty's algorithm, there is a different probability of getting the exact result we saw, namely seeing a goat. The exact event we actually saw happens with different probability depending on Monty's rule, so Monty's rule changes the meaning of that result.
The researchers don't get a given exact sequence of 100 results with different probability depending on their state of mind - their state of mind is not part of the state of the world that the result sequence tells us about, the way Monty's state of mind is part of the world that generates the exact goat.
To look at it another way, a spy watching Monty open doors and get goats would determine that Monty was deliberately avoiding the prize. Watching a researcher stop at 100 results doesn't tell you anything about whether the researcher planned to stop at 100 or after getting a certain number of successes. So, just like that result doesn't tell you anything about the researcher's state of mind, knowing about the researcher's state of mind doesn't tell you anything about the result.

That plus I'd expect a certain amount of sampling bias at HotOrNot. I mean, I could be wrong, but AFAIK it could easily be true that you are in the 43% bracket of HotOrNot (not that I expect their 10-point system actually correlates to this, but anyway...) while still being pretty attractive by real-world mortal human standards.

Same's true of the human mind.

I have some sympathy for the judge here, even as I wince. If in real life juries don't understand Bayes and the actual effect of its use in court is to be grossly misused or make wild guesses sound formal and authoritative, then in the end you can't have Bayes's Theorem used formally in courts.

I have. There are still people publishing books that go in the SF section of the bookstore, but I could count off on one hand the number of authors I can remember who are feeding people hope instead of stylish postmodern cynicism.

I don't think you're a Christian. I do think you want Christianity to have a chance in hell, because... well, I'm not going to speculate. Meta-contrarianism would be one reason. Everyone voting down shminux, please note that they never said they thought Goetz was a Christian.

Luke will never be able to break up with any future girlfriends because it would require too many preliminaries before he could even start the sequence which would explain why they should break up.

Agreed that the ev-psych was bad. But...
If your true and actual reason for breaking up with someone is that her breasts are too small, consider that (a) saying "It was because you were too clingy" may cause them to try and mess with an aspect of their personality that doesn't even need fixing, and (b) total silence, which you may fondly imagine to be mercy, may result in her frantically imagining dozens of possible flaws all of which she tries desperately to correct, just on the off-chance it was that one. As opposed to, say, looking for a guy who's into smaller breasts next time.
Maybe I'm just being inordinately naive, but telling someone honestly, softly, and believably, your true reason for rejecting them, seems like it really should have certain advantages for them, if not for you. I mean, compared to either silence or lying. Calling it "grossly insensitive" is too quick a rejection of the possibility of telling a truth.

This is the exact reverse, in every way, of Erin collaborating with a friend of hers to write up an elaborate argument tree for the job of persuading me that she ought to be my girlfriend, which she ended up not actually needing to use.
She also doesn't have that document any more. I so wanted to see it...

Does this not count as an indirect argument or something?
I for one would be sort of annoyed if somebody announced that due to an alleged unamiable personality of mine they're declining to debate me, and then in the same public venue proceeded to state that my whole argument was an obvious non-sequitur.

I was debating wedifrid.

And here I thought there wasn't anything besides c I'd bet on at 99-to-1 odds.
Two! Two things in the universe I'd bet on at 99-to-1 odds. Though I'm not actually going to do it for more than say $2k if anyone wants it, since I don't bet more than I have, period.


For my part it strikes me that there is something rather different between the thing that Eliezer ridicules and the thing that species selection can be considered an instance of.

Correct. Species don't breed - they are not part of a sexually reproducing population - so the theory and simulations ruling out group selection against a countervailing individual selection pressure (invaders take over the gene pool under all realistic conditions) doesn't rule out species-level selection. Modus ponens to modus tollens, observed examples of species selection don't argue for group selection being realistic and Phil Goetz's entire argument is a nonsequitur. This was obvious to me at a glance but experience has taught me I do not enjoy arguing with Phil Goetz.

And Einstein was better at the same sort of philosophy and used it to predict new physical laws that he thought should have the right sort of style (though I'm not trying to do that, just read off the style of the existing model). But anyway, I'd pay $20,000 to find out I'm that wrong - what I want to eliminate is the possibility of paying $20,000 to find out I'm right.

This is still a benevolent dictatorship.
I hope that answers your question.

I declined to answer because I gave up arguing with Phil Goetz long before there was a Less Wrong, back in the SL4 days - I'm sorry, but there are some people that I don't enjoy debating. I'll leave it at that.


The sadder thing is that Eliezer doesn't seem particularly bothered with numbers either.

I beg your pardon. Check Ch. 30 and you should see some non-canonical first-year student cameos in Draco's army. For, may I mention, exactly that reason - I was explicitly familiar with the dilemma of the discordant Rowling statements and decided to resolve in favor of Hogwarts having around a thousand students, so that having around half the students sign up for the armies would give you 72 first-year soldiers.

It's not about transmitting information into the past - it's about the locality of causality. Consider Judea Pearl's classic graph with SEASONS at the top, SEASONS affecting RAIN and SPRINKLER, and RAIN and SPRINKLER both affecting the WETness of the sidewalk, which can then become SLIPPERY. The fundamental idea and definition of "causality" is that once you know RAIN and SPRINKLER, you can evaluate the probability that the sidewalk is WET without knowing anything about SEASONS - the universe of causal ancestors of WET is entirely screened off by knowing the immediate parents of WET, namely RAIN and SPRINKLER.
Right now, we have a physics where (if you don't believe in magical collapses) the amplitude at any point in quantum configuration space is causally determined by its immediate neighborhood of parental points, both spatially and in the quantum configuration space.
In other words, so long as I know the exact (quantum) state of the universe for 300 meters around a point, I can predict the exact (quantum) future of that point 1 microsecond into the future without knowing anything whatsoever about the rest of the universe. If I know the exact state for 3 meters around, I can predict the future of that point one nanosecond later. And so on to the continuous limit: the causal factors determining a point's infinitesimal future are screened off by knowing an infinitesimal spatial neighborhood of its ancestors.
This is the obvious analogue of Judea Pearl's Causality for continuous time; instead of discrete causal graphs, you have a continuous metric of relatedness (space) which shrinks to an infinitesimal neighborhood as you consider infinitesimal causal succession (time).
This, in turn, implies the existence of a fundamental constant describing how the neighborhood of causally related space shrinks as time diminishes, to preserve the locality of causal relatedness in a continuous physics.
This constant is, obviously, c.
I've never read this anywhere else, by the way. It clearly isn't universally understood, because if all physicists understood the universe in these terms, none of them would believe in a "collapse of the wavefunction", which is not locally related in the configuration space. I would be surprised neither to find that the above statement is original, nor that it has been said before.
I am attempting to bet that physics still looks like this after the dust settles. It's a stronger condition than global noncircularity of time - not all models with globally noncircular time have local causality.
If violating Lorentz invariance means that physics no longer looks like this, then I will bet at 99-to-1 odds against violations of Lorentz invariance. But I can't make out from the Wikipedia pages whether Lorentz violations mean the end of local causality (which I'll bet against) or if they're random weird physics (which I won't bet against).
I am also willing to bet that the fundamental constant c as it appears in multiple physical equations is the constant of time/space locality, i.e., the constant we know as c is fundamentally the shrinking constant by which an infinitesimal neighborhood in space causally determines an infinitesimal future in time. I am willing to lose the bet if there's still locality but the real size of the infinitesimal spatial neighborhood goes as 2c rather than c (though I'm not actually sure whether that statement is even meaningful in a Lorentz-invariant universe) and therefore you can use neutrinos to transmit information at up to twice the speed of light, but no faster. The clues saying that c is the fundamental constant that we should expect to see in any continuous analogue of a locally causal universe, are strong enough that I'll bet on them at 99-to-1 odds.
What I can't make out is whether Lorentz violation throws away locality; employs a more complicated definition of c which is different in some directions than others; makes the effect of the constant different on neutrinos and photons; or, well, what exactly.
I would happily amend the bet to be annulled in the case that any more complicated definition of c is adopted by which there is still a constant of time/space locality in causal propagation, but it makes photons and neutrinos move at different speeds.
The trouble is that physicists don't read books like Causality and don't understand local causality as part of the apparent character of physical law, which is why some of them still believe in the "collapse of the wavefunction" - it would be an exceptional physicist whom we could simply ask whether the Standard Model Extension preserves locally continuous causality with c as the neighborhood-size constant.

There are many definitions of c - it appears as a constant in many different physical equations. Right now, all of these definitions are consistent. If you have a new physics where all these definitions remain consistent and you can still transmit information faster than c, then certainly I have lost the bet. Other cases would be harder to settle - I did state that weird physics along the lines of "this is why photons are slowed down in a vacuum by dark matter, but neutrinos aren't slowed" wouldn't win the bet.

That might preserve before-and-after. It wouldn't preserve the locality of causality. Once you throw away c, you might need to take the entire frame of the universe into account when calculating the temporal successor at any given point, rather than just the immediate spatial neighborhood.


Minerva remembered what Harry had told her... how people were usually too optimistic, even when they thought they were being pessimistic. It was the sort of information that preyed on your mind, dwelling in it and spinning off nightmares...
But what was the worst that could happen?


Yeah, see, I'm not betting against random cool new physics, I wouldn't offer odds like that on there not being a Higgs boson, I'm betting on the local structure of causality. Could I be wrong? Yes, but if I have to pay out that entire bet, it won't be the most interesting thing that happened to me that day.
How confident am I of this? Not just confident to offer to bet at 99-to-1 odds. Confident enough to say...
"Well, that was an easy, risk-free $202."
Or to put it even more plainly:

"You turned into a cat! A SMALL cat! You violated Conservation of Energy! That's not just an arbitrary rule, it's implied by the form of the quantum Hamiltonian! Rejecting it destroys unitarity and then you get FTL signaling! And cats are COMPLICATED! A human mind can't just visualize a whole cat's anatomy and, and all the cat biochemistry, and what about the neurology? How can you go on thinking using a cat-sized brain?"
McGonagall's lips were twitching harder now. "Magic."
"Magic isn't enough to do that! You'd have to be a god!"


I agree with your sentiment but respectfully disagree with the details. First, Yasser Arafat got a Nobel Peace Prize so the system is already an utter farce. And second, if it wasn't an utter farce, you could make a good case for Petrov getting an honorable mention rather than the main prize, because there are people who've spent decades working hard for peace.

c is the constant as it appears in fundamental physical equations, relativistic or quantum. Anything slowing down the propagation of photons through an apparent vacuum (such as interaction with dark matter) which did not affect, for example, the mass-energy equivalence of E=MC2, would not win the bet.

I'll take bets at 99-to-1 odds against any information propagating faster than c. Note that this is not a bet for the results being methodologically flawed in any particular way, though I would indeed guess some simple flaw. It is just a bet that when the dust settles, it will not be possible to send signals at a superluminal velocity using whatever is going on - that there will be no propagation of any cause-and-effect relation at faster than lightspeed.
My real probability is lower, but I think that anyone who'd bet against me at 999-to-1 will probably also bet at 99-to-1, so 99-to-1 is all I'm offering.
I will not accept more than $20,000 total of such bets.


No, no; the fic has to be My Little Pony: Friendship is Science.

Done.

Friendship is Science has now been revised to contain a bit more plot.

I... I don't even... he Photoshopped the evidence into their actual hiking expedition... but... look, how far does this have to go before your kids are justified in wondering whether the world around them was created by you for the sole purpose of deceiving them?

Just the first two episodes so far.

Or buy the journal article and upload it... you'd think there'd be better centralized pirated repositories of science by now.

Only to the same degree that first-order logic requires an ambient group of models (not necessarily sets) to make sense. It's just that the ambient models in the second-order theory include collections of possible predicates of any objects that get predicates attached, or if you prefer, people who speak in second-order logic think that it makes as much sense to say "all possible collections that include some objects and exclude others, but still include and exclude only individual objects" as "all objects".

Thus demonstrating that a sort of species selection can exist, because species can't breed with each other and therefore can't be infected by SC genes from other species, while group selection can't exist, SC always infects.

I don't believe it's good math until it becomes possible to talk about the first uncountable ordinal, in the way that you can talk about the integers. Any first-order theory of the integers, like first-order PA, will have some models containing supernatural numbers, but there are many different sorts of models of supernatural numbers, you couldn't talk about the supernaturals the way you can talk about 3 or the natural numbers. My skepticism about "the first uncountable ordinal" is that there would not exist any canonicalizable mathematical object - nothing you could ever pin down uniquely - that would ever contain the first uncountable ordinal inside it, because of the indefinitely extensible character of well-ordering. This is a sort of skepticism of Platonic existence - when that which you thought you wanted to talk about can never be pinned down even in second-order logic, nor in any other language which does not permit of paradox.

SS0 isn't a free variable like "x", it is, in any given model of arithmetic, the unique object related by the successor relation to the unique object related by the successor relation to the unique object which is not related by the successor relation to any object, which is how mathematicians say "Two".

Only in the same sense that you can talk about kittens by saying "Those furry things!" There'll always be some ambiguity over whether you're talking about kittens or lions, even though kittens are in fact furry and have all the properties that you can deduce to hold true of furry things.

I don't think people really understood what I was talking about in that thread. I would have to write a sequence about

the difference between first-order and second-order logic
why the Lowenheim-Skolem theorems show that you can talk about integers or reals in higher-order logic but not first-order logic
why third-order logic isn't qualitatively different from second-order logic in the same way that second-order logic is qualitatively above first-order logic
the generalization of Solomonoff induction to anthropic reasoning about agents resembling yourself who appear embedded in models of second-order theories, with more compact axiom sets being more probable a priori
how that addresses some points Wei Dai has made about hypercomputation not being conceivable to agents using Solomonoff induction on computable Cartesian environments, as well as formalizing some of the questions we argue about in anthropic theory
why seeing apparently infinite time and apparently continuous space suggests, to an agent using second-order anthropic induction, that we might be living within a model of axioms that imply infinity and continuity
why believing that things like a first uncountable ordinal can contain reality-fluid in the same way as the wavefunction, or even be uniquely specified by second-order axioms that pin down a single model up to isomorphism the way that second-order axioms can pin down integerness and realness, is something we have rather less evidence for, on the surface of things, than we have evidence favoring the physical existability of models of infinity and continuity, or the mathematical sensibility of talking about the integers or real numbers.


I know very well the difference between a collection of axioms and a collection of models of which those axioms are true, thank you.
A lot of people seem to have trouble imagining what it means to consider the hypothesis that SS0+SS0 = SSS0 is true in all models of arithmetic, for purposes of deriving predictions which distinguish it from what we should see given the alternative hypothesis that SS0+SS0=SSSS0 is true in all models of arithmetic, thereby allowing internal or external experience to advise you on which of these alternative hypotheses is true.

Yeah, I discovered that when researching HPMOR.

It is more subtle and I do prefer it. The problem is that a substantial fraction of reviewers are still saying they've got no idea what's happening during the ellipses, and I care about that.
Your version is a little too unsubtle, but the fact that people were buying the "last descendant of Merlin thing" had me wondering what it would've taken to actually trigger their skepticism.
"For you, my Lady, are the last descendant of Cthulhu -"
"For you alone must stand against the vampires, the demons, and the forces of darkness -"
"Only you can prevent forest fires -"

Erm, I'd guess what gave my brain the idea originally was the fact that in canon they are flying corpses in grave shrouds.

That's not "adversity", that's "solvable problems requiring initiative".

I regret that I only have one upvote to give this comment.

So I'm thinking to myself, around six years ago, "I can at least manage to publish timeless decision theory, right? That's got to be around the safest idea I have, it couldn't get any safer than that while still being at all interesting. I mean, yes, there's these possible ways you could let these ideas eat your brain but who could possibly be smart enough to understand TDT and still manage to fall for that?"
Lesson learned.

I spent a year or so diligently studying rationality as a SingInst Visiting Fellow followed by realizing that I was a few levels above nearly any other aspiring rationalist.

And this is what several levels above me looks like? I'm not omnipotent, yet, but I have a deed or two to my name at this point; for example, when I write Harry Potter fanfiction, it reliably ends up as the most popular HP fanfiction on the Internet. (Those of you who didn't get here following HPMOR can rule out selection effects at this point.) Several levels above me should make it noticeably easier to show your power in a third-party-noticeable fashion, and the fact that you can't do so should cause you to question yourself.
It's the opposite of the lesson I usually try to teach, but in this one case I'll say it: it's not the world that's mad, it's you.

Actually I looked up in Wikipedia how many licks it takes to get the tootsie roll center of a tootsie pop, and picked a number that seemed commensurate with the human-licker experiments.
The Chilling Implications you point out (how many licks does it take to get to the center of Hermione?) were totally lost on my consciousness until now. I wonder if that subconscious imagery had anything to do with why my brain produced that response from Hermione?
But still, probably not 187.

Dumbledore does not understand "Parseltongue" in MoR and Ron could not have memorized a phrase in it. Parseltongue is not audio structure. Snakes can't talk.

"One step up and one step down" sounds like a valuable heuristic; it's what I actually did in the post, in fact. Upvoted.

A few months later, I've been teaching Anna and Luke and Will Ryan and others this rule as the "concrete-abstract pattern". Give a specific example with enough detail that the listener can visualize it as an image rather than as a proposition, and then describe it on the level of abstraction that explains what made it relevant. I.e., start with an application of Bayes's Theorem, then show the abstract equation that circumscribes what is or isn't an example of Bayes's Theorem.


Acting in a way to make yourself immune to criticism hardly fits the claim of being "a few levels above nearly any other aspiring rationalist". Rather, it shows that you're failing even the very rudiments of rationalist practice 101.

Being levels above in rationalism means doing rationalist practice 101 much better than others as much as being a few levels above in fighting means executing a basic front-kick much better than others.

http://en.wikipedia.org/wiki/Second-order_logic#Expressive_power - you can't talk about the integers or the reals in first-order logic. You can have first-order theories with the integers as a model, but they'll have models of all other cardinalities too. http://en.wikipedia.org/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem

You seem to accept the notion that all finite numbers have a supremum. Why not just iterate whatever process accounts for that?

First of all, I've never seen an aleph-null, just one, two, three, etc. Accepting that the integers have a supremum is a whole different kettle of fish from accepting that the collection of finite integers seems to go on without bound. Second, taking a supremum once, using a clearly defined computable notation and a halting machine that can compare any two representations, is a whole different kettle of fish than talking about the supremum of all possible ways to define countable well-orderings to and beyond computable recursion.

I think we may have something of a clash of backgrounds here. The reason I'm inclined to take the real continuum seriously is that there are numerous physical quantities that seem to be made of real or complex numbers. The reason I take mathematical induction seriously is that it looks like you might always be able to add one minute to the total number of minutes passed. The reason I take second-order logic seriously is that it lets me pin down a single mathematical referent that I'm comparing to the realities of space and time.
The reason I'm not inclined to take the least uncountable ordinal seriously is because, occupying as it does a position above the Church-Kleene ordinal and all possible hypercomputational generalizations thereof, it feels like talking about the collection of all collections - the supremum of an indefinitely extensible quality that shouldn't have a supremum any more than I could talk about a mathematical object that is the supremum of all the models a first-order set theory can have. If set theory makes the apparent continuum from physics collide with this first uncountable ordinal, my inclination is to distrust set theory.

Is this because you can't prove aleph-one = beta-one? I'm Platonic enough that to me, "well-order an uncountable set" and "well-order the reals" sound pretty similar.

Perhaps you could start by saying, "I can only tell you if you're asking for information and you promise not to argue." I don't know how practical that is in real life.
LWers could have a convention for saying to each other, "Please tell me so that I know how I was perceived by you. I will not argue and tell you that you perceived me differently, I will not blame the messenger, and I will not subject you to the unpleasant experience of hearing me offer to change."

This makes it sound like believing in an uncountable ordinal is equivalent to AC, which would make things easier - lots of mathematicians reject AC. But you might not need AC to assert the existence of a well-ordering of the reals as opposed to any set, and others have claimed that weaker systems than ZF assert a first uncountable ordinal. My own skepticism wasn't so much the existence of any well-ordering of the reals (though I'm willing to believe that no such exists), my skepticism was about the perfect, canonical well-ordering implied by there being an uncountable ordinal onto whose elements all the countable ordinals are mapped and ordered. Of course that could easily be equivalent to the existence of any well-ordering of the reals.

I wonder how much of this is just a function of what math you've ended up working with a lot.
Humans have really bad intuition about math. This shouldn't be that surprising. We evolved in a context where selection pressure was on finding mates and not getting eaten by large cats.
Speaking from personal experience as a mathematician (ok a grad student but close enough for this purpose) it isn't that uncommon for when I encounter a new construction that has some counterintuitive property to look at it and go "huh? Really?" and not feel like it works. But after working with the object for a while it becomes more concrete and more reasonable. This is because I've internalized the experience and changed my intuition accordingly.
There are a lot of very basic facts that don't involve infinite sets that are just incredibly weird. One of my favorite examples are non-transitive dice. We define a "die" to be a finite list of real numbers. To role a dice we pick a die a random number from the list, giving each option equal probability. This is a pretty good representation of what we mean by a dice in an intuitive set. Now, we say a die A beats a die B if more than half the time die A rolls a higher number than die B. Theorem: There exist three 6-sided dice A, B and C with positive integer sides such that A beats B, B beats C and C beats A. Constructing a set of these is a fun exercise. If this claim seems reasonable to you at first hearing then you either have a really good intuition for probability or you have terrible hindsight bias. This is an extremely finite, weird statement.
And I can give even weirder examples including an even more counterintuitive analog involving coin flips.
I just don't see "my intuition isn't happy with this result" to be a good argument against a theorem. All the axioms of ZF seem reasonable and I can get the existence of uncomputable ordinals from much weaker systems. So if there's a non-intuitive aspect here, that's a reason to update my intuition not to reduce my confidence in set theory.
ETA: If you want to learn more about this (and see solution sets for the three dice problem) see this shamelessly self-promoting link to my own blog or this more detailed and better written Wikipedia article.

I would be interested in knowing if there is any second-order system which is strong enough to talk about continuity, but not to prove the existence of a first uncountable ordinal.

The existence of the real number line is one thing. The existence of an uncountable ordinal is another. When you consider the hierarchies of uncomputable ordinals to their various Turing degrees that are numbered among the countable ordinals, and that which countable ordinals you can constructively well-order strongly corresponds to the strength of your proof theory and which Turing machines you believe to halt, and when you combine this with the Burali-Forti paradox saying that the predicate "well-ordered" cannot be self-applicable, even though any given collection of well-orderings can be well-ordered...
...I just have trouble believing that there's actually any such thing as an uncountable ordinal out there, because it implies an absolute well-ordering of all the countable well-orderings; it seems to have a superlogical character to it.

James Miller says:

Hi,
It wasn't me. Garett Jones, an economist at George Mason University, has been making these points. See
http://mason.gmu.edu/~gjonesb/JonesADBSlides
http://mason.gmu.edu/~gjonesb/JonesADR


Variants I'd like to see:
1) You can observe rounds played by other bots.
2) You can partially observe rounds played by other bots.
3) (The really interesting one.) You get a copy of the other bot's source code and are allowed to analyze it. All bots have 10,000 instructions per turn, and if you run out of time the round is negated (both players score 0 points). There is a standard function for spending X instructions evaluating a piece of quoted code, and if the evaled code tries to eval code, it asks the outer eval-ing function whether it should simulate faithfully or return a particular value. (This enables you to say, "Simulate my opponent, and if it tries to simulate me, see what it will do if it simulates me outputting Cooperate.")

It's not Choice I have the problem with here, it's set theory.

I've just decided to eliminate the "fourth-year" qualifier. I'd previously meant Ranma to be separate from Tonks, but on reflection it's kind of funnier if she is Ranma. More importantly, I want Metamorphmagi to be rarer and more unexpected than if two different ones are attending Hogwarts at the same time.

I'm impressed. That's WH40K-level crapsackiness.

You know, the concept of the first uncountable ordinal is actually one of the strongest reasons I've ever heard to disbelieve in set theory. ZFC, or rather NBG, does imply the existence of a first uncountable ordinal, right, or am I mistaken?

This is something I hadn't realized explicitly until you pointed it out. But yes, lazy authors don't bother to give their characters conflicting goals or personalities or deep beliefs, so they give them conflicting surface beliefs and then come up with bad excuses for them not to communicate.

Wrote the quadruple-disclaimerized version of that conversation, deleted the disclaimers because it didn't flow as writing. Justification: Harry finds it very easy to imagine that Hermione is just as terrified of losing control as he is, even though that's not quite what's going on at the other end.

Actually, you can solve this problem just by snapping your fingers, and this will give you all the same benefits as the placebo effect! Try it - it's guaranteed to work!

Was this meant to be a Discussion post?

ERROR: POSTULATION OF GROUP SELECTION DETECTED

<mental model of Michael Vassar says>This strikes me as a nerdism. If you don't find less intelligent people easier to manipulate, you must be working on sympathetic models of them instead of causal ones. I expect that experience would cure this, and after a few months of empirical practice and updating on the task of reasoning with fools, you would find it was actually easier to get them to do whatever you wanted - if you could manage to actually try a lot of different things and notice what worked, instead of being incredulous and indignant at their apparent reasoning errors.</Vassar>

Space Viking has got to be one of the leading "way more rational than its title sounds like" books out there. I wonder if Piper actually named it that or if it was some bright-eyed publisher.

It's a nice list, but I think the core point strikes me as liable to be simply false. I forget who it was presenting this evidence - it might even have been James Miller, it was someone at the Winter Intelligence conference at FHI - but they looked at (1) the economic gains to countries with higher average IQ, (2) the average gains to individuals with higher IQ, and concluded that (3) people with high IQ create vast amounts of positive externality, much more than they capture as individuals, probably mostly in the form of countries with less stupid economic policies.
Maybe if we're literally talking about a pure speed and LTM pill that doesn't affect at all, say, capacity to keep things in short-term memory or the ability to maintain complex abstractions in working memory, i.e., a literal speed and disk space pill rather than an IQ pill.

I keep running into problems with various versions of what I internally refer to as the "placebo paradox", and can't find a solution that doesn't lead to Regret Of Rationality. Simple example follows:
You have an illness from wich you'll either get better, or die. The probability of recovering is exactly half of what you estimate it to be due to the placebo effect/positive thinking. Before learning this you have 80% confidence in your recovery.
Since you estimate 80%, your actual chance is 40% so you update to this.
Since the estimate is now 40%, the actual chance is 20%, so you update to this.
Then it's 10%, so you update to that. etc. Until both your estimated and actual chance of recovery are 0. then you die.
An irrational agent, on the other hand, upon learning this could self delude to 100% certainty of recovery, and have a 50% chance of actually recovering.
This is actually causing me real world problems, such as inability to use techniques based on positive thinking, and a lot of cognitive dissonance.
Another version of this problem features in HP:MoR, in the scene where harry is trying to influence the behaviour of dementors.
And to show this isn't JUST a quirk of human mind design, one can envision Omega setting up an isomorphic problem for any kind of AI.

On at least three occasions in this fic that I can think of offhand, people have confidently identified references which seemed very clear and obvious and fitting after I looked them up, despite the fact that I hadn't the slightest idea of what I was "referring" to at the time. It is enough to expand my concept of coincidence.

Some do.
Some are stupid and will shoot the messenger even though they're emotionally better off knowing for certain than just wandering in an unhappy fog, wondering over and over what they're doing wrong.
If they ask directly, I'd say, tell them honestly.

A.T. Field. What do you mean, Hammertime?

And no matter who you are, there's someone out there who thinks you're hot.
(while talking about the Harry Potter movies, before she'd started on MoR)
Erin: ...I did like the fluffy things, though.
Me: Fluffy things?
Erin: I forget what they're called.
Me: (thinks for a bit...)
Me: Dementors? The flying corpses in shrouds?
Erin: Yeah! Dementors are cute.
Me: Puppies are cute. Dementors are not cute.
Erin: Puppies are food.
Me: Help me, I've been shipped to Bellatrix.


It's none of someone's business why unless you choose to volunteer that information, and needing to know why you've just been turned down is a massive low-self-perceived-status signal.

Okay, seriously? This kind of "No you can't know what you did wrong, asking means you're even lower-status" dynamic to sexuality has probably been responsible for a number of geek/Aspie suicides over the last century. The existence and popularity of PUA isn't so much a response to men who feel deprived of sex, it's targeted at men who feel deprived of sex and romance and any idea of what they're doing wrong and any known strategy for even getting started on fixing things. A major reason why people hurt is that there's no known gentle slope into sex, and not getting any feedback is part of that.
I've informed a number of male college students that they have large, clearly detectable body odors. In every single case so far, they say nobody has ever told them that before. (And my girlfriend has confirmed a number of these, so it's not just a unique nose.)
If you don't need to ask yourself, that's fine. If someone else does need to ask, try to be more sympathetic. And if someone asks you, TELL THEM.

Right. In canon, Snape didn't want to win that fight. Also in canon, Snape is the only wizard besides Voldemort who can fly without a broomstick, although the movie subverts this. One also notes that a half-blood was accepted into the Death Eaters and presumably not for his wealth or family connections. More to the point, in MoR there's a sharper distinction between powerful wizards and non-powerful ones; powerful wizards have taken an interest in ancient riddles, they have delved into secrets, they have found sources of lore that cannot be learned from books. Professor McGonagall knows a hell of a lot about Transfiguration, but she hasn't gone down that road.
Even so, the gap between a "powerful wizard" like Snape and, say, Dumbledore, is rather large. Dumbledore strolls through Amelia Bones's wards like they were water, and Bones, in this fic, is an ancient and experienced witch. Forty-four simultaneous strikes from upper-year Hogwarts students will definitely bring down Madam Bones, maybe even if she does have time to reinforce and strengthen her shields. Dumbledore would wave the Elder Wand, once.

Well, I'm glad someone got Zelazny, but that leaves at least three references I haven't seen anyone decrypt yet, two obscure and one much more mainstream.

The thought had occurred to me. And if you were a double witch, wouldn't you think it was pretty darned plausible that there were triple witches?

You can't raise and strengthen very much in the way of advanced shielding and be invisible at the same time, and massed fire from forty-four fifth-year through seventh-year wizards is nothing to sneeze at. Snape would have noticed Quirrell taking him out, but it wasn't necessary for Quirrell to do so.

There's been an article in the online? version of The Atlantic about it. See "Press Coverage" in the user profile.


(And if Silas isn't a donor... Thomas %&$@! Bayes why does anyone care?!)

I don't, but if I don't say that out loud, other people go on loudly caring, and if I do say it out loud, I get downvoted. (Shrug.)

The answer I would make to "why?" (but have never had to, as women tend to be much less clueless than men about dating) would be something like: "Because it seemed as though you were the sort of person who would feel entitled to ask me why, instead of merely accepting my answer."
It's none of someone's business why unless you choose to volunteer that information, and needing to know why you've just been turned down is a massive low-self-perceived-status signal.
The only exception to that rule would be someone that you already have a deep and long standing relationship (just not sexual or romantic) with. Such a person might be justified in starting a "Why" conversation as your friend. But even that is dicey, and the sort of conversation that could destroy the friendship, as it can so easily ride the knife edge of trying to make you defend your answer, or guilt you into changing it if you can't convince them that is both reasonable and not a negative judgement of them.

Your standards are probably higher than mine? As far as I can tell, most women are attractive. I can think of ones who aren't but they seem like exceptions. You can kinda see why it would work that way.


Dreyfus was still convicted, though his sentence was reduced, but the public was outraged and the president issued a pardon two weeks later.

???
Dreyfus spent years on a prison island and emerged looking rather the worse for wear. http://en.wikipedia.org/wiki/Dreyfus_affair

I find this oddly cheerful. Go for it, then!




I think tolerance of another's multiple entanglements is more important component of poly than the desire to oneself have multiple entanglements.

Never mind tolerance, to me it feels better for its own sake to not be my girlfriends' only boyfriend. It was a surprisingly large weight off my mind to know that if I can't take her to Yosemite, or escort her to BENT SF, then she has other paramours who can do so. I know that I'm not personally responsible for matching every one of her sexual facets, just some of them, and that she won't be forever sexually unsatisfied if there's something I happen not to enjoy. If you asked me "Is it more important to your happiness that that your girlfriend be able to have more than one boyfriend or that you be able to have more than one girlfriend?" I might well reply "The former."

It's not? I mean, there's some people, though probably considerably less than half of the population, who are genuinely and naturally well-suited to monogamous closed relationships. But the point that immortal superbeings would do something polyish actually does strike me as a clear argument in favor of "poly is More Highly Evolved". I mean, you're then that much closer to doing things the way immortal superbeings would do it. This is why I've always felt vaguely guilty about not being bisexual, since immortal superbeings clearly would be.


If he'd said this before I hacked poly, I wouldn't have hacked poly... Given that I'm now poly, and that we both have other partners/prospects who we'd be somewhat distressed to give up, I'm not planning to reverse the hack.

Sounds like a pretty definitive answer to the "You just went poly for the guy!" objection.


Don't poly folks want to feel special to their partners?

Yes. Why would my being special to someone imply that they couldn't have sex and/or long-term relationships with people they found attractive?


Seriously... being poly doesn't add extra hours to the day!

You know, I had assumed that Time-Turners were invented by a Hogwarts Headmaster who despaired of getting the school schedules straight and one day before deadline stayed up until 6AM inventing the Time-Turner, and that he (gender coinflip-generated) succeeded because he was the first person to try for time travel just to get extra time and not to change the past, and that the invention within Hogwarts is why they get a traditional free pass on using them. But some polyamorous past wizard would be just as reasonable an inventor.
I like love pentagons and poly chains within the community. It creates a familial feeling. Of course nothing's actually gone wrong in my immediate poly family yet. You can easily see how this could go wrong.

Congratulations on the hack. I would have expressed doubt that this could work, and am correspondingly updating my priors.

[1] I'm counting willingness that one's sole partner have other partners (e.g. being an arm of a V) to be a low-key flavor of being poly oneself, not a variety of tolerant monogamy. I think this is the more reasonable way to divide things up given a two-way division, but if you feel that I mischaracterize the highly simplified taxonomy, do tell.

It happens that I agree with you on this, in fact I think tolerance of another's multiple entanglements is more important component of poly than the desire to oneself have multiple entanglements. In the poly circles I am aware of, there is no broad agreement on either of these points though. I thought I should mention that there are a non-trivial proportion of couples who self-ID as "one of us is poly and the other is not" where the poly one is involved with other people.
This is similar to the labeling disputes that occur when (say) two bisexual women are said to be in a "lesbian relationship". They might reasonably object that people will hear "lesbian relationship" and assume they are lesbians - "only lesbians can be in a lesbian relationship" is something I've heard some bi women say; but then again I can think of as many counter-examples where two bi women deliberately identify as being in a lesbian relationship.
So perhaps there is a similar scope issue with "poly person" vs. "poly relationship"; I was certainly startled to see you assert a poly person can only be involved with a poly person. I know many poly people currently involved in monogamous relationships with monogamous people, so perhaps this should be "one can only have a poly relationship with a poly person"?

Sure, and I also didn't mean to imply that this should happen on a first meeting, only at the point where you find yourself thinking, "Hm, I think I would prefer having sex with this person to not having sex with them," regardless of whether that takes a long or a short time.

Duly noted!

That's a good question. I am hard pressed to think of any nerd females I've known well enough to observe them in any detail, who I would actually consider non-pretty. So to rephrase the test: If you go to nerd parties and male nerds who don't already know you seem to gravitate in your direction and then continue to be there despite not having an obvious personal stake in the ongoing conversation, this is because you are pretty.
Also, short of actually having half your face burned off a la Two-Face in the Batman series, being visibly smart and funny will boost your apparent prettiness by quite a lot.


(Also, spontaneously all kinds of popular. If I'd known I could get this many people interested by hacking poly I might have done it sooner.)

The following is a public service announcement to all women who naturally like at least some shy nerds.
If you are (1) polyamorous and (2) able to directly ask men you find attractive to sleep with you (instead of doing the sheep dance where you freeze motionless and wait for them to approach) - or if you can hack yourself to be like that without too much effort - it is vastly easier than you imagine to acquire an entire harem of high-status and/or handsome nerds.
(For some but not all nerds, this may require that you be reasonably attractive. Most nerd girls I know are reasonably attractive and think they are not. So if you think that you're overweight and hideous and yet oddly enough nerds spend a lot of time talking to you at nerd parties, this means you are pretty.)
This concludes the public service announcement.

I thought the part about "hopefully Ch. 78 will be finished by the time the Interlude is posted" ought to clearly imply that the schedule only applies until I run out of completed chapters.

Do you want credit with HPMOR's grateful readers, and if so should it be under Rain or your true name?

By which you mean, we haven't gotten the check yet, and the current total doesn't take into account pengvado's donation?

Does the current page take into account http://lesswrong.com/lw/78s/help_fund_lukeprog_at_siai/4p9a and http://lesswrong.com/lw/78s/help_fund_lukeprog_at_siai/4p1x in the total?

I just put in a pledge of $1,000 per month.

That was hard to decide. I eventually figured on "No" - the Four Founders are too recent, and shouldn't have the magic level necessary to produce large-scale nuke-proof structures.

The key thing is for your voice to make it clear that you're not at all afraid and that you think this is what the high-prestige smart people do. Show the tiniest trace of defensiveness and they'll pounce.

Upvoted for correctly subverting the standard madness of inaction.

I'm quite often asked about my necklace, and I'll say "It's my contract of immortality with the Cult of the Severed Head", or in some contexts, "It's my soul" or "It's my horcrux".

By check? Can you PM or email me with the name? The reason I ask is so that I can figure out how close HPMOR is to the 4-day update threshold, add it into my calculations in advance, and make sure it doesn't get double-counted when the actual check arrives. (BTW, do you want credit with my thousands of fanatic readers for bringing the threshold closer?)

"It would be silly if anyone could win the whole war at any time just by owling him a hand grenade," Harry thinks in Ch. 37. A Fate Zero style conflict between a sane and a non-sane wizard ends very quickly, and if two sane wizards ever fight each other...

See "Secrecy and Openness". I directly contradicted Rowling in that chapter for exactly that reason. Roughly, a good wizard or witch who knows what's coming can easily raise a shield against bullets. Bombs are more difficult, although e.g. the Castle Hogwarts would just shrug them off. And there are ancient devices and certain old structures that could stand up to point-blank nuclear weapons, but they're rare.

Agreed.

Voting buttons have now been removed from user profile pages. There was no good reason for them to be there in the first place, really.

#include "dripgrind.h"

Administrating volunteers also requires staff hours. Sometimes more than the original task. Why, are you volunteering to administrate them?

Since Yudkowsky's 30-day Karma is currently 6055, and he hasn't posted that much over the last month, the answer must be "all votes on all posts within the past 30 days, regardless of the age of the post or comment".
This doesn't look that useful, I think it should only count votes on recent posts/comments. Also, this measure, if amended this way, looks informative enough to allow an option of viewing a longer list of active contributors, which could be linked to from caption of the sidebar section, like with "recent posts".

Maybe I'm pushing my luck, but "Nihil supernum"?

"Nihilitas superna"?

Modifying the English isn't unthinkable. What sounds right in Latin?

Thanks! Here for comparison is Google's translation:
Non habet soter salvator.
Vindex est dominus no,
nec mater nec pater,
modo nihil est.

If "Soter" or "Sotehr" means "savior", as I seem to recall from Aristoi, that might suit the meaning well; and if the first line makes sense grammatically, of which no clue hath I, it has a good ring. "Defensori" does sound closer to the intended meaning than "victori" or "vindex". And whether "modo nihil est" means at all the same thing as "modo nihilitas supera", I've likewise no clue but it sounds like the "above" part was left out. If it actually does convey the same meaning, it is more compact.
If this version works, it would have a powerful ring to it:
Non habet soter salvator.
Neque defensori dominus,
nec pater, nec mater,
modo nihil est.

But one suspects that what's actually needed is:
Non est salvatori salvator.
Neque defensori dominus,
nec pater, nec mater,
modo nihilitas supera.


Well, my probability that you could or would do anything useful, given money, just dropped straight off a cliff. But perhaps you're just having trouble communicating. That is to say: What the hell are you talking about.
If you're going to ask for money on LW, plain English response, please: What's the output here that the money is paying for; (1) a Friendly AI, (2) a theory that can be used to construct a Friendly AI, or (3) an analysis that purports to say whether or not Friendly AI is "feasible"? Please pick one of the pre-written options; I now doubt your ability to write your response ab initio.

Sorry, could you say again what exactly you want to do? I mean, what's the output here that the money is paying for; a Friendly AI, a theory that can be used to construct a Friendly AI, or an analysis that purports to say whether or not Friendly AI is "feasible", or what?


Money is the unit of caring and it really is impossible to overstate how much things change when you add money to them.

Can you give an example relevant to the context at hand to illustrate what you have in mind? I don't necessarily disagree, but I presently think that there's a tenable argument that money is seldom the key limiting factor for philanthropic efforts in the developed world.

BTW, note that I deleted the "impossible to overstate" line on grounds of its being false. It's actually quite possible to overstate the impact of adding money. E.g., "Adding one dollar to this charity will CHANGE THE LAWS OF PHYSICS."

Leaving aside Aumann questions: If people like that think that the Future of Humanity Institute, work on human rationality, or Giving What We Can has a large probability of catalyzing the creation of an effective institution, they should quite plausibly be looking there instead. "I should be doing something I think is at least medium-probably remedying the sheerly stupid situation humanity has gotten itself into with respect to the intelligence explosion" seems like a valuable summary heuristic.
If you can't think of anything medium-probable, using that as an excuse to do nothing is unacceptable. Figure out which of the people trying to address the problem seem most competent and gamble on something interesting happening if you give them more money. Money is the unit of caring and I can't begin to tell you how much things change when you add more money to them. Imagine what the global financial sector would look like if it was funded to the tune of $600,000/year. You would probably think it wasn't worth scaling up Earth's financial sector.

Quick comment one:
This jumped out instantly when I looked at the charts: Your prior and evidence can't possibly both be correct at the same time. Everywhere the prior has non-negligible density has negligible likelihood. Everywhere that has substantial likelihood has negligible prior density. If you try multiplying the two together to get a compromise probability estimate instead of saying "I notice that I am confused", I would hold this up as a pretty strong example of the real sin that I think this post should be arguing against, namely that of trying to use math too blindly without sanity-checking its meaning.
Quick comment two:
I'm a major fan of Down-To-Earthness as a virtue of rationality, and I have told other SIAI people over and over that I really think they should stop using "small probability of large impact" arguments. I've told cryonics people the same. If you can't argue for a medium probability of a large impact, you shouldn't bother.
Part of my reason for saying this is, indeed, that trying to multiply a large utility interval by a small probability is an argument-stopper, an attempt to shut down further debate, and someone is justified in having a strong prior, when they see an attempt to shut down further debate, that further argument if explored would result in further negative shifts from the perspective of the side trying to shut down the debate.
With that said, any overall scheme of planetary philanthropic planning that doesn't spend ten million dollars annually on Friendly AI is just stupid. It doesn't just fail the Categorical Imperative test of "What if everyone did that?", it fails the Predictable Retrospective Stupidity test of, "Assuming civilization survives, how incredibly stupid will our descendants predictably think we were to do that?"
Of course, I believe this because I think the creation of smarter-than-human intelligence has a (very) large probability of an (extremely) large impact, and that most of the probability mass there is concentrated into AI, and I don't think there's nothing that can be done about that, either.
I would summarize my quick reply by saying,
"I agree that it's a drastic warning sign when your decision process is spending most of its effort trying to achieve unprecedented outcomes of unquantifiable small probability, and that what I consider to be down-to-earth common sense is a great virtue of a rationalist. That said, down-to-earth common-sense says that AI is a screaming emergency at this point in our civilization's development, and I don't consider myself to be multiplying small probabilities by large utility intervals at any point in my strategy."


If you're thinking truly reductionistically about programming an AI, you'll realize that "probability" is nothing more than a numerical measure of the amount of information the AI has.

And here I thought it was a numerical measure of how credible it is that the universe looks a particular way. "Probability" is what I plug into expected utility calculations. I didn't realize that I ought to be weighing futures based on "the amount of information" I have about them, rather than how likely they are to come to pass.

I still don't think you're saying something sophisticated and true. I think you're saying something sophisticated and nonsensical. I think it's meaningless to assign a probability to the assertion "understand up without any clams" because you can't say what configurations of the universe would make it true or false, nor interpret it as a question about the logical validity of an implication. Assigning probabilities to A, B, C as in your linked writing strikes me as equally nonsensical. The part where you end up with a probability of 25% after doing an elaborate calculation based on having no idea what your symbols are talking about is not a feature, it is a bug. To convince me otherwise, explain how an AI that assigns probabilities to arbitrary labels about which it knows nothing will function in a superior fashion to an AI that only assigns probabilities to things for which it has nonzero notion of its truth condition.
"If you know nothing, 50% prior probability" still strikes me as just plain wrong.


A statement, any statement, starts out with a 50% probability of being true, and then you adjust that percentage based on the evidence you come into contact with.

That's wildly wrong. "50% probability" is what you assign if someone tells you, "One and only one of the statements X or Y is true, but I'm not going to give you the slightest hint as to what they mean" and it's questionable whether you can even call that a statement, since you can't say anything about its truth-conditions.
Any statement for which you have the faintest idea of its truth conditions will be specified in sufficient detail that you can count the bits, or count the symbols, and that's where the rough measure of prior probability starts - not at 50%. 50% is where you start if you start with 1 bit. If you start with 0 bits the problem is just underspecified.
Update a bit in this direction: That part where Rational Rian said "What the hell do you mean, it starts with 50% probability", he was perfectly right. If you're not confident of your ability to wield the math, don't be so quick to distrust your intuitive side!


But these are Yale cognitive science students. Surely they don't think the mind runs on magic, right?

Wow. I had no idea Yale cognitive science students had reached the astronomical level of competence where we ought to be surprised when they make simple mistakes. I assume that none of them are religious, either?

Well, he's probably kinda right. I mean, I can visualize some 50-year-old getting along just fine but they'd have to be an unusually cool 50-year-old who doesn't make the others present feel uncomfortable when the topic of polyamory comes up.


A student study at the University of Cambridge concluded that it takes 3,481 licks to get to the center of a Tootsie Pop.[7] Another study by Purdue University concluded that it takes an average of 364 licks to get to the center of a Tootsie Pop using a "licking machine", while it takes an average of 252 licks when tried by 20 volunteers. Yet another study by the University of Michigan concluded that it takes 411 licks to get to the center of a Tootsie Pop. A 1996 study by undergraduate students at Swarthmore College concluded that it takes a median of 144 licks (range 70-222) to get to the center of a Tootsie Pop.[8] Harvard Grad students created a rotating mechanical tongue and concluded 317 licks.

-- Wikipedia, on the reproducibility of scientific results

See, there you're just confirming the original quote.



I don't think that's a complete explanation. I would say it's more along the lines of "If you start with somebody working a three-day week, it's much easier to employ them for another two days, than to hire a new person to work two days because that requires creating a whole new business relationship." Then both corporations and governments, I think, tend to be as inefficient as they can possibly get away with without dying, or maybe a little more inefficient than that. Work expands to fill the time available...
I would have to sit down and write this out if I really wanted to think it through, but roughly I think that there are forces which tend to make people employed for a full workweek, everyone want to be employed, and society to become as inefficient as it can get away with. Combine these factors and it's why increasing productivity doesn't increase leisure.

If there were a one-and-done answer, I think this'd be it.


Technological advances can't shorten the work hours because even in a society wealthy and technologically advanced enough that basic subsistence is available for free, people still struggle for zero-sum things, most notably land and status. Once a society is wealthy enough that basic subsistence is a non-issue, people probably won't work as much as they would in a Malthusian trap where constant toil is required just to avoid starvation, but they will still work a lot because they're locked in these zero-sum competitions.

That is the clearest explanation I've seen so far for this. (I've read a lot of SF, and asked myself the question.)


You don't have to be a genius to be evil

Right, I'm just saying, that's how I know it's not the real Voldemort posting.

in my experience those few people who are both geniuses and evil, usually tell you exactly what they are about. They may not say, "I intend to torture and kill you," but they very often will tell you with relish how they've tortured others,

We may have different standards for "genius"; I don't think I've ever heard of someone who I would classify as both malicious (negated utility function, actually wants to hurt people rather than just being selfish) and brilliant. I also doubt that any such person exists nowadays, because, you see, we're not all dead.

XKCD 871: The problem of scaling the sane use of money is a problem of not crushing people's wills, not a problem of money being a limited resource. It simply isn't true that money spent on cryonics comes out of Givewell's or SIAI's pockets, unless you're Rain, which is why I'll accept that answer from Rain but not from you.

True evil geniuses don't reveal their intentions openly. (They also don't post this blog comment.)

This is a fantastically burdensome explanation for why people don't sign up for cryonics. Do people who do sign up for cryonics usually have happier lives? (Not that I've heard of.) Do the same people who turn down cryonics turn down other forms of medical care? (Not that I've heard of.) If we found that people signing up for cryonics were less happy on average, would we be able to construct an equally plausible-sounding symmetrical argument that people with happy, fulfilled lives see no need for a second one? (Yes.)
I hate to go into psychologizing, but I suspect that Mike Darwin wants a grand narrative of Why, Oh Why Cryonics Fails, a grand narrative that makes sense of this shocking and incomprehensible fact and gives some info on what needs to be done to relieve the frustration.
The truth is that people aren't anything like coherent enough to refuse cryonics for a reason like that.
Asking them about cryonics gets their prerecorded verbal behaviors about "immortality" which bear no relation whatsoever to their feelings about whether or not life is fun.
Remember the fraction of people that take $500 for certain over a 15% chance of $1 million? How could you possibly need any elaborate explanation of why they don't sign up for cryonics? Risk-aversion, loss-aversion, ambiguity-aversion, status quo bias.
Cryonics sounds strange and not-of-our-tribe and they don't see other people doing it, a feeling expressed in words as "weird". It's perceptually categorized as similar to religions or other scams they've heard about from the newspaper, based purely on surface features and without any reference to, or remediability by, the strength of the underlying logic; that's never checked. Mike Darwin thinks that if you have better preservation techniques, people will sign up in droves, because right now they're hearing about cryonics and rejecting it because the preservation techniques aren't good enough. This is obviously merely false, and the sort of thing which makes me think that Mike Darwin needs a grand narrative which tells him what to do to solve the problem, the way that Aubrey de Grey thinks that good enough rejuvenation results in mice will grandly solve deathism.
I recently got a phone call saying that, if I recall correctly, around a quarter - or maybe it was half - of all Alcor's cryonics signups this year, are originating from LW/Yudkowsky/rationality readers. If you want people to sign up for cryonics, the method with by far the strongest conversion ratio is to train them from scratch in advanced sanity techniques. Nothing else that cryonics advocates have tried, including TV ads, has ever actually worked. There's no simple reason people don't sign up, no grand narrative, nothing that makes sense of cryonicists' frustration, people are just crazy in rather simple and standard ways. The only grand narrative for beating that is "soon, your annual signups will equal 10% of the people who've gone through a rationality bootcamp plus 1% of the people who've read both Eliezer's nonfiction book and Harry Potter and the Methods of Rationality."


I give away all my earnings and my husband gives about 20% of his, so we live on a much smaller budget than most people we know.

You have my great respect for this, and if you moreover endorse

But it would be good if people laid off the end-of-life spending even without cryonics.

and you've got some sort of numerical lives-saved estimate on the charities you're donating to, then I will accept "Cryonics is not altruistically maximizing" from you and your husband - and only from you two.
Unless you have kids, in which case you should sign them up.

I decided to adopt "Together, we can punch the sun!" as a personal motto even before I scrolled back up and saw the relevant photo.
Now I just need to decide what it's a motto for.

I don't think that Pascal's Mugging puts pressure on Bayesianism, I think it puts pressure on Solomonoff-type priors - Robin's anthropic answer is the one I currently find most appealing. The Lifespan Dilemma puts a lot more pressure on EU, in my book.

Episode #7 of Madoka, and I'm thinking, "It's amazing how many anime problems can be solved by polyamory and the pattern theory of identity."

Upvoted for evil brilliance.

We simply do not have a scientific process any more.

This sounds like a sensible position to me as well.

This robot is not a consequentialist - it doesn't have a model of the world which allows it to extrapolate (models of) outcomes that follow causally from its choices. It doesn't seem to steer the universe any particular place, across changes of context, because it explicitly doesn't contain a future-steering engine.

That is among the reasons why I keep telling SIAI people to never reply to "AI risks are small" with "but a small probability is still worth addressing". Reason 2: It sounds like an attempt to shut down debate over probability. Reason 3: It sounds like the sort of thing people are more likely to say when defending a weak argument than a strong one. Listeners may instinctively recognize that as well.
Existential risks from AI are not under 5%. If anyone claims they are, that is, in emotional practice, an instant-win knockdown argument unless countered; it should be countered directly and aggressively, not weakly deflected.


Sadly, psychology has not yet advanced to the point where we can give people electric shocks for thinking things

So long as one could convince someone psychology (or magic) has advanced to the point where we can give people electric shocks for thinking things, one could possibly trick people into monitoring their thoughts themselves.
"So, let's test this out. Practice thinking of a jar of pennies and we'll see if it triggers the shock." "OK." bzzzzzzz.
"Ow, that hurt!" "Brace yourself next time. It helps. Try now. Brace, but don't think of the pennies." "OK." "Now think of the pennies." bzzzzzzzz "@#$%! You're right, bracing helped a lot"
"Good. A few more tests to make sure it works, then you go wear it around all day."
The subject then braces every time he thinks of the pennies, which the monitor detects.
Neither deception nor electric shocks are really necessary, of course. People very often change what mental associations they have without them.

My salary is my own, to do with as I wish. I'm not put out by the rudeness, per se, but I will not entertain further questions along these lines - it is not something on which I'm interested in having other people vote.

I am not sure that you properly appreciate what happens to people when they get old. There was once a Marvin Minsky who helped write the first paper ever on "artificial intelligence". I look forward to meeting him after he comes out of cryonic suspension, but he isn't around to talk to right now.

Agreed, moved.

I've met Minsky. He's, well, old. I tried asking him what he thought of Bayesianism. He said he regarded it as a failed approach. I didn't try pushing any further.

Villains do exist in this world. So do heroes, although they're a lot rarer. That villains have good sides, or heroes have flaws, does not change this point. And yes, Gould is a bad guy. Not Voldemort, but still someone whose scientific works contain lies and misdirections and mis-implications subtle enough that I would consider it to be a foolishly overconfident risk to try to read them.


Normally, it's a four-year liberal arts degree to learn the subtle arts of weighing up unreliable human-generated evidence and turning it into useful information

I've never heard of that being taught in college. Is there a Bayesian stats class involved? Could these alleged evidence-weighers combine two likelihood ratios with a prior?
I mean, I'm sorry, but the above is just a ridiculous assertion. If there were any four-year university degree which taught people how to weigh evidence correctly, the world would look very different from the way it currently does.

It worries me a tad that nobody in the discussion group corrected what I consider to be the obvious basic inaccuracy of the model.
Success on FAI is not a magical result of a researcher caring about safety. The researcher who would have otherwise first created AGI does not gain the power to create FAI just by being concerned about it. They would have to develop a stably self-improving AI which learned an understandable goal system which actually did what they wanted. This could be a completely different set of design technologies than what would have gone into something unstable that improved itself by ad-hoc methods well enough to go FOOM and end the game. The researcher who would have otherwise created AGI might not be good enough to do this. The best you might be able to convince them to do would be to retire from the game. It's a lot harder to convince someone to abandon the incredibly good idea they're enthusiastic about, and start over from scratch or leave the game, then to persuade people to be "concerned about safety", which is really cheap (you just put on a look of grave concern).
If I thought all you had to do to win was to convince the otherwise-first creator of AGI to be "take safety seriously", this problem would be tremendously easier and I would be approaching it in a very different way. I'd be putting practically all of my efforts into PR and academia, not trying to assemble a team to solve basic FAI problems over however-many years and then afterward build FAI. A free win just for convincing someone to take something seriously? Hot damn, that'd be one easy planet to save; there'd be no point in pursuing any other avenue until you'd totally exhausted that one.
As it stands, though, you're faced with (a) the much harder sell of convincing AGI people that they will destroy the world and that being concerned is not enough to save them, that they have to tackle much harder problems than they wanted to face on a problem that seems to them hard-enough-already; and (b) if you do convince the AGI person who otherwise would've destroyed the world, to join the good guys on a different problem or retire, you don't win. The game isn't won there. It's just a question of how long it takes the next AGI person in line to destroy the world. If you convinced them? Number three. You keep dealing through the deck until you turn up the ace of spades, unless the people working on the ace of hearts can solve their more difficult problem before that happens.
All academic persuasion does is buy time, and not very much of that - the return on effort invested seems to be pretty low.

Well, I recently read through Erfworld because I'd heard it compared to MoR, and to be precise, Erfworld is not rationalist fiction. Erfworld is Munchkin fiction.

Posts like this go to the discussion section now.


Wikipedia's Epistemology - How Wikipedia determines truth. I'll let David Gerard tell us what that was about

Um, OK. This is an inchoate thing I've been bouncing around in my head for about the past six months. To attempt to summarise ...
Normally, it's a four-year liberal arts degree to learn the subtle arts of weighing up unreliable human-generated evidence and turning it into useful information. The way Wikipedia works means that you have to explain all that from scratch to argumentative teenagers with Wikipedia-induced aspergism in three paragraphs, and they'll still argue it, 'cos it's not like there's people who really do know more than them about abstracting knowledge from data, is it.
This means that Wikipedia has evolved its own epistemology of where knowledge comes from. It means there's a massive systemic bias against fields that aren't favoured by people who don't think like that. It also generates absurdities like regarding newspapers as "reliable sources", which anyone who's ever been quoted in one will laugh hysterically in horror at.
This is treated as though it is not just one epistemology of many, but the epistemology of how to abstract truth for an encyclopedia.
This is enough of a problem that I know humanities scholars who know Wikipedia in depth but are having to work out what the hell they can do about this, as academic experts in various fields start bringing themselves to Wikipedia even if it gets idiots in their faces, just to get their field properly represented.
A further problem is that early Wikipedians were encyclopedia nerds who could answer "What's an encyclopedia?" by pointing to Britannica and saying "It's a bit like that." There are kids now who have never had any other encyclopedia than Wikipedia. So "what is an encyclopedia?" is coming loose from history. This may be good or bad. I suspect it's bad but would be willing to be convinced it wasn't.
The above needs work and, the hard bit, proposed solutions. That last is what I've been stuck on.

There was a Twilight fanfiction with 55,000 reviews but it seems to have disappeared off FF.net; I don't know what was #2.

As of 4:17am PDT, HPMOR is the #1 most-reviewed Harry Potter fanfiction on the entire Internet.

Which one's this an exercise for?

Skill: Being strategic. I.e., life-consequentialism, where you actually do things based on their expected future consequences, as opposed to drifting into a PhD program because your friends are doing it.
Exercise materials: We either need to develop efficient probes for getting people to list out major life choices that they could actually remake (are under serious reconsideration) or we need to develop hypothetical life stories and policy decisions to use in exercises.

Subskill: Musashi's "cut through in the same motion".

Subskill: Use fungibility. There are different ways to achieve many goals. E.g. instead of wasting an evening with a relative in a way you resent, send them a postcard.

Subskill: Before the final moment of doing something that has any sort of cost or downside, ask whether you're doing it because of its consequence or merely because you previously decided to do it.

Exercise: Before ordering food in a restaurant, check whether you want the food, or just have a cached belief that you like it.

Skill: Being strategic. I.e., life-consequentialism, where you actually do things based on their expected future consequences, as opposed to drifting into a PhD program because your friends are doing it.
Exercise materials: We either need to develop efficient probes for getting people to list out major life choices that they could actually remake (are under serious reconsideration) or we need to develop hypothetical life stories and policy decisions to use in exercises.

Subskill: Notice foreign goals. Are your parents making you do it? Are you doing it because you read it in a book? Did you just drift into doing that?
Converse subskill: Notice feeling of actually caring about something. Notice whether that caring is giving energy to what you're doing. Notice its absence.

Subskill: Maximize on big things, satisfice on small things.

Running list of major skill areas not yet discussed:

Curiosity - the true and honest feeling, or failing that, how to get as close there as possible. Litany of Tarski, "ask whether, not why", Update Yourself Incrementally, etc.
Bottom-line stuff - noticing when you already know your destination, hold off on proposing solutions, etc.
Connecting belief to anticipation.
Use-of-words skills - people trying to milk definitional arguments for inferences, etc.
Empiricism - keeping a constant eye out for ways to test things. Not big official scientific reliable tests, just keeping your eyes open.
Concreteness / specificity - managing your levels of abstraction (this is surprisingly important in practice).
Productivity - adding new good habits, breaking old bad habits, scholarship
Argument flow awareness - things like motivated stopping, motivated continuation, flinching away from a counterargument that might carry; also positive aspects like knowing which question an argument is intended to resolve
Nonconformity
Cooperation
Saying oops
Munchkinism, minmaxing, "burn the spirit of the game", zs'hanh, assume the problem is solvable and continue solving it
Fun


Skill: Other introspection / reflectiveness. (Catchall for internal self-knowledge habits that don't go elsewhere.)

Subskill: Self-investigation of emotions, beliefs, anticipations. Forming hypotheses about what you feel, believe, or expect; questioning those hypotheses to see if they're actually true.

Subskill: Accept that an accurate description of yourself during any given frame-instant will involve some emotions, reasons why you act, and reasons why you believe, which are less than perfectly virtuous.

Don't lie about your life story to yourself - don't spruce up your history to make yourself look better. It may be easier to practice the above skill on the past than on the present, and it gets you used to the general idea.
Accepting that "What is the cause of my belief?" and "What is the justification of my belief?" will not always have the same answers - you have to accept this non-virtuous fact before you can properly process them as different questions.


Exercise: Find at least one thing which you do for less than perfectly noble reasons, which you are going to admit to everyone else in the study group, and not fix for at least a week. Everyone else in the study group will admit something similar to you. Possibly followed by group hug. (The idea isn't that any given flaw is okay, the idea is that it's okay to have a running accurate description of yourself which involves known flaws you haven't fixed yet, and all of you are in that same boat together.)

Subskill: Avoid pitfalls of verbal strategic reasoning.

List consequences without shifting from intuitive-sum to verbal-justification mode.
Don't exacerbate scope insensitivity or attending to rare events.

There are studies showing that people who consider their decisions more make worse decisions. As I understand it, the main explanation for this is that people shift from an intuitive sum of costs and benefits, to seeking verbally justifiable decisions, which in turn might lead them to one-reason-decisionmaking, ignoring some of their costs and benefits which are important to them but seem less "sensible". I also suspect it may exacerbate other biases like scope insensitivity or rare events - thinking about cases which are rare or short in duration.
The classic case being "Let's get a bigger house, further away from work, so it has an extra bedroom in case Grandma comes over", which she does once a year, but the 20 minutes of extra commute time happen every day and are not acclimated-to.

Exercise A: Give somebody two hypothetical package deals to choose from. First, have them choose quickly and intuitively. Then, have them think about consequences and list out desiderata and alternatives... but at the end of that, have them do the intuitive sum and state a preference, rather than coming up with a verbal reason for the decision.
Exercise B: Have some of the desiderata be rare cases or cases of short duration. Detect these, cross them out with a black marker.

Subskill: Chain from feelings of angst and frustration into saying, "I need to be strategic!" and the other skills listed.

Subskill: Unbundling; optimize separate things separately.
Example 1: Optimize fuzzies and utilons separately.
Example 2: Optimize grades and learning separately (instead of just optimizing grades, or haphazardly optimizing both at the same time).
5SL: Notice when you're optimizing two things at once. (Maybe because you (a) have a sense of awkwardness or of not getting enough done, and when you list out the desirable consequences, there's more than one?) Then, find two different things you can do which optimizes each one individually and without worrying about the other one.

Subskill: Don't express emotions as policy decisions. (Anna.) Find some other outlet for the emotion besides the policy decision, i.e., screaming. (Eliezer.)

Exercise: Recall acts you've done while your emotions were running high, in cases where it seems like something that might be worth optimizing. Of those acts, ask whether the action/policy/response can best be interpreted as maximizing a worthwhile criterion, or as direct expressions of the emotion.

Skill: Being strategic. I.e., life-consequentialism, where you actually do things based on their expected future consequences, as opposed to drifting into a PhD program because your friends are doing it.
Exercise materials: We either need to develop efficient probes for getting people to list out major life choices that they could actually remake (are under serious reconsideration) or we need to develop hypothetical life stories and policy decisions to use in exercises.

Subskill: Detach from sunk costs.
Material for exercises: Requires a case where the person has sunk costs and the option of continuing or not continuing. Might want to choose cases slightly less fraught than a marriage or a PhD program - you want to work up to those gradually.

Exercise A: For a case where sunk costs exist, imagine that you are a new person who was just now teleported into this person's life. Variant: Imagine you were just teleported into this person's life, and everyone else knows this, and they all expect you to execute sudden changes of course. See what this changes about your thinking, regardless of whether it changes your decision.
Exercise B: For whatever sunk costs you're in the middle of expending, look at the same scenario and rephrase it as a sunk benefit, the purchased option to finish a task more quickly than before. E.g., if you already paid $100 on a $150 item, change from "I paid $100" to "I now have the option of purchasing this item for $50". Again, the stated objective of the exercise will be, "Notice the difference in your thinking", not, "try to change your decision".
Exercise C: As above, but imagine that you bought the option for a penny on eBay. E.g. if you're one year into a four-year PhD program, imagine that you paid a penny on eBay to purchase an option to get a PhD in three years rather than the usual four years. Would you exercise that option if you paid a penny for it?

Subskill: Before the final moment of doing something that has any sort of cost or downside, ask whether you're doing it because of its consequence or merely because you previously decided to do it.

Subskill: "What is the consequence, what is the goal?"

Exercise A: For some policy that someone is carrying out today, starting as close to the object level as possible (answering an email, making a phone call, buying something at the store), ask about the consequences, and the consequences of the consequences. Identify the consequences that are desirable or that the action is being carried out for-the-sake-of. State the goal in abstract terms. Ask whether achieving the goal has further consequences - even things terminally desirable often have other, instrumentally desirable or undesirable consequences. Trace out the chain of specific events and the abstract instrumental and terminal goals they correspond to.
Exercise B: For each goal node, find some other policy - not necessarily a superior policy, but some other policy - that would be helpful for the same goal, not necessarily in the same way. (The point being to unanchor your concept of that goal from the exact, specific means of achieving it. This also obviously starts on the habit of searching for superior alternatives.)

Anna's subskill list from "Humans Are Not Automatically Strategic":

Ask ourselves what we're trying to achieve; 
Ask ourselves how we could tell if we achieved it ("what does it look like to be a good comedian?") and how we can track progress; 
Find ourselves strongly, intrinsically curious about information that would help us achieve our goal; 
Gather that information (e.g., by asking as how folks commonly achieve our goal, or similar goals, or by tallying which strategies have and haven't worked for us in the past); 
Systematically test many different conjectures for how to achieve the goals, including methods that aren't habitual for us, while tracking which ones do and don't work; 
Focus most of the energy that isn't going into systematic exploration, on the methods that work best;
Make sure that our "goal" is really our goal, that we coherently want it and are not constrained by fears or by uncertainty as to whether it is worth the effort, and that we have thought through any questions and decisions in advance so they won't continually sap our energies;
Use environmental cues and social contexts to bolster our motivation, so we can keep working effectively in the face of intermittent frustrations, or temptations based in hyperbolic discounting.


Skill: Other introspection / reflectiveness. (Catchall for internal self-knowledge habits that don't go elsewhere.)

Subskill: Notice new judgments and ask about them. E.g., "Huh, I have a bad feeling about this person cleaning this apartment."

Perceptually notice the new judgment, emotion, or intuition.
Put a box around it, that is, try to describe what you just felt in verbal language, to promote it to primary awareness.
Wonder where the feeling came from - search for internal causes.
Ask about the reference class - the general reliability of 'feelings like this'.


Skill: Other introspection / reflectiveness. (Catchall for internal self-knowledge habits that don't go elsewhere.)

Exercise: Acting lessons.

Skills this might teach:
Body language / vocal tone / facial expressions, awareness and control.
Awareness of social roles as you are carrying them out.
Social confidence, possibly performance in front of crowds (if you actually perform).

Exercise: PUA. Amy suggests that an exercise of equivalent difficulty for women might be getting a handsome guy to buy you a drink without promising him anything else.

Skills this may teach / has successfully taught:
Social confidence.
General self-confidence.
Body language, vocal tone, facial expressions.
Charisma.
Navigating social conversations.
Willingness to hug people at LW meetups.

Exercise: Improv (improvised comedy).

Skills this might teach:
Speed, non-hesitation.
Originality.
Rapid adaptation to shifts of context.
Social confidence.
Some of the same things as acting lessons?

Exercise: Say "Ducks" whenever someone sneezes.
(No, this was not my idea. Jasen again.)

Exercise: Say "Greetings" or "Salutations" instead of "Hello" or "How are you", because people don't sneeze often enough. Also note that "How are you" itself is an unusually autopiloted question.
(From Shannon.)

Skill: Mindfulness / not doing things on autopilot.
An underlying capacity that seems worth exercises to train explicitly, if we can figure out how to do that.

Subskill / contributing talent: Maintain a focus of attention. (Exercise: Dual N-back.)

Of course not.

Subskill: Maintain awareness of things that would ordinarily zip right past.

Exercise A: Have a set of hand signals describing conversational modes and use them during conversation; something along the lines of the Philosophy Referee signals only more relevant, like a hand signal meaning "You are attempting to refute what I just said" or "I am accepting that implicit premise."
Exercise B: Hand signals to describe body language, tone of voice, facial expressions.
(What other continuously changing variables would be good to learn to pay attention to?)

Skill: Mindfulness / not doing things on autopilot.
An underlying capacity that seems worth exercises to train explicitly, if we can figure out how to do that.

Subskill: Retrain habitual responses.

Exercise: Say "Ducks" whenever someone sneezes.
(No, this was not my idea. Jasen again.)

Subskill: Meditation.
Exercise: Meditation.
(Jasen's bailiwick.)

Skill: Anti-rationalization (1): Prevent your mind from selectively searching for support of only one side of an argument.

Subskill: Analyze the underlying reasons why you're trying to rationalize for or against something - why a conclusion feels required, or disallowed.
Important subskill: Notice when a candidate "the reason I'm trying to rationalize something" is a poor guess or itself a rationalization - when it's a guess that sounds plausible about someone like you in your position, but doesn't seem to ring true.

Subskill: Notice the process of selectively searching for support, and halt it. Chains into "Ask whether, not why", or into trying to unbind the need to rationalize.
Level 1, notice the active process after performing a conscious check whether it's running; Level 2, detect it perceptually and automatically; Level 3, never run this process.

Exercise material: List of suggested conclusions to rationalize. I'm not quite sure what the prerequisites here would be; one of my own childhood epiphanies was finding that I could argue reasons why there ought to be a teapot in the asteroid belt, and thinking to myself, "Well, I better not do that, then." I suspect the primary desideratum would be more that the topic offers plenty of opportunity to come up with clever rationalizations, rather than that the topic offers motivation to come up with clever rationalizations.
Exercise A: Pick a conclusion from the list. Come up with a clever argument why it is true. Notice what it feels like to do this. Then don't do it.
(This is distinct from noticing the feeling of being tempted to rationalize, a separate subskill.)
Exercise B: In the middle of coming up with clever arguments why something is true, stop and chain into the Litany of Tarski, or some other remedy. Variant: Say out loud "Ew!" or "Oops!" during the stop part.
Exercise C: For a conclusion on the list, argue that it is true. Then "Ask whether, not why" - try to figure out whether it is true. Notice the difference between these two processes. Requires a question whose answer is less than immediately obvious, but which someone can find info about by searching their memory and/or the Internet.

Subskill: Be able to identify the internal feeling of having a required conclusion, of an argument only having one allowed answer, and (a slightly different internal sensation) of other answers being disallowed.
Level one: After this skill is explicitly mentioned and invoked by some other process, be able to notice this internal sense of required-conclusion-ness (disallowed-ness) when you consciously focus on it.
Level two: Have a constant perceptual eye out for feelings like this, notice automatically without needing to be "on guard", chain into applying other anti-rationalization skills (e.g. Litany of Tarski).

Exercise idea: For items on the hot-topic list, identify ones that you care about, and:
Exercise A: Try to identify directly, just from looking at the issue and imagining the potential answers to it, the emotional sense that there's only one allowed answer to it, and the emotional sense that a different answer is not allowed.
Exercise B: Imagine being in the process of losing an argument about that issue. Identify the drive (desperation, need) to regain the lost territory and win. Then imagine being in the process of winning an argument about that issue. Identify the sense of triumph and the prior commitment which makes that particular conclusion "winning".
Exercise C: Get into a simulated argument about the issue with someone taking the opposite side from the one you care about. Maintain awareness of your overall emotional state, try to be aware of the internal drive to produce a particular answer, be aware of the sense of revulsion or flinch-away that associates with other answers.

Skill: Anti-rationalization (1): Prevent your mind from selectively searching for support of only one side of an argument.

Exercise material (prerequisite for multiple exercises below): Have a hot-topic list such that incoming students at the expected level (e.g. level = typical LW reader) would be tempted to rationalize at least some of them. This requires both that someone care about the topic, and that the topic isn't so cut-and-dry that there's no temptation to distort anything. E.g., I care about atheism but I don't have any emotional fear of that argument coming out "the wrong way" - on the other hand, putting me in an actual argument with, say, my parents, or someone who was a really clever theistic arguer in front of an audience, might generate the emotional temptation to cheat to ensure winning on every single point.

Subskill: Be able to identify the internal feeling of having a required conclusion, of an argument only having one allowed answer, and (a slightly different internal sensation) of other answers being disallowed.
Level one: After this skill is explicitly mentioned and invoked by some other process, be able to notice this internal sense of required-conclusion-ness (disallowed-ness) when you consciously focus on it.
Level two: Have a constant perceptual eye out for feelings like this, notice automatically without needing to be "on guard", chain into applying other anti-rationalization skills (e.g. Litany of Tarski).

Um... I may be a bit prejudiced, here, especially considering that this has been highly upvoted, but I have to admit I have two major problems with promoting this post:
1) It says "Constraining Anticipation" in the title, and after reading it, I cannot think of anything I expect to see happen.
2) More importantly, I don't feel I know anything about rationality which I didn't know earlier. I'm not sure LW should be in the business of fully general scientific intros.

Upvoted, but I can't begin to imagine how you take your data to be an argument that cryonics is too costly. 0.5% of planetary GDP to be a post-death society?
Also, a society which actually embraced cryonics would save huge amounts on horribly painful and expensive medical treatments that have no hope of saving the patient but only keep them alive for another month on a ventilator. There's no way that a society which actually understood cryonics would be spending more money on healthcare, net, than we do.

Liquid nitrogen is cheap, and heat loss scales as the 2/3 power of volume. Cryonically preserving 150,000 people per day would, I fully expect, be vastly cheaper than anything else we could do to combat death.

The bound would also have to be substantially less than 3^^^^3.

You know how there are people who, even though you could train them to carry out the steps of a Universal Turing Machine, you can't manage to teach them linear algebra, so there are problems they can't even represent compared to people who know linear algebra? I can't exhibit to you specifically what it is for obvious reasons, but there's going to be lots of stuff like that where a human brain just can't grok it, even though - like a sufficiently well-trained dog - we could be trained to carry out the operations of a UTM that did grok it, given infinite time and paper. You could train a chimp to simulate a human brain given infinite time and paper, I've little doubt. So what? There was still a huge jump in qualitative ability.

Doesn't have a name as far as I know. But I'm not sure it deserves one; would CDT really be a probable output anywhere besides a verbal theory advocated by human philosophers in our own Everett branch? Maybe, now that I think about it, but even so, does it matter?

A causal decision theorist can have an accurate abstract understanding of both TDT and CDT and can calculate the expected utility of applying either.

But it will calculate that expected value using CDT!expectation, meaning that it won't see how self-modifying to be a timeless decision theorist could possibly affect what's already in the box, etcetera.

Yes, you're a freak and nobody but you and a few other freaks can ever get any useful thinking done and didn't we sort of cover this territory already?

You said:

Causal decision theorists don't self-modify to timeless decision theorists. If you get the decision theory wrong, you can't rely on it repairing itself.

but you also said:

...if you build an AI that two-boxes on Newcomb's Problem, it will self-modify to one-box on Newcomb's Problem, if the AI considers in advance that it might face such a situation.

I can envision several possibilities:

Perhaps you changed your mind and presently disagree with one of the above two statements.
Perhaps you didn't mean a causal AI in the second quote. In that case I have no idea what you meant.
Perhaps Newcomb's problem is the wrong example, and there's some other example motivating TDT that a self-modifying causal agent would deal with incorrectly.
Perhaps you have a model of causal decision theory that makes self-modification impossible in principle. That would make your first statement above true, in a useless sort of way, so I hope you didn't mean that.

Would you like to clarify?

Causal decision theorists self-modify to one-box on Newcomb's Problem with Omegas that looked at their source code after the self-modification took place; i.e., if the causal decision theorist self-modifies at 7am, it will self-modify to one-box with Omegas that looked at the code after 7am and two-box otherwise. This is not only ugly but also has worse implications for e.g. meeting an alien AI who wants to cooperate with you, or worse, an alien AI that is trying to blackmail you.
Bad decision theories don't necessarily self-repair correctly.
And in general, every time you throw up your hands in the air and say, "I don't know how to solve this problem, nor do I understand the exact structure of the calculation my computer program will perform in the course of solving this problem, nor can I state a mathematically precise meta-question, but I'm going to rely on the AI solving it for me 'cause it's supposed to be super-smart," you may very possibly be about to screw up really damned hard. I mean, that's what Eliezer-1999 thought you could say about "morality".

Causal decision theorists don't self-modify to timeless decision theorists. If you get the decision theory wrong, you can't rely on it repairing itself.


(3) If it were the case that (not-P), S would not believe that P
(4) If it were the case that P, S would believe that P

I'm genuinely surprised. Condition 4 seems blatantly unnecessary and I had thought analytic philosophers (and Nozick in particular) more competent than that. Am I missing something?

Yep. Another case in point of the danger of replying, "Tell me how you define X, and I'll tell you the answer" is Parfit in Reason and Persons concluding that whether or not an atom-by-atom duplicate constructed from you is "you" depends on how you define "you". Actually it turns out that there is a definite answer and the answer is knowably yes, because everything Parfit reasoned about "indexical identity" is sheer physical nonsense in a world built on configurations and amplitudes instead of Newtonian billiard balls.
PS: Very Tarskian and Bayesian of them, but are you sure they didn't say, "A belief in X is knowledge if one would never have it whenever not-X"?


According to the provided link, zs'hanh means "contemptuous indifference to the activity of others". I'm not sure how that's supposed to apply here, since the entire subject of discussion is the activity of others (namely, Gary Drescher's writings).

Not those others.

I work 4-5 hours at a stretch when writing.
By the way, I think we can all recognize this as the leading criticism of my ideas, to which all newcomers, requesting to know what my critics have said in response to me, should be directed.


"And it concerns me that neither the evidentialist camp nor the causalist camp seem to see a need to rebut or comment on Drescher's ideas."

Doesn't concern me in even the tiniest, most infinitesimal amount. Remind me to post on the rationalist virtue of zs'hanh at some point.
Difference between PD and one-shot Newcomb: Agree the incentives are different; agree that the logical structure of the problem is potentially more complicated because of that; suggest that the decision to expend cognitive resources searching for a way to defect could be treated as a defection or a probabilistic defection itself.
Drescher on subjunctives - I agree, this strikes me more as Drescher trying to make partial progress toward a solution than presenting something well-defined in a logical sense. I'm not sure Drescher would disagree with that.
I've spoken to Drescher at length and I think he's trying to derive way too much "ought" from TDT, to the point of thinking TDT yields morality itself.
That said, "Good and Real" is still the reductionist book for now.

Rational expected-utility-maximizing agents get to care about whatever the hell they want. Downvoted.


In general, though, I am sceptical that "producing world-saving actions" is what we should be aiming for. Maybe I am biased by the fact that I am a cautious person, but I think that if only we could make everyone a lot more cautious

Aaaand not to put too fine a point on it, but how much research is that caution getting done, exactly? Philanthropic donations produced by this philosophy? Anything?

Well, like I said. How's that careful avoidance of any phrasing that potentially smacks of egotism, working out for you in terms of producing world-saving actions?

If I have a choice between actions, and one of them is more likely to save the world than the other, I will take the one that is more likely to save the world.
Even I don't live up to that every time, not even close, but it sure sounds a lot scarier than "wanting to save the world", doesn't it?

I would very strongly advise that you donate something while you're trying to accumulate money. Otherwise I would bet against a generic person in your situation ever following through (Outside View).

I praise this right action.

How's that philosophy working out for you in terms of producing world-saving actions?

If this doesn't work, give it another shot, after declaring your meetup before the Friday before your meetup. As it stands it's a bit late to be put on the Meetup collection page, though I'm putting it there anyway.

Which of the Sequences have you read so far?

Nope, he showed up at a Thursday LW meetup in Mountain View and he was like "Actually I just got back from a two-year stint organizing self-sustaining Mormon communities in India" and I was like "Awesome, got any advice for us?" and he was like "Yeah" and then it became clear the discussion was going to go on for a while and we decided to reconvene Tuesday so we could talk in detail.

Could you please be more specific? What sort of action is being taken here as a result of your worry?


I would suggest listing the cities in alphabetical order in the heading rather than time order.

Sounds like a good idea to me too.

Erm, that's supposing the religious person would actually want to suicide or do the ridiculous thing, rather than this itself being an expression of belief, affirmation, and argument of the religion. (I.e., as appeal to consequences, or saying negative things about the negation.)


This isn't an organisation, it's a blog.

Some of us would like a %$^&ing organization, pardon my French.

I kind of think that's already happening. All over the place. All the time. What kind of policy implications did you want to draw from it in this particular instance?

I kinda disagree here. If you show me an exact Bayesian network, I can read off it the degree to which evidence for one proposition is evidence against another. If you don't give an exact interpretation in probability theory, then isn't talking about "probability" instead of "power" just pretending to precision? Jumping to "probability" is something that has to be earned, and to me it's not yet obvious that for all Bayesian graphs, if P(A) > P(B) > 0.5, then learning the truth of a descendant node which proves !(A & B) will cause B to decrease in probability more than A.


I'm further surprised that the LessWrong community at large was so enthusiastic in upvoting these insights into how to seduce impressionable people into a false, irrational, and personally costly religious cult.

Because "Telling people to greet first-time attendees and be nice to them vastly improves the rate at which new attendees come back" is useful for seducing people into attending Less Wrong meetups as well as costly religious cults. I wouldn't exactly call it Dark Arts, either.
We've been considering learning from Toastmasters too. If we ever want to be more effective than an online discussion, we need to go learn from (not imitate) real-world groups that are more effective than that.

Because Divia and Will and I talked to him for a couple of hours and he had tremendously useful practical advice, like "Telling people to greet first-time newcomers and be nice to them is the difference between a 50% retention rate and a 90% retention rate."

I think you're confusing the criticism "This evidence is not surprising enough to be strong evidence that lifts the prior improbability of Mormonism" with the criticism "You are not answering this question honestly." The answer was to the point. It doesn't lift Mormonism. It doesn't even come close. But it wasn't leaving anything out, I expect, because I expect that there isn't anything else.

I don't understand why you posted that paragraph.

Sixthed. Actually I'd be interested in hearing from anyone who wouldn't so promise.
The only caveat is that I'd have to be separately convinced of the factual and ethical sides, i.e., showing evidence to me that the Mormon God exists is not the same as justifying that the Mormon God's policies are good.


His allusions to his missionary work are provoking an immune response from many here, including me (not that I write much). I think this is why (from a quote thread):

I have not been particularly bothered by the missionary allusions but obviously don't consider the posts nearly as valuable as you do. There is an undesirable emphasis on norms and a constant pressure to move things in the direction of 'making the group do set projects' and 'consensus'. This isn't an organisation, it's a blog.


When you do this in person, hopefully you give them as long as they'd like to respond?

30 seconds is actually a pretty long time in conversation, so if they don't say anything like "I'm thinking" I will have probably made a suggestion myself, or asked other questions intended to narrow things down.

Confirmed true.

I did it.

Yup, even a business shirt and tie. But I think the point is that it's the consistency which creates the vulnerability. Suppose we took the look of anyone in our organization, including say Lukeprog, and duplicated it on all the members...

When I use this technique I'm usually using it on someone who is perfectly capable of coming up with an example, and whose mind has just taken the path of least resistance; I do it to help them think and they usually think successfully, and I make suggestions if they don't. Using this to call bluffs, when I think the other person's got nothin', is a rarer practice.

The word "moralize" has now been eliminated from the blog post. Apparently putting a big warning sign up saying "Don't argue about how to verbally define this problem behavior, it won't be fun for anyone and it won't get us any closer to having a relaxed rationalist community where people worry less about stepping in potholes" wasn't enough.

I made the mistake of using a word for something people shouldn't do. Then they started disputing the definition of the word, even after I told them not to. I will edit to take out the evil word.

Most tools can be. Tools with moral dimensions are rare.

Agreed.

Upvoted for being the only one to try the exercise.

Not everything that is not purely consequentialist reasoning is moralizing. You can have consequentialist justifications of virtue ethics or even consequentialist justifications of deontological injunctions, and you are allowed to feel strongly about them, without moralizing. It's a 5-second-level emotional direction, not a philosophical style.
Sigh. This is why I said, "But trying to define exactly what constitutes 'moralizing' isn't going to get us any closer to having nice rationalist communities."

The goal invoked in the post, though, is to avoid moralizing in conversations between rationalists so that they don't feel like they're walking through a minefield. Having the anger and suppressing it, doesn't work for that. The person next to you is still walking the minefield. They're just not getting feedback.

"One step up and one step down" sounds like a valuable heuristic; it's what I actually did in the post, in fact. Upvoted.


The 5-second method is sufficiently general to coax someone into believing any world view, not just a rationalist one.

Um, yes. This is supposed to increase your general ability to teach a human to do anything, good or bad. In much the same way, having lots of electricity increases your general ability to do anything that requires electricity, good or bad. This does not make electrical generation a Dark Art.

Actually, it occurs to me that this can be generalized. We might feel morally worried about a technique for initial epistemic persuasion which can operate equally to convince people of true statements or false statements, which is being used without the person's knowledge and before they've come to an initial decision about the worth of the idea (i.e., it's not like they already believe it and you're trying to help them alieve it). This is what some people (not me, please note) termed the Dark Arts.
Instrumental techniques which are useful for accomplishing anything, good or bad, depending on the user's utility function? Those are fine. Those are great. Nothing Dark about them.

IAWYC, and introspective access to what my mind was doing on this timescale was one of the bigger benefits I got out of meditation. (Note: Probably not one of the types of meditation you've read about). However, I don't think you've correctly identified what went wrong in the example with red. Consider this analogous conversation:
What's a Slider? It's a Widget. 
What's a Widget? It's a Drawable. 
What's a Drawable? It's an Object.
In this example, as with the red/color example, the first question and answer was useful and relevant (albeit incomplete), while the next two were useless. The lesson you seem to have drawn from this is that looking down (subclassward) is good, and looking up (superclassward) is bad. The lesson I draw from this is that relevance falls off rapidly with distance, and that each successive explanation should be of a different type. It is better to look a short distance in each direction rather than to look far in any one direction. Compare:
X is a color. This object is X. (One step up, one step down) 
X is a color. A color is a quality that things have. (Two steps up) 
This object is X. That object is also X. (Two steps down)
I would expect the first of these three explanations to succeed, and the other two to fail miserably.

I fully expect that less than 0.1% of mathematicians are working on math anywhere near as important as starting a chain of paleo coffee shops. What are you working on?

Well-written!


I've never met Eliezer, but although he comes off as extremely intelligent, judging by his writings and level of achievement (which are impressive) he still does not come off to me as, say, Von Neumann intelligent.

Wouldn't dispute that.

In accordance with the general fact that "calories in - calories out" is complete bullshit, I've had to learn that sweet things are not their caloric content, they are pharmaceutical weight-gain pills with effects far in excess of their stated caloric content. So no, I wouldn't be able to eat a triple chocolate muffin, or chocolate cake, or a donut, etcetera. But yes, when I still believed the bullshit and thought the cost was just the stated caloric content, I sometimes didn't resist.

Yep. The way it actually works is that I'm on the critical path for our organizational mission, and paying me less would require me to do things that take up time and energy in order to get by with a smaller income. Then, assuming all goes well, future intergalactic civilizations would look back and think this was incredibly stupid; in much the same way that letting billions of person-containing brains rot in graves, and humanity allocating less than a million dollars per year to the Singularity Institute, would predictably look pretty stupid in retrospect. At Singularity Institute board meetings we at least try not to do things which will predictably make future intergalactic civilizations think we were being willfully stupid. That's all there is to it, and no more.

So there wasn't anything especially related to explicit rationality in the quote. Or rather, it wasn't extraordinarily rational enough to make up for its extraordinary politicalness.


If you think something's supposed to hurt, you're less likely to notice if you're doing it wrong. -- Paul Graham


Cryonics should just work if everything we currently already believe about the brain is true and there are no surprises. It is not a small probability. It is the default mainline probability.


What he should have done was contingently committed to selling his organs on the black market before committing suicide. Then, there would have been a net benefit to his death, instead of it being zero-sum, and his actions would have been admirable.

Does not follow - the breakup value of your organs is not necessarily greater than your organs working together. Just because someone gets paid doesn't mean that game is positive-sum.

The reason my salary shows as $95K in 2009 is that Paychex screwed up and paid my first month of salary for 2010 in the 2009 tax year. My actual salary was, I believe, constant or roughly so through 2008-2010.

Erm, if this is purporting to be for the whole Bay Area, you ought to list the Tortuga weekly meetups too?

As direct moderator censorship seems to provoke a lot of bad feeling, I would encourage everyone to downvote this to oblivion, or for the original poster to voluntarily delete it, for reasons given in highly upvoted comments below. Or search on "UTTERLY FUCKING STUPID", without quotes.

Long, long ago, I think at around age nine or something, I was taking a Tae Kwon Do class.
At one point the master said to me, "Attack me."
I gave him a skeptical look.
He said, "Attack me the way you would attack me in real life."
I raised my finger and thumb, pointed it at him, and said "Bang."
He laughed and called up the next student.

I think that willingness to burn the spirit of the game when it comes to things like signing up for cryonics instead of confronting the inevitability of your mortality, drinking extra-light olive oil instead of trying to diet by sheer willpower, or building a recursively self-improving AI instead of trying to solve the world's problems the normal way, is exactly what distinguishes Munchkinism from mere hacking.


The Lowenheim-Skolem theorem says (loosely interpreted) that even if you CLAIM to be talking about uncountably infinite things, there's a perfectly self-consistent interpretation of your talk that refers only to finite things (e.g. your definitions and proofs themselves).

Only in first-order logic. In second-order logic, you can actually talk about the natural numbers as distinguished from any other collection, and the uncountable reals.
Amusingly, if you insist that we are only allowed to talk in first-order logic, it is impossible for you to talk about the property "finite", since there is no first-order formula which expresses this property. (Follows from the Compactness Theorem for first-order logic - any set of first-order formulae which are true of unboundedly large finite collections also have models of arbitrarily large infinite cardinality.) Without second-order logic there is no way to talk about this property of "finiteness", or for that matter "countability", which you seem to think is so important.

Is this ("It has a small probability of success, but we should pursue it, because the probability if we don't try is zero") not a standard pro-cryonics argument? Given a sufficiently large expected payoff, it seems perfectly valid ...

Okay. Professor McGonagall knows that Harry Potter is the one with the power to vanquish the Dark Lord, that he is marked as the Dark Lord's equal, and that he will have power the Dark Lord knows not. (She has, in fact, heard that Prophecy spoken in the terrible hollow echoing voice of Sybill Trelawney.)
Harry isn't acting like a normal eleven-year-old, or any kind of eleven-year-old, and Professor McGonagall has noticed that as well, in as many words.
That's all. If you think, under those circumstances, that the boy ought to be given the back of your hand and told to shut up, you're welcome to write your own fanfiction where that's what Professor McGonagall does. The further consequences seem predictable enough. The thing is, in my story, Professor McGonagall can see that too.

In case you didn't notice due to lack of Potterverse familiarity, it was established back in chapter 5 that Professor McGonagall knows the prophecy.

Eek, what did I write? I got the last one wrong first time around as well.

You didn't include a date.

Edited to read "Sunday May 1", change this if incorrect.

You can encode Turing machines in Life.

Zvi is one of the leaders of the New York Less Wrong community, actually. Munchkinism generalizes.
I played Magic against Zvi using one of his own decks, and the deck won - I was there, but I wasn't involved.

Vassar's story sounds like a parable. Is it real?

You seem to be optimizing this entire problem for avoiding the mental pain of worrying about whether you're being cheated. This is the wrong optimization criterion.

Large air-conditioned living space, healthy food, both for 2 people (myself and girlfriend). My salary is at rough equilibrium with my spending; I am not saving for retirement. The Bay Area is, generally speaking, expensive.

Hm. $95K still sounds too high, but if I recall correctly, owing to a screwup in our payments processor at that time, my salary for the month of January 2010 was counted into the 2009 tax year instead of 2010.
No apology is required; you wrote without malice.


Where did you think the money was going?!

They thought it wasn't going to paying me $180K. Correctly. +1 epistemic point to everyone who expressed surprise at this nonfact, -1 point for hindsight bias to anyone who claimed not to be shocked by it.

Should've noticed your own confusion, that didn't actually happen.
http://lesswrong.com/r/discussion/lw/5fo/siai_fundraising/40v2
(Base compensation rates of increase sound about right, though.)


Note that it appears that Eliezer was paid $83,934 in 2009 as additional compensation for the completion of the Sequences.

Okay, that didn't happen. I got my standard salary in 2009, no more. I think my standard salary must've been put down as payment for the Sequences... or something; I don't know. But I didn't get anything but my standard salary in 2009 and $84K sounds right for the total of that salary.

Not really. It involves the ability to do things that would make other people look at you funny, and a relentlessly optimizing attitude toward all of real life and not just computer science problems or particular locks. There may be something more to it, too. In any case Timothy Ferriss != John McCarthy (albeit McCarthy himself may also have the Munchkin-nature) and people who build championship Magic decks don't think in quite the same way as great programming hackers, though you can also be both.

You should definitely write at least the first post in this attempted sequence; either it will work or it won't.
Though I will advise that you lead with your strongest and most useful point - you can try writing things in an optimal educational order after that; first you have to hook your readers.

Erm, maybe my standards are too high, but this didn't seem overwhelmingly well-written as fiction and I really worry when material that attacks a target that's supposed to be attacked gets a free pass as art. Or maybe you all actually enjoyed that, and I'm being unreasonable in expecting blog comments to meet publishable quality standards.

Feel free to make concrete alternative suggestions. "Hacking" is taken.

It's not the same thing. Picking locks is a hack. Cryonics is something more, which is why even most people who can pick locks don't go for it.

In top-level posts, automatically replace "div" tags (which screw up the rest of the HTML) with "p", strip out all the font-specification crap that Microsoft Word and similar apps try to stuff in (the original font is good enough for everyone), and in general auto-simplify the HTML. This will save editors some work.

Hm. I think it's fair to say that I would probably be about equally reluctant to wreck any other artwork containing an equal amount of painstaking effort.
(Whew!)

Interesting. I seem to have the same flinch effect JoshuaZ described, despite believing that religion in general and Judaism in particular are great evils of the world which separated my family from me.

Post removed from main and discussion on grounds that I've never seen anything voted down that far before. Page will still be accessible to those who know the address.


This. In fact you should still be able to login and edit, in case you put specific private info in a comment you want to delete. Just your display name should change to (Account deleted).

Your preferences in this thread appear incompatible with each other.

Hm. True. Perhaps someone could request an unlock from a moderator. The way the interface is currently set up, I can't edit other user's comments.

This. In fact you should still be able to login and edit, in case you put specific private info in a comment you want to delete. Just your display name should change to (Account deleted).

I would like to see edits frozen after 24 hours.

This comment is more likely if Douglas Hofstadter is Clippy than if he isn't.


One who is satisfied cannot be seduced.

Erm, this statement is clearly false as soon as you reflect on it?
Personally, if I was going to come up with a clever rationalization for BFDs, it would be something like, "Any boyfriend who keeps her locked up in a closed relationship must clearly be a patriarchal bastard."

Well, in true fact, there is no lost planet of Magrathea.


Do you know any counterpart to the Solomonoff distribution for universes based on higher-order logic?

That I didn't invent? No.

Worst case, nobody shows up and you waste one (1) evening.

Yes, of course.

My current view on this is that you can shift to the language of higher-order logic, with the ordinals for the order of logic that are provable in some proof system. Not sure you can do any better of that.

Note for future customs: When you're ready to declare a specific place and time, take it to the main section, anything before that should go to the Discussion section.

Don't ask. It doesn't work when you ask. Pick a time, pick a place, commit to being there with an identifying book (like Demon-Haunted World or something). Worst case, nobody shows up. If anyone prefers meeting in another time or place, they can say so in the comments.

The point is that it's not commonly internalized to the point where someone will correctly use DAG as a synonym for "universe".

I can't see how a small Python script that calculates pi could assign utility to anything. It doesn't replan in a complex way that implies a utility function. It calculates bleedin' pi.

Actually it takes longer than that.

Look, sometimes you've just got to do things because they're awesome.

Eliezer still does.

...that was written by a Less Wrong reader. Or if not, someone who independently reinvented things to well past the point where I want to talk to them. Do you know the author?

The "plus" is right, the main idea is wrong.

He thinks so. And in a situation like that, if you think so, you're probably right in thinking so.

But that's just wrong. If you're doing it right your mean creeps steadily upward and that's how you hit high points.

Well said.

Upvoted for Just Trying It.

Actually I think the next step is to run the experiment. It's a perfectly good experiment - even though I'd expect it not to happen inside a real school, because the students would complain they weren't being graded fairly.
Concept is still important enough that I promoted it immediately.


Depression is partly about not being able to resist (or even being attracted to) ideas which cause damage.

This sounds very true and important.

Excuse me, but XiXiDu is taking for granted ideas such as Pascal's Mugging - in fact Pascal's Mugging seems to be the main trope here - which were explicitly rejected by me and by most other LWians. We're not quite sure how to fix it, though Hanson's suggestion is pretty good, but we did reject Pascal's Mugging!
It's not obvious to me that after rejecting Pascal's Mugging there is anything left to say about XiXiDu's fears or any reason to reject expected utility maximization(!!!).

I don't tell people this very often. In fact I'm not sure I can recall ever telling anyone this before, but then I wouldn't necessarily remember it. But yes, in this case and in these exact circumstances, you need to get laid.

What on EARTH are you trying to -
Important note: Currently in NYC for 20 days with sole purpose of finding out how to make rationalists in Bay Area (and elsewhere) have as much fun as the ones in NYC. I am doing this because I want to save the world.

Just reading Church of the Subgenius literature, praise "Bob".

Long ago, I forget where, I saw a blog post that applied this to writing. It pointed out that if we model the quality of your writing as having a mean X and variance Y, then the only way to hit those unlikely exceptionally good texts is to write a lot. Yes, while doing so you might also come up with the same number of exceptionally bad texts, but nobody forces you to show those to anyone. Plus writing a lot will give you practice, gradually pushing up the mean.
From personal experience, I'd also err on the side of publishing even texts you're not personally all that impressed by. I've noticed that I'm relatively bad at estimating what's going to be popular. Some of my biggest hits have been blog posts I'd never have thought would be popular.

In my case? Yeah.


For others you see miracle-level improvements within days.

Where 'days' indicates 7 at the very least and 14 would still count as a 'rapid miracle level recovery'.

No, this was faster.
No, I really don't think the placebo effect did that.

It's possible, but I worry that our friendly local countersignalers are underestimating the power of being sane.

NYLW has done some preliminary testing, asking people what they think of when they hear the word "rational". So far the results have been positive.

Permission requested to move to main section, promote.

Note also that this could be transformed from "good" to "timeless classic" by going through, taking all paragraphs about something abstract, and inserting at least one concrete example into each of them.

Suggestion: Would FreeMind (open-source mindmapping software that produces trees) work for representing your graphs?
Request: Concrete example of some links.

Humean, yep.

I now count it as a major life error that I didn't tell someone to try Prozac five years earlier.
For some people it doesn't work.
For others you see miracle-level improvements within days.
Just try it and see what happens.


The philosophers I study under criticise the sciences for not being rigorous enough.

Acid test 1: Are they complaining about experimenters using arbitrary subjective "statistical significance" measures instead of Bayesian likelihood functions?
Acid test 2: Are they chiding physicists for not decisively discarding single-world interpretations of quantum mechanics?
Acid test 3: Are all of their own journals open-access?
It may be ad hominem tu quoque, but any discipline that doesn't pass the three acid tests has not impressed me with its superiority to our modern, massively flawed academic science.

Richard, I'm pretty sure I remember you treating the apparent conceivability of zombies as a primary fact about the conceivability of zombies to which you have direct access, rather than treating it as an output of some cognitive algorithm in your brain and asking what sort of thought process might have produced it.



Found it.

Looks better. Thank you!

...well that ain't a very large effect size in the direction of the hypothesis, even under the best conditions.
Don't suppose you've got "Changes in interpersonal perception as a means of reducing cognitive dissonance"?

Googling around from these citations did produce:
Glass, D. (1964). Changes in liking as a means of reducing cognitive discrepancies between self-esteem and aggression. Journal of Personality, 32, 531-549.
Which I think is what I'm looking for.


Yudkowsky said he believed there was a 5 percent chance the Singularity was about to happen and human existence would be forever changed.

Note: This is a LIE.
The correct quote was that I said on SL4 that when Douglas Lenat switched on Eurisko, essentially the first time anyone had ever built a Turing-complete freeform genuinely recursive self-modifier with heuristics modifying heuristics, he ought to have evaluated a 5% chance of it going FOOM.
I was 4 years old when Eurisko was switched on, and could not possibly have said anything at the time.
Declan McCullagh. Write it down. Never trust him.
No matter how many terrible things you've heard about the mainstream press, you truly cannot appreciate how bad it really, really is until you have been reported on yourself. It is at least two orders of magnitude worse than you think it is from reading Reddit.

Matches nothing once you tell it to search for exact spelling. Thankfully.

Too late. http://www.google.com/search?q=Yudkowskian

I realize that I am in the habit of occasionally answering "Yes?" when someone says "Oh, God" but even so: Ugh.

TDT doesn't think you can change the past either. TDT behaves as if it decides the past, which it does.

Didn't spend much time around you (yet) but I can (surprisingly, given my terrible memory) remember your face and I'm reasonably sure you weren't ugly. But then you listed this as a known irrationality, so you know that, right?

Got no idea what you mean by either of those clauses. Why wouldn't the most painful times in your life be someone else's well-meant disaster, forced on you without your control? And then the second clause I don't understand at all.

The last time someone told me "Good luck", I replied, "I don't believe in an ontologically fundamental tendency toward positive outcomes."

Depends on what your luck attribute is. I'm well above average, both in life and the poker hands I get dealt.
(Yes, median Less Wrong user, this comment is a joke, mostly. You need to play a large amount of hands for your variance to go down to acceptable levels, especially with no limit poker).

Tarski did philosophical work on truth? Apart from his mathematical logic work on truth? Haven't read it if so.
What does Talbot say about a cognitive algorithm generating the appearance of free will? Is it one of the cognitive algorithms referenced in the LW dissolution or a different one? Does Talbot talk about labeling possibilities as reachable? About causal models with separate nodes for self and physics? Can you please take a moment to be specific about this?

Stalnaker's name sounds familiar from Pearl, so I'll take your word for this and concede the point.

Done.


Quine's naturalized epistemology. Epistemology is a branch of cognitive science

Saying this may count as staking an exciting position in philosophy, already right there; but merely saying this doesn't shape my expectations about how people think, or tell me how to build an AI, or how to expect or do anything concrete that I couldn't do before, so from an LW perspective this isn't yet a move on the gameboard. At best it introduces a move on the gameboard.

Tarski on language and truth.

I know Tarski as a mathematician and have acknowledged my debt to him as a mathematician. Perhaps you can learn about him in philosophy, but that doesn't imply people should study philosophy if they will also run into Tarski by doing mathematics.

Chalmers' formalization of Good's intelligence explosion argument...

...was great for introducing mainstream academia to Good, but if you compare it to http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate then you'll see that most of the issues raised didn't fit into Chalmers's decomposition at all. Not suggesting that he should've done it differently in a first paper, but still, Chalmers's formalization doesn't yet represent most of the debates that have been done in this community. It's more an illustration of how far you have to simplify things down for the sake of getting published in the mainstream, than an argument that you ought to be learning this sort of thing from the mainstream.

Dennett on belief in belief.

Acknowledged and credited. Like Drescher, Dennett is one of the known exceptions.

Bratman on intention. Bratman's 1987 book on intention has been a major inspiration to AI researchers working on belief-desire-intention models of intelligent behavior...

Appears as a citation only in AIMA 2nd edition, described as a philosopher who approves of GOFAI. "Not all philosophers are critical of GOFAI, however; some are, in fact, ardent advocates and even practitioners... Michael Bratman has applied
his "belief-desire-intention" model of human psychology (Bratman, 1987) to AI research on planning (Bratman, 1992)." This is the only mention in the 2nd edition. Perhaps by the time they wrote the third edition they read more Bratman and figured that he could be used to describe work they had already done? Not exactly a "major inspiration", if so...

Functionalism and multiple realizability.

This comes under the heading of "things that rather a lot of computer programmers, though not all of them, can see as immediately obvious even if philosophers argue it afterward". I really don't think that computer programmers would be at a loss to understand that different systems can implement the same algorithm if not for Putnam and Lewis.

Explaining the cognitive processes that generate our intuitions... Talbot describes the project of his philosophy dissertation for USC this way: "...where psychological research indicates that certain intuitions are likely to be inaccurate, or that whole categories of intuitions are not good evidence, this will overall benefit philosophy."...

Same comment as for Quine: This might introduce interesting work, but while saying just this may count as an exciting philosophical position, it's not a move on the LW gameboard until you get to specifics. Then it's not a very impressive move unless it involves doing nonobvious reductionism, not just "Bias X might make philosophers want to believe in position Y". You are not being held to a special standard as Luke here; a friend named Kip Werking once did some work arguing that we have lots of cognitive biases pushing us to believe in libertarian free will that I thought made a nice illustration of the difference between LW-style decomposition of a cognitive algorithm and treating biases as an argument in the war of surface intuitions.

Pearl on causality.

Mathematician and AI researcher. He may have mentioned the philosophical literature in his book. It's what academics do. He may even have read the philosophers before he worked out the answer for himself. He may even have found that reading philosophers getting it wrong helped spur him to think about the problem and deduce the right answer by contrast - I've done some of that over the course of my career, though more in the early phases than the later phases. Can you really describe Pearl's work as "building" on philosophy, when IIRC, most of the philosophers were claiming at this point that causality was a mere illusion of correlation? Has Pearl named a previous philosopher, who was not a mathematician, who Pearl thought was getting it right?

Drescher's Good and Real.

Previously named by me as good philosophy, as done by an AI researcher coming in from outside for some odd reason. Not exactly a good sign for philosophy when you think about it.

Dennett's "intentional stance."

For a change I actually did read about this before forming my own AI theories. I can't recall ever actually using it, though. It's for helping people who are confused in a way that I wasn't confused to begin with. Dennett is in any case a widely known and named exception.

Bostrom on anthropic reasoning. And global catastrophic risks. And Pascal's mugging. And the doomsday argument. And the simulation argument.

A friend and colleague who was part of the transhumanist community and a founder of the World Transhumanist Association long before he was the Director of the Oxford Future of Humanity Institute, and who's done a great deal to precisionize transhumanist ideas about global catastrophic risks and inform academia about them, as well as excellent original work on anthropic reasoning and the simulation argument. Bostrom is familiar with Less Wrong and has even tried to bring some of the work done here into mainstream academia, such as Pascal's Mugging, which was invented right here on Less Wrong by none other than yours truly - although of course, owing to the constraints of academia and their prior unfamiliarity with elementary probability theory and decision theory, Bostrom was unable to convey the most exciting part of Pascal's Mugging in his academic writeup, namely the idea that Solomonoff-induction-style reasoning will explode the size of remote possibilities much faster than their Kolmogorov complexity diminishes their probability.
Reading Bostrom is a triumph of the rule "Read the most famous transhumanists" not "Read the most famous philosophers".
The doomsday argument, which was not invented by Bostrom, is a rare case of genuinely interesting work done in mainstream philosophy - anthropic issues are genuinely not obvious, genuinely worth arguing about and philosophers have done genuinely interesting work on it. Similarly, although LW has gotten further, there has been genuinely interesting work in philosophy on the genuinely interesting problems of Newcomblike dilemmas. There are people in the field who can do good work on the rather rare occasions when there is something worth arguing about that is still classed as "philosophy" rather than as a separate science, although they cannot actually solve those problems (as very clearly illustrated by the Newcomblike case) and the field as a whole is not capable of distinguishing good work from bad work on even the genuinely interesting subjects.

Ord on risks with low probabilities and high stakes.

Argued it on Less Wrong before he wrote the mainstream paper. The LW discussion got further, IMO. (And AFAIK, since I don't know if there was any academic debate or if the paper just dropped into the void.)

Deontic logic

Is not useful for anything in real life / AI. This is instantly obvious to any sufficiently competent AI researcher. See e.g. http://norvig.com/design-patterns/img070.htm, a mention that turned up in passing back when I was doing my own search for prior work on Friendly AI.
...I'll stop there, but do want to note, even if it's out-of-order, that the work you glowingly cite on statistical prediction rules is familiar to me from having read the famous edited volume "Judgment Under Uncertainty: Heuristics and Biases" where it appears as a lovely chapter by Robyn Dawes on "The robust beauty of improper linear models", which quite stuck in my mind (citation from memory). You may have learned about this from philosophy, and I can see how you would credit that as a use of reading philosophy, but it's not work done in philosophy and, well, I didn't learn about it there so this particular citation feels a bit odd to me.

No, I'm suggesting that you do focus on the short-term stuff, like sitting in a way that strains your thigh. Try it. See what happens.

Note the way I speak with John Baez in the following interview, done months before the present post:
http://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/

In terms of what I would advocate programming a very powerful AI to actually do, the keywords are "mature folk morality" and "reflective equilibrium"...
In terms of Google keywords, my brand of metaethics is closest to analytic descriptivism or moral functionalism...

I was happy to try and phrase this interview as if it actually had something to do with philosophy.
Although I actually invented the relevant positions myself, on the fly when FAI theory needed it, then Googled around to find the philosophical nearest neighbor.
The fact that you are skeptical about this, and suspect I suppose that I accidentally picked up some analytic descriptivism or mature folk morality elsewhere and then forgot I'd read about it, even though I hadn't gone anywhere remotely near that field of philosophy until I wanted to try speaking their language, well, that strikes at the heart of why all this praise of "mainstream" philosophy strikes me the wrong way. Because the versions of "mature folk morality" and "reflective equilibrium" and "analytic descriptivism" and "moral functionalism" are never quite exactly right, they are built on entirely different premises of argument and never quite optimized for Friendly-AI thinking. And it seems to me, at least, that it is perfectly reasonable to simply ignore the field of philosophy and invent all these things the correct way, on the fly, and look up the nearest neighbor afterward; some wheels are simple enough that they're cheaper to reinvent than to look up and then modify.
Can philosophers be useful? Yes. Is it possible and sometimes desirable to communicate with people who've previously read philosophy in philosophical standard language? Yes. Is Less Wrong a branch from the mighty tree of mainstream philosophy? No.

I think this is what people are complaining about when they complain that "status" is being used isomorphically to "magic".

IIRC there are stats and it is that bad.

Okay, but you did the consequentialist reasoning first, right?

Hrm... just a thought re point 2: in the case of group1 of gender A enjoying lowering the status of their partners, and group2 of gender B enjoying having their status lowered, if size group 1 < size group 2, that could work out.
ie, I'd imagine that a situation where members of group 1 having harems of members of group 2 could potentially work well on both sides of the equation.
size group 1 > size group 2, however, could potentially be more of a problem since in that case the analogous solution does not seem to present itself as working as well for both groups.
(Or did I miss some obvious aspect of the relevant psychology?)

Well, the problem with e.g. the number of women who enjoy lowering male status and the number of men who enjoy their status being lowered is that group 1 << group 2 to a degree unsolvable with any realistic harem size.

Not that one fetish in particular, no. But speaking much more generally, part of the concept behind the rationalist mate is that we're supposed to do a bit of consequentialist reasoning before going "Ew!", and try to set things up so that people are happy instead of making them do the ideologically correct thing.
The main way "objectifying women as sexual fetish" is a problem ("problem": something that prevents people from being happy) is if (1) the person doesn't understand the difference between having a sexual fetish and stating an ethical value or (2) if there's a large difference between the number of men who have that fetish and the number of women, so that they can't pair up.

May or March?

Doesn't cost you much to try.

PM me with a quick description of what got fixed, maybe, never mind the gory details? ("I used to hear voices and listen to them, now I ignore them" would be an interesting data point just by itself.)

Pick a time, pick a place, declare you'll be there with a sign reading "LW", and I'll promote it to the front page. Either someone else will show or they won't.

Who cares?

Actually, if LW techniques can ameliorate clinically diagnosed, organic mental illness then this is extremely interesting. Most methods can't.

I hope this is being downvoted for the second paragraph and not the first paragraph. There are women out there whose fetish is their status being lowered, and they need boyfriends too.

If you're wondering why I'm afraid of philosophy, look no further than the fact that this discussion is assigning salience to LW posts in a completely different way to I do.
I mean, it seems to me that where I think an LW post is important and interesting in proportion to how much it helps construct a Friendly AI, how much it gets people to participate in the human project, or the amount of confusion that it permanently and completely dissipates, all of this here is prioritizing LW posts to the extent that they happen to imply positions on famous ongoing philosophical arguments.
That's why I'm afraid to be put into any philosophical tradition, Quinean or otherwise - and why I think I'm justified in saying that their cognitive workflow is not like unto my cognitive workflow.

We're not, or at least I'm not, just abstractly debating the perils of scholarship.

I can't believe people are getting so far up your nose about mentioning Jeremy Bentham.

In other words, there's both showing off by obscurity and showing off by association with well-known names.

"Everybody knows who Bentham is", is precisely the factor that makes association with him prestigious.

I'm part of Roger Bacon's lineage too, and not ashamed of it either, but time passes and things improve and then there's not much point in looking back.

I think there's a regular meeting at Columbia on Saturday evenings, but I could make a Saturday lunch. Will check.


When I wrote the post I didn't know that what you meant by "reductionist-grade naturalistic cognitive philosophy" was only the very narrow thing of dissolving philosophical problems to cognitive algorithms.

No, it's more than that, but only things of that level are useful philosophy. Other things are not philosophy or more like background intros.
Amy just arrived and I've got to start book-writing, but I'll take one example from this list, the first one, so that I'm not picking and choosing; later if I've got a moment I'll do some others, in the order listed.

Predicate logic.

Funny you should mention that.
There is this incredibly toxic view of predicate logic that I first encountered in Good Old-Fashioned AI. And then this entirely different, highly useful and precise view of the uses and bounds of logic that I encountered when I started studying mathematical logic and learned about things like model theory.
Now considering that philosophers of the sort I inveighed against in "against modal logic" seem to talk and think like the GOFAI people and not like the model-theoretic people, I'm guessing that the GOFAI people made the terrible, horrible, no good, very bad mistake of getting their views of logic from the descendants of Bertrand Russell who still called themselves "philosophers" instead of those descendants who considered themselves part of the thriving edifice of mathematics.
Anyway. If you and I agree that philosophy is an extremely sick field, that there is no standardized repository of the good stuff, that it would be a desperate and terrible mistake for anyone to start their life studying philosophy before they had learned a lot of cognitive science and math and AI algorithms and plain old material science as explained by non-philosophers, and that it's not worth my time to read through philosophy to pick out the good stuff even if there are a few small nuggets of goodness or competent people buried here and there, then I'm not sure we disagree on much - except this post sort of did seem to suggest that people ought to run out and read philosophy-qua-philosophy as written by professional philosophers, rather than this being a terrible mistake.
Will try to get to some of the other items, in order, later.

I'll await your next post, but in retrospect you should have started with the big concrete example of mainstream philosophy doing an LW-style dissolution-to-algorithm not already covered on LW, and then told us that the moral was that we shouldn't ignore mainstream philosophy.
I did the whole sequence on QM to make the final point that people shouldn't trust physicists to get elementary Bayesian problems right. I didn't just walk in and tell them that physicists were untrustworthy.
If you want to make a point about medicine, you start by showing people a Bayesian problem that doctors get wrong; you don't start by telling them that doctors are untrustworthy.
If you want me to believe that philosophy isn't a terribly sick field, devoted to arguing instead of facing real-world tests and admiring problems instead of solving them and moving on, whose poison a novice should avoid in favor of eating healthy fields like settled physics (not string theory) or mainstream AI (not AGI), you're probably better off starting with the specific example first. "I disagree with your decision not to cover terminal vs. instrumental in CEV" doesn't cover it, and neither does "Quineans agree the world is made of atoms". Show me this field's power!


Can I expect a reply to my claim that a central statement of your above comment was both clearly false and misrepresented Quinean naturalism?

Reply to charge that it is clearly false: Sorry, it doesn't look clearly false to me. It seems to me that people can get along just fine knowing only what philosophy they pick up from reading AI books.
Reply to charge that it misrepresented Quinean naturalism: Give me an example of one philosophical question they dissolved into a cognitive algorithm. Please don't link to a book on Amazon where I click "Surprise me" ten times looking for a dissolution and then give up. Just tell me the question and sketch the algorithm.
The CEV article's "conflation" is not a convincing example. I was talking about the distinction between terminal and instrumental value way back in 2001, though I made the then-usual error of using nonstandard terminology. I left that distinction out of CEV specifically because (a) I'd seen it generate cognitive errors in people who immediately went funny in the head as soon as they were introduced to the concept of top-level values, and (b) because the original CEV paper wasn't supposed to go down to the level of detail of ordering expected-consequence updates versus moral-argument-processing updates.


If you're not that familiar with Quinean naturalistic philosophy, why do you assume in advance that it's a bad idea to read through it for insights?

Because I expect it to teach very bad habits of thought that will lead people to be unable to do real work. Assume naturalism! Move on! NEXT!

My own vague recollection of this event says it was a Hanson post on the original OB.

That was my intuition. Just wanted to know if there's more out there.

What, you mean in mainstream philosophy? I don't think mainstream philosophers think that way, even Quineans. The best ones would say gravely, "Yes, goals are important" and then have a big debate with the rest of the field about whether goals are important or not. Luke is welcome to prove me wrong about that.


Are you saying that my claim that LW-style philosophy shares many central assumptions with Quinean naturalism in contrast to most of philosophy doesn't hinge on whether or not I can present a long list of things on which LW-style philosophy and Quinean naturalism agree on, in contrast to most of philosophy?

I'm saying that the claim that LW-style philosophy shares many assumptions with Quinean naturalism in contrast to most of philosophy is unimportant, thus, presenting the long list of basic assumptions on which LW-style and Quinean naturalism agree is from my perspective irrelevant.

Do you disagree with my claim that "standard Less Wrong positions on philosophical matters have been standard positions in a movement within mainstream philosophy for half a century"?

Yes. What I would consider "standard LW positions" is not "there is no libertarian free will" but rather "the philosophical debate on free will arises from the execution of the following cognitive algorithms X, Y, and Z". If the latter has been a standard position then I would be quite interested.

Or perhaps you disagree with my claim that "Less Wrong-style philosophy is part of a movement within mainstream philosophy to massively reform philosophy in light of recent cognitive science - a movement that has been active for at least two decades"?

The kind of reforms you quote are extremely basic, along the lines of "OMG there are cognitive biases and they affect philosophers!" not "This is how this specific algorithm generates the following philosophical debate..." If the movement hasn't progressed to the second stage, then there seems little point in aspiring LW rationalists reading about it.
GJM's suggestion is correct but the thing which you seem to deny and which I think is true is that LW is at a different stage of doing this sort of philosophy than any Quinean naturalism I have heard of, so that the other Quineans "doing things that nobody else have thought of" don't seem to be doing commensurate work.
I am not asking for an example of someone who agrees with me that, sure, cognitive philosophy sounds like a great idea, by golly. There's a difference between saying "Sure, evolution is true!" and doing evolutionary biology.
I'm asking for someone who's dissolved a philosophical question into a cognitive algorithm, preferably in a way not previously seen before on LW.
Did you read the LW sequence on free will, both the setup and the solution? Apologies if you've already previously answered this question, I have a vague feeling that I asked you before and you said yes, but still, just checking.
On the whole, you seem to think that I should be really enthusiastic about finding philosophers who agree with my basic assumptions, because here are these possible valuable allies in academia - why, if we could reframe LW as Quineanism, we'd have a whole support base ready-made!
Whereas I'm thinking, "If you ask what sort of activity these people perform in their daily work, their skills are similar to those of other philosophers and unlike those of people trying to figure out what algorithm a brain is running" and so they can't be hired to do the sort of work we need without extensive retraining; and since we're not out to reform academic philosophy, per se, it's not clear that we need allies in a fight we could just bypass.

There's lots of people I think have valuable insights - cognitive scientists, AI researchers, statistical learning experts, mathematicians...
The question is whether high-grade academic philosophy belongs on the scholarship list, not whether scholarship is a virtue. The fact that they have managed to produce a minority school that agrees with Gary Drescher on the extremely basic question of whether there's libertarian free will (no) and people are made of atoms (yes), does not entitle them to a position next to "Artificial Intelligence: A Modern Approach".

What you care about determines what your explorations learn about. An AI that didn't care about anything you thought was important, even instrumentally (it had no use for energy, say) probably wouldn't learn anything you thought was important. A probability-updater without goals and without other forces choosing among possible explorations would just study dust specks.

I affirm this interpretation.

Okay, so what have they done that I would consider cognitive philosophy? It doesn't matter how many verbal-type non-dissolved questions we agree on apart from that. I'm taking free will as an exemplar and saying, "But it's all like that, so far as I've been able to tell."

Look, if someone came to me and said, "I'm great at LW-style philosophy, and the proof of this is, I can argue there's no libertarian free will" I would reply "You have not yet done any difficult or worthwhile cognitive work." It's like saying you don't believe in astrology. Well, great, and yes there's lots of people who disagree with you about that, but there's a difference between doing grade school arithmetic and doing calculus, and "There is no libertarian free will" is grade school arithmetic. It doesn't interest me that this philosophical school agrees with me about that. It's too simple and basic, and part of what I object to in philosophy is that they are still arguing about problems like this instead of moving onto real questions.

Luke,
From my perspective, the idea that we do not have libertarian free will is too obvious to be interesting. If you want to claim that places me in a particular philosophical camp, fine, but that doesn't mean they do the same sort of cognitive labor I do when I'm doing philosophy. I knew there wasn't libertarian free will the instant I first considered the problem, at I think maybe age fourteen or thereabouts; if that made me a master philosopher, great, but to me it seems like the distance from there to being able to resolve the algorithms of the brain into their component parts was the interesting part of the journey.
(And Judea Pearl I have quite well acknowledged as an explicit shoulder to stand upon, but so far as I know he's another case of an AI researcher coming in from outside and solving a problem where philosophers just spun their wheels because they didn't think in algorithms.)

That's good for standard philosophy, but it doesn't rise to the level of LW-style cognitive philosophy.

My theory of truth is explicitly Tarskian. I'm explicitly influenced by Korzybski on language and by Peirce on "making beliefs pay rent", but I do think there are meaningful and true beliefs such that we cannot experientally distinguish between them and mutually exclusive alternatives, i.e., a photon going on existing after it passes over the horizon of the expanding universe as opposed to it blinking out of existence.

I'm highly skeptical. I suspect that you may have failed to distinguish between sensory empiricism, which is a large standard movement, and the kind of thinking embodied in How An Algorithm Feels From the Inside which I've never seen anywhere else outside of Gary Drescher (and rumors that it's in Dennett books I haven't read).
Simple litmus test: What is the Quinean position on free will?
"It's nonsense!" = what I think standard "naturalistic" philosophy says
"If the brain uses the following specific AI-ish algorithms without conscious awareness of it, the corresponding mental ontology would appear from the inside to generate the following intuitions and apparent impossibilities about 'free will'..." = Less Wrong / Yudkowskian

That's me.

I'm not sure the Youtube example is a good thing, and if not a good thing it is certainly a bad thing. It could very well be worse than a dust speck in the eye.

Rewrite this to make definite statements instead of tentative offers (see Less Wrong NYC) and I'll promote it.

Forced to? No.

Guess: Lucidfox is a member of the Dark Lord Potter forum.

No, this is not a mere worldview phenomenon. Apparently the NYC LW group has successfully proved, contrary to all stereotypes, that rationality done right makes you a more attractive mate.
Contrast:
1) "How could you say that! You terrible mean person who wants to hurt my feelings!"
2) "You need to understand that when you say something like that, it makes me feel as though you're trying to hurt my feelings, whether or not you do."
3) "I'm sorry about how I reacted; even though I know on the level of rational probabilities that it's extremely unlikely you meant to hurt my feelings, I'm still working on getting my brain to alieve that and not just believe it."
Let's say you've got a mate at level 1. Then you join a group in which you find (a) single people at level 3 and (b) a widespread understanding of the concept of the "sunk cost fallacy" and the importance of saying oops and changing your policies occasionally.
What do you think happens next?
And yes, NYC LW is demonstrating that this also works with women realizing that they can no longer stand to be around non-rationalist guys.
PS: Cultish countercultishness. I'm actually pretty sick of hearing someone yell "Cult!" every time rationalists try to coordinate as well as a model railroad club. Why Our Kind Can't Cooperate.

I think I formed the idea pretty early on that my teachers were idiots, like age five or thereabouts. Never realized before how important that could've been in preventing this particular bit of early brain damage!

Fix deployed.

Don't call her "kid", grup.

Hey Eliezer,
Interesting point. I think part of the problem is that sex theorists have to work very hard to get ourselves taken seriously, so many of us overcompensate. Another problem is that while sex is totally fun, sex also comes with an enormous potential to harm, so it's important to take it seriously at least somewhat.
Also, sex is a highly-triggering area for most people. I specifically try to include some humor and/or sexy anecdotes in my writing, but I find that I am considerably likely to be misinterpreted when I do so, and when I'm misinterpreted it can get really bad really fast ("I CAN'T BELIEVE YOU JUST MADE LIGHT OF ABUSIVE RELATIONSHIPS!11").
One of the projects I'm outlining right now is a BDSM erotica novella in which I try to include as much theory as I possibly can while still keeping it sexy. We'll see if I succeed.


One of the projects I'm outlining right now is a BDSM erotica novella in which I try to include as much theory as I possibly can while still keeping it sexy.

Harry Potter and the Methods of Sexuality?

I'd just call 'em "dull feminists" and get on with my life.

What determines how much of each sort of everything exists?
http://lesswrong.com/lw/py/the_born_probabilities/ http://lesswrong.com/lw/19d/the_anthropic_trilemma/

Erm... it's harder for me to say this, because of the high-status thing where I've been turned into some kind of giant lumbering elephant who has to keep trying not to step on people, but nonetheless I beg you to stop, reflect, consider what you have just said, and realize that you don't actually know anything about this problem.

Last time I talked to Zvi, Laura, and Will and those were the only people I got to know in any real sense - I didn't really get a chance to meet the community as a community and I had no idea that all this stuff was going on. Mostly, last time I ran around giving talks and meeting potential SIAI donors, etc.

Bleah. Still not used to this high status thing.
But seriously, if you are not metabolically privileged, what happens if you try this is that your body shuts down and goes into starvation mode instead of losing weight. Your fat cells do not release fat under any circumstances, though they're happy to hoover up blood sugar so you always feel tired. We're not talking "feeling hungry", we're talking that you stop feeling hungry and lie down, feeling very very cold and having a hard time moving. Literal starvation, instead of your fat cells releasing fat. I've never tried starving myself that much (I worry that it will cause my brain to cannibalize irreplaceable neurons or something, the way the rest of the body cannibalizes muscle) but I've just recently watched that happen to someone else who tried to lose weight by not eating and wasn't metabolically privileged enough to get away with it.
A calorie is not a calorie. The thermodynamic theory of metabolism is a fucking lie. And it seriously does wear away on your nerves like sandpaper, after a while, to be blamed for it, when the exact same diet can make one person thin and cause the other to blow up like a balloon...
Eh, just read "Beware of Other-Optimizing."

Disagree with Sean Carroll. The property that Judea Pearl defines in "Causality" is a central part of the character of physical law.
And even if what Sean Carroll said was true, there'd still be a big important problem to be resolved somehow.
It's okay to have big outstanding problems. You don't have to say "God" and you don't have to sweep them under the rug either.

Erm, I strongly suspect that doing great work in your 20s is not just about status.

Disagree. LW could stand to see giant lists of references more often.

Rejoinder to them is at http://lesswrong.com/lw/jp/occams_razor/ and http://lesswrong.com/lw/19m/privileging_the_hypothesis/.

rolls eyes
My dear fellow, I assure you that I take TDT very seriously. Some of us cannot lose weight that way. See "Beware of Other-Optimizing", linked at the top of the post.
I am beginning to think that metabolisms vary about as much as minds.

I'll be in NYC Apr 1-18 inclusive with the sole agenda of actually meeting the community this time.

This is really impressive, and a great example of how rationalists really do will. From what I gather, even the great Eliezer himself have not been able to take this idea so seriously! Truly an example for us all.

For the reasons given by McAllister and Weissman, I think your answer is every bit as terrible as the religious one. I'm sure we're asking the wrong question but if we knew the right question we'd be done. And meanwhile, there's a very real problem and trying to sweep it under the rug like this is every bit as bad as claiming that "God did it" cleans it up.

Google shadow, of course.

Yeah, I think Michael Shermer's wrong about what went wrong exactly. Chemists have access to the indisputable Truth about the atomic theory of chemistry and it hasn't turned them into a cult. http://lesswrong.com/lw/m1/guardians_of_ayn_rand/.


With what probability will you anticipate success?

Higher probability than people who've despaired of theists believe is possible, still less than 50%. Keep in mind that we're starting with someone who hasn't sunk too far into belief in self-deception.

Most folk tell themselves they are being rational, Miss Granger. They do not thereby rise above the ordinary.
(Not literally true, but true within certain subcommunities.)

My suggested resolution is as follows:
1) "Don't ask and for God's sake don't tell." This is a group where people come to speak freely about rationality. If you don't talk about your beliefs about God, no one will press you on it or demand that you affirm anything.
2) However, part of our zeitgeist is that it's okay to question beliefs, or even try your hardest to destroy beliefs you think are false, because that which can be destroyed by the truth should be. There are no exceptions for anything, and if you say anything indicating that you think religious beliefs should be exempt, people are not going to nod along, instead they are going to start talking about "The rule that you have to look at a city in order to draw an accurate map of it has no exceptions".
3) Criticism of religion is not taboo - it goes against both the ideals of rationality we believe in, and the atmosphere of freedom that draws us to the group, to have that sort of taboo for that reason. So, to put it bluntly, you will overhear other people comparing belief in God to the Tooth Fairy, and if you contradict them they will contradict you back, and if you say that everyone has a right to their own opinion they will start trying to explain to you the concept of "anti-epistemology".
4) Atheism might not be mandatory inside a rationalist community - but what is mandatory is the idea that it's allowed to argue beliefs, and that attacking the belief isn't the same as attacking the person. And if you decide that you're offended, you will not get sympathy or agreement on that point from the group, because the idiom of "I Am Offended, Shut Up" is something we have explicitly decided to give up.
5) Atheists are obliged to argue carefully with theists, and hold their arguments contradicting theism to the same sort of skeptical standard that they would use for arguments in favor of a dislikable conclusion instead of a likable one. If you, say, claim that "Time didn't exist before the Big Bang" is a complete and satisfactory solution to the problem "Why does something exist instead of nothing?", rather than (as is the correct answer) trying to explain why saying "God" (a) doesn't help and (b) constitutes the cardinal sin of Just Making Stuff Up, then you have lost all claim to any moral victory. (This last point is really a more general one, but it is an example of the sort of respect that you do owe to a religious newcomer, and if you deny them that respect - if they see that you are allowed to throw bad arguments at them that you wouldn't throw at anyone else - they are quite justified in walking away in a huff.)
6) Don't sweat losing some possible recruits. There's enough atheists in the world, or theists who can tolerate disrespect for theism, that in the present stage of the community's growth it is definitely not worth compromising community values of rationality in order to hang onto people who still have a sense of entitlement to their Offense.

That's what Barbara says, but only if you press her. Mostly she just doesn't talk about it.
If she spontaneously announces this to the group and challenges them to a debate, believing that the truth ought to be obvious enough that anyone should be able to see it, I shall endeavor to turn her into Caroline in 15 minutes and Donna a day later.

Log-lines-written? Probably around 5.

Numerous cases in Methods of Rationality, especially during the early days. It's as if they had priors suggesting that most Harry Potter fanfiction authors were female.

When I read this, I remembered also being told to "move your arms and legs".
Seriously?
Seriously?
WTF WAS WRONG WITH THOSE PEOPLE oh never mind OP said it better.

Learned to program at five. If someone has the programming gear, five is a perfectly good time to teach them to program. Just show them some Python code (I was reading BASIC, bleah) and see if they can deduce the rules and try writing their own. If someone is meant to be a programmer then a programmer they shall be.

I'm going to quote you on this in the rationality book. Email me with who you want credited if it's not "gwern on LessWrong.com".

Um... that body part is not supposed to stay in that state for 5 hours. Maintaining an erection means diminished bloodflow to the area and anything over 4 hours calls for a trip to the emergency room.

Language Log and Strunk and White are not playing the same game.
Strunk and White are playing "Does this look right nowadays?"
Language Log apparently thinks there are official rules determined by history.
I, of course, think the singular "they" looks just fine, nowadays.

Just read through these links, and I have to say that the concept of "fun" leapt out at me as being largely missing.
I suspect there's a major problem where a lot of the people who spend the most time writing about polyamory or BDSM or, hell, sexuality in general, are people who literally have nothing more important in their identities. They're trying way too hard to sound adult and serious. You want to scream at them to just lighten up.
I'm starting to get that dreadful "I could do better than that" feeling which makes me do things like write Harry Potter and the Methods of Rationality or explain Bayes's Theorem...

It says something about the way I think, that to me it seems like a primary reductio of monogamy that it wouldn't scale to a million years.

A simpler technique is to just substitute x = y = 1 in each step and look for the first false equation, i.e., the first step where a false statement is derived from a true statement. Clearly that reasoning step is not valid (does not produce only true conclusions from true premises).

Click through the "antitheism" tag for more. This is just one post.

Gotcha.

Or to look at it another way, Solomonoff was the first mathematical specification of a system that could, in principle if not in the physical universe, learn anything learnable by a computable system.

AIXI is based on Solomonoff, and to the extent that you regard all other AGIs as approximations to AIXI...

Bostrom writes clearly.
But I will suggest for the record that we can probably get away with just ignoring anything that was written for other philosophers rather than for the general public or competent AGI researchers, since those are the only two constituencies we care about. If anyone in philosophy has something to contribute to the real FAI discussion, let them rewrite it in English. I should also note that anything which does not assume analytic naturalism as a matter of course is going to be rejected out of hand because it cannot be conjugate to a computer program composed of ones and zeroes.
Philosophers are not the actual audience. The general public and competent AGI researchers are the actual audience. Now there's some case to be made for trying to communicate with the real audience via a complicated indirect method that involves rewriting things in philosophical jargon to get published in philosophy journals, but we shouldn't overlook that this is not, in fact, the end goal.
Relevance. It's what's for dinner.

If we're talking about CEV, I agree. It needs rewriting. So does the Intuitive Explanation of Bayesian Reasoning, or any number of other documents produced by earlier Eliezers.
It was the linked Sobel paper which called forth that particular comment by me, if you're wondering. I looked at it in hopes of finding useful details about how to construe an extrapolated volition, and got a couple of pages in before I decided that I wasn't willing to read this paper unless someone had produced a human-readable version of it. (Scanning the rest did not change my mind.)
I'm not sure I want to import FAI ethics into philosophical academia as a field where people can garner prestige by writing papers that are hard to read. Maybe it makes sense to put up a fence around it and declare that if you can't write plain-English papers you shouldn't be writing about FAI.

Simply put, we don't have anyone who can except Carl Shulman and myself. I'm busy writing a book. I think Carl actually is doing papers but I'm not sure this is his highest-priority subject.
It might be that hiring a real professor to supervise would enable us to get this sort of work out of a postdoc going through the Visiting Fellows program, but that itself is not easy.

Agree or disagree with the following statement?
"After publishing the paper in a philosophy journal so that academics would be allowed to talk about it without losing face, you would have to write a separate essay to explain the ideas to anyone who actually wanted to know them, including those philosophers."

This and Mitchell Porter's are the main comments I've seen so far that seem to display a grasp of the real emotions involved, as opposed to arguing.

Don't commit to repeating it. That just gives people an option to ignore the first one.

Point 1 - it wasn't stylistically consistent with later chapters. When I wrote the original Chapter 1 I didn't realize that this story was going to be funny. The part where Harry bites a math teacher in the original Chapter 2 is the exact part where I realized this story was going to be funny.
Point 2 - I got tripped up by the differences between the published SF I knew and the expectations of fanfiction. If you saw a character talking like that in a published SF novel, you would know that he was an alien or genetically engineered or that the author meant you to know something was funny about him. In fanfiction they assume that it's either the author's conceit or, more probable yet, you're just a terrible author who can't write realistic eleven-year-olds. I thought it was so blatantly lampshaded that nobody could possibly mistake it for an accident, but no, fanfiction readers just don't think like that - they don't look for clues and they do assume lousy authors. So I made Harry's intellect slightly more subtle in the first chapter and let it dawn slightly more slowly.

The "Discussion" section has taken over from the old open thread system.
EDIT: All right, but at least post Open Threads in the Discussion section?

Basically incompetent organizations that try to build AI just won't do anything.

If you want to offer a concrete proposal for verifying the trustworthiness of nine people in a basement, offer it. Otherwise you're just giving people an excuse to get lost in thought and implement the do-nothing option instead of implementing the best time-sensitive policy proposal offered so far.

They are ideas for allaying fears that SIAI is incompetent or worse. Which, since it is devoted to building an AI, would tend to allay fears that it is building an evil one.

That is not even on the Internet, I checked. 100 nerd points to you.


that his prediction timeframes are driven by his own hope for immortality

This is not an important criticism; it is ad hominem in its purest form.

"And I said yayus, yayus, everybody yayus! I said you must have faith in the creative spirit, because it is creativity... that gets things... created."

And they ask me: Can it be done?

Interesting; it seemed to me that combat in the ancestral environment would be the main case where the tribe would shut up about status for five seconds and allow fighters to get stuff done.

"Ordinary claims require merely ordinary evidence" is an overlooked and tremendously important corollary.

If you only go to one, I'd guess you should go to the NYU one.

Keep in mind that Blaise's plan was Dumbledore's.

This sounds more like a bias (people want to know they're successful; ambiguity aversion; want a solid warm glow) than a consideration which could reasonably compete against the economies of scale in existential risk. The penalty for increased uncertainty owing to difficulty of measurement is simply not as large as the orders of magnitude of scope involved.

Today I met a relative of mine named Eliezer Yudkowsky. First words out of his mouth: "Oh, it's you! You're the one who ruined my life!"
I also met Avi, who (I was told) used to come over to babysit me, and I would do his math homework for him.
And I was told that at one point during my distant youth, I was holding a camera and kept tilting it, and Uncle David kept telling me "Hold it steady!" without effect, and then Dad said "Hold it in a plane perpendicular to the floor" and that worked.
Just in case anyone was still claiming that my eleven-year-olds are unrealistic.

A major problem of many future existential threat charities is evaluating whether they actually are reducing existential risk, or whether they will actually increase it. The evidence of history, for example, indicates that even the best foreign policy experts are not very good at evaluating a policy's secondary effects and perverse incentives. The result that it is very hard to evaluate whether the net effect of spending money on what is supposed to be "reducing the risk of global thermonuclear war" will actually increase or decrease the risk of global thermonuclear war. The very same multipliers that leverage to massive utility on the assumption of intended consequences leverage to massive disutility on the assumption of perverse consequences.
On the other hand, it's rather easy to evaluate the net value of a heavily-studied vaccination against an endemic disease, and you can be reasonably certain you're not actually spreading the very disease you're trying to fight.

Important note: This is not the same Eliezer Yudkowsky. This Eliezer Yudkowsky is like 13 years old or something.

Yep, B'harne.

I think of this as Senior Author Syndrome. Chief exhibits are Anne McCaffrey, Mercedes Lackey and Orson Scott Card. The first symptom is forgetting how to hurt your characters. Just pick up a late book and an early book and compare how much jeopardy the characters get into.

Erm, sorry, I was just linked there or Googled there or something, don't read it on a regular basis.

Moved to Discussion.

"Permutation City", followed by http://www.fanfiction.net/s/5389450/1/The_Finale_of_the_Ultimate_Meta_Mega_Crossover

I'm saying that the doctors are the original and IEBT is the copy, not the other way around.

You can do it yourselves, or I'll do it when I promote the post.


I don't usually advocate lying, but if your companions are being sufficiently silly, maybe you should get ice water, drop in some blue food coloring you brought from home, and tell them it's Romulan ale.

This is hilarious, but I suspect would not help in the situations where this was actually a problem.

You could try it and see what happens. (Or use a different color and make up a different name.)

Well, to be precise, the Intuitive Explanation describes exactly this error, previously found to have been made by doctors.

Interesting. I sometimes get questioned on why I don't drink, but my response of "I don't dare lose a single brain cell" or "I have an addictive personality, I don't dare try World of Warcraft either" is usually accepted with a smile. Following Michael Vassar's theory of how excessive visible virtue is disliked as rivalry / implicit criticism / showing-up, I'm probably already seen as belonging to a sufficiently different category that I have dispensation to be virtuous without it counting as an implicit criticism of normal people.
I don't usually advocate lying, but if your companions are being sufficiently silly, maybe you should get ice water, drop in some blue food coloring you brought from home, and tell them it's Romulan ale.

I wasn't saying that they were similar questions, just that one reminded me of the other. (Though I can see why one would think that.)
I'd say the answer to this is pretty simple. Laura ABF (if I remember the handle correctly) suggested of the original Torture vs. Specks dilemma that the avoided specks be replaced with 3^^^3 people having really great sex - which didn't change the answer for me, of course. This provides a similar template for the current problem: Figure out how many victims you would dustspeck in exchange for two beneficiaries having a high-quality sexual encounter, then figure out how many dustspecks correspond to a month of torture, then divide the second number by the first.

This seems like an excellent idea. Will do unless there are objections.

This reminds me of the horrible SIAI job interview question: "Would you kill babies if it was intrinsically the right thing to do? If so, how right would it have to be, for how many babies?"

Affirmed.

Roko's original proposed basilisk is not and never was the problem in Roko's post. I don't expect it to be part of CEV, and it would be caught by generic procedures meant to prevent CEV from running if 80% of humanity turns out to be selfish bastards, like the Last Jury procedure (as renamed by Bostrom) or extrapolating a weighted donor CEV with a binary veto over the whole procedure.
EDIT: I affirm all of Nesov's answers (that I've seen so far) in the threads below.

Original version of grandparent contained, before I deleted it, "Besides the usual 'Eating babies is wrong, what if CEV outputs eating babies, therefore a better solution is CEV plus code that outlaws eating babies.'"

That's nothing, I once saw a restaurant called Genghis Khan's Mongolian BBQ.

As for concrete failure scenarios, yes - that will be the point of that chapter.
As for a computational procedure that does better, probably not. That is beyond the scope of this book. The book will be too long merely covering the ground that it does. Detailed alternative proposals will have to come after I have laid this groundwork - for myself as much as for others. However, I'm not convinced at all that CEV is a failed project, and that an alternative is needed.

Can you give me one quick sentence on a concrete failure mode of CEV?

You say you'll present some objections to CEV. Can you describe a concrete failure scenario of CEV, and state a computational procedure that does better?

I take 0.4 mg an hour or two before sleep, then 0.3 mg timed-release (sold by LEF) just before getting into bed.
That took a lot of tweaking to find.

Well, the part about you being a fundamentalist Christian three years ago is damned impressive and does a lot to convince me that you're moving at a reasonable clip.
On the other hand, a good metaethical answer to the question "What sort of stuff is morality made out of?" is essentially a matter of resolving confusion; and people can get stuck on confusions for decades, or they can breeze past confusions in seconds. Comprehending the most confusing secrets of the universe is more like realigning your car's wheels than like finding the Lost Ark. I'm not entirely sure what to do about the partial failure of the metaethics sequence, or what to do about the fact that it failed for you in particular. But it does sound like you're setting out to heroically resolve confusions that, um, I kinda already resolved, and then wrote up, and then only some people got the writeup... but it doesn't seem like the sort of thing where you spending years working on it is a good idea. 15 years to a piece of paper with the correct answer written on it is for solving really confusing problems from scratch; it doesn't seem like a good amount of time for absorbing someone else's solution. If you plan to do something interesting with your life requiring correct metaethics then maybe we should have a Skype videocall or even an in-person meeting at some point.
The main open moral question SIAI actually does need a concrete answer to is "How exactly does one go about construing an extrapolated volition from the giant mess that is a human mind?", which takes good metaethics as a background assumption but is fundamentally a moral question rather than a metaethical one. On the other hand, I think we've basically got covered "What sort of stuff is this mysterious rightness?"
What did you think of the free will sequence as a template for doing naturalistic cognitive philosophy where the first question is always "What algorithm feels from the inside like my philosophical intutions?"

I remember getting the word from Bostrom.

Pretty sure I've had some type of allegedly-comprehensive-but-cheap blood scan done, which didn't turn up anything interesting. Is there somewhere I go for a more comprehensive blood scan?

I shall have to quote this a good deal more when dealing with people who chide me for not mentioning all the possible objections that philosophers consider to still be in play.

It's very nearly one of the only pieces of rationalist fiction out there.

I don't think we have open threads any more now that there's a Discussion section.

Yes, I find it quite amusing that some people of a certain political bent refer to "corporations" as superintelligences, UFAIs, etcetera, and thus insist on diverting marginal efforts that could have been directed against a vastly underaddressed global catastrophic risk to yet more tugging on the same old rope that millions of other people are pulling on, based on their attempt to reinterpret the category-word; and yet oddly enough they don't think to extend the same anthropomorphism of demonic agency to large organizations that they're less interested in devalorizing, like governments and religions.

I'm curious: leaving aside weight and social stigma, have you found that the different levels of diet and exercise you've experimented with had any positive or negative effects? (E.g., mood, energy levels, endurance, etc.?)

I can walk farther after getting in a couple of weeks of regular walking.
That's it.
Basically, "no effect that I can detect with the naked eye".

I have given up, and it was indeed a great improvement in quality of life when I stopped trying to manage my weight - gave up and ate whatever, stopped going to the gym - and observed that my weight behaved in exactly the same way as before, the same slow upward creep at the same rate.
I don't know to what degree being overweight would be less painful if there wasn't a social stigma attached to it, but we don't actually live in that world.

I think that asking the community to downvote timtyler is a good deal less disruptive than an outright ban would be. It makes it clear that I am not speaking only for myself, which may or may not have an effect on certain types of trolls and trolling. And doing nothing is not a viable option..

I use Dvorak. It's no faster and no more accurate, but it does tire out your fingers a whole lot less, and just typing one sentence in Dvorak will enable you to see why. I switched to Dvorak after a bout of RSI, and the RSI never came back.

At this point, my Expectancy for positive results from single changes like "just use a trainer at the gym" has hit essentially zero - I've tried all sorts of stuff, nothing ever fucking works - so I'm not willing to spend the incremental money. If I have a lot of money to spend, I'll try throwing a higher level of money at all aspects of the problem - get a trainer on weights, try the latest fad of "short interval bursts" for aerobic exercise, get LASIK and a big TV and a separate room of the apartment to make exercising less unpleasant (no, dears, I don't get any endorphins whatsoever), buy a wide variety of grass-fed organic meats and take one last shot at the paleo diet again, and... actually I think that's most of what I'd do. That way I'd be able to scrape up enough hope to make it worth a shot. Trying one item from that list doesn't seem worth the bother.
I did try Shangri-La again when Seth Roberts contacted me personally and asked me to take another shot. It was just wearing tight, uncomfortable noseplugs while eating all my food and clearing out time at night to make sure I took oil 1 hour away from eating any other food or brushing my teeth, a trivial inconvenience when I'd walk over broken glass to lose weight. I lost 20 pounds and then despite trying out around 10 different things Seth Roberts said to do, my weight slowly started creeping up again, and when after a while I gave up and stopped taking the oil to see what would happen, there was no change in the behavior of my weight - the same slow creep. It's clear that Shangri-La worked initially but then, contrary to all theory, it just mysteriously stopped working. So far I've gained 10 of those 20 pounds back, in accordance with the one truly reliable law of dietary science: 95% of the people who manage to lose weight put it back on shortly thereafter. BTW, exercise didn't lead me to lose any weight whatsoever, even when combined with an attempt at the paleo diet (albeit not one that spent lots of money, or involved a personal trainer).
So far as I can tell, all the advice here is from metabolically privileged folks who don't know they're metabolically privileged and don't comprehend the nothing fucking works phenomenon that obtains if you're not metabolically privileged.
If you want to give advice, that's fine. Don't tell me how well it's going to work or how easy it's going to be; that just tells me you're clueless.

Well, it looks to me like SIAI core people got it, but there's trouble being sure about that sort of thing.


So we need another word to filter out those kinds of somewhat-arbitrary proposed meta-ethical systems. "Objective" probably is not the best word for the job, but it is the only one I can think of right now.

This is where I stopped reading.
I suggest that you actually read the SEP entry on meta-ethics instead of just linking there - if you did read it, feel free to correct my guess. Metaethics does not mean what you said it did (metaethics is a theory of what morality is, not a way of comparing moralities), moral realism does not mean what you said it did (your belief that morality is a real thing out there constitutes moral realism), naturalistic metaethics do not mean what you said it did, CEV is totally not about convergence in all possible minds, etcetera. I also have to ask whether you read the Metaethics Sequence, but I mostly regard that sequence as having failed so I won't be surprised if the answer is yes.

Yeah, that kind of advice is not going to fill any procedural knowledge gaps, sorry.
Previously I've tried "exercise" with fitness machines, aerobic and resistance both, an hour apiece on both, and it doesn't seem to do anything at all. I currently walk a couple of hours every other day. I have no idea whether this does anything (besides exhausting me so much I don't get any work done for the rest of the day, of course). I once read that 40% of the population is "immune to exercise" and I suspect I'm one of the 0.40.
If I have enough money at some point I'll try hiring a fitness trainer, and then getting a larger apartment with an extra bedroom for exercise equipment (and maybe get Lasik so I don't have to wear glasses and use a TV and Dance Dance Revolution) but such expenses are beyond the reach of my current financial balance.
EDIT: Wow, lots of advice here from metabolically privileged folks who don't comprehend the nothing fucking works phenomenon that obtains if you're not metabolically privileged.

Craigslist has worked for me. Expect to spend some money on repairing the car. Taking it to a mechanic first seems like a big deal but you will need to take it there and you may as well take it there before buying it.

Timtyler is trolling again, please vote down. This exact point was addressed in the interview and in http://lesswrong.com/lw/xr/in_praise_of_boredom/. The math of exploration-exploitation does not give you anything like humanlike boredom.


HapMax seemed like a good idea at first, but this example shows that it is very wrong.

This has nothing to do with instability. HapMax is undesirable under your preferences, not its own preferences. HapMax itself just feeds the Utility Monster.
http://lesswrong.com/lw/ta/invisible_frameworks/


Azathoth wants you to maximize your number of descendants; if you fail to have descendants, Azathoth will try not to have created you.

But this seems merely false. Azathoth just creates descendants whose ancestors reproduced. Azathoth isn't exerting any sort of foresight as to whether you reproduce. I can't figure out who or what you're trying to trade with. Not having children simply does not make you retroactively less likely to have existed.
I suppose you could be in a Newcomblike situation with your parents making a similar decision to have birthed you. I don't see how you could be in one with respect to Azathoth/evolution. It's not modeling you, it doesn't contain a computation similar to you, there is no logical update on what it does after you know your own decision.


Interesting. I always assumed that raising a question was the first step toward answering it

Only if you want an answer. There is no curiosity that does not want an answer. There are four very widespread failure modes around "raising questions" - the failure mode of paper-writers who regard unanswerable questions as a biscuit bag that never runs out of biscuits, the failure mode of the politically savvy who'd rather not offend people by disagreeing too strongly with any of them, the failure mode of the religious who don't want their questions to arrive at the obvious answer, the failure mode of technophobes who mean to spread fear by "raising questions" that are meant more to create anxiety by their raising than by being answered, and all of these easily sum up to an accustomed bad habit of thinking where nothing ever gets answered and true curiosity is dead.
So yes, if there's an interim solution on the table and someone says "Ah, but surely we must ask more questions" instead of "No, you idiot, can't you see that there's a better way" or "But it looks to me like the preponderance of evidence is actually pointing in this here other direction", alarms do go off inside my head. There's a failure mode of answering too prematurely, but when someone talks explicitly about the importance of raising questions - this being language that is mainly explicitly used within the failure-mode groups - alarms go off and I want to see it demonstrated that they can think in terms of definite answers and preponderances of evidence at all besides just raising questions; I want a demonstration that true curiosity, wanting an actual answer, isn't dead inside them, and that they have the mental capacity to do what's needed to that effect - namely, weigh evidence in the scales and arrive at a non-balanced answer, or propose alternative solutions that are supposed to be better.
I'm impressed with your blog, by the way, and generally consider you to be a more adept rationalist than the above paragraphs might imply - but when it comes to this particular matter of metaethics, I'm not quite sure that you strike me as aggressive enough that if you had twenty years to sort out the mess, I would come back twenty years later and find you with a sheet of paper with the correct answer written on it, as opposed to a paper full of questions that clearly need to be very carefully considered.

Harry got the note from himself at around 3:10pm. He left for lunch with Professor Quirrell in the late morning, went back in time, went to Azkaban, went back in time again to Mary's Room, and was grabbed by Dumbledore and rescued at around lunchtime. Those two time-loops did not intersect; they are separate time-loops.


As it expands across the galaxy, perhaps encountering other creatures, it could do some behavioral psychology and neuroscience on these creatures to decode their intentional action systems

Now, it's just a wild guess here, but I'm guessing that a lot of philosophers who use the language "reasons for action" would disagree that "knowing the Baby-eaters evolved to eat babies" is a reason to eat babies. Am I wrong?

I'm merely raising questions that need to be considered very carefully.

I tend to be a bit gruff around people who merely raise questions; I tend to view the kind of philosophy I do as the track where you need some answers for a specific reason, figure them out, move on, and dance back for repairs if a new insight makes it necessary; and this being a separate track from people who raise lots of questions and are uncomfortable with the notion of settling on an answer. I don't expect those two tracks to meet much.

Okay, see, this is why I have trouble talking to philosophers in their quote standard language unquote.
I'll ask again: How would a computer program enumerate all reasons for action?

How large is the poly community? It seems like one of the Common Interest in Rationality groups; but I don't know if it's large enough that marginal investments in evangelism should be targeted there.

It's by Rick Cook, first novel Wizard's Bane.


or should we instead program an AI to figure out all the reasons for action that exist and account for them in its utility function

...this sentence makes me think that we really aren't on the same page at all with respect to naturalistic metaethics. What is a reason for action? How would a computer program enumerate them all?

I thought the punchline was going to be that the men were cats.

What should it mean in a world like that?

So let's say that you go around saying that philosophy has suddenly been struck by a SERIOUS problem, as in lives are at stake, and philosophers don't seem to pay any attention. Not to the problem itself, at any rate, though some of them may seem annoyed at outsiders infringing on their territory, and nonplussed at the thought of their field trying to arrive at answers to questions where the proper procedure is to go on coming up with new arguments and respectfully disputing them with other people who think differently, thus ensuring a steady flow of papers for all.
Let us say that this is what happens; which of your current beliefs, which seem to lead you to expect something else to happen, would you update?

As this is currently in the top position on the front page, I took the liberty of editing the top slightly to trigger fewer perceptual spam-detectors - there's no real reason not to tell people what the article is about up front.

Humans and Babykillers are not talking about the same subject matter when they debate what-to-do-next, and their doing different things does not constitute disagreement.

You're not writing my book, so why not start posting on it now? (As indeed many are already doing.)

I think Eliezer's meta-ethics is wrong because it's possible that we live in a world where Eliezer's "right" doesn't actually designate anything. That is, where a typical human's morality, when extrapolated, fails to be coherent. "Right" should still mean something in a world like that, but it doesn't under Eliezer's theory.
Also, to jump the gun a bit, your own meta-ethics, desirism, says:

Thus, morality is the practice of shaping malleable desires: promoting desires that tend to fulfill other desires, and discouraging desires that tend to thwart other desires.

What does this mean in the FAI context? To a super-intelligent AI, it's own desires, as well as those of everyone else on Earth, can be considered "malleable", in the sense that it can change all of them if it wanted to. But there might be some other super-intelligent AIs (created by aliens) whose desires it is powerless to change. I hope desirism doesn't imply that it should change my desires so as to fulfill the alien AIs' desires...

What I'm saying is that in the physical world there are only causes and effects, and the primeness of a heap of pebbles is not an ontologically basic fact operating as a separate and additional element of physical reality, but it is nonetheless about as "intrinsic" to the heap of pebbles as anything.
Once morality stops being mysterious and you start cashing it out as a logical function, the moral awfulness of a murder is exactly as intrinsic as the primeness of a heap of pebbles. Just as we don't care whether pebble heaps are prime or experience any affect associated with its primeness, the Pebblesorters don't care or compute whether a murder is morally awful; and this doesn't mean that a heap of five pebbles isn't really prime or that primeness is arbitrary, nor yet that on the "moral Twin Earth" murder could be a good thing. And there are no little physical primons associated with the pebble-heap that could be replaced by compositons to make it composite without changing the number of pebbles; and no physical stone tablet on which morality is written that could be rechiseled to make murder good without changing the circumstances of the murder; but if you're looking for those you're looking in the wrong closet.

Do you think a heap of five pebbles is intrinsically prime, or does it get its primeness from some extrinsic thing that attaches a tag with the five English letters "PRIME" and could in principle be made to attach the same tag to composite heaps instead? If you consider "beauty" as the logical function your brain's beauty-detectors compute, then is a screensaver intrinsically beautiful?
Does the word "intrinsic" even help, considering that it invokes bad metaphysics all by itself? In the physical universe there are only quantum amplitudes. Moral facts are logical facts, but not all minds are compelled by that-subject-matter-which-we-name-"morality"; one could as easily build a mind to be compelled by the primality of a heap of pebbles.

I voted this up, and the immediate parent down, and I DON'T NEED A REASON.

1-4 yes.
5 is questionable. When you say "Nothing is fundamentally moral" can you explain what it would be like if something was fundamentally moral? If not, the term "fundamentally moral" is confused rather than untrue; it's not that we looked in the closet of fundamental morality and found it empty, but that we were confused and looking in the wrong closet.
Indeed my utility function is generally indifferent to the exact state of universes that have no observers, but this is a contingent fact about me rather than a necessary truth of metaethics, for indifference is also a value. A paperclip maximizer would very much care that these uninhabited universes contained as many paperclips as possible - even if the paperclip maximizer were outside that universe and powerless to affect its state, in which case it might not bother to cognitively process the preference.
You seem to be angling for a theory of metaethics in which objects pick up a charge of value when some valuer values them, but this is not what I think, because I don't think it makes any moral difference whether a paperclip maximizer likes paperclips. What makes moral differences are things like, y'know, life, consciousness, activity, blah blah.


As a reader, I hate hate hate this strategy

I don't know about "hate" but it always feels empty and hollow, every time I read about a fictional supergenius, and that is where MoR came from.


McGonnaggal would likely have achieved better results if she had asked Harry to devise an elaborate prank that would have dubiously potentially helped Hagrid in some ambiguous way.

Well, you've surely got that right.


how much fun would Moby Dick have been if Ahab had just said "Eh, it's a dumb whale, shit happens" and got on with life?

Congratulations, you just wrote "Moby Dick and the Methods of Rationality".


What's the policy/netiquette on fanfiction based on HPMoR?

APPROVED

Haven't really thought about it until now, but I'll assume that Finite is a brute-force method requiring strength proportional to the original spell to cancel (so a Transfiguration that takes minutes would require a mass casting to cancel, perhaps) and sometimes won't work at all, while specialized counter-jinx just works if the caster has sufficient strength to cast it.

Yep, see Ch. 17.

I endorse the above.

"Hogwarts is full of people who'd find it awesome if Harry Potter gave them some of his personal attention."
Harry has given lots of people his personal attention, which he justified by the fact it would help them -- Neville, whom he pranked, Padma Patil, whom he pranked, Gregory Goyle, whom he pranked, Lesath Lestrange who he pranked others for... Even his own past self he pranked.
So why not Hagrid? I don't see this really being about Harry time-budgeting, it's more about the fact that he can't be simply nice to people -- McGonnaggal would likely have achieved better results if she had asked Harry to devise an elaborate prank that would have dubiously potentially helped Hagrid in some ambiguous way.


Can anything besides Gary's preferences provide a justification for saying that "Gary should_gary X"? (My own answer would be "No.")

This strikes me as an ill-formed question for reasons I tried to get at in No License To Be Human. When Gary asks "What is right?" he is asking the question e.g. "What state of affairs will help people have more fun?" and not "What state of affairs will match up with the current preferences of Gary's brain?" and the proof of this is that if you offer Gary a pill to change his preferences, Gary won't take it because this won't change what is right. Gary's preferences are about things like fairness, not about Gary's preferences. Asking what justifies shouldGary to Gary is either answered by having shouldGary wrap around and judge itself ("Why, yes, it does seem better to care about fairness than about one's own desires") or else is a malformed question implying that there is some floating detachable ontologically basic property of rightness, apart from particular right things, which could be ripped loose of happiness and applied to pain instead and make it good to do evil.

By saying "Gary should_gary X", do you mean

Shouldness does incorporate a concept of reflective equilibrium (people recognize apparent changes in their own preferences as cases of being "mistaken"), but should_Gary makes no mention of Gary (except insofar as Gary's welfare is one of Gary's terminal values) but instead is about a large logical function which explicitly mentions things like fairness and beauty. This large function is rightness which is why Gary knows that you can't change what is right by messing with Gary's brain structures or making Gary want to do something else.

Or, perhaps you are saying that one cannot give a concise definition of "should"

You can arrive at a concise metaethical understanding of what sort of thing shouldness is. It is not possible to concisely write out the large function that any particular human refers to by "should", which is why all attempts at definition seem to fall short; and since for any particular definition it always seems like "should" is detachable from that definition, this reinforces the false impression that "should" is an undefinable extra supernatural property a la Moore's Open Question.
By far the hardest part of naturalistic metaethics is getting people to realize that it changes absolutely nothing about morals or emotions, just like the fact of a deterministic physical universe never had any implications for the freeness of our will to begin with.
I also note that although morality is certainly not written down anywhere in the universe except human brains, what is written is not about human brains, it is about things like fairness; nor is it written that "being written in a human brain" grants any sort of normative status. So the more you talk about "fulfilling preferences", the less the subject matter of what you are discussing resembles the subject matter that other people are talking about when they talk about morality, which is about how to achieve things like fairness. But if you built a Friendly AI, you'd build it to copy "morality" out of the brains where that morality is written down, not try to manually program in things like fairness (except insofar as you were offering a temporary approximation explicitly defined as temporary). It is likewise extremely hard to get people to realize that this level of indirection, what Bostrom terms "indirect normativity", is as close as you can get to getting any piece of physical matter to compute what is right.
If you want to talk about the same thing other people are talking about when they talk about what's right, I suggest consulting William Frankena's wonderful list of some components of the large function:
"Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in one's own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc."
(Just wanted to quote that so that I didn't entirely fail to talk about morality in between all this stuff about preferences and metaethics.)

The closest point I've found to my metaethics in standard philosophy was called "moral functionalism" or "analytical descriptivism".
Cognitivism: Yes, moral propositions have truth-value, but not all people are talking about the same facts when they use words like "should", thus creating the illusion of disagreement.
Motivation: You're constructed so that you find some particular set of logical facts and physical facts impel you to action, and these facts are what you are talking about when you are talking about morality: for example, faced with the problem of dividing a pie among 3 people who all worked equally to obtain it and are all equally hungry, you find the mathematical fact that 1/3, 1/3, 1/3 is an equal division compelling - and more generally you name the compelling logical facts associated with this issue as "fairness", for example.
(Or as it was written in Harry Potter and the Methods of Rationality:
"Mr. Potter, in the end people all do what they want to do. Sometimes people give names like 'right' to things they want to do, but how could we possibly act on anything but our own desires?"
"Well, obviously I couldn't act on moral considerations if they lacked the power to move me. But that doesn't mean my wanting to hurt those Slytherins has the power to move me more than moral considerations!")
Moral epistemology: Statements can be true only when there is something they are about which makes them true, something that fits into the Tarskian schema "'X' is true iff X". I know of only two sorts of bearers of truth-value, two sorts of things that sentences can be about: physical facts (chains of cause and effect; physical reality is made out of causes a la Judea Pearl) and logical validities (which conclusions follow from which premises). Moral facts are a mixture of both; if you throw mud on a painting it becomes physically less beautiful, but for a fixed painting its "beauty" is a logical fact, the result of running the logical "beauty" function on it.

High probability this comment had something to do with the surprise creation of SPHEW.

Yup. Harry is allowed to know about cognitive psychology from arbitrary time periods, too, and have read anachronistic science books, etcetera. Science is Timeless.

I'd like to say "Let it be done!" but I don't recall who implemented the Discussion section originally. Possibly Louie?

There weren't any changes to that chapter...?

Oh, it's a critique all right, but it's not a feminist critique. One free karma point if you can guess what it's a critique of.

In lieu of an extended digression about how to adjust Solomonoff induction for making anthropic predictions, I'll simply note that having God create the world 5,000 years ago but fake the details of evolution is more burdensome than having a simulator approximate all of physics to an indistinguishable level of detail. Why? Because "God" is more burdensome than "simulator", God is antireductionist and "simulator" is not, and faking the details of evolution in particular in order to save a hypothesis invented by illiterate shepherds is a more complex specification in the theory than "the laws of physics in general are being approximated".
To me it seems nakedly obvious that "God faked the details of evolution" is a far more outre and improbable theory than "our universe is a simulation and the simulation is approximate". I should've been able to leave filling in the details as an exercise to the reader.

I'd planned to try and get HPMOR nominated for Best Novel in the 2011 awards (which will be awarded in 2012). I'm not sure if my personally being nominated for Best Fan Writer makes the work ineligible, or if it would be considered bad sport to try twice.

Get back in the box!


we discover that many of the counter-arguments that we advance against theist apologetics are (objectively speaking) equally effective against simulationist speculation

I've argued rather extensively against religion on this website. Name a single one of those arguments which is equally effective against simulationism.

"Gods are ontologically distinct from creatures, or they're not worth the paper they're written on." -- Damien Broderick
If you believe in a Matrix or in the Simulation Hypothesis, you believe in powerful aliens, not deities. Next!
There's also no hint of worship which everyone else on the planet thinks is a key part of the definition of a religion; if you believe that Cthulhu exists but not Jehovah, and you hate and fear Cthulhu and don't engage in any Elder Rituals, you may be superstitious but you're not yet religious.
This is mere distortion of both the common informal use and advanced formal definitions of the word "atheism", which is not only unhelpful but such a common religious tactic that you should not be surprised to be downvoted.
Also http://www.smbc-comics.com/index.php?db=comics&id=1817

Consider rewriting this as a post?

Though it's worth noting that this was a "That shouldn't happen" quote and not a "What a good idea" quote.

Everything you say after the 'No." is true but doesn't support your contradiction of:


I can't think of one single case in my experience when the argument "It has a small probability of success, but we should pursue it, because the probability ifwe don't try is zero" turned out to be a good idea.

Er ... isn't that the argument for cryonics?

There is no need to defend cryonics here. Just relax the generalisation. I'm surprised you 'can't think of a single case in your experience' anyway. It took me 10 seconds to think of three in mine. Hardly surprising - such cases turn up whenever the payoffs multiply out right.

Name one? We might be thinking of different generalizations here.

No. If everything else we believe about the universe stays true, and humanity survives the next century, cryonics should work by default. Are there a number of things that could go wrong? Yes. Is the disjunction of all those possibilities a large probability? Quite. But by default, it should simply work. Despite various what-ifs, ceteris paribus, adding carbon dioxide to the atmosphere would be expected to produce global warming and you would need specific evidence to contradict that. In the same way, ceteris paribus, vitrification at liquid nitrogen temperatures ought to preserve your brain and preserving your brain ought to preserve your you, and despite various what-ifs, you would need specific evidence to contradict that it because it is implied by the generalizations we already believe about the universe.

So Chuunin Exam Day, then? I've never read it, but I've heard of it.
Considering that I was able to identify the author and possibly the exact fic from the information that the morality was being heavily lambasted, may I suggest that readers noticing nonlampshaded evil doesn't actually happen all that often? TV Tropes is good at noticing Moral Dissonance, but literally nowhere else that I've ever heard of. It took a critic on the order of David Brin to point out that Aragorn wasn't democratically elected.

I.e., agree with the morals -> don't notice the bad writing?

I don't have permission to view that, says the board. But, just taking a wild guess here, that wouldn't be a Perfect Lionheart fic would it? Because unless the same forumgoers are also lambasting the Bible and David Eddings, one can't help but suspect that it's not the content so much as the writing which triggers the hate.

I beg to be allowed to post this (with attribution of course) in Omake Files #3.

This happens when <div> tags are included, anywhere. I've deleted them.

There's this:
People are always amazed by how much "free time" I have.
They're also amazed that I don't know who Ally McBeal is.
Frankly, I'm amazed that they can't make the connection."
-- Robert Wenzlaff


'Twas sarcasm, friend. http://lesswrong.com/lw/hi/futuristic_predictions_as_consumable_goods/

Given Vast quantities of computing power, it would qualify as a very silly AGI which will eventually try dropping an anvil on its own head just to see what happens.
Roughly, no.

Now there was a book that was not like what the title would lead you to expect.

Then only programs for that particular UTM are "precisely formulated". How do you interpret them as questions, even?

This could well be the first good bit of Singularity fanfiction. Ever.

Why? We already consumed those delicious predictions.

In which case the nonprovision of that info is also information.
But it wouldn't at all resemble marketing as we know it, either way.


General AI will not be made in 2011. Confidence: 90%.

I hope to hell you're underconfident about that.

When somebody makes a statement you don't understand, don't tell him he's crazy. Ask him what he means.
-- H Beam Piper, Space Viking

It is necessary at all times to distinguish whether we are talking about humans or rational agents, I think.
<humans>
If you expect that larger organizations mount more effective marketing campaigns and do not attend to their own diminishing marginal utility and that most people don't attend to the diminishing marginal utility either, you should look for maximum philanthropic return among smaller organizations doing very important, almost entirely neglected things that they have trouble marketing, but not necessarily split your donation up among those smaller organizations, except insofar as, being a human, you can donate more total money if you split up your donations to get more glow.
</humans>
<rational agents>
Marketing campaign? What's a marketing campaign?
</rational agents>


If your goal is not to maximize altruism, but rather to ensure a certain minimum level of altruism given massive uncertainty about the effectiveness of charities

No one who cares about helping people cares about that.
Only people trying to ensure a satisficing level of warm glow care about that.
What part of "Steven Landsburg was simply correct about what a rational agent should do" is so hard for people to come to terms with? Not every mistake can be excused as an amazing clever strategy in disguise.


Consider going swimming and having to overcome the pain of entering water colder than surrounding. This pain, less momentary than the one in question and (more or less) equally discounted, doesn't produce problematic hesitation.

...I just realized why I so rarely go swimming.

http://lesswrong.com/lw/jp/occams_razor/
http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/

Fixed.

Hm. Right now, you can't downvote more than you've been upvoted. Suppose a Plonk costs 1000 downvotes, could only be applied once per user-pair, and increased the minimum viewability threshold of a user by 1. So if two people Plonked timtyler, his comments would start disappearing once they'd been voted down to -1, instead of -3. The opposite of a Plonk would be an Accolade and that would make comments harder to hide, lowering the threshold by 1?
Doesn't actually sound like a good idea to me, but I do sometimes get the sense that there ought to be better incentives for people to take hints.


pain of continuing to procrastinate, which is, once again, usually less painful than being in the middle of doing the work.

I believe you meant this to be the other way around.

Look at his actual comments.


I am intimately familiar with how splitting donations into fine slivers is very much in the interests of all charities except the very largest;

Not the largest, the neediest.
As charities become larger, the marginal value of the next donation goes down; they become less needy. In an efficient market for philanthropy you could donate to random charities and it would work as well as buying random stocks. We do NOT have an efficient market in philanthropy.

With due respect to your wit, I would point out that although Slytherin the fictional character lived centuries ago, he was, in fact, invented after Nietzsche, and it is possible the correlation is noncoincidental and begins with Nietzsche as a cause.

Risk aversion would apply if you were an egoist trying to make sure you got at least some warm glow. It does not apply if you're an altruist trying to help people as much as they can be helped.
This is not a complicated issue.

A logic which applies only to people who are interested in getting a warm glow and not for people interested in helping. Diversifying charitable investments maximizes your chance of getting at least some warm glow from "having helped". It does not help people as best they can be helped.
I'm beginning to think that LW needs some better mechanism for dealing with the phenomenon of commenters who are polite, repetitive, immune to all correction, and consistently wrong about everything. I know people don't like it when I say this sort of thing, but seriously, people like that can lower the perceived quality of a whole website.

Moved to LW main and promoted.
Let me know if this seems like a bad idea for some reason, but when something gets 34 upvotes...

Landsburg is correct about what rational agents should do. (Period.)
Human altruists may have to resort to more complex tactics like http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/.

Moved to Discussion section.

Suggest turning this into LW main post. (You can Edit and re-save it there.)


Even Jesus said, "The poor you will have with you always", to justify spending an INCREDIBLE amount of money (enough to buy ten people's entire lives, in an era with no inflation, making it comparable to ten million US dollars today) on pouring perfume once on Jesus' feet

"Even Jesus"? Does it occur to you that making this a religious example is actually even MORE likely to get us to notice the moral dissonance, not convince us to excuse it?

Why Our Kind Can't Cooperate, exhibit #37B.

Obvious guess: Your blood then contains antibodies to the blood type of your babies.


Giving blood is important to me. It is so important that I have chosen not to pursue relationships with other men in order than I can continue to give blood without lying to do so.

On the margins, I expect that each marginal pint of blood saves only a very small fraction of a life. As several readers pointed out, this doesn't mean that we should ordinarily be calculating on the margins, since it's not like you can use a pint of blood for something else instead; in terms of moral credit, you should think of yourself as part of a reference class of people who all choose to donate blood for around the same reasons, and who all get an equal share of the lives saved.
However, the Red Cross has already decided that they're willing to X out the entire homosexual community, and I would expect the reference class of those who refrain from sexual activity in order to continue donating blood to be small, and I would guess that if this entire reference class refrained from donating blood, not a single additional life might be lost.
Modern-day hospitals are not, so far as I know, blood-limited. They need a routine flow of blood in order to routinely save lives. They do not need more blood to save more lives. That's the impression I got, anyway; some quick Googling even said that they usually have enough blood to just use O-negative instead of matching types.
I hate to say this, but I think you're making the wrong sacrifices here. I estimate a very high information value for further investigation on your part; I would expect it to show that you were safe to stop donating blood and resume sexual activity without costing anyone one-twentieth of a life. If you're really feeling guilty or worried, resume sexual activity and send a donation to the Singularity Institute as a carbon offset. If you can speed up a positive Singularity by one minute that works out to around 100 lives, never mind increasing the probability.

Maybe I'm just being naive here, but in a case that straightforward and that possible for the average person to understand, where there's nothing odd or unprestigious about the action and lots of people are doing it already, where, on the margins, an additional American life is saved each time another person donates blood, I have trouble believing that even a world this insane wouldn't push blood donations a little harder.


I always try to give blood as often as possible under the assumption that I save at least one life each time

That can't possibly be right, not on the margins.

Thx fixed.

Added reference to the writeup of this experiment in Wikipedia: http://en.wikipedia.org/wiki/Hong%E2%80%93Ou%E2%80%93Mandel_effect
HT Manfred.

Manfred (below) posted a complete analysis of the exact setup from Joint Configurations, given in Wikipedia, but failed to put enough exclamation marks on it.
http://en.wikipedia.org/wiki/Hong%E2%80%93Ou%E2%80%93Mandel_effect


I've been an egoist for as long as I can remember

No offense intended, but: If you could take a pill that would prevent all pain from your conscience, and it could be absolutely guaranteed that no one would ever find out, how many twelve-year-olds would you kill for a dollar?
(Perhaps you meant to say that you were mostly egoist, or that your deliberatively espoused moral principles were egoistic?)
PS: Welcome to Less Wrong!

(If you're wondering why this is being voted down, it's for a comment of the general form "Oh, how can you claim to be 'less wrong' and yet do X, which is wrong?" which will automatically get downvoted to oblivion. Someone else is welcome to explain why in more detail, I don't feel like putting in that effort at the moment.)

suggest turning this into a post


The intelligence to calculate the CEV needs to be pre-FOOM.

No, a complete question to which CEV is an answer needs to be pre-FOOM. All an AI needs to know about morality before it is superintelligent is (1) how to arrive at a CEV-answer by looking at things and doing calculations and (2) how to look at things without breaking them and do calculations without breaking everything else.


Eliezer's personal desire might be to implement CEV of humanity (whatever it turns out to be). I believe, however, that for well over 99% of humans this would not be the best possible outcome they might desire.

I'm not clear on what you could mean by this. Do you mean that you think the process just doesn't work as advertised, so that 99% of human beings end up definitely unhappy and with there existing some compromise state that they would all have preferred to CEV? Or that 99% of human beings all have different maxima so that their superposition is not the maximum of any one of them, but there is no single state that a supermajority prefers to CEV?


2) How can anyone sincerely want to build an AI that fulfills anything except their own current, personal volition?

I am honestly not sure what to say to people who ask this question with genuine incredulity, besides (1) "Don't be evil" and (2) "If you think clever arguments exist that would just compel me to be evil, see rule 1."

Greetings, fellow thinkers! I'm a 19-year-old undergraduate student at Clemson University, majoring in mathematics (or, as Clemson (unjustifiably) calls it, Mathematical Sciences). I found this blog through Harry Potter and the Methods of Rationality about three weeks ago, and I spent those three weeks doing little else in my spare time but reading the Sequences (which I've now finished).
My parents emigrated from the Soviet Union (my father is from Kiev, my mother from Moscow) just months before my birth. They spoke very little English upon their arrival, so they only spoke Russian to me at home, and I picked up English in kindergarten; I consider both to be my native languages, but I'm somewhat more comfortable expressing myself in English. I studied French in high school, and consider myself "conversant", but definitely not fluent, although I intend to study abroad in a Francophone country and become fluent. This last semester I started studying Japanese, and I intend to become fluent in that as well.
My family is Jewish, but none of my relatives practice Judaism. My mother identifies herself as an agnostic, but is strongly opposed to the Abrahamic religions and their conception of God. My father identifies as an atheist. I have never believed in Santa Claus or God, and was very confused as a child about how other people could be so obviously wrong and not notice it. I've never been inclined towards mysticism, and I remember espousing Physicalist Reductionism (although I did not know those words) at an early age, maybe when I was around 9 year old.
I've always been very concerned with being rational, and especially with understanding and improving myself. I think I missed out on a lot of what Americans consider to be classic sci-fi (I didn't see Star Wars until I got to college, for example), but I grew up with a lot of good Russian sci-fi and Orson Scott Card.
I used to be quite a cynical misanthrope, but over the past few years I've grown to be much more open and friendly and optimistic. However, I've been an egoist for as long as I can remember, and I see no reason why this might change in the foreseeable future (this seems to be my primary point of departure from agreement with Eliezer). I sometimes go out of my way to help people (strangers as much as friends) because I enjoy helping people, but I have no illusions about whose benefit my actions are for.
I'm very glad to have found a place where smart people who like to think about things can interact and share their knowledge!

A tad too much cynicism, there. You can't think of any other reason to move it to the Discussion page, under those circumstances?

Yes.

No.
In fact it should be provided for them, as it should be for all human beings and, just in case, chimpanzees.
Any other questions?

Either my views changed since that time, or what I was trying to communicate by it was that 3 was the most inscrutable part of the problem to people who try to tackle it, rather than that it was the blocker problem. 1 is the blocker problem, I think. I probably realize that now to a greater degree than I did at that time, and probably also made more progress on 3 relative to 1, but I don't know how much my opinions actually changed (there's some well-known biases about that).

So would it be fair to say that figuring out how to build a good paperclipper, as opposed to a process that does something we don't understand, already requires solving the hard part?

Current estimate says yes. There would still be an inscrutable problem to solve too, but I don't think it would have quite the same impenetrability about it.

It wasn't the topicness so much as the degree to which the post seemed written in the form of... natter, maybe, would be the word to describe it? It read like Discussion, and not like a main LW post.

Heck, who would think that a bunch of savanna apes would manage to edit DNA using their fingers?


I personally suspect that it will basically come down to solving the stability under self-modification problem... This may already be the part of the problem that people in the know think is difficult

Yes, it is already the part we suspect will be difficult. The other part may or may not be difficult once we solve that part.

is the above discussion just me starting to suspect how hard friendliness is?

No, you still haven't started to suspect that.
Also, moving this post to the Discussion section.

I expect an algorithm polynomial in the size of the table would exist.

It's not particularly on-topic for the main site.

Logical uncertainty is an open problem, of course (I attended a workshop on it once, and was surprised at how little progress has been made).
But so far as Dutch-booking goes, the obvious way out is 2 with the caveat that the probability distribution never has P(X) < (PX&Y) at the same time, i.e., you ask it P(X), it gives you an answer, you ask it P(X&Y), it gives you a higher answer, you ask it P(X) again and it now gives you an even higher answer from having updated its logical uncertainty upon seeing the new thought Y.
It is also clear from the above that taw does not comprehend the notion of subjective uncertainty, hence the notion of logical uncertainty.

I suggest you look up the concept of "subjective Bayesian". Probabilities are states of knowledge. If you don't know an answer, it's uncertain. If someone who doesn't know anything you don't can look over your odds and construct a knowably losing bet anyway, or construct a winning bet that you refuse, then you are Dutch-bookable.
Also, considering that you have apparently been reading this site for years and you still have not grasped the concept of subjective uncertainty, and you are still working with a frequentist notion of probability, nor yet have you even grasped the difference, I would suggest to you in all seriousness that you seek enlightenment elsewhere.
(Sorry, people, there's got to be some point at which I can express that. Downvote if you must, I suppose, if you think such a concept is unspeakable or unallowed, but it's not a judgment based on only one case of incomprehension, of course.)

Correct.

It would be valuable training in the fact that everyone else in the world is crazy.

...isn't that just what we want to train the kids to spot, though?

Upvoted for catching on to the nonexistence of God before you figured out Santa Claus. I think that's the first time I've heard that one.


I found a page stuffed into a drawer that had been ripped out of a book--it explained how to tell your kids that Santa wasn't real.

Shades of what happened when I found the secret hidden volume of my beloved Childcraft set, the "Guide to Parents"! Only in my case, that's how I found out what teenagers were supposed to be like and that's when I decided never to go there.


I can write down many such utility functions easily, as contrasted with the difficulty of describing friendliness, so I can at least hope to design an AI which has one of them.

What you can do when you can write down stable utility functions - after you have solved the self-modification stability problem - but you can't yet write down CEV - is a whole different topic from this sort of AI-boxing!


Each genie you produce will try its best to find a solution to the problem you set it, that is, they will respond honestly. By hypothesis, the genie isn't willing to sacrifice itself just to destroy human society.

Thanks to a little thing called timeless decision theory, it's entirely possible that all the genies in all the bottles can cooperate with each other by correctly predicting that they are all in similar situations, finding Schelling points, and coordinating around them by predicting each other based on priors, without causal contact. This does not require that genies have similar goals, only that they can do better by coordinating than by not coordinating.

I had my suspicions about Santa pretty early--as a too-curious preschooler snooping in my parents' bedroom, I found boxes for some gifts that had been "from Santa"; my mother had made up some story explaining it. Later (6 or 7 years old?) I found a page stuffed into a drawer that had been ripped out of a book--it explained how to tell your kids that Santa wasn't real. I read all of the books on the shelf at home, including the parenting book; that was the bit of knowledge my parents wanted to hide from me! (I suppose they thought I would be too young to understand some of the other stuff I'd find, but that I would understand that if I found it. My parents really had no idea how to deal with a young voracious reader.)
So I knew that Santa wasn't real but that my parents (my mother, really) cared that I not find out. I don't think Santa in particular affected me much in part because I was a voracious reader--I knew a lot of things that were different than what my parents told me, and I also knew that most parents were advised that kids might not be ready to know them. (Like I said, I read their parenting book.)
Knowing that I couldn't trust my parents to tell me the truth about a lot of things (not just Santa) because they thought it better that I know a pleasant lie, and also that they really had no idea what I was and wasn't ready to hear, had a tremendous effect on me. I didn't trust them even when I should have, in fact; I rarely trusted people to be telling me the whole truth or to have good judgment about what I should and shouldn't be doing. (I also grew up in a weird household, the main thing being that my mother was hospitalized for mental illness when I was 11.) It was good for me in some ways--but there are some big things I should have sought someone's guidance for, if not my parents', but I simply had no idea how to go about it or even that I should.

Tried writing a paragraph or two of explanation, gave it up as too large a chunk. It also feels to me like I've explained this three or four times previously, but I can't remember exactly where.

Remind me if I or the other admins forget to do this henceforth.

HPMOR demonstrates:
1) People usually don't recognize faked genius as faked when they see it; they don't realize what's missing from "genius" characters in their fiction.
2) However, if you then show them real genius, they can recognize it as new, different, better, and important (though they may not realize what the added ingredient was).

B is false.

Does this still hold if you remove the word "hostile", i. e. if the "friendliness" of the superintelligence you construct first is simply not known?

Heh. I'm afraid AIs of "unknown" motivations are known to be hostile from a human perspective. See Omohundro on the Basic AI Drives, and the Fragility of Value supersequence on LW.

Oh, bravo! True rationalist fiction, and that is very rare.

Jungle wa Itsumo Hale Nochi Guu!
I consider her one of my role models.

Subproblems like that seem likely to be rare.


The probability I assign to achieving a capability state where it is (1) possible to prove a mind Friendly even if it has been constructed by a hostile superintelligence, (2) possible to build a hostile superintelligence, and (3) not possible to build a Friendly AI directly, is very low.

A general theory of quarantines would nevertheless be useful.

For what?


a certifiably friendly AI: a class of optimization processes whose behavior we can automatically verify will be friendly

The probability I assign to achieving a capability state where it is (1) possible to prove a mind Friendly even if it has been constructed by a hostile superintelligence, (2) possible to build a hostile superintelligence, and (3) not possible to build a Friendly AI directly, is very low.
In particular the sort of proof techniques I currently have in mind - what they prove and what it means - for ensuring Friendliness through a billion self-modifications of something that started out relatively stupid and built by relatively trusted humans, would not work for verifying Friendliness of a finished AI that was handed you by a hostile superintelligence, and it seems to me that the required proof techniques for that would have to be considerably stronger.
To paraphrase Mayor Daley, the proof techniques are there to preserve the Friendly intent of the programmers through the process of constructing the AI and through the AI's self-modification, not to create Friendliness. People hear the word "prove" and assume that this is because you don't trust the programmers, or because you have a psychological need for unobtainable absolute certainty. No, it's because if you don't prove certain things (and have the AI prove certain things before each self-modification) then you can't build a Friendly AI no matter how good your intentions are. The good intentions of the programmers are still necessary, and assumed, beyond the parts that are proved; and doing the proof work doesn't make the whole process absolutely certain, but if you don't strengthen certain parts of the process using logical proof then you are guaranteed to fail. (This failure is knowable to a competent AGI scientist - not with absolute certainty, but with high probability - and therefore it is something of which a number of would-be dabblers in AGI maintain a careful ignorance regardless of how you try to explain it to them, because the techniques that make them enthusiastic don't support that sort of proof. "It is hard to explain something to someone whose job depends on not understanding it.")

I'm afraid not.

e^-x is its own second derivative. sin(x) is its own fourth derivative (note relation to e^ix).
And welcome to LW! (he said)

That was more or less precisely the thought going through my mind when I was imagining how I would design the system if I was doing it from scratch. Though not with the "mark and delete" part, just "check to see if love is reciprocable before allowing process to proceed".

Upvoted for thinking about the problem for five minutes.

Probabilities of 1 and 0 are considered rule violations and discarded.

Should people be allowed to have their hearts broken, ever, whether or not a society does it to them deliberately?

I should like to know Yvain's prior on this.

Same reaction here. Since it is physically possible to break RSA, it seems obvious to me that RSA will be broken... eventually.

Saved everyone on the third day.
(Just kidding; I got the best available ending, thought I'd lost, and found only by checking online that it was the best available one.)

I can't help but ask whether you've ever found this advice personally useful, and if so, how.


If you can prove that a theory has no models, then you can prove a contradiction in a finite number of steps.

No, if a first-order theory has no models, then you can prove a contradiction from it. Not, if it provably has no models. Just, if it has no models, period, then it proves a contradiction in a finite number of steps.

In the language of model theory, Nelson believes that one can prove that Peano arithmetic has no models. He does not believe one can prove that Peano arithmetic minus induction has no models.

That... is a very strange combination of beliefs. I honestly cannot imagine what he could possibly be thinking. It's like someone sat around thinking, "Hm, what could I come up with that would be even dumber than believing PA to be inconsistent?" Indeed, I notice my own confusion and suspect that you may have misunderstood something that was stupid but not quite that stupid.

But we do not have to have a model, or even to know any model theory, to "talk about something."

Then what is it talking about?

If you believe in second-order logic then you believe there's only one second-order logic.


We see cause and effect relationships everywhere, and it is natural to wonder about the first cause.

It is. It's not natural to wonder if the first cause is a complex structured intelligent being, because such complicated and internally correlated structures demand simpler preceding causes of which to be the effects, for if we try to model the structure as uncaused we have unexplained internal correlations, which is a no-no in causal graphs.
If you then start making special pleading excuses about an intelligence that you predict using a complex structured internally correlated model but which you claim to have no structure so that you can pretend it's simple even though you can't exhibit any simple computer program that does the same thing, it's really unnatural - not just physically unnatural, but epistemically unnatural.

Hence "so young or so isolated as to actually believe that stuff". People who genuinely believe out of fear of hell will not long survive exposure to Reddit.

Theories with no models are not talking about anything, and semantically imply all consequences.
In first-order logic, every theory with no models syntactically proves a contradiction in a finite number of steps.

I... I just realized... there's no evidence whatsoever of the Glowing Purple Space Cannibals, nobody's ever even postulated their existence...

Except for religionites so young or so isolated as to actually believe that stuff, people are not believers because they fear hell. Rather, they fear hell in order to go on believing.


There are models of axioms 1-5 that are not the intersection of all models of axioms 1-4.

In second-order logic I believe your statement above to be simply false. It's false just for using the plural "models"; there is only one model of the second-order axioms 1-5.
Do you claim your statement is true in second-order logic?

Saved everyone on the third day.
(Just kidding; I got the best available ending, thought I'd lost, and found only by checking online that it was the best available one.)

I can't help but ask whether you've ever found this advice personally useful, and if so, how.


If you can prove that a theory has no models, then you can prove a contradiction in a finite number of steps.

No, if a first-order theory has no models, then you can prove a contradiction from it. Not, if it provably has no models. Just, if it has no models, period, then it proves a contradiction in a finite number of steps.

In the language of model theory, Nelson believes that one can prove that Peano arithmetic has no models. He does not believe one can prove that Peano arithmetic minus induction has no models.

That... is a very strange combination of beliefs. I honestly cannot imagine what he could possibly be thinking. It's like someone sat around thinking, "Hm, what could I come up with that would be even dumber than believing PA to be inconsistent?" Indeed, I notice my own confusion and suspect that you may have misunderstood something that was stupid but not quite that stupid.

But we do not have to have a model, or even to know any model theory, to "talk about something."

Then what is it talking about?

If you believe in second-order logic then you believe there's only one second-order logic.


We see cause and effect relationships everywhere, and it is natural to wonder about the first cause.

It is. It's not natural to wonder if the first cause is a complex structured intelligent being, because such complicated and internally correlated structures demand simpler preceding causes of which to be the effects, for if we try to model the structure as uncaused we have unexplained internal correlations, which is a no-no in causal graphs.
If you then start making special pleading excuses about an intelligence that you predict using a complex structured internally correlated model but which you claim to have no structure so that you can pretend it's simple even though you can't exhibit any simple computer program that does the same thing, it's really unnatural - not just physically unnatural, but epistemically unnatural.

Hence "so young or so isolated as to actually believe that stuff". People who genuinely believe out of fear of hell will not long survive exposure to Reddit.

Theories with no models are not talking about anything, and semantically imply all consequences.
In first-order logic, every theory with no models syntactically proves a contradiction in a finite number of steps.

I... I just realized... there's no evidence whatsoever of the Glowing Purple Space Cannibals, nobody's ever even postulated their existence...

Except for religionites so young or so isolated as to actually believe that stuff, people are not believers because they fear hell. Rather, they fear hell in order to go on believing.


There are models of axioms 1-5 that are not the intersection of all models of axioms 1-4.

In second-order logic I believe your statement above to be simply false. It's false just for using the plural "models"; there is only one model of the second-order axioms 1-5.
Do you claim your statement is true in second-order logic?

Saved everyone on the third day.
(Just kidding; I got the best available ending, thought I'd lost, and found only by checking online that it was the best available one.)

I can't help but ask whether you've ever found this advice personally useful, and if so, how.


If you can prove that a theory has no models, then you can prove a contradiction in a finite number of steps.

No, if a first-order theory has no models, then you can prove a contradiction from it. Not, if it provably has no models. Just, if it has no models, period, then it proves a contradiction in a finite number of steps.

In the language of model theory, Nelson believes that one can prove that Peano arithmetic has no models. He does not believe one can prove that Peano arithmetic minus induction has no models.

That... is a very strange combination of beliefs. I honestly cannot imagine what he could possibly be thinking. It's like someone sat around thinking, "Hm, what could I come up with that would be even dumber than believing PA to be inconsistent?" Indeed, I notice my own confusion and suspect that you may have misunderstood something that was stupid but not quite that stupid.

But we do not have to have a model, or even to know any model theory, to "talk about something."

Then what is it talking about?

If you believe in second-order logic then you believe there's only one second-order logic.


We see cause and effect relationships everywhere, and it is natural to wonder about the first cause.

It is. It's not natural to wonder if the first cause is a complex structured intelligent being, because such complicated and internally correlated structures demand simpler preceding causes of which to be the effects, for if we try to model the structure as uncaused we have unexplained internal correlations, which is a no-no in causal graphs.
If you then start making special pleading excuses about an intelligence that you predict using a complex structured internally correlated model but which you claim to have no structure so that you can pretend it's simple even though you can't exhibit any simple computer program that does the same thing, it's really unnatural - not just physically unnatural, but epistemically unnatural.

Hence "so young or so isolated as to actually believe that stuff". People who genuinely believe out of fear of hell will not long survive exposure to Reddit.

Theories with no models are not talking about anything, and semantically imply all consequences.
In first-order logic, every theory with no models syntactically proves a contradiction in a finite number of steps.

I... I just realized... there's no evidence whatsoever of the Glowing Purple Space Cannibals, nobody's ever even postulated their existence...

Except for religionites so young or so isolated as to actually believe that stuff, people are not believers because they fear hell. Rather, they fear hell in order to go on believing.


There are models of axioms 1-5 that are not the intersection of all models of axioms 1-4.

In second-order logic I believe your statement above to be simply false. It's false just for using the plural "models"; there is only one model of the second-order axioms 1-5.
Do you claim your statement is true in second-order logic?



If not, can you summarize your arguments in favor of choosing B?

Well, if I choose B, I'll be alive for a very large number of years. I'll be alive so long, that I expect that I'll get used to anything deployed to torture me. And I'll be alive so long, I'd need to study a fair amount of cosmology just to understand what my lifetime will involve, by way of the deaths and rebirths of whole universes or whatever. Some of that would be interesting to see.
The easy thought experiment would be dust speck vs. 3 years of torture followed by death. I think there, I'd go with the speck.

Nope, ritual magic = permanent sacrifice.

Unbreakable Vows are ridiculously broken, as Harry briefly observes in Ch. 74. They're even more ridiculous in fanfictions where people can just grab a wand and swear something on their life and magic and thereby create a magically binding vow. I had to nerf the hell out of their activation costs just to make the MoR-verse keep running. I can't depict a society with zero agency problems, a perfect public commitment process and an infinite trust engine unless the whole story is about that.

Web of Angels.

FYI to other readers: Citation does not support claim, it's about linear models of wine-tasting rather than experimental support for psychotherapy.

I am totally using that as my rejoinder there - "If Dudley can get a Playstation in 1993, clearly Playstations are timeless in canon."

Since you have already replied to the grandparent with a partial affirmation could you please confirm or (I hope) deny the primary contention of said comment?

My interpretation (which Eliezer's above comment seems to have confirmed) was, Eliezer deleted Roko's comment for the exact same reason he would have deleted an epileptic-fit-inducing animation.

That is another idiot ball which I have assumed you are not guilty of bearing. But if you are giving support to a comment which presents such an interpretation it warrants clarification.

Depends what you mean by "exact same". I deleted the basilisk strictly to protect readers, yes. I didn't realize at the time that there was also an immediate damage mode for unusually vulnerable readers.

Yes, whole rafts of stuff are being made-up here.

If I thought whole-brain emulation were far more effective I would be pushing whole-brain emulation, FOR THE LOVE OF SQUIRRELS!

On the one hand, everything you say would be true, if we were assigning consistent probabilities.
On the other hand, I've never been able to assign consistent probabilities over the LHC and knowing this hasn't helped me either.

I question the explanandum. It wouldn't surprise me to learn that I was more upvoted, but it wasn't my impression that I was.

Sure, and people feel safer driving than riding in an airplane, because driving makes them feel more in control, even though it's actually far more dangerous per mile.
Probably a lot of people would feel more comfortable with a genie that took orders than an AI that was trying to do any of that extrapolating stuff. Until they died, I mean. They'd feel more comfortable up until that point.
Feedback just supplies a form of information. If you disentangle the I-want-to-drive bias and say exactly what you want to do with that information, it'll just come out to the AI observing humans and updating some beliefs based on their behavior, and then it'll turn out that most of that information is obtainable and predictable in advance. There's also a moral component where making a decision is different from predictably making that decision, but that's on an object level rather than metaethical level and just says "There's some things we wouldn't want the AI to do until we actually decide them even if the decision is predictable in advance, because the decision itself is significant and not just the strategy and consequences following from it."

It gets to possibly say "No", once. Nothing else.
Are you under the impression that jumping on statements like this, after the original statement explicitly disclaimed them, is a positive contribution to the conversation?

As soon as I saw "Witching was turning out to be..." in the "Recent Comments" bar, I said, "Hey, I bet that's a Pratchett quote".

Michael Vassar is usually the voice within SIAI of such concerns. It hasn't been formally written up yet, but besides the Last Judge notion expressed in the original CEV paper, I've also been looking favorably on the notion of giving a binary veto over the whole process, though not detailed control, to a coherent extrapolated superposition of SIAI donors weighted by percentage of income donated (not donations) or some other measure of effort exerted.
And before anyone points it out, yes I realize that this would require a further amendment to the main CEV extrapolation process so that it didn't deliberately try to sneak just over the veto barrier.
Look, people who are carrying the Idiot Ball just don't successfully build AIs that match up to their intentions in the first place. If you think I'm an idiot, worry about me being the first idiot to cross the Idiot Finish Line and fulfill the human species' destiny of instant death, don't worry about my plans going right enough to go wrong in complicated ways.

I'm glad someone noticed that. It was NOT EASY to compress that entire metaethical debate down into two paragraphs of text that wouldn't distract from the main story.

I've been transparent about CEV and intend to continue this policy.

You're totally misunderstanding the situation.


If we assume that culture is adaptive

Well, sure, but couldn't you assume something more ridiculous if you tried hard enough?

Does this theory of yours require that Eliezer Yudkowsky plus several other old-time Less Wrongians are holding the Idiot Ball and being really stupid about something that you can just see as obvious?
Now might be a good time to notice that you are confused.


most people are irrational, hypocritical and selfish, if you try and tell them they shoot the messenger, and if you try and do anything you bear all the costs, internalize only tiny fractions of the value created if you succeed,

So? They're just kids!
(or)
He glanced over toward his shoulder, and said, "That matter to you?"
Caw!
He looked back up and said, "Me neither."


Expand? Are you talking about saying things about the output of CEV, or something else?

Not just the output, the input and means of computation are also potential minefields of moral politics. After all this touches on what amounts to the ultimate moral question: "If I had ultimate power how would I decide how to use it?" When you are answering that question in public you must use extreme caution, at least you must if you have any real intent to gain power.
There are some things that are safe to say about CEV, particularly things on a technical side. But for most part it is best to avoid giving too many straight answers. I said something on the subject of what can be considered the subproblem ("Do you confess to being consequentialist, even when it sounds nasty?"). Eliezer's responses took a similar position:


then they would be better off simply providing an answer calibrated to please whoever they most desired to avoid disapproval from

No they wouldn't. Ambiguity is their ally. Both answers elicit negative responses, and they can avoid that from most people by not saying anything, so why shouldn't they shut up?

When describing CEV mechanisms in detail from the position of someone with more than detached academic interest you are stuck between a rock and a hard place.
On one hand you must signal idealistic egalitarian thinking such that you do not trigger in the average reader those aversive instincts we have for avoiding human tyrants.
On the other hand you must also be aware that other important members (ie. many of those likely to fund you) of your audience will have a deeper understanding of the practical issues and will see the same description as naive to the point of being outright dangerous and destructive.


You intend to give it a morality based on the massed wishes of humanity -

See the "Last Judge" section of the CEV paper.

Therefore, you are, by your own statements, raising the risk of my infinite torture from zero to a tiny non-zero probability.

As Vladimir observes, the alternative to SIAI doesn't involve nothing new happening.

Reminder: Exposure to the basilisk can cause and has caused immediate severe mental torment to people with OCD or strong OCD tendencies. Again, this has already happened (at least two reports that I know of). So that's like posting a video that gives vulnerable people epileptic fits, like that infamous Pokemon episode.
"Please remember", he said in a dryly sarcastic voice, "that not everyone's mind is an invincible fortress like yours."

Aw, look, it's someone sane.

No, WFG committed to that before I said anything in Mod Voice.

I invite anyone who still sides with WaitingForGodel at this point to leave and find a site more suited to their intellects. I am sure it will only frustrate them and us to have them stick around.

Your post has been moved to the Discussion section, not deleted.

Moved post to Discussion section. Note that user's karma has now dropped below what's necessary to submit to the main site.

Note that comments like these are still not being deleted, by the way. LW censors Langford Basilisks, not arbitrarily great levels of harmful stupidity or hostility toward the hosts - those are left to ordinary downvoting.

WFG gave this reply which was downvoted under the default voting threshold:

Agree except for the 'terrorism' and 'allegedly' part.
I just emailed a right-wing blogger some stuff that probably isn't good for the future. Not sure what the increase was, hopefully around 0.0001%.
I'll write it up in more detail and post a top-level discussion thread after work.
-wfg

I'm also reposting this just in case wfg tries to delete it or modify it later, since I think it's important for everyone to see. Ordinarily I'd consider that a violation of netiquette, but under these here exact circumstances...

I should have taken this bet

Consider that clarification made; no such threat is intended.

(Shrugs.)
Your decision. The Singularity Institute does not negotiate with terrorists.

Yes, the attempt to censor was botched and I regret the botchery. In retrospect I should have not commented or explained anything, just PM'd Roko and asked him to take down the post without explaining himself.

Reposting comments deleted by the authors or by moderators will be considered hostile behavior and interfering with the normal and intended behavior of the site and its software, and you will be asked to leave the Less Wrong site.
-- Eliezer Yudkowsky, Less Wrong Moderator.

No, the rationale for deletion was not based on the possibility that his exact, FAI-based scenario could actually happen.

To the best of my knowledge, SIAI has not planned to do anything, under any circumstances, which would increase the probability of you or anyone else being tortured for the rest of infinity.
Supporting SIAI should not, to the best of my knowledge, increase the probability of you or anyone else being tortured for the rest of infinity.
Thank you.

Pretty much correct in this case. Roko's original post was, in fact, wrong; correctly programmed FAIs should not be a threat.


Do you predict, then, that if you put a person in a group where every other person disapproves more of attempts to dodge the question than to provide either answer, and makes this known, then they will never refuse to answer the question on its own terms?

I predict this will have a large effect on the number who refuse to answer the question, increasing with the closeness of the peer group and the level of disapproval. Enough to flip 75% nonresponse to 25% nonresponse or something like that.


then they would be better off simply providing an answer calibrated to please whoever they most desired to avoid disapproval from

No they wouldn't. Ambiguity is their ally. Both answers elicit negative responses, and they can avoid that from most people by not saying anything, so why shouldn't they shut up?
EDIT: In case it's not clear, I consider this tactic borderline Dark Arts (please note who originally said that ambiguity-ally line in HPMOR!), a purely political weapon with no role in conversations trying to be rational. I wouldn't criticize its use as a defense against some political nitwit who's trying to hurt you in front of an inexperienced audience; I would be unhappy with first-use of it as a primary political strategy.

2pm of any particular Sunday, or is this weekly now? If the latter, suggest inserting "weekly" as the second word of the post's body (please do not change the title because that makes RSS readers reload it).

It gets rid of good leaders, too. As Marcus Aurelius put it when I asked him about it, "I just don't know what would've happened to Rome if I'd had a finite lifespan."
Think of it as preventing the long-term operation of differential compound interest.

Because Moody wonders whether the Muggles are just pretending to have wards - that's a Schneierism.

Say more?

Let me try to rephrase: correct FAI theory shouldn't have dangerous ideas. If we find that the current version does have dangerous ideas, then this suggests that we are on the wrong track. The "Friendly" in "Friendly AI" should mean friendly.

RationalWiki was the only place I saw this mistake made, so the i.e. seemed deserved to me.

There's a substantial fraction of the total people who know me who believe I'm a beacon of human salvation, and even though that's exactly who I try to be, it still weirds me out.


Maybe it's a question of definitions, like the question about a tree making a sound

First thought that occurred to me while reading. If you know who you are apart from categorizations, why does it make so much difference whether it fits into a particular category? If someone told me that I wasn't really male, but fleem, and had been fleem all along, I would still be me.

I didn't think of that.
EDIT: And it wouldn't have worked because the Time-Turner is sealed to a single user alone.

I think I can go ahead and say, now that Ch. 63 is up, that there are exactly two characters in this fic who will reliably think of that sort of thing, and they are Professor Quirrell and Mad-Eye Moody.
It's only the Idiot Ball if it has been previously established that the character is too intelligent to make a mistake. Not all characters in this fic are supposed to be that intelligent!

And as soon as they burst into the Slytherin common room, Tracey Davis took a deep breath and shouted, "Everyone! Harry Potter couldn't cast the Patronus Charm and the Dementor almost ate him and Professor Quirrell saved him but then Potter was all evil until Granger brought him back with a kiss! It's true love for sure!"
It was ordered storytelling of a sort, Daphne supposed.
The news failed to produce the expected reaction. Most of the girls glanced over and then stayed in their couches, or the boys simply kept reading in their chairs.
"Yes," said Pansy sourly, from where she was sitting with Gregory's feet in her lap, leaning back and reading what seemed to be a coloring book, "Millicent already told us."
How -


It could be that someone saw this and said 'F* it, just give them time machines.'

I now declare this MoR!canon.

I'll fix it.

It's like Harry is Harry Potter instead of Eliezer or something!

Tom Riddle got a Time-Turner in his third year and Legilimized himself.


point to Eliezer outright making things up on costs of cryonics

Citation needed. Please do. I pay those costs out of pocket, they can be verified with my insurance agent if need be, and I should very much like to know what on Earth you think you are talking about.

I congratulate you on noticing your own confusion. Yes, I thought of that.

The time-turner could be (should've been!) tested without Harry's participation.

Molecules constantly significantly changing their spatial orientation relative to each other.

He doesn't have a Time-Turner. The thought that he could turn into a snake Animagus in Harry's pouch is not in their hypothesis space.

That was because they didn't have the same impending doom of existential risk hanging directly over their heads and people weren't dying all the time, it wasn't a function of "yay more people are HAPPY".

Note for the clueless (i.e. RationalWiki): This is photoshopped. It is not an actual slide from any talk I have given.

Having a spiral of green energy is subtle.
Calling it the Breaking Drill Hex is less subtle.
Having the incantation being "Lagann!" is not subtle at all.
There are subtle references in there, but that ain't one of them.

I'm a little baffled about how Dumbledore and Co. aren't at least CONSIDERING the possibility that Quirrel is involved, especially since Dumbledore was already suspicious of him.

And if you don't know? I care about possibilities where bad things happen without my knowing about them, I would not choose to have the knowledge erased from my brain and call it a success.


Eliezer may have altered the requirements to make achieving the same goal more difficult for Quirrelmort, because if he wanted to he could easily have already gathered all the ingredients the original Voldemort did, and probably without anyone noticing to boot.

First, ask yourself how you would set things up if you were Lord Voldemort. Then, reread Ch. 53.

So far as SIAI is concerned, I have to say that the storylike qualities of the situation have provided no bonus at all to our PR rolls, just a penalty we have to be careful to avoid because of all the people who perceptually recognize it as a mere story. In other words, we lose because of all the people trying to compensate for a bias that doesn't actually seem to be there. People who really are persuaded by stories, go off and become religious or something, find people with much better-refined attractive stories than us. Our own target audience, drawn from the remainder, tends to see any assertion classifiable as storylike as forbidden-in-reality because so many stupid people believe in them.
And of course much of your story simply and obviously isn't applicable. There will be no robot war, there don't seem to be any hostile human parties trying to bring about the apocalypse either, the question gets resolved in a research basement somewhere and then there's no final battle one way or another.
But then if you'd left out the part about the Robot War and the final battle, your opening paragraph wouldn't have sounded as clever. And this is also something that happens to us a LOT, which is that people arguing against us insist on mapping the situation very exactly onto a story outline, at the expense of accuracy, so that it looks stupider.
All the bias here seems to be in the overcompensation.

Quirrell's stated reason is that he wants Bahry to take down his Occlumency barrier.

Both theories are trying to pay rent on the same house; that's the problem here, which is quite distinct from neither theory paying rent at all.

Never mind usefulness, it seems to me that "Evolution by natural selection occurs" and "God made the world and everything in it, but did so in such a way as to make it look exactly as if evolution by natural selection occured" are not the same hypothesis, that one of them is true and one of them is false, that it is simplicity that leads us to say which is which, and that we do, indeed, prefer the simpler of two theories that make the same predictions, rather than calling them the same theory.

I really have to remember that frame for cryonics.

Harry already tried this in Ch. 28, failed, and concluded that it wasn't possible to Transfigure things out of moving molecules. It's safe to assume that he tried it again after (1) figuring out partial Transfiguration and (2) asking McGonagall, and found that it was still impossible; otherwise he wouldn't have bothered touching his wand to the metal stairs to get his mirror.


Eliezer making a comment to the effect that symbolic logic is something computers can do so it must not be what makes humans more special than chimps.

I did not say that. I said that symbolic logic probably wasn't It. You made up your own reason why, and a poor one.

The quote is wrong.

And hardware overhang (faster computers developed before general cognitive algorithms, first AGI taking over all the supercomputers on the Internet) and fast infrastructure (molecular nanotechnology) and many other inconvenient ideas.
Also if you strip away the talk about "imbalance" what it works out to is that there's a self-contained functioning creature, the chimpanzee, and natural selection burps into it a percentage more complexity and quadruple the computing power, and it makes a huge jump in capability. Nothing is offered to support the assertion that this is the only such jump which exists, except the bare assertion itself. Chimpanzees were not "lopsided", they were complete packages designed for an environment; it turned out there were things that could be done which created a huge increase in optimization power (calling this "symbolic processing" assumes a particular theory of mind, and I think it is mistaken) and perhaps there are yet more things like that, such as, oh, say, self-modification of code.

But they wouldn't rediscover the mythic overlay, which is what makes the original quote a lie and an attempt to steal credit.

So far as I know, he wasn't, just placed under house arrest. It jumped out at me too; you really have to get these poems exactly right on a factual level or it takes a lot away.

Don't feed the trolls, people, this is not reddit.


Harry recovered way too easily, if the story were consistent he'd be screaming on the floor until the Aurors arrived.

Well, to the accusation of inconsistency I will respond that (a) Harry is not standing five paces away from a Dementor this time and (b) he has been strengthened somewhat by previous realizations, thus he does not instantly fall over and gets a chance to recover.

Don't forget that if it works, you probably get immortality too. If you were already immortal, would you be willing to become mortal for $500 000?

I hereby declare this to be fact. Not least because otherwise Harry would be tempted to send his Patronus into the Dementors' pit at any time, which problem I had thought about and planned to have him just not think of.

Azkaban's future cannot interact with its past.

I hadn't thought of that, but (in canon) only members of the Order of the Phoenix can use their Patronuses that way.


evolved adaptations that make human society work better

ERROR: POSTULATION OF GROUP SELECTION IN MAMMALS DETECTED

I affirm that this is what I think the policy should be. Speculation does not require spoilers.

Whaaaa? I don't think you and I have been reading the same terrible BDSM erotica.

One of the systematic changes in MoR is that things which are sufficiently powerful are artifacts, and things which are artifacts are sufficiently powerful: The Marauder's Map was originally devised by Slytherin as part of the creation of Hogwarts and only slightly twisted by the Marauders (Ch. 25), and the Cloak of Invisibility is now in a class of its own compared to standard invisibility cloaks or Disillusionment (Ch. 54).
Rowling, of course, wrote that thing with Moody's eye before she decided the Cloak of Invisibility was a major artifact. So if Moody's eye can still see through it in MoR, it's going to be because either Moody's eye is also a major artifact, or, more likely, a specialized artifact devoted to seeing through invisibility (a specialized, specific artifact can defeat a generally more powerful artifact if the specialization is narrow enough).

Huh. Guessed that.

So write it. Nothing wrong with having an AU of an AU.

A related nitpick: I was wondering why McGonagall's Patronus found this Harry if there are multiple Harrys around at this time because of his use of the Time-Turner. It seems likely that either the earlier Harry is still around at this time, or the later Harry has come back and is around at this time, or both. Is it because this Harry is using his Patronus?
(A similar issue about communicating with people who have time traveled naq oebhtug gurve pryy cubarf came up in another work of fiction, but I won't say anything more about that to avoid spoilers.)

Saw someone else do it already.
Actually this is a fallacy I've seen coming up a lot in discussions of the fic. People are so enchanted with being able to come up with many possibilities that they forget to ask which are the probable possibilities. Sure, the laws change somewhat when you're matching wits with an author instead of reality; but when it comes to people talking about lots of other possible explanations for the Mendelian pattern in wizard genetics, for example, they seem to be doing a Culture of Objections thing where they declare victory as soon as they come up with an overlooked possibility that can be used to reject the paper, never mind the prior probability on it.
I've seen this answer gotten, so I know it's gettable.

Conservation of Detail. Obviously it means something; with some imagination on your part you could deduce what.

Indeed. Both sides of the inversion need plausible rationalizations.

And WOW did I ever get called on it. It leaves a small but ugly hole, too, because now it looks like something that ought to stem from the single point of departure, but was actually meant to be unchanged from canon. I should probably just take the "boyfriend" line out of Ch. 17, or rewrite it or something.

I came to realize in time that what I thought was a bug was a feature, however frustrating that may be for me, so please rot13 that comment with the warning "spoilers even if you've read all chapters".

You're welcome. You warm my heart.

Another good test of rationality is whether you're more confused by fiction than by reality. Just thought I should mention that in this connection.

FYI: Version 1 of Ch. 50 had Harry approaching Padma directly... and having to be considerably more threatening in order to have a smaller impact on her, which is what got him in trouble with Hermione in the original version.
Version 2 won out over Version 1 because it was weirder, and therefore more awesome; and also because it got him into less trouble with Hermione - I didn't like having her be quite so clearly in the right in Version 1, i.e., so right that even Harry would notice. It had to end on a note of ambiguity from Harry's perspective.
The thing a reader suggested that I'm embarrassed not to have thought of as an option was that Harry should have gotten a teacher Padma respected to do it. But then Harry would not have thought of this over an even longer time period than I didn't. And it probably still wouldn't have worked as well as the ghost, on a purely individual level for Padma, simply because Mysterious Visitations are supposed to be Life-Changing Events and having a teacher talk to you isn't.

I can't help but notice that Bella's feelings of social awkwardness about trying to proselytize her fellow vampires not to eat people are exactly identical to how I feel about the tradeoff between social awkwardness and human life when it comes to persuading people to sign up for cryonics. Did you think that too while you were writing it?
Being a transhumanist puts you into sympathy with the strangest people, doesn't it?

Yeah, I can only imagine how disappointed they'll be when they learn stuff about how to build benevolent superintelligences instead.

Out of those four colors? Blue/blue, of course, because I'm RAVENCLAW!

The standard Patronus "feels wrong" when Hermione tries to cast it, just as it does to Harry.

thanks, but no quoting LWers in this post

From a hacker news thread on the difficulty of finding or making food that's fast, cheap and healthy.
"Former poet laureate of the US, Charles Simic says, the secret to happiness begins with learning how to cook." -- pfarrell
Reply:
"Well, I'm sure there's some economics laureate out there who says that the secret to efficiency begins with comparative advantage." -- Eliezer Yudkowsky

My response is probably concentrated to one in five or something like that.

Apparently my theory was wrong.
When I'm feeling down, what snaps me out of it will be something which exhibits what TV Tropes would call the Determinator trope. Shinji 40k, Fate/stay Night UBW route, etcetera. 8 minutes of courage is that in highly concentrated form, and the silliness doesn't bother me. I guess, however, that this is an idiosyncratic response which doesn't extend to anyone else.
But yes, when I get up in the morning, and didn't get quite enough sleep, and the prospect of working on the rationality book fills me with weary dread, watching "8 Minutes of Courage" is enough to make me think, "YOU CALL THIS AN OBSTACLE? I CALL IT A SPEEDBUMP".
Maybe when MoR finishes, I'll find someone with the flash skills to do some slogans set to the Rationality Patronus and the music of Emiya #0.

Agreed that all-zero utility function is more or less just wrong.
People like this can still remember what happiness is and wish that they were happy; they can dislike feeling nihilistic.
They can still experience all sorts of things as unpleasant, such as making an effort.
A state of mind in which happiness is very difficult to obtain and drive/motivation is at an extremely low ebb is not a zero utility function.
Nonetheless I find it very easy to understand why "zero utility function" would be used in this case as a poetic metaphor.

I'm interested in testing what happens if you watch "8 minutes of courage":
http://www.youtube.com/watch?v=LI1FP3czJnY

Would you mind if I edited this post in order to express more strongly that the vast majority of this reading is not required to keep up with the vast majority of LW posts?

God grant me the strength to change the things I can,
The intelligence to know what I can change,
And the rationality to realize that God isn't the key figure here.

I confirm that Grautry's answer is the conventional one, and that I often worry that I am overusing adverbs or verbs that are not simply "said", which is what we are told to worry about.


Though I remember from canon that Lucius wanted to send Draco to Durmstrang instead, so he wouldn't be under Dumbledore's authority, and it was just Narcissa who vetoed the idea.

Well, there's a fact I did not know. Then again this Lucius is more competent anyway. Shrugs.

May I suggest mentioning OB and LW on the sign too? More people have read LW than HPMOR, I'm pretty sure.

Ah yes, the standard argument against consequentialism: X has expected positive consequences, so consequentialism says we should do it. But clearly, if people do X, the world will be a worse place. That's why practicing consequentialism makes the world a worse place, and people shouldn't be consequentialists.
Personally I'll stick with consequentialism and say a few extra words in favor of maintaing the consistency of your abstract arguments and your real expectations.

Someone asked me to delete this comment. I do not think deletion is necessary in cases like this. It's enough for the readers to downvote it into oblivion. Note that after it hits -3 most readers will stop seeing it, so don't panic if it only gets downvoted to -3 instead of -40.

I suggest "Open Discussion" with the last 5 comments in the sidebar, underneath "Recent Posts" and above "Wiki Edits".

Okay granted. I also think this would be a good idea. Actually, I'd be against having an easy way to delete all contributions in general, it's too easy to wreck things that way.

Acausal influence stems from other processes similar to you. This can be a simulated version of you, on whose action the simulating agent's choices depend. Or it can just be someone else like you, who's likely to some degree to decide the same thing for some of the same reasons.

Ah, didn't see this earlier.
I don't think it multiplies the expected payoff for both in the same way. Some Bostromian division-of-responsibility principle should apply in both cases. The apparent gains are from the probability of making an important shift via group action where individual action would be unlikely to go over a tipping point, not because you're multiplying by the number of people involved.

I take it that you're using the standard wrong classical causal decision theory (in which no one is responsible for the election outcome unless one side wins by a single vote, in which case millions of voters are all solely responsible for the entire election outcome) out of either misguided humility about the probability of an SIAI-originating decision theory being correct, or because you're planning to publish this paper elsewhere and you don't want to invoke Hofstadterian superrationality in place of the standard wrong decision theory?

I think we've still got the bug.

Accidentally promoted this whilst trying to add an "end summary" bar. Demoted thereafter. No offense meant, sorry for the inconvenience.


Would this have worked just as well if a sincerely religious individual who believed in an eventual resurrection of all had cast the patronus?

It would require that they cognitively mapped the existence of the Dementor onto the concept of soul-death and that they forcefully rejected this event on an emotional level instead of just having a quiet factual opinion that it never happened. Such a hypothetical individual is simply a non-reductionist isomorph of Harry's reductionist belief. It would just be difficult for a religious individual to get into that state of mind in the first place. It probably would help a lot if they believed that the Dementor's Kiss actually does destroy a soul.
I mention this because I did think about what would happen if someone like a Buddhist acknowledged the existence of true Death, soul-death, and still accepted that without the tiniest bit of sour grapes; and concluded that although that wouldn't make a Dementor-destroying Patronus, they would be able to see the Dementor's true form and cast a perfect shield against its fear.
Incidentally, Harry didn't say at any point that any of what he said was a certainty.

I printed that out and put it on my bedroom wall at one point.

If you want to be more practical and useful, then anything of or relating to real-world jobs is a large step forward.

"Maximum likelihood" totally != "report likelihood ratios".

Oh, it's intended.

BTW, this point should be elementary, but no guessing as to the poster of the ad, please.
I'd disable comments on this post if I saw a simple way to do it, but I don't.


With some luck Berkeley will become the Singularitarian/Neorationalist nexus.

Berkeley? Center of the next great rationalist movement? That's going to take some luck.


Okay, anyone who cares about helping people in Africa and can multiply should be giving their money to x-risk charities. Because saving the world also includes saving Africa.

But... but... but saving the world doesn't signal the same affiliations as saving Africa!

"What is your super origin / how did you become a rationalist?" is a question that seemed to spark a lot of interesting conversation.

Awww.
I considered buying 911birthday.something with the slogan, "If you don't celebrate our birthdays, the terrorists have already won." Looks like I'm not the only one with that idea. I wonder if I share a birthday with someone at Reddit?

When I get insignificant amounts of change, like a nickel, I leave it on the nearest outdoor object, thus teaching people that they can collect small amounts of money by searching random objects.

Have I ever remarked on how completely ridiculous it is to ask high school students to decide what they want to do with the rest of their lives and give them nearly no support in doing so?
Support like, say, spending a day apiece watching twenty different jobs and then another week at their top three choices, with salary charts and projections and probabilities of graduating that subject given their test scores? The more so considering this is a central allocation question for the entire economy?

What lapse? People don't know these things until I explain them! Have you been in a mental state of having-already-read-LW for so long that you've forgotten that no one from outside would be expected to spontaneously describe in Bayesian terms the problem with saying that "complexity" explains something? Someone who'd already invented from scratch everything I had to teach wouldn't be taken as an apprentice, they'd already be me! And if they were 17 at the time then I'd probably be working for them in a few years!


EY is astounded that someone can understand this after a thorough explanation. Can it honestly be that hard to find someone who can follow that?

Yes. Nobody arrives from the factory with good rationality skills, so I look for learning speed. Compare the amount I had to argue with Marcello in the anecdote to the amount that other people are having to argue with you in this thread.

See, at first I went "Awww" and then a second later, "Wait a minute, why I am dying at age 10^63?"

These days I am apparently known by the title LORD-SAVIOR-GOD-COMBATMAID.

Eh? No, there were just a few kids, like 2 or 3.

Goblet of Fire, Bertha. We could possibly assume that her Obliviation was intended to be of the undoable sort to begin with, like what canon!Hermione did to her parents (so Bertha could still work on her job while she was at work, or regain the memories after the Tournament).

If a horcrux of Type 1 is found, that greatly increases the chance another horcrux of Type 1 is findable. You actually do want to use as many different modes as possible, not randomize across modes, because the probabilities are not independent.


I just added the slash.

Good! Now your fic is artistically complete as well.

Sounds related to the failure class I call "living in the should-universe".

And Gryffindors, Hufflepuffs, and Ravenclaws.


I'd expect people to develop serious plans of taking over the world at some age older than 11

Erm, not taking over the world per se, but I was certainly thinking in long-range terms. If you look at my grade school graduation yearbook (age 12), my ambition is listed as building the first faster-than-light starship. Ah, the innocence of youth, before I got ambitious, and before I understood the local nature of causality.


Should businessmen collude on one-shot pricing?

All they need to do is find someone who can help enforce the decision, or make it matter reputationally to friends, or iterate it, and they don't need to worry about whether they're doing the same thing for the same reasons.


You could - "with a lot of additional adjustments". You would to have to actually work at turning them into Slytherins, and doubly so if there are no natural Slytherins there at all to lead the way. And probably not everyone anyway.

My claim is that most humans outside of fairy tales already are Slytherins.

True. You have persuaded me back to my original position. Whoever made house elves was disgusting. It could have been done right (complementary intelligent species that enjoys doing a lot of necessary things we don't, while still having rich lives of their own), but it wasn't.

It's probably already been done.

You're confusing different questions. Each question should isolate a single potential motivation and show that it is not, of itself, sufficient reason to refuse. If you fear signaling, don't tell people about the necklace. If you fear quacks, don't make the question be about a necklace or about signaling.

I feel reluctant to directly contradict canon in this case by having McGonagall not transform the pig, never mind the havoc it wreaks on all neighboring paragraphs. Do you have a different suggested fix?

I told someone in advance; in particular, I told them there would be a completely wrong ship, they guessed Fred/George, I told them "not wrong enough", and then said "Sirius and Pettigrew". They said, "OH, THAT'S JUST WRONG". So I figured that, yes, that was justifiably describable as "completely wrong". Also, the fact that I googled obvious spellings of the ship and found that it only seemed to have been done once or twice (I forgot the exact number), in a fandom that has a convention for using "Snumbledore" to indicate Snape+Lupin+Dumbledore, seemed to suggest that it was pretty damned wrong.

Sadly, being not logically omniscient, I am not capable of always perfectly living up to my endeavor to write a story with perfectly consistent rules. There are these things called mistakes.

Really? Fifteen percent of all yaoi fans are yaoi fanboys and eighty-five percent are yaoi fangirls? I'd like to see that statistic before I believe it. Also, you've got to keep in mind that we're talking about the set of yaoi fans who are squeeing over Harry and Draco while they're still eleven. This, to me, says "yaoi fangirl", though I fully admit I'm working from 100% stereotypes and 0% experience.

Er, who says Draco's grabbing Hermione because she's a girl? He's grabbing her because she's falling off the roof.

I'm assuming that Harry had a background plan-- to get Draco and Hermione to cooperate, and to feel as though they are on each others' side on a gut level.
Even though the "drop me" scene is cute, I'm not convinced it's plausible behavior for Draco. I bet he hasn't had the Anglo/muggle training about not hurting girls, and learning that blood purity isn't true may not have affected his reflexes.
Harry's apology may be strategic. Or he may still be learning to navigate between empathy and rationality. Or (not an attractive hypothesis) tropes may be taking over some of the story.
ETA: Perhaps that should be empathy-signaling rather than just empathy.

I know nothing about PUA except what I read in other people's blog comments, and this part honestly leaves me baffled. Wha? Amplify please?

This is honestly making me feel a bit aspie over here. As I understand the rules for social interaction, dropping a 12-year-old girl off the roof should generally be recognized as a misdeed. Hermione, as I was modeling her, was annoyed enough by having to climb the icy castle walls, and after falling off the roof, had gone beyond annoyance into a kind of detached curiosity. Bear in mind that she doesn't know anything about Harry's plans for Malfoy; so far as she knows, Harry is doing all of this for no other reason than to be annoying. I had trouble understanding wedifrid's reaction to Harry and hypothesized that he enjoyed empathizing with a dominant character and didn't want that dominant character to apologize, but now Nancy thinks Hermione is being unfair and, well. I feel a bit aspie because I don't quite understand where it's all coming from. Canon!Hermione in particular seems like she'd be really really annoyed with Harry making her climb up an icy wall and then getting her dropped off the roof. She'd forgive it in a flash as part of the war against Voldemort, but not if it was, say, part of a prank. What am I missing here?


Is it the author's opinion that the creation of house elves was a terribly evil deed?

It had been, but...

Even if we accept that creating conscious entities which are forced by means of their preferences to do menial work is wrong, it would seem to be better to create them, than to force those who don't enjoy such work to do it.

...is a powerful argument I had never considered.

Makes sense. I thought I had a distant memory of that, but wasn't sure of the context.

Now I've written certain unfinished and unpublished stories set in Brennan's world, so I know there's an Erotic Conspiracy and that it's called the Erotic Conspiracy, but may I inquire as to how you know there's such a thing as the Erotic Conspiracy when Google shows that the first reference on LW is in this very thread? Lucky guess, or did the information escape some other way?

(7) If you have a fatal disease that can only be cured by wearing a bracelet or necklace under your clothing, and anyone who receives an honest explanation of what the item is will think you're weird, do you wear the bracelet or necklace?
Answering yes to (7) means that you shouldn't refrain from cryonics for fear of being thought weird.


When I first started watching Blogging Heads discussions featuring Eliezer I would often have moments where I held my breath thinking "Oh god, he can't address that directly without sounding nuts, here comes the abhorrent back-peddling and waffling". Instead he met it head on with complete honesty

I am so glad that someone notices and appreciates this.

I feel that anyone advocating for public hypocrisy among the SIAI staff is working to disintegrate the organization (even if unintentionally).

Agreed.

Thank you for your specifications, and I'll try to keep them in mind!

BTW, when people start saying, not, "You offended me, personally" but "I'm worried about how other people will react", I usually take that as a cue to give up.

This sort of conversation just makes me feel tired. I've had debates before about my personal psychology and feel like I've talked myself out about all of them. They never produced anything positive, and I feel that they were a bad sign for the whole mailing list they appeared on - I would be horrified to see LW go the way of SL4. The war is lost as soon as it starts - there is no winning move. I feel like I'm being held to an absurdly high standard, being judged as though I were trying to be the sort of person that people accuse me of thinking I am, that I'm somehow supposed to produce exactly the right mix of charming modesty while still arguing my way into enough funding for SIAI... it just makes me feel tired, and like I'm being held to a ridiculously high standard, and that it's impossible to satisfy people because the standard will keep going up, and like I'm being asking to solve PR problems that I never signed up for. I'll solve your math problems if I can, I'll build Friendly AI for you if I can, if you think SIAI needs some kind of amazing PR person, give us enough money to hire one, or better yet, why don't you try being perfect and see whether it's as easy as it sounds while you're handing out advice?
I have looked, and I have seen under the Sun, that to those who try to defend themselves, more and more attacks will be given. Like, if you try to defend yourself, people sense that as a vulnerability, and they know they can demand even more concessions from you. I tried to avoid that failure mode in my responses, and apparently failed. So let me state it plainly for you. I'll build a Friendly AI for you if I can. Anything else I can do is a bonus. If I say I can't do it, asking me again isn't likely to produce a different answer.
It was very clearly a mistake to have participated in this thread in the first place. It always is. Every single time. Other SIAI supporters who are better at that sort of thing can respond. I have to remember, now, that there are other people who can respond, and that there is no necessity for me to do it. In fact, someone really should have reminded me to shut up, and if it happens again, I hope someone will. I wish I could pull a Roko and just delete all my comments in all these threads, but that would be impolite.


this comment has done more to persuade me to stop being a monthly donor to SIAI than anything else I've read or seen.

It is certainly Eliezer's responses and not multi's challenges which are the powerful influence here. Multi has effectively given Eliezer a platform from which to advertise the merits of SIAI as well as demonstrate that contrary to suspicions Eliezer is, in fact, able to handle situations in accordance to his own high standards of rationality despite the influences of his ego. This is not what I've seen recently. He has focussed on retaliation against multi at whatever weak points he can find and largely neglected to do what will win. Winning in this case would be demonstrating exactly why people ought to trust him to be able to achieve what he hopes to achieve (by which I mean 'influence' not 'guarantee' FAI protection of humanity.)
I want to see more of this:

With Michael Vassar in charge, SIAI has become more transparent, and will keep on doing things meant to make it more transparent

and less of this:

I have to say that my overall impression here is of someone who manages to talk mostly LW language most of the time, but when his argument requires a step that just completely fails to make sense, like "And this is why if you're trying to minimize existential risk, you should support a charity that tries to stop tuberculosis" or "And this is where we're going to assume the worst possible case instead of the expected case and actually act that way", he'll just blithely keep going.

I'll leave aside ad hominim and note that tu quoque isn't always fallacious. Unfortunately in this case it is, in fact, important that Eliezer doesn't fall into the trap that he accuses multi of - deploying arguments as mere soldiers.

Can you please be more precise about what you saw as the problem?

How much will you donate to cover the costs? It's always easy to spend other people's money.


SIAI does not presently exhibit high levels of transparency and accountability... For this reason together with the concerns which I express about Existential Risk and Public Relations, I believe that at present GiveWell's top ranked charities VillageReach and StopTB are better choices than SIAI

I have difficulty taking this seriously. Someone else can respond to it.

agree with what I interpret to be Dario's point above: that in evaluating charities which are not transparent and accountable, we should assume the worst.

Assuming that much of the worst isn't rational. It would be a convenient soldier for your argument, but it's not the odds to bet at. Also, you don't make clear what constitutes a sufficient level of transparency and accountability, though of course you will now carefully look over all of SIAI's activities directed at transparency and accountability, and decide that the needed level is somewhere above that.
You say you assume the worst, and that other people should act accordingly. Would you care to state "the worst", your betting odds on it, how much you're willing to bet, and what neutral third party you would accept as providing the verdict if they looked over SIAI's finances and told you it wasn't true? If you offer us enough free money, I'll vote for taking it.
I have to say that my overall impression here is of someone who manages to talk mostly LW language most of the time, but when his argument requires a step that just completely fails to make sense, like "And this is why if you're trying to minimize existential risk, you should support a charity that tries to stop tuberculosis" or "And this is where we're going to assume the worst possible case instead of the expected case and actually act that way", he'll just blithely keep going.
With Michael Vassar in charge, SIAI has become more transparent, and will keep on doing things meant to make it more transparent, and I have every confidence that whatever it is we do, it will never be enough for someone who is, at that particular time, motivated to argue against SIAI.

Obviously I'm not trying to keep it a secret. I just haven't gotten around to editing.

Oh, I know it's not your fault, but seriously, have "the Internet" ask you the same question 153 times in a row and see if you don't get slightly frustrated with "the Internet".


Imagine an AI as intelligent and well informed as an FAI, but one without much power - as a result of physical safeguards, say

There's some part of my brain that just processes "the Internet" as a single person and wants to scream "But I told you this a thousand times already!"
http://yudkowsky.net/singularity/aibox

If you want it done, feel free to do it yourself. :)


I assign a probability of less than 10^(-9) to you succeeding in playing a critical role on the Friendly AI project that you're working on.

I wish the laws of argument permitted me to declare that you had blown yourself up at this point, and that I could take my toys and go home. Alas, arguments are not won on a points system.

My impression is that you've greatly underestimated the difficulty of building a Friendly AI.

Out of weary curiosity, what is it that you think you know about Friendly AI that I don't?
And has it occurred to you that if I have different non-crazy beliefs about Friendly AI then my final conclusions might not be so crazy either, no matter what patterns they match in your craziness recognition systems?

I've encountered people who think Singularitarians think that, never any actual Singularitarians who think that.

I wouldn't put it in terms of forbidden premises or forbidden conclusions.
But if each of these statements has a 90% of being true, and if they are assumed to be independent (which admittedly won't be exactly true), then the probability that all three are true would be only about 70%, which is not an extremely high degree of confidence; more like saying, "This is my opinion but I could easily be wrong."
Personally I don't think 1) or 3), taken in a strict way, could reasonably be said to have more than a 20% chance of being true. I do think a probability of 90% is a fairly reasonable assignment for 2), because most people are not going to bother about Friendliness. Accounting for the fact that these are not totally independent, I don't consider a probability assignment of more than 5% for the conjunction to be reasonable. However, since there are other points of view, I could accept that someone might assign the conjunction a 70% chance in accordance with the previous paragraph, without being crazy. But if you assign a probability much more than that I would have to withdraw this.
If the statements are weakened as Carl Shulman suggests, then even the conjunction could reasonably be given a much higher probability.
Also, as long as it is admitted that the probability is not high, you could still say that the possibility needs to be taken seriously because you are talking about the possible (if yet improbable) destruction of the world.

I certainly do not assign a probability as high as 70% to the conjunction of all three of those statements.
And in case it wasn't clear, the problem I was trying to point out was simply with having forbidden conclusions - not forbidden by observation per se, but forbidden by forbidden psychology - and using that to make deductions about empirical premises that ought simply to be evaluated by themselves.
I s'pose I might be crazy, but you all are putting your craziness right up front. You can't extract milk from a stone!


there seems to be something about this whole endeavour (including but not limited to Eliezer's writings) that makes people think !!!CRAZY!!! and !!!DOOMSDAY CULT!!!,

Yes, and it's called "pattern completion", the same effect that makes people think "Singularitarians believe that only people who believe in the Singularity will be saved".


Second, if someone really believes that he is literally saving the world, then he can be sure that he has a minor personality disorder [1], regardless of whether he will eventually save the world or not.

So you'd prohibit someone of accurate belief? I generally regard that as a reductio.

Success is not assured. I'm not sure what's meant by confessing to being "ambitious". Is it like being "optimistic"? I suppose there are people who can say "I'm being optimistic" without being aware that they are instantiating Moore's Paradox but I am not one of them.
I also disclaim that I do not believe myself to be the protagonist, because the world is not a story, and does not have a plot.

Dozens isn't sufficient. I asked Marcello if he'd run into anyone who seemed to have more raw intellectual horsepower than me, and he said that John Conway gave him that impression. So there are smarter people than me upon the Earth, which doesn't surprise me at all, but it might take a wider net than "dozens of other smart people" before someone comes in with more brilliance and a better starting math education and renders me obsolete.


I would say that no one can reasonably believe all of the following at the same time with a high degree of confidence: 1) I am critical to this Friendly AI project that has a significant chance of success. 2) There is no significant chance of Friendly AI without this project. 3) Without Friendly AI, the world is doomed.

I see. So it's not that any one of these statements is a forbidden premise, but that their combination leads to a forbidden conclusion. Would you agree with the previous sentence?
BTW, nobody please vote down the parent below -2, that will make it invisible. Also it doesn't particularly deserve downvoting IMO.

Unknown reminds me that Multifoliaterose said this:

The modern world is sufficiently complicated so that no human no matter how talented can have good reason to believe himself or herself to be the most important person in human history without actually doing something which very visibly and decisively alters the fate of humanity. At present, anybody who holds such a belief is suffering from extreme delusions of grandeur.

This makes explicit something I thought I was going to have to tease out of multi, so my response would roughly go as follows:

If no one can occupy this epistemic state, that implies something about the state of the world - i.e., that it should not lead people into this sort of epistemic state.
Therefore you are deducing information about the state of the world by arguing about which sorts of thoughts remind you of your youthful delusions of messianity.
Reversed stupidity is not intelligence. In general, if you want to know something about how to develop Friendly AI, you have to reason about Friendly AI, rather than reasoning about something else.
Which is why I have a policy of keeping my thoughts on Friendly AI to the object level, and not worrying about how important or unimportant that makes me. In other words, I am reluctant to argue on this level not just for the obvious political reasons (it's a sure loss once the argument starts), but because you're trying to extract information about the real world from a class of arguments that can't possibly yield information about the real world.
That said, as far as I can tell, the world currently occupies a ridiculous state of practically nobody working on problems like "develop a reflective decision theory that lets you talk about self-modification". I agree that this is ridiculous, but seriously, blame the world, not me. Multi's principle would be reasonable only if the world occupied a much higher level of competence than it in fact does, a point which you can further appreciate by, e.g., reading the QM sequence, or counting cryonics signups, showing massive failure on simpler issues.
That reflective decision theory actually is key to Friendly AI is something I can only get information about by thinking about Friendly AI. If I try to get information about it any other way, I'm producing noise in my brain.
We can directly apply multi's stated principle to conclude that reflective decision theory cannot be known to be critical to Friendly AI. We were mistaken to start working on it; if no one else is working on it, it must not be knowably critical; because if it were knowably critical, we would occupy a forbidden epistemic state.
Therefore we have derived knowledge about which problems are critical in Friendly AI by arguing about personal psychology.
This constitutes a reductio of the original principle. QEA. (As was to be argued.)


Er... I can't help but notice a certain humor in the idea that it's terrible if I'm self-deluded about my own importance because that means I might destroy the world.

It seems like an implication of this and other assumptions made by multi, and apparently shared by you, is that no one can believe themselves to be critical to a Friendly AI project that has a significant chance of success. Do you agree that this is an implication? If not, why not?

I remember there being lots of bad comments on the old OB, and I think that putting a karma system in place, and requiring registration, helped an awful lot.

It seems like an implication of your post that no one is ever allowed to believe they're saving the world. Do you agree that this is an implication? If not, why not?

Downvoted for retaliatory downvoting; voted everything else up toward 0.

If a thousand species in nature with a thousand different abilities were to cooperate, would they equal the capabilities of a human? If not, what else is missing?


By no means do I want to downplay the difficulty of P vs NP; all the same, I think we have different meanings of "vast" in mind.

I hate to go all existence proofy on you, but we have an existence proof of a general intelligence - accidentally sneezed out by natural selection, no less, which has severe trouble building freely rotating wheels - and no existence proof of a proof of P != NP. I don't know much about the field, but from what I've heard, I wouldn't be too surprised if proving P != NP is harder than building FAI for the unaided human mind. I wonder if Scott Aaronson would agree with me on that, even though neither of us understand the other's field? (I just wrote him an email and asked, actually; and this time remembered not to say my opinion before asking for his.)

Scott says that he thinks P != NP is easier / likely to come first.

For Congress to implement good policy in this area would be performance vastly exceeding what we've previously seen from them. They called prediction markets terror markets. I expect more of the same, and expect to have little effect on them.

I agree with your main point and was refining it.

That seems bizarre to me too. But if Jimrandomh is filling his boxes on the basis of what most people would do, and most people do one-box, then perhaps they are just behaving as rational, highly correlated, timeless decisionmakers.

I love that "makes it reasonable" part. Especially in a discussion on what you shouldn't say in public.
Now we're to avoid stating any premises from which any absurd conclusions seem reasonable to infer?
This would be a reducto of the original post if the average audience member consistently applied this sort of reasoning; but of course it is motivated on XiXiDu's part, not necessarily something the average audience member would do.
Note that saying "But you must therefore argue X..." where the said person has not actually uttered X, but it would be a soldier against them if they did say X, is a sign of political argument gone wrong.

In a sane world where everyone had the same altruistic component of their values, the marginal EU of all utilities would roughly balance up to the cost of discriminating them more closely. I'd have to think about what would happen if everyone had different altruistic components of their values; but if large groups of people had the same values, then there would exist some class of charities that was marginally balanced with respect to those values, and people from that group would expend the cost to pick out a member of that class but then not look too much harder. If everyone who works for a charity is optimistic and claims that their charity alone is the most marginally efficient in the group, that raises the cost of discriminating among them and they will become more marginally unbalanced.


who has never written a single computer program

utterly false, wrote my first one at age 5 or 6, in BASIC on a ZX-81 with 4K of RAM
The fact that a lot of these reactions are based on false info is worth noting. It doesn't defeat any arguments directly, but it says that the naive model where everything happens because of the direct perception of actions I directly control is false.

And saddened once again at how people seem unable to distinguish "multi claims that something Eliezer said could be construed as claim X" and "Eliezer claimed X!"
Please note that for the next time you're worried about damaging an important cause's PR, multi.


if you think that the most important thing we can be doing right now is publicizing an academically respectable account of existential risk, then you should be funding the Future of Humanity Institute. Funding SIAI is optimal only if you think that the pursuit of Friendly AI is by far the most important component of existential risk reduction

Agreed. (Modulo a caveat about marginal ROI eventually balancing if FHI got large enough or SIAI got small enough.)


I think controlling Earth's destiny is only modestly harder than understanding a sentence in English.

Well said. I shall have to try to remember that tagline.


By no means do I want to downplay the difficulty of P vs NP; all the same, I think we have different meanings of "vast" in mind.

I hate to go all existence proofy on you, but we have an existence proof of a general intelligence - accidentally sneezed out by natural selection, no less, which has severe trouble building freely rotating wheels - and no existence proof of a proof of P != NP. I don't know much about the field, but from what I've heard, I wouldn't be too surprised if proving P != NP is harder than building FAI for the unaided human mind. I wonder if Scott Aaronson would agree with me on that, even though neither of us understand the other's field? (I just wrote him an email and asked, actually; and this time remembered not to say my opinion before asking for his.)


Honestly, I see the problem of AGI as the fairly concrete one of assembling an appropriate collection of thousands-to-millions of "narrow AI" subcomponents.

What do you think you know and how do you think you know it? Let's say you have a thousand narrow AI subcomponents. (Millions = implausible due to genome size, as Carl Shulman points out.) Then what happens, besides "then a miracle occurs"?


I'm not sure why it is a requirement that an organization concerned with the behavior of hypothetical future engineered minds would need to be in contact with these researchers.

You have to know some of their math (some of it is interesting, some not) but this does not require getting on the phone with them and asking them to explain their math, to which of course they would tell to you to RTFM instead of calling them.

I don't mean to dismiss the points of this post, but all of those points do need to be reinterpreted in light of the fact that I'd rather have a few really good rationalists as allies than a lot of mediocre rationalists who think "oh, cool" and don't do anything about it. Consider me as being systematically concerned with the top 5% rather than the average case. However, I do still care about things like propagation velocities because that affects what population size the top 5% is 5% of, for example.

Solid, bold post.
Eliezer's comments on his personal importance to humanity remind me of the Total Perspective Device from Hitchhiker's. Everyone who gets perspective from the TPD goes mad; Zaphod Beeblebrox goes in and finds out he's the most important person in human history.
Eliezer's saying he's Zaphod Beeblebrox. Maybe he is, but I'm betting heavily against that for the reasons outlined in the post. I expect AI progress of all sorts to come from people who are able to dedicate long, high-productivity hours to the cause, and who don't believe that they and only they can accomplish the task.
I also don't care if the statements are social naivete or not; I think the statements that indicate that he is the most important person in human history - and that seems to me to be what he's saying - are so seriously mistaken, and made with such a high confidence level, as to massively reduce my estimated likelihood that SIAI is going to be productive at all.
And that's a good thing. Throwing money into a seriously suboptimal project is a bad idea. SIAI may be good at getting out the word of existential risk (and I do think existential risk is serious, under-discussed business), but the indicators are that it's not going to solve it. I won't give to SIAI if Eliezer stops saying these things, because it appears he'll still be thinking those things.
I expect AI progress to come incrementally, BTW - I don't expect the Foomination. And I expect it to come from Google or someone similar; a large group of really smart, really hard-working people.
I could be wrong.
--JRM


(This might also explain why he think it's cool to write BSDM scenes featuring a 16-year-old schoolgirl as part of an ostensibly respectable work of SF, so it's a pet suspicion of mine.)

Point of curiosity: Does anyone else still notice this sort of thing? I don't think my generation does anymore.

I was embarrassed by most of the facts. The one about my holding up a blank sheet of paper and saying "a blank map does not correspond to a blank territory" and thus creating the universe is one I still tell at parties.

See also, "The Riddle of Kyon".

Yes, but those are polished outputs, and (no offense) have your halo-effect to back them up. I'm talking about sketching in a more generalized algorithm which accepts highly technical explanations as input, and produces output which a member of the general public would intuitively recognize as 'wise,' while retaining the input's truth-value.

There are algorithms for that? My brain just does it automatically on request.
(Also, I presented HPMOR to a new audience with my name stripped off just to check if people still liked what I wrote without the halo effect.)

I'm currently preparing for the Summit so I'm not going to hunt down and find links. Those of you who claimed they wanted to see me do this should hunt down the links and reply with a list of them.

Given my current educational background I am not able to judge the following claims (among others) and therefore perceive it as unreasonable to put all my eggs in one basket:

You should just be discounting expected utilities by the probability of the claims being true, and then putting all your eggs into the basket that has the highest marginal expected utility per dollar, unless you have enough resources to invest that the marginal utility goes down. This is straightforward to anyone who knows about expected utility and economics, and anyone who knows about scope insensitivity knows why this result is counterintuitive to the human brain. We don't emphasize this very hard when people talk in concrete terms about donating to more than one organization, because charitable dollars are not substitutable from a limited pool, the main thing is the variance in the tiny fraction of their income people donate to charity in the first place and so the amount of warm glow people generate for themselves is important; but when they talk about "putting all eggs in one basket" as an abstract argument we will generally point out that this is, in fact, the diametrically wrong direction in which abstract argument should be pushing.


Superhuman Artificial Intelligence (the runaway kind, i.e. God-like and unbeatable not just at Chess or Go).


Read the Yudkowsky-Hanson AI Foom Debate. (Someone link to the sequence.)


Advanced real-world molecular nanotechnology (the grey goo kind the above intelligence could use to mess things up).


Read Eric Drexler's Nanosystems. (Someone find an introduction by Foresight and link to it, that sort of thing is their job.) Also the term you want is not "grey goo", but never mind.


The likelihood of exponential growth versus a slow development over many centuries.


Exponentials are Kurzweil's thing. They aren't dangerous. See the Yudkowsky-Hanson Foom Debate.


That it is worth it to spend most on a future whose likelihood I cannot judge.


Unless you consider yourself entirely selfish, any altruistic effort should go to whatever has the highest marginal utility. Things you spend on charitable efforts that just make you feel good should be considered selfish. If you are entirely selfish but you can think past a hyperbolic discount rate then it's still possible you can get more hedons per dollar by donating to existential risk projects.
Your difficulties in judgment should be factored into a probability estimate. Your sense of aversion to ambiguity may interfere with warm glows, but we can demonstrate preference reversals and inconsistent behaviors that result from ambiguity aversion which doesn't cash out as a probability estimate and factor straight into expected utility.


That Eliezer Yudkowsky (the SIAI) is the right and only person who should be leading, respectively institution that should be working, to soften the above.


Michael Vassar is leading. I'm writing a book. When I'm done writing the book I plan to learn math for a year. When I'm done with that I'll swap back to FAI research hopefully forever. I'm "leading" with respect to questions like "What is the form of the AI's goal system?" but not questions like "Do we hire this guy?"

My judgement of and attitude towards a situation is necessarily as diffuse as my knowledge of its underlying circumstances and the reasoning involved. The state of affairs regarding the SIAI and its underlying rationale and rules of operation are not sufficiently clear to me to give it top priority. Therefore I perceive it as unreasonable to put all my eggs in one basket.

Someone link to relevant introductions of ambiguity aversion as a cognitive bias and do the detailed explanation on the marginal utility thing.

What I mean to say by using that idiom is that I cannot expect, given my current knowledge, to get the promised utility payoff that would justify to make the SIAI a prime priority. That is, I'm donating to the SIAI but also spend considerable amounts of resources maximizing utility at present. Enjoying life, so to say, is therefore a safety net given that my inability to judge the probability of a positive payoff will be answered negative in future.

Can someone else do the work of showing how this sort of satisficing leads to a preference reversal if it can't be viewed as expected utility maximization?

Much of all arguments on this site involve a few propositions and the use of probability to legitimate action in case of their asserted accuracy. Here much is uncertain to an extent that I'm not able to judge any nested probability estimations. I'm already unable to judge what the likelihood of something like the existential risk of exponential evolving superhuman AI is compared to us living in a simulated reality. Even if you tell me, am I to believe the data you base those estimations on?

Simplify things. Take the version of reality that involves AIs being built and not going FOOM, and the one that involves them going FOOM, and ask which one makes more sense. Don't look at just one side and think about how much you doubt it and can't guess. Look at both of them. Also, read the FOOM debate.

And this is what I'm having trouble to accept, let alone look through. There seems to be a highly complicated framework of estimations to support and reinforce each other. I'm not sure how you call this in English, but in German I'd call this a castle in the air.

Do you have better data from somewhere else? Suspending judgment is not a realistic policy. If you're looking for supporting arguments on FOOM they're in the referenced debate.

You could tell me to learn about Solomonoff induction etc., I know that what I'm saying may simply be due to a lack of education. But that's what I'm arguing and inquiring about here. And I dare to bet that many who support the SIAI cannot interpret the reasoning which lead them to support the SIAI in the first place, or at least cannot substantiate the estimations with other kinds of evidence than a coherent internal logic of reciprocal supporting probability estimations.

Nobody's claiming that having consistent probability estimates makes you rational. (Having inconsistent estimates makes you irrational, of course.)

I can however follow much of the reasoning and arguments on this site. But I'm currently unable to judge their overall credence. That is, are the conclusions justified? Is the coherent framework build around the SIAI based on firm ground?

It sounds like you haven't done enough reading in key places to expect to be able to judge the overall credence out of your own estimates.

There seems to be no critical inspection or examination by a third party. There is no peer review. Yet people are willing to donate considerable amounts of money.

You may have an unrealistic picture of what it takes to get scientists interested enough in you that they will read very long arguments and do lots of work on peer review. There's no prestige payoff for them in it, so why would they?

I'm concerned that although consistently so, the LW community is updating on fictional evidence. This post is meant to inquire the basic principles, the foundation of the sound argumentation's and the basic premises that they are based upon . That is, are you creating models to treat subsequent models or are the propositions based on fact?

You have a sense of inferential distance. That's not going to go away until you (a) read through all the arguments that nail down each point, e.g. the FOOM debate, and (b) realize that most predictions are actually antipredictions (someone link) and that most arguments are actually just defeating anthropomorphic counterarguments to the antiprediction.


An example here is the treatment and use of MWI (a.k.a. the "many-worlds interpretation") and the conclusions, arguments and further estimations based on it. No doubt MWI is the only consistent non-magic interpretation of quantum mechanics. But that's it, an interpretation. A logical consistent deduction. Or should I rather call it an induction, as the inference seems to be of greater generality than the premises, at least as understood within the LW community? But that's besides the point. The problem here is that such conclusions are, I believe, widely considered to be weak evidence to base further speculations and estimations on.

Reading the QM sequence (someone link) will show you that to your surprise and amazement, what seemed to you like an unjustified leap and a castle in the air, a mere interpretation, is actually nailed down with shocking solidity.

What I'm trying to argue here is that if the cornerstone of your argumentation, if one of your basic tenets is the likelihood of exponential evolving superhuman AI, although a valid speculation given what we know about reality, you are already in over your head with debt. Debt in the form of other kinds of evidence. Not to say that it is a false hypothesis, that it is not even wrong, but that you cannot base a whole movement and a huge framework of further inference and supportive argumentation on such premises, on ideas that are themselves not based on firm ground.

Actually, now that I read this paragraph, it sounds like you think that "exponential", "evolving" AI is an unsupported premise, rather than "AI go FOOM" being the conclusion of a lot of other disjunctive lines of reasoning. That explains a lot about the tone of this post. And if you're calling it "exponential" or "evolving", which are both things the reasoning would specifically deny (it's supposed to be faster-than-exponential and have nothing to do with natural selection), then you probably haven't read the supporting arguments. Read the FOOM debate.

Further, do you have an explanation for the circumstance that Eliezer Yudkowsky is the only semi-popular person who has figured all this out? The only person who's aware of something that might shatter the utility of the universe, if not multiverse? Why is it that people like Vernor Vinge, Charles Stross or Ray Kurzweil are not running amok using all their influence to convince people of the risks ahead, or at least give all they have to the SIAI?

After reading enough sequences you'll pick up enough of a general sense of what it means to treat a thesis analytically, analyze it modularly, and regard every detail of a thesis as burdensome, that you'll understand people here would mention Bostrom or Hanson instead. The sort of thinking where you take things apart into pieces and analyze each piece is very rare, and anyone who doesn't do it isn't treated by us as a commensurable voice with those who do. Also, someone link an explanation of pluralistic ignorance and bystander apathy.

I'm talking to quite a few educated people outside this community. They are not, as some assert, irrational nerds who doubt all all those claims for no particular reason. Rather they tell me that there are too many open questions to worry about the possibilities depicted on this site and by the SIAI rather than other near-term risks that might very well wipe us out.

An argument which makes sense emotionally (ambiguity aversion, someone link to hyperbolic discounting, link to scope insensitivity for the concept of warm glow) but not analytically (the expected utility intervals are huge, research often has long lead times).

I believe that hard-SF authors certainly know a lot more than I do, so far, about related topics and yet they seem not to be nearly as concerned about the relevant issues than the average Less Wrong member. I could have picked Greg Egan. That's besides the point though, it's not just Stross or Egan but everyone versus Eliezer Yudkowsky and some unknown followers. What about the other Bayesians out there? Are they simply not as literate as Eliezer Yudkowsky in the maths or maybe somehow teach but not use their own methods of reasoning and decision making?

Good reasoning is very rare, and it only takes a single mistake to derail. "Teach but not use" is extremely common. You might as well ask "Why aren't there other sites with the same sort of content as LW?" Reading enough, and either you'll pick up a visceral sense of the quality of reasoning being higher than anything you've ever seen before, or you'll be able to follow the object-level arguments well enough that you don't worry about other sources casually contradicting them based on shallower examinations, or, well, you won't.

What do you expect me to do? Just believe Eliezer Yudkowsky? Like I believed so much in the past which made sense but turned out to be wrong? And besides, my psychic condition wouldn't allow me to devote all my resource to the SIAI, or even a substantial amount of my income. The thought makes me reluctant to give anything at all.

Start out with a recurring Paypal donation that doesn't hurt, let it fade into the background, consider doing more after the first stream no longer takes a psychic effort, don't try to make any commitment now or think about it now in order to avoid straining your willpower.

Maybe after a few years of study I'll know more. But right now, if I was forced to choose the future over the present, the SIAI or to have some fun. I'd have some fun.

I forget the term for the fallacy of all-or-nothing reasoning, someone look it up and link to it.

All right, I'll note that my perceptual system misclassified you completely and consider that concrete reason to doubt it from now on.
Sorry.
If you are writing a post like that one it is really important to tell me that you are an SIAI donor. It gets a lot more consideration if I know that I'm dealing with "the sort of thing said by someone who actually helps" and not "the sort of thing said by someone who wants an excuse to stay on the sidelines, and who will just find another excuse after you reply to them", which is how my perceptual system classified that post.
The Summit is coming up and I've got lots of stuff to do right at this minute, but I'll top-comment my very quick attempt at pointing to information sources for replies.

Then you should have written your own version of it. Bad posts that get upvoted just annoy me on a visceral level and make me think that explaining things is hopeless, if LWers still think that bad posts deserve upvotes. People like XiXiDu are ones I've learned to classify as noisemakers who suck up lots of attention but who never actually change their minds enough to start pitching in, no matter how much you argue with them. My perceptual system claims to be able to classify pretty quickly whether someone is really trying or not, and I have no concrete reason to doubt it.
I guess next time I'll try to remember not to reply at all.
Everyone else, please stop upvoting posts that aren't good. If you're interested in the topic, write your own version of the question.

Robert Freitas seemed to be trying to argue that it would be difficult, and he couldn't argue it very well - for example, he used Eric Drexler's assumptions that were conservative for Nanosystems and anticonservative for grey goo, about a single radiation strike being enough to make a nanosystem fail, in calculating the amount of shielding required for aerovores that were mostly shielding (if I recall my reactions upon reading correctly). And despite that, the best he could come up with was "the heat bloom would be detected and stopped by our police systems", like they couldn't spread through the jet stream first and go into their final reproductive phase later, etcetera.
Unless Freitas is missing something that he seemed heavily motivated to find, I have to conclude that turning the biosphere into grey goop does not seem to be very difficult given what we currently know of the rules.

No, nothing, and because while religion does contain some confusion, after you eliminate the confusion you are left with claims that are coherent but false.

I'm a cognitivist. Sentences about goodness have truth values after you translate them into being about life and happiness etc. As a general strategy, I make the queerness go away, rather than taking the queerness as a property of a thing and using it to deduce that thing does not exist; it's a confusion to resolve, not an existence to argue over.

"There is no intangible stuff of goodness that you can divorce from life and love and happiness in order to ask why things like that are good. They are simply what you are talking about in the first place when you talk about goodness."
And then the long arguments are about why your brain makes you think anything different.

I suspect that Robin would not actually act-as-if those odds with a gun to his head, and he is being conveniently modest.


If you haven't read through the MWI sequence, read it. Then try to talk with your smart friends about it. You will soon learn that your smart friends and favorite SF writers are not remotely close to the rationality standards of Less Wrong, and you will no longer think it anywhere near as plausible that their differing opinion is because they know some incredible secret knowledge you don't.

I'm curious what evidence you actually have that "You will soon learn that your smart friends and favorite SF writers are not remotely close to the rationality standards of Less Wrong." As far as I can tell, LWians are on a whole more rational than the general populace, and probably more rational than most smart people. But I'd be very curious as to what evidence you have that leads to conclude that the rationality standards of LW massively exceed those of a random individual's "smart friends." Empirically, people on LW have trouble telling when they have sufficient knowledge base about topics and repeat claims that aren't true that support their pre-existing worldview (I have examples of both of these which I'll link to if asked). LWians seem to be better than general smart people at updating views when confronted with evidence and somewhat better about not falling into certain common cognitive ruts.
That said, I agree that XiXi should read the MWI sequence and am annoyed that XiXi apparently has not read the sequence before making this posting.

Well, I could try to rephrase as "Below the standards of promoted, highly rated LW posts", i.e., below the standards of the LW corpus, but what I actually meant there (though indeed I failed to say it) was "the standards I hold myself to when writing posts on LW", i.e., what XiXiDu is trying to compare to Charles Stross.

Because the fact that you're mentioning Charles Stross means that you need to do basic reading, not complicated reading.

"Near"? Where'd we say that? What's "near"? XiXiDu thinks we're Kurzweil?
What kind of evidence would you want aside from a demonstrated Singularity?
Grey goo? Huh? What's that got to do with us? Read Nanosystems by Eric Drexler or Freitas on "global ecophagy". XiXiDu thinks we're Foresight?
If this business about "evidence" isn't a demand for particular proof, then what are you looking for besides not-further-confirmed straight-line extrapolations from inductive generalizations supported by evidence?

Google "global ecophagy".

If he wasn't convinced about MWI it would start to become a serious possibility.

Most of the long arguments are concerned with refuting fallacies and defeating counterarguments, which flawed reasoning will always be able to supply in infinite quantity. The key predictions, when you look at them, generally turn out to be antipredictions, and the long arguments just defeat the flawed priors that concentrate probability into anthropomorphic areas. The positive arguments are simple, only defeating complicated counterarguments is complicated.
"Fast AI" is simply "Most possible artificial minds are unlikely to run at human speed, the slow ones that never speed up will drop out of consideration, and the fast ones are what we're worried about."
"UnFriendly AI" is simply "Most possible artificial minds are unFriendly, most intuitive methods you can think of for constructing one run into flaws in your intuitions and fail."
MWI is simply "Schrodinger's equation is the simplest fit to the evidence"; there are people who think that you should do something with this equation other than taking it at face value, like arguing that gravity can't be real and so needs to be interpreted differently, and the long arguments are just there to defeat them.
The only argument I can think of that actually approaches complication is about recursive self-improvement, and even there you can say "we've got a complex web of recursive effects and they're unlikely to turn out exactly exponential with a human-sized exponent", the long arguments being devoted mainly to defeating the likes of Robin Hanson's argument for why it should be exponential with an exponent that smoothly couples to the global economy.

Your skepticism is aimed in the wrong direction and MWI does not say what you think it does. Read the sequence. When you're done you'll have a much better gut sense of the gap between SIAI and Charles Stross.

Oh, is that the substantive point? How the heck was I supposed to know you were singling that out?
That one's easy: We're doing complex multi-step extrapolations argued to be from inductive generalizations themselves supported by the evidence, which can't be expected to come with experimental confirmation of the "Yes, we built an unFriendly AI and it went foom and destroyed the world" sort. This sort of thing is dangerous, but a lot of our predictions are really antipredictions and so the negations of the claims are even more questionable once you examine them.

Like what? Why he should believe in exponential growth? When by "exponential" he actually means "fast" and no one at SIAI actually advocates for exponentials, those being a strictly Kurzweilian obsession and not even very dangerous by our standards? When he picks MWI, of all things, to accuse us of overconfidence (not "I didn't understand that" but "I know something you don't about how to integrate the evidence on MWI, clearly you folks are overconfident")? When there's lots of little things scattered through the post like that ("I'm engaging in pluralistic ignorance based on Charles Stross's nonreaction") it doesn't make me want to plunge into engaging the many different little "substantive" parts, get back more replies along the same line, and recapitulate half of Less Wrong in the process. The first thing I need to know is whether XiXiDu did the reading and the reading failed, or did he not do the reading? If he didn't do the reading, then my answer is simply, "If you haven't done enough reading to notice that Stross isn't in our league, then of course you don't trust SIAI". That looks to me like the real issue. For substantive arguments, pick a single point and point out where the existing argument fails on it - don't throw a huge handful of small "huh?"s at me.

XiXiDu wants to know why he can trust SIAI instead of Charles Stross. Reading the MWI sequence is supposed to tell him that far more effectively than any cute little sentence I could write. The first thing I need to know is whether he read the sequence and something went wrong, or if he didn't read the sequence.


I have found, strangely enough, that switching from percentages (or values between 0 and 1) to something that feels more "discrete", like a scale of 0 to 10 or a "five-star" system, sometimes helps with this.

This makes perfect sense to me. I feel far more comfortable converting my sense of credibility to an intensity scale of 1 to 100 than converting those intensities to probabilities.

They're saying "I love you" in an irrational way. This can hurt because there is no easy way to quibble with the second part and not violate cultural conventions about how to express your acceptance of the first.


Would any of you really trade being well-informed for the convenience of not having to hold your tongue?

Well said. That's a 5-second response right there to quite a lot of people in the econoblogging community who think they're clever.

Ooh, same embedded system crasher as "I couldn't fail to disagree with you less."

People seemed to like Twelve Virtues of Rationality and Harry Potter and the Methods of Rationality.

I use the conjunction fallacy for my first illustration.

I think that statement becomes a lot stronger if you say "most of your ancestors".


When you ask why you're being ordered to do something, and you happen to be beneath the age that society considers you a real person, that's taken as an attack on the dominance of the person bossing you around.

Harry may or may not get a chance to say this at some point, but it sure is going in my quotes file.

Did you actually read through the MWI sequence before deciding that you still can't tell whether MWI is true because of (as I understand your post correctly) the state of the social evidence? If so, do you know what pluralistic ignorance is, and Asch's conformity experiment?
If you know all these things and you still can't tell that MWI is obviously true - a proposition far simpler than the argument for supporting SIAI - then we have here a question that is actually quite different from the one you seem to try to be presenting:

I do not have sufficient g-factor to follow the detailed arguments on Less Wrong. What epistemic state is it rational for me to be in with respect to SIAI?

If you haven't read through the MWI sequence, read it. Then try to talk with your smart friends about it. You will soon learn that your smart friends and favorite SF writers are not remotely close to the rationality standards of Less Wrong, and you will no longer think it anywhere near as plausible that their differing opinion is because they know some incredible secret knowledge you don't.

(I definitely should have thought of this earlier; interestingly enough it was this comment that was the trigger.)
Use probabilities! (Or likelihood ratios.) Especially when arguing. Yes, do so with care, i.e. without deceiving yourself into thinking you're better calibrated than you are -- but hiding the fact that you're not perfectly calibrated doesn't make your calibration any better. You brain is still making the same mistakes whether you choose to make them verbally explicit or not. So instead of reacting with indignation when someone disagrees, just ask "how confident are you?" Then, if necessary, follow up with "what was your prior?" "How many bits of evidence is this piece of information worth?" Make your argument a numerical game rather than a barefaced status war.
People often profess to be uncomfortable with assigning a numerical value to their confidence level. I have found, strangely enough, that switching from percentages (or values between 0 and 1) to something that feels more "discrete", like a scale of 0 to 10 or a "five-star" system, sometimes helps with this.


put all my eggs in one basket

Keep reading Less Wrong sequences. The fact that you used this phrase when it nakedly exposes reasoning that is a direct, obvious violation of expected utility maximization (with any external goal, that is, rather than psychological goals) tells me that rather than trying to write new material for you, I should rather advise you to keep reading what's already been written, until it no longer seems at all plausible to you that citing Charles Stross's disbelief is a good argument for remaining as a bystander, any more than it will seem remotely plausible to you that "all your eggs in one basket" is a consideration that should guide expected-utility-maximizing personal philanthropy (for amounts less than a million dollars, say).
And of course I was not arguing that you should give up movie tickets for SIAI. It is exactly this psychological backlash that was causing me to be sharp about the alleged "cryonics vs. SIAI" tradeoff in the first place.


There are maybe two or three people in the entire world who spend only the bare possible minimum on themselves, and contribute everything else to a rationally effective charity. They have an excuse for not signing up. No one else does.

If you are opening the scope to the entire world it would seem fair to extend the excuse to all those who don't even have the bare possible minimum for themselves and also don't live within 100 km of anyone who understands cryonics.

Agreed; your correction is accepted.

Short but not true. Cryonics is one of the ways that, in the self-directed part of your life, you can pretend to be part of a smarter civilization, be the sort of sane person who also fights existential risk in the other-directed part of their life. Anyone who spends money on movie tickets does not get to claim that they have no self-directed component to their life.

I'm saying he'll get them to do neither.
Easy way for multi to provide an iota of evidence that what he's doing is effective: Find at least one person who says they canceled a cryo subscription and started sending an exactly corresponding amount of money to the Singularity Institute. If you just start sending an equal amount of money to the Singularity Institute, without canceling the cryo, then it doesn't count as evidence in his favor, of course; and that is just what I would recommend anyone feeling guilty actually do. And if anyone actually sends the money to Africa instead, I am entirely unimpressed, and I suggest that they go outside and look up at the night sky for a while and remember what this is actually about.

So be it first noted that everyone who complains about trying to trade off cryonics against charity, instead of movie tickets or heart transplants for old people, is absolutely correct about cryonics being unfairly discriminated against.
That said, reading through these comments, I'm a bit disturbed that no one followed the principle of using the Least Convenient Possible World / strongest argument you can reconstruct from the corpse. Why are you accepting the original poster's premise of competing with African aid? Why not just substitute donations to the Singularity Institute?
So I know that, obviously, and yet I go around advocating people sign up for cryonics. Why? Because I'm selfish? No. Because I'm under the impression that a dollar spent on cryonics is marginally as useful as a dollar spent on the Singularity Institute? No.
Because I don't think that money spent on cryonics actually comes out of the pocket of the Singularity Institute? Yes. Obviously. I mean, a bit of deduction would tell you that I had to believe that.
Money spent on life insurance and annual membership in a cryonics organization rapidly fades into the background of recurring expenses, just like car insurance. To the extent it substituted for anything, it would tend to substitute for buying a house smaller by $300/year on the mortgage, or retirement savings, or something else that doesn't accomplish nearly as much good as cryonics.
There are maybe two or three people in the entire world who spend only the bare possible minimum on themselves, and contribute everything else to a rationally effective charity. They have an excuse for not signing up. No one else does.
And if you do sign up for cryonics, that contributes to a general frame of mind of "Wait, there are clever solutions to all the world's problems, this planet I'm living in doesn't make any sense, it's okay to do something that other people aren't doing, I'm part of the community of people who are part of the future, and that's why I'm going to donate to SIAI." It's a gateway drug; it's part of the ongoing lifestyle of someone with one foot in the future, staring back at a mad world and doing what they can to save it.
The basic fact about rational charity is that charity is not a matter of people starting out with fixed resources for charity and allocating them optimally. It is about the variance in the tiny little percentage of their income people give to rationally effective charity in the first place. And if I had to place my bets on empirical outcomes, I would bet that this blog post helped decrease that percentage in its readers, more than it actually resulted in any dollars going to an effective charity (i.e., SIAI, who is anyone kidding with this talk about development aid?) by helping to foster a sense of guilt and "ugh" around rational charity.
And finally, with all that said, if we actually did forget about the Singularity and the expected future value of the galaxy and take the original post at face value, if you consider the interval between a planet with slightly more developed poor countries and a planet signed up for cryonics, and ask about marginal impacts you can have on both relative to existing resources, then clearly you should be signing up for cryonics. I am tempted to add a sharp "Duh" to the end of this statement.
But of course, the actual impact of cryonics, just like the actual impact of development aid, in any rational utilitarian calculation, is simply its impact on the future of the galaxies, i.e., its impact on existential risk. Do I think that impact is net negative? Obviously not.


I have no problem with people signing up for cryonics as long as they recognize that it's something that they're doing for themselves.

In your version of the story, what mistake am I making that causes me to go around urging other people to sign up for cryonics?


after one is revived the human race could go extinct

Given the tech level required for revival, I'd assign a pretty low probability of getting revived before we're through the window of vulnerability.

http://lesswrong.com/lw/x5/nonsentient_optimizers/

Then it would've been trivial to leave at least one nanomachine and a radio detector in every solar system, which is all it takes to wipe out any incipient civilizations shortly after their first radio broadcast.

Tricycle has the data. Also if an event of JCW magnitude happened to me I'm pretty sure I could beat it. I know at least one rationalist with intense religious experiences who successfully managed to ask questions like "So how come the divine spirit can't tell me the twentieth digit of pi?" and discount them.

I already wrote this fic ("The Grand Finale of the Ultimate Meta Mega Crossover").

Rowling's money is wildly inconsistent. I use the figure of one Galleon = $100USD and stick with it.

Wands cost 7 Galleons. People throw around comparable sums all the time in canon. Percy Weasley bets 10 Galleons on a Quidditch game, heck, Harry buys three sets of Omnioculars (wizarding binoculars) at 10 Galleons apiece to watch the Quidditch World Cup. Many wizarding supplies less useful than a wand cost considerably more. There really is no good reason for witches and wizards not to carry multiple wands except for tradition. Even the Weasleys could afford multiple wands if they made it a priority.

10^-5, not 10^-8.

"No."
Life insurance salesmen are used to hearing that. If they act offended, it's a sales act. If you're reluctant to say it, you're easily pressured and you're taking advantage. You say "No". If they press you, you say, "Please don't press me further." That's all.

I did that research, thank you. I always read the Wikia.

Okay, I think we can end this thread now.

They're still right. If that's what happened to the reader and broke their suspension of disbelief, that's what happened. It doesn't matter if the reader made a mistake. Your text caused that mistake.

Seems highly likely.

I've sometimes pondered the bizarrely high level of rationality on TV Tropes, and my guess is that it has something to do with people zooming in, thinking about details, trying to find the obvious consequences and moral implications that no one else sees, thinking in "near mode" about things that would usually be considered in "far mode", and possibly just being made up of nerds.
Cases in point:

http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan
http://tvtropes.org/pmwiki/pmwiki.php/Main/FantasticAesop


That was the complaint.
Personally I think a lot of people are confusing expert skepticism with expert science, but if the reader says you're messing with their suspension of disbelief, the reader is always right. Substituting Michael Shermer just makes it a Shout Out instead of an actual conspiracy theory.

Btw, after some complaints about suspension of disbelief, I substituted Michael Shermer for James Randi.

Can you do worse? Try harder.

Okay, cat!McGonagall/Dobby and Crabbe/Fawkes are both completely wrong enough to be competitive.
But of course, it's more impressive if the completely wrong ship is less high-entropy - that is, if you don't have to dip so low in the search ordering to find it.
Incidentally, I tried to look it up, and as far as I can tell, this particular ship has been done exactly once before. Surprising, considering how near the characters are in canon. I suppose it's just that wrong.

Feel free to repost without the hints.

Hermione/Griphook isn't completely wrong enough to qualify.

And you'd better believe Methods of Rationality obeys it.

Go for it. Most people complain about the pace being too slow - I think you might even be the first to complain about it being too fast - but that's certainly one way to fix it, if you're inspired with the vision of a battle. There's a chance though hardly a certainty that I would answer questions before you wrote, or declare the story canon afterward.

That's all correct, modulo the actual psychological damage already caused to people with personalities closer than average to the OCD end of the spectrum, who were much more vulnerable than someone further away from that end of the spectrum might expect, to thoughts of the form "if you think X bad things will happen". Ordinarily you just tell them that's not how a reductionist universe works and try to help them understand reductionism, but in this case that cure doesn't work!

J. K. Rowling once said that Harry Potter was a story about death.

I don't understand this thread.

Does the obvious Lobian proof for 1 still go through if there's a bound on the proof length built into all the sentences?

Actually, someone shot down my proposed my.C = (my.C == your.C) algorithm, because if two prisoners both defect, that is in a mathematical sense "cooperating if and only if you cooperate". So if the other prisoner is Defection Rock, then my.C = (my.C == your.C) has no consistent solution.


There's little he could do that would make us think "Wow, I didn't know Voldemort was that evil".


Read chapter 22 and 23, then look at what you just wrote. Are you sure that's the same Draco? If the passage actually does need more shock value (and I'm not quite sure that it does, especially given the wide variance in reader taste and the number who enjoyed the rest of the fic but thought that one part was too shocking) then it has to be more in-character for the later-revealed Draco. There's a simple way to increase the shock without adding vulgarities that polite young Death Eaters don't use - namely, substitute "torture and rape" for "rape" - but I already think this whole conversation is getting a bit off-topic for LW.

MoR points things out for the sake of pointing them out, by way of trying to teach the art of the awakened mind. Actually I'm not clear on what if anything we're arguing about at this point.


What we don't expect is for him to talk casually about committing rape, considering that nobody in the series does

Those are the children's books version, and MoR is not a children's book, nor is most fanfiction. If you'll pardon the size of the hypothetical: If the Death Eaters actually existed, no way in hell are the males not committing rape.

Yes.

Oh, some other change I made in the past.

Correct. Do I need to point out again that no one even noticed the thing with the monastery?

I'm not sure the Draco I've been writing for the last few chapters would use the term "bitch" in front of Harry Potter, it doesn't sound dignified enough for the heir of Malfoy.
(But yes I did explicitly consider that alternative. If I get enough votes for keeping the shock value I'll put it back in, or figure out something, I guess.)

Well, for one thing, sometimes Orwellian Retcons are there for important future reasons. It's one thing if someone reads Chapter 7 well enough, before and after, to spot the other change I made to Draco; but having a little automatic computer notifier makes my life more nervous.
Also, what kind of load does Update Scanner put on FFN's servers?


Notice: I am not Professor Quirrell in real life.

And that is exactly what Professor Quirrell would say!

Professor Quirrell wouldn't give himself away by writing about Professor Quirrell, even after taking into account that this is exactly what he wants you to think.

I see. A side effect of banning one post, I think; only one post should've been banned, for certain. I'll try to undo it. There was a point when a prototype of LW had just gone up, someone somehow found it and posted using an obscene user name ("masterbater"), and code changes were quickly made to get that out of the system when their post was banned.
Holy Cthulhu, are you people paranoid about your evil administrator. Notice: I am not Professor Quirrell in real life.
EDIT: No, it wasn't a side effect, Roko did it on purpose.


A number of people have asked about that feature of weirdtopia, and as far as I know, Eliezar has never answered.

Um, grandparent of above comment much? http://lesswrong.com/lw/y8/interlude_with_the_confessor_48/qtf

So... the part I found potentially convincing was that if you ran off a logical view of the world instead of a Solomonoff view (i.e., beliefs represented in e.g. higher-order logic instead of Turing machines) and lived in a hypercomputable world then it might be possible to make better decisions, although not better predictions of sensory experience, in some cases where you can infer by reasoning symbolically that EU(A) > EU(B), presuming that your utility function is itself reasoning over models of the world represented symbolically. On the other hand, cousin_it's original example still looks wrong.

AIXI with a TM-based universal prior will always produce predictions about the black box, and predictions about the rest of the universe based on what the black box says, that are just as good as any prediction the human can come up with. After all, the human is in there somewhere. If you think of AIXI as embodying all computable ways of predicting the universe, rather than all computable models of the universe, you may begin to see that's not quite as narrow as you thought.

No posts on Riddle Theory.

Still the sort of thing that will send people close to the OCD side of the personality spectrum into a spiral of nightmares, which, please note, has apparently already happened in at least two cases. I'm surprised by this, but accept reality. It's possible we may have more than the usual number of OCD-side-of-the-spectrum people among us.

I've evolved to stop talking to people who abuse evolutionary psychology.

I actually use "the sign bit in the utility function" as one of my canonical examples for how not to design an AI.

The original version of this post caused actual psychological damage to at least some readers. This would be sufficient in itself for shutdown even if all issues discussed failed to be true, which is hopefully also the case.
Please discontinue all further discussion of the banned topic.
All comments on the banned topic will be banned.
Exercise some elementary common sense in future discussions. With sufficient time, effort, knowledge, and stupidity it is possible to hurt people. Don't.
As we used to say on SL4: KILLTHREAD.


If you were worried that they did, you'd just try to apply an equal and opposite impulse in the other direction to reduce the incentive to blackmail you, and then rely on an FAI to try to cancel it out exactly later.

I didn't quite understand this.
I also don't understand how exactly to avoid being blackmailed. If one went on and didn't do anything different, the AI would be precommited to punishment, but if one refused to be blackmailed and went on and didn't do anything different, there would be no point in the AI doing that?
Is it just about thinking "I refuse to be blackmailed?"
I know maybe I shouldn't've been thinking about it, but I'd already thought about it, so I figure it wouldn't hurt.

Right, no point in the AI doing that. If there's a leftover marginal expected effect then an FAI comes along later and applies an impulse in the other direction, erring on the side of overcompensation. That last trick is (a) slightly less likely to work than just not thinking the exactly wrong thought and (b) possibly expensive, so it's a retreat to second-best.

Time, mental energy, focus. I cannot work two jobs and do justice to either of them.
I am feeling viscerally insulted by your assertion that anything I do can be done in my spare time. Let's try that with nuclear engineers and physicists and lawyers and electricians, shall we? Oh, I'm sorry, was that work actually important enough to deserve a real effort or something?

There's more than one countable infinity?

You're silly. I suppose if you started doing things in your free time that are as interesting as what I do in my professional full-time workdays I would pay attention to you again.

I'd certainly never take a positive action aimed in that direction. I do not commit to trying to write in any special-purpose safeguards.
If you don't want CEV to try blackmailing you, there is a very simple solution. DON'T THINK ABOUT IT. And don't worry about accidentally thinking about it in "sufficient detail", I'm pretty sure if you just hum a tune that'll be enough to stop you from accidentally getting it exactly right.

ERROR: FINITE MERMAID NUMBER

I begin to worry that there is a certain sort of rationalist who is driven insane by the thought of other people doing better than them, and who, perhaps, is having romantic problems that are not best addressed by the same sort of thinking that created them.

I would not have been able to write and pursue a day job at the same time. You seem to have incredibly naive ideas about the amount of time and energy needed to accomplish worthwhile things. There are historical exceptions to this rule, but they are (a) exceptions and (b) we don't know how much faster they could have worked if they'd been full-time.


Popularized evo-psych seems to be a lot like appealing that a certain way of life is "natural" and thus "good".

Does anyone actually do this? Do I just hang out in the wrong circles? Are there people who do this and yet I never talk to any of them or read anything they write?

If it produces successful advance predictions - if you do new experiments to test the idea and they seem to come out pretty much the right way - then it's probably working well enough.


higher popularity of pornography with one female and multiple males compared to one male and multiple females

Supply-driven. Male actors are much cheaper.

Omega lets me decide to take only one box after meeting Omega, when I have already updated on the fact that Omega exists, and so I have much better knowledge about which sort of god I'm likely to encounter. Upsilon treats me on the basis of a guess I would subjunctively make without knowledge of Upsilon. It is therefore not surprising that I tend to do much better with Omega than with Upsilon, because the relevant choices being made by me are being made with much better knowledge. To put it another way, when Omega offers me a Newcomb's Problem, I will condition my choice on the known existence of Omega, and all the Upsilon-like gods will tend to cancel out into Pascal's Wagers. If I run into an Upsilon-like god, then, I am not overly worried about my poor performance - it's like running into the Christian God, you're screwed, but so what, you won't actually run into one. Even the best rational agents cannot perform well on this sort of subjunctive hypothesis without much better knowledge while making the relevant choices than you are offering them. For every rational agent who performs well with respect to Upsilon there is one who performs poorly with respect to anti-Upsilon.
On the other hand, beating Newcomb's Problem is easy, once you let go of the idea that to be "rational" means performing a strange ritual cognition in which you must only choose on the basis of physical consequences and not on the basis of correct predictions that other agents reliably make about you, so that (if you choose using this bizarre ritual) you go around regretting how terribly "rational" you are because of the correct predictions that others make about you. I simply choose on the basis of the correct predictions that others make about me, and so I do not regret being rational.
And these questions are highly relevant and realistic, unlike Upsilon; in the future we can expect there to be lots of rational agents that make good predictions about each other.

Be sure to add some extra complexity so that you can get more emergence out!


But then, I don't want to become some horrible robot that doesn't truly care about paperclips.

Er, I think you just blew your pretense. Paperclip maximizers care about paperclips, they don't use phrases like "horrible robot that doesn't truly care", they'd be happy to have a universe containing nothing sentient and lots of paperclips.
Or they would be, if they ever bothered to experience happiness, I mean. As opposed to just outputting the action that leads to the most expected paperclips. Hence the term, "expected paperclip maximizer". Don't think of it as having a little ghost inside that maximizes paperclips, think of it as a ghostless device that maximizes paperclips.

Haven't read the book yet, but here's the supporting evidence I gathered from the Amazon.com preview and the authors' website and blog. (I probably missed some so please add to the list.)

females have potential for multiple orgasms
higher popularity of pornography with one female and multiple males compared to one male and multiple females
female copulatory vocalizations
male anatomy indicating sperm competition
Coolidge effect in females
it fits better with "fierce egalitarianism" of forager bands: female sexual exclusivity is necessary for males to determine paternity, but if all resources are equally shared, then there is little point in knowing paternity
ETA: our closest primate relations, chimps and bonobos, have "multimale-multifemale mating systems"

On a separate note, while it seems plausible that the authors of the book are right that our forager ancestors were polyamorous, it's not clear why that matters to us in making our own choices, given that our ancestors switched over to monogamy/polygyny as soon as agriculture was invented.

"Frogs have subjective experience" is the biggy, there's a number of other things I already know myself to be confused about which impact on that, and so I don't know exactly what I should be looking for in the frog that would make me think it had a sense of its own existence. Certainly there are any number of news items I could receive about the frog's mental abilities, brain complexity, type of algorithmic processing, ability to reflect on its own thought processes, etcetera, which would make me think it was more likely that the frog was what a non-confused person of myself would regard as fulfilling the predicate I currently call "capable of experiencing pain", as opposed to being a more complicated version of neural network reinforcement-learning algorithms that I have no qualms about running on a computer.
A simple example would be if frogs could recognize dots painted on them when seeing themselves in mirrors, or if frogs showed signs of being able to learn very simple grammar like "jump blue box". (If all human beings were being cryonically suspended I would start agitating for the chimpanzees.)

To me it seems obvious that the intended meaning is "most parents are sufficiently non-indifferent in some direction".

I don't consider frogs to be objects of moral worth.

I have no trouble visualizing a society composed mostly of people with high status, or a society composed mostly of people with low status, with very different sums of total status.

Was trying to say Pentateuch, not Nach.


There are a lot of religious Jews who have not read much of the Bible

I'd guess most adult Orthodox Jews have read the whole Old Testament.


I haven't checked this calculation at all, but I'm confident that it's wrong, for the simple reason that it is far more likely that some "mathematician" gave them the wrong numbers than that any compactly describable event with odds of 1 in 18 septillion against it has actually been reported on, in writing, in the history of intelligent life on my Everett branch of Earth.

Hm. Have you looked at the multiverse lately? It's pretty apparent that something has gone horribly weird somewhere along the way. Your confidence should be limited by that dissonance.
It's the same with MWI, and cryonics, and moral cognitivism, and any other belief where your structural uncertainty hasn't been explicitly conditioned on your anthropic surprise. I'm not sure to what extent your implied confidence in these matters is pedagogical rather than indicative of your true beliefs. I expect mostly pedagogical? That's probably fine and good, but I doubt such subtle epistemic manipulation for the public good is much better than the Dark Arts.
(Added: In this particular case, something less metaphysical is probably amiss, like a math error.)

Whowha?


Kachelmeier, S.J., & Shehata, M. (1992). Examining risk preferences under high monetary incentives: Experimental evidence from the People's Republic of China. American Economic Review, 82, 1120-1141.

That's what my notes says was the famous experiment which tested some biases, I forget which, in China using offers of up to several months equivalent salary. Just a fast answer grabbed from my notes, I didn't try to Google or check notes for what the experiment was about.
EDIT: Paper here, seems to be about pricing some simple lotteries. http://decisions.epfl.ch/ExpFinance/readings/KachelmeierShehata.pdf

I always figured that was a rather large sector of people's negative reaction to cryonics; I'm amazed to find someone self-aware enough to notice and work through it.

From a recent newspaper story:

The odds that Joan Ginther would hit four Texas Lottery jackpots for a combined $21 million are astronomical. Mathematicians say the chances are as slim as 1 in 18 septillion -- that's 18 and 24 zeros.


http://www.msnbc.msn.com/id/38229644/ns/us_news-life

I haven't checked this calculation at all, but I'm confident that it's wrong, for the simple reason that it is far more likely that some "mathematician" gave them the wrong numbers than that any compactly describable event with odds of 1 in 18 septillion against it has actually been reported on, in writing, in the history of intelligent life on my Everett branch of Earth. Discuss?

I increased the number of soldiers Hermione had left to help make this clearer.

In medias res this one. Start it in the middle of something interesting happening.

Literally laughing out loud, here.
But just to be clear, this story represents my outrage at all scientifically uncurious characters everywhere, and even more than that, my unfilled need to read a story where for just once the alleged "genius" characters are actual geniuses.
I was not picking on J. K. Rowling in particular in any way.
It is a work of Harry Potter fanfiction for the following simple reason:
I knew I needed a rapid feedback loop to motivate my brain to write. That was why I was bogging down on the rationality book.
And to the best of my knowledge of the entire world of online fiction, if you were posting an incomplete story chapter-by-chapter, it would get the most reviews if...
...it were a work of Harry Potter fanfiction posted on fanfiction.net.
QED.

Actually, on reviewing this remark later, it's not quite true. My brain generated an idea set in the HPverse because I'd been reading a lot of HP fanfiction, and I accepted it and stopped the search because it was also optimal for getting reviews. However, I've since read analyses showing that Twilight stories are getting more new reviews on FF.net than Harry Potter, and I don't think I'd have been the smallest bit tempted if I'd known the fact in advance.

As the comments by this user have been consistently voted down and he cannot seem to take the hint, comments by him will be deleted/banned.

I think you're Rokomorphizing an awful lot. You just need to be in a state of mind where smashing a cyro container seems cool, something that can score points with your friends, and where you think you can get away with it.


And think of the required radiation shielding! All of your stuff is getting irradiated, so you need lots of lead.

Is the radiation going to cause significant information-theoretic damage? In how long?

Things like this are 90% self-confidence and 10% innate weirdness. Talk about it like it's obvious, normal, and you're part of a community of smart people out there, and they'll pick up on the cues.
I know saying that won't help a lot of people, but it's what I do. When I introduce cryonics to someone, I don't sound nervous and timid and censure-expecting, I take off my necklace and say "This is my contract of immortality with the cult of the severed head."

Social reasons? You're scared they'll think you're weird? I'd think most programmers would be open to a discussion about the brain as a program, at least. Is it really that weird?

I'm getting a pretty sharp lesson in unintended ambiguity here. Although some of your suggestions are not compatible with a straight reading of the text.


I'm aware of no previous work of fiction that jootzed all the way to being genre-aware of its genre-awareness and I think it would be it would be hilarious to read one :-)

Yes, well, you may have to write yourself the work you always wanted to read.

Okay, if no one on LW got Aftermath 2, even after working out that the aftermath was supposed to display a change in Snape, then it was too subtle.


Come to think of it, some of the MWI proponents here should agree that by their criteria, there was nothing irrational about Paulos's investment at all.

Anyone with diminishing returns on the utility of money doesn't like volatility, whether probabilistic or MWI.

No, Harry's experimental result scared the hell out of him and he decided not to do any more clever experiments until he was fifteen.
This is actually more rational than what you are advocating.


Good heavens Mr. Yudkowsky, I thought the inventor of Timeless Decision Theory would have a better grasp on how being the kind of person who would make a certain decision can determine what happens

I do indeed.
I've written unpublished fiction about it.
From before TDT was invented, actually.
Harry has not worked all that stuff out yet.
He did work out one important principle so far.
It is called DO NOT MESS WITH TIME.
And considering that he got that result, you seem to have missed some of the implications for how time travel works in that universe which would make it potentially dangerous to try and blackmail reality.
Time travel was the first optimization process I considered which was truly alien enough to deanthropomorphize my thinking; evolutionary biology didn't do the job, but the unpublished story I was writing about time travel did.
What you're suggesting is a bit more potentially incredibly dangerous than you seem to think.

Time travel in this universe has a consistent single line; once McGonagall sees Harry disappear, he can't undo it.

Good heavens, Mr. Wedrifid, you can't change time! Do you think students would be allowed Time-Turners if that was possible? What if someone tried to change their test scores?


"Harry, give me your time t..."
shit. shit. shit. Activate time turner. Escape.

...don't take this the wrong way, but even Harry knows better than that.
If you genuinely think this is a smart thing to do in real life, it makes me seriously worry about your safety and the safety of people around you.

He can't have inner dialogue during that section, it's in Minerva's point of view!

...I am not sure if Professor Quirrell whistles Bach. I shall have to think about it.

To be precise, the argument would run that the universe will end up being dominated by beings that care more about their measure, and so there is a categorical imperative for happier beings to care more about their measure.

Second Blueberry's question.

This doesn't warrant one sentence worth of inner dialog? Seriously not buying it. It's a different Harry hacked together for a different parable and as a plot hack to get rid of the Get Out Of Jail Free Card of Awesomeness +4.

That's definitely going on.

Hey, not to sound intimidating or anything, but it's a sad fact that while Michael Vassar and I have gigantic webs of precomputed original ideas, we can also generate original ideas in real time.

I'm sorta discouraged by what a shoddy hate blog it is.

Ouch. I am burned.

Your original quote asserts a definite fate, not a fate which would occur if some particular technology were to remain uninvented.

That is not dead which can eternal lie 
And with strange aeons even death may die

-- H. P. Lovecraft

I've asked this question before, but where the hell does the high-quality rationality on TV Tropes come from?

I try not to downvote people when they are right.

There's a general rule in writing that if you don't know how many items to put in a list, you use three. So if you're giving examples and you don't know how many to use, use three. Don't know if that helps, but it's the main heuristic I know that's actually concrete.

keyword = "werther effect"

Literally laughing out loud, here.
But just to be clear, this story represents my outrage at all scientifically uncurious characters everywhere, and even more than that, my unfilled need to read a story where for just once the alleged "genius" characters are actual geniuses.
I was not picking on J. K. Rowling in particular in any way.
It is a work of Harry Potter fanfiction for the following simple reason:
I knew I needed a rapid feedback loop to motivate my brain to write. That was why I was bogging down on the rationality book.
And to the best of my knowledge of the entire world of online fiction, if you were posting an incomplete story chapter-by-chapter, it would get the most reviews if...
...it were a work of Harry Potter fanfiction posted on fanfiction.net.
QED.

What evidence exists for an afterlife in the HP universe? I haven't actually read the novels beyond 1-3, just fanfiction, and movies 4-6.
EDIT: Since people have asked why:
I read books 1-3 while I was still living in Chicago, and the whole family read them together. Then I moved out in 2000, and didn't get around to trying to read book 4 until it was time to write the fic... and found that I couldn't seem to read it. Maybe it was a change in the literary quality (I've heard others say that) or it was the fact that I'd already watched the movie and that got rid of the plot tension. Or (my personal suspicion) the fact that I'd read a lot of fanfiction aimed at a more grownup audience meant that the children's-book version of the Potterverse just didn't feel right to me any more.
I feel guilty about not reading the later books, obviously, but my brain doesn't want to do it and one of the major points of this whole endeavor is that it's fun, not something I have to make myself do. So I've been getting along on movies 4-6, other fanfiction, and above all the Harry Potter Wikia.
Mentioning this because TV Tropes is now giving me a "Did Not Do The Research" trope, which is supposed to be for people who didn't care enough to find out something they could've gotten in 10 minutes, and that stings a bit. I tried, I really did, and now I read the Wikia and try my best to get things right, but I'm just not enjoying the original Potter novels. Not every children's book, even ones that have taught millions of children to enjoy reading, ends up being enjoyable to every adult.
"Harry Potter and the Methods of Rationality" is in the Potterverse because I was attracted to the universe of Harry Potter fanfiction.

Not to mention, the ancient wizards made the levitation spell be called "Wingardium Leviosa"?

Hate spinach, love ice cream, love mother. What's so difficult?

Outsource would be desirable.


our perceived history matches the Born rules too well for it to be reasonable that "probabilities are meaningless", so either the universe is OK with measures on infinite sets or it's somehow finite after all

I like this; it is an excellently compact way of putting it.

TV Tropes claims that I have "a few" Shout Outs. You'd think TV Tropes would do better than this.

I probably need to write up a "For those who have never seen the books" summary page of years 1-7, containing all the information that will be needed mixed with enough other information that it looks like a summary instead of "here is exactly and only what you will need to know".

Slightly edited the original post to avoid giving away what my readers have finally convinced me is, in fact, an undesirable spoiler. I also hope you didn't mind my removing the mention of FAI, because I feel fairly strongly about not mixing that into the fic. "A fanatic is someone who can't change their mind and won't change the subject"; if we can't shut up about FAI while talking about Harry Potter, we may have a problem.

Search on "tentacle".

Maybe I should start a startup that requires you to pay a $1000 fee if you do business with anyone who signs a precommitment contract intended to give them a negotiating advantage.


Dan Ariely, who apparently learned his teaching methodology from Professor Quirrell

Epic reversed causality much?
Heuristics and biases researchers -> Eliezer Yudkowsky -> Professor Quirrell.

Not even the term "rationalism" can manage to sever itself from the Soviet Union. Both the French Revolution and the Communist Revolution were genuine bastard children of the Enlightenment - missing what we would now regard as some key elements, yes, turning into pure power plays as they spiraled downward, but way back at the dawn of time they both started in, metaphorically speaking, the coffeeshops of Oxford.


On the other hand, I think it's close enough to non-falsifiable that I don't worry about it.

A hypothesis which is non-falsifiable has an alternative which is also non-falsifiable. You don't get to "not worry about it" unless you can show that the prior is low, which is a separate matter from any difficulties of setting up an experiment that everyone will agree is definitive.

Or mass killers are anti-Church for the same reason that they are, say, anti-labor-union: they permit no competing organized power structures.

Um... Chapter 7 is not the child-friendliest chapter in the world. Teen-friendly, maybe. Not child-friendly.

It's almost done, actually. Here's a sneak preview of the next chapter:

Dumbledore peered over his desk at young Harry, twinkling in a kindly sort of way. The boy had come to him with a terribly intense look on his childish face - Dumbledore hoped that whatever this matter was, it wasn't too serious. Harry was far too young for his life trials to be starting already. "What was it you wished to speak to me about, Harry?"
Harry James Potter-Evans-Verres leaned forward in his chair, looking bleak. "Headmaster, I got a sharp pain in my scar during the Sorting Feast. Considering how and where I got this scar, it didn't seem like the sort of thing I should just ignore. I thought at first it was because of Professor Snape, but I followed the Baconian experimental method which is to find the conditions for both the presence and the absence of the phenomenon, and I've determined that my scar hurts if and only if I'm facing the back of Professor Quirrell's head, whatever's under his turban. Now it could be that my scar is sensitive to something else, like Dark Arts in general, but I think we should provisionally assume the worst - You-Know-Who."
"Great heavens, Harry!" gasped Dumbledore. He sat there with his head whirling. The boy was right that this was nothing to ignore. He dared not confront Professor Quirrell within the halls of Hogwarts, around the other students - he would have to figure out some way to lure Quirrell out of the castle -
But the grim young boy was still speaking. "Now, if the worst is true, then we know exactly where You-Know-Who is right now. And I don't think that's an opportunity we should pass up. Destroying his body didn't work last time, so I asked Hermione if she'd ever heard of anything that would destroy a soul, and she mentioned a method of executing criminals called the Dementor's Kiss..."

just kidding

AAAAAIIIIIIIIEEEEEEEE
BOOM

I think you underestimate the real-world value of Just Testing It. If I got a mysterious letter in the mail and Mom told me I was a wizard and there was a simple way to test it, I'd test it. Of course I know even better than rationalist!Harry all the reasons that can't possibly be how the ontologically lowest level of reality works, but if it's cheap to run the test, why not just say "Screw it" and test it anyway?
Harry's decision to try going out back and calling for an owl is completely defensible. You just never have to apologize for doing a quick, cheap experimental test, pretty much ever, but especially when people have started arguing about it and emotions are running high. Start flipping a coin to test if you have psychic powers, snap your fingers to see if you can make a banana, whatever. Just be ready to accept the result.

The probability of magic should make any effort on testing the hypothesis unjustified. Testing theories no matter how improbable is generally incorrect dogma. (One should distinguish improbable from silly though.)

I think I agree with that: There's nothing necessarily delusive about believing you got lucky, but it should generally require (at least) an amount of evidence proportional to the amount of purported luck.

Then if that qualifies, what would falsify Gall's Law?

Do a top-level post.

Yeah, I don't think I can plausibly deny responsibility for this one.
Googling either (rationality + fanfiction) or even (rational + fanfiction) gets you there as the first hit, just so ya know...
Also, clicking on the Sitemeter counter and looking at "referrals" would probably have shown you a clickthrough from a profile called "LessWrong" on fanfiction.net.
Want to know the rest of the plot? Just guess what the last sentence of the current version is about before I post the next part on April 3rd. Feel free to post guesses here rather than on FF.net, since a flood of LW.com reviewers would probably sound rather strange to them.

Counterexample: Space shuttle.

But that still doesn't need to be luck. I got my priors offa evolution and they are capable of noticing when something works or doesn't work a hundred times in a row. True, if I had a different prior, I wouldn't care about that either. But even so, that I have this prior is not a question of luck.

THAT'S NOT TRUE!
(Flees in tears.)

Trust your intuition.

I'd further specify that the bottom levels should not be fundamentally mental (or living). In other words, the bottom levels should resemble bowling balls or water more than it resembles fish or human beings; to look at it another way, we should end up explaining how human-like things are made out of water-like things since all is water, rather than how water-like things are made out of human-like things since all is mind.

Can't answer until I know the laws of time travel.
No, seriously. Is the resulting universe randomly selected from all possible self-consistent ones? By what weighting? Does the resulting universe look like the result of iteration until a stable point is reached? And what about quantum branching?
Considering that all I know of causality and reality calls for non-circular causal graphs, I do feel a bit of justification in refusing to just hand out an answer.

I'd still deny this. You need the right (wrong) fallacies to jump to those conclusions. Maybe the fallacies are easy to invent, or maybe our civilization ubiquitously primes people with them, but it still takes an extra and mistaken step.

I'm not worried about sounding effusive and I'll omit the "borderline" part.

I think we already take this for granted around here.

Here's a puzzle that involves time travel:
Suppose you have just built a machine that allows you to see one day into the future. Suppose also that you are firmly committed to realizing the particular future that the machine will show you. So if you see that the lights in your workshop are on tomorrow, you will make sure to leave them on; if they are off, you will make sure to leave them off. If you find the furniture rearranged, you will rearrange the furniture. If there is a cow in your workshop, you will spend the next 24 hours getting a cow into your workshop.
My question is this: What is your prior probability for any observation you can make with this machine? For example, what are the odds of the windows being open?

Well, not quite option two, but yes, "You make a convincing case that it should be legal to eat month-old infants." One person's modus ponens is another's modus tollens...


My mother made this argument to me probably when I was in high school. Given my position as past infanticide candidate, it was an odd conversation.

Hey, now you know you were kept around because you were actually wanted, not out of a dull sense of obligation. It's like having a biological parent who is totally okay with giving up children for adoption - and stuck around!

Despite some jokes I made earlier, things that could arguably depend on values don't make good litmus tests. Though I did at one point talk to someone who tried to convert me to vegetarianism by saying that if I was willing to eat pork, it ought to be okay to eat month-old infants too, since the pigs were much smarter. I'm pretty sure you can guess where that conversation went...

I am constantly amazed by the number of people who commit suicide without getting on the evening news.

I assume the problem is to be interpreted as Omega saying, "Either (1) (I have predicted you will refuse the $10, and there is $1000,000 in the envelope) xor (2) (I have predicted you will take the $10, and there is $0 in the envelope)", rather than asserting some sort of entanglement above and beyond this.
If so, I take the $10 and formulate the counterfactual, "If I were the sort of person who rejected the $10, Omega would have told me something else to begin with, like 'if you refuse the $10 then the envelope will be empty', but the digit of pi would have been the same".
As previously noted, though, I can't quite say how to compute this formally.

My father was once involved in an UFO sighting - he built the UFO, and did the sound effects too, when the other kids got close. Summer camp was involved.
Hope no one ever told those kids it was a flock of birds...


and in any case wouldn't do fancy acrobatics with their exterior lights on.

How can you reason about the motives of alien interstellar travelers? Maybe traveling to earth to freak out the humans is just some kind of alien prank, like cow-tipping. Maybe they get some aesthetic satisfaction from fancy aerial acrobatics.

How can you reason about the motives of alien interstellar travelers? Maybe they've been poking holes in my socks and interfering with my TV reception.

I have trouble imagining how I would feel if heterosexuals were persecuted and one of my gay male friends kissed a woman to show solidarity.


My head for my future self, my innards for the sick, my penis and anus for lovers, and my arms and legs for the hungry.

NEW UTILITARIAN LITMUS TEST


If this observation is correct, beliefs about sexuality are a very strong indicator of rationality.

The problem is if the supposedly rational beliefs also happen to be the tribal belief system of a large, pre-existing tribe. Then someone was rational, sometime back in the history, but it isn't necessarily the person you're talking to right now.
A better test would be to ask them to defend a sexual view of theirs that they see as unconventional, or at least, not a typical view of their tribe as yet.

If you uploaded, would you be willing to let someone else eat your body if they were, y'know, into that sort of thing?

Point of curiosity if anyone knows the answer: How promiscuous are bisexual men and do they tend to have more m-m than m-f sex because the m-m sex is much easier to obtain? If not, why not?

Actually, what you really need is the sexchange pill, but that's a lot harder than it sounds.


Is there any way to arrange for followup comments (I suppose, the contents of my account inbox) to be emailed to me?

Not that I know of, I'm afraid. There are lots of requested features that we would implement if we had the programmatic resources, but alas, we don't. One just has to check if the envelope is red once in a while.


I still haven't decided how I feel about it

I call that a win for literature.


I've read some posts here that I thought had really awful attitudes about sexuality and BDSM in particular

?? Not any of mine, I hope.
EDIT: I see, Phil Goetz on masochism. Well, I downvoted it. Not much else to say, aside from noting that it had net 4 points and that karma rules do make it easier to upvote than downvote.
This is a community blog and I think it's pretty fair to say that what has not been voted high or promoted ought not to be blamed on "Less Wrong".



I'm somewhat sympathetic to that idea (I haven't felt guilty about being straightish, but I've wished I were more bisexual once in a while, and succeeded in pushing myself in that direction in some cases), but I'm curious now: is gender the only dimension you'd apply that to? Would you also take a pill (again assuming it's really really safe) that would make all outward physical attributes irrelevant to how attractive you find someone? Would you take a pill that would make you enjoy every non-harmful sexual practice/fetish (not necessarily seeking them out, but able to enjoy it if a partner initiated it)?
(I originally started writing this comment thinking something like "hmm, I'd take the bi-pill, but let's take that reasoning to its vaguely-logical conclusion and see if it's still palatable", but now I'm actually thinking I'd probably take both of those pills too.)

Well, to ask the non-mainstream-relative-to-this-community version of the question, ask "Would I take the loli pill?"

So I wouldn't miss out on half the fun.


I'm sure you know this, but I don't think it makes any sense to think you should enjoy X.

Why doesn't it make sense? If there were a pill to turn me bisexual, I'd take it, modulo the fact that in general I take almost no pills (it'd have to be really really safe, but I hold all mind-affecting substances to that standard, don't drink etcetera, it's not a special case for the bisexuality pill).


I honestly wish I never saw the damn thing.

This sounds like you're a bit too scared that it has an "unnatural" explanation. If it did happen, there's a normal explanation for it. Curious, yes, scared, no.

It's not a good litmus test until you also point to what you consider the best honest skeptical response - albeit this is often damned hard to do with poor skepticism, cryonics being exhibit A in point.

Nothing that travels from one star to another has cause to be scared of us. If they're worried about future war, they'd just wipe us out, and in any case wouldn't do fancy acrobatics with their exterior lights on.

Perhaps, but to the same extent, we should discount reporters' accounts as informative or worthy of being taken as serious arguments. In other words, you want to play-a the grownup game, you play-a by the grownup rules; if your editor says you can't, too bad, go sit at the kids' table.

Heh. My tribal beliefs are from reading Spider Robinson books as a teen. Ciphergoth is an example of the sort of person I grew up thinking of as normal, and I've always felt a little guilty about not being bisexual. You have to get up pretty early in the morning to go outside that mainstream, which is one reason I went to the lengths of postulating legalized rape in Three Worlds Collide.


Mercier and Sperber argue that, when you look at research that studies people in the appropriate settings, we turn out to be in fact quite good at reasoning when we are in the process of arguing; specifically, we demonstrate skill at producing arguments and at evaluating others' arguments.

I observed some time ago that Roger Penrose seemed to be a much better explainer of physics when he was using it to argue something (even though the conclusion was completely bogus) than people who graciously write textbooks that will be required reading for the students who have to buy it.
If you want good textbooks, make sure the author is trying to persuade the students of something, I'd say. I usually am.

Looks fine to me.


The fifth fact is a consequence of the previous ones.

Um, no. Again, I think you may have misunderstood that point there. The point is not that all Countesses can inevitably and inescapably be blackmailed. It is just that a Countess designed a particular way can be blackmailed. The notion of a superior epistemic vantage point is not that there is some way for the Baron to always get it, but that if the Baron happens to have it, the Baron wins.

Could the countess plausibly raise herself to a superior epistemic vantage over the baron, and get out from under his thumb? Alas no.

Again, this just wasn't a conclusion of the workshop. A certain fixed equation occupies a lower epistemic vantage. Nothing was said about being unable to raise yourself up.

Alas no. Once the countess allows herself to use tactics conditional on the baron's actions, the whole set-up falls apart: the two start modelling the other's actions based on their own actions which are based on the other's actions, and so on. The baron can no longer assume that the countess has no influence on his decision, as now she does, so the loop never terminated.

Or the Countess just decides not to pay, unconditional on anything the Baron does. Also, if the Baron ends up in an infinite loop or failing to resolve the way the Baron wants to, that is not really the Countess's problem.
As I did say at the decision workshop, the resolution that seems most likely is "respond to offers, not to threats".

Heh. I got this right originally, then reread it just recently while working on the book, saw what I thought was an error (1 numeral? just one second? why?) and "fixed" it.

This is a good and important article, but it would be improved by having a point. Examples of points are "Behold this compactly stated surprising truth I have now demonstrated" or "And that's why you should do X" or "Thus the common belief Y is dumber than a sack of bricks."

What is the point of this post?

Wedifred's remarks above seem obvious to me. Furthermore, your reply seems to consist of "for some reason a group cannot solve a coordination problem rationally, but if I suppose that they are allowed to take the same action that a rational group would perform for irrational reasons only, then the irrational group wins".

I've got a weaker form of this, but I manage. The number one thing that seems to work is a tight feedback loop (as in daily) between action and reward, preferably reward by other people. That's how I was able to do OBLW. Right now I'm trying to get up to a reasonable speed on the book, and seem to be slowly ramping up.

Um... I think it's a worthwhile point, at this juncture, to observe that Turing machines are humanly comprehensible and lambda calculus is not.
EDIT: It's interesting how many replies seem to understand lambda calculus better than they understand ordinary mortals. Take anyone who's not a mathematician or a computer programmer. Try to explain Turing machines, using examples and diagrams. Then try to explain lambda calculus, using examples and diagrams. You will very rapidly discover what I mean.

Richard Dawkins talking to an astrologer. Best part at 10m28s.

Transcript:
--
Dawkins: We could devise a little experiment where we take your forecasts and then give some of them straight, give some of them randomized, sometimes give Virgo the Pisces forecast et cetera. And then ask people how accurate they were.
Astrologer: Yes, that would be a perverse thing to do, wouldn't it.
Dawkins: It would be - yes, but I mean wouldn't that be a good test?
Astrologer: A test of what?
Dawkins: Well, how accurate you are.
Astrologer: I think that your intention there is mischief, and I'd think what you'd then get back is mischief.
Dawkins: Well my intention would not be mischief, my intention would be experimental test. A scientific test. But even if it was mischief, how could that possibly influence it?
Astrologer: (Pause.) I think it does influence it. I think whenever you do things with astrology, intentions are strong.
Dawkins: I'd have thought you'd be eager.
Astrologer: (Laughs.)
Dawkins: The fact that you're not makes me think you don't really in your heart of hearts believe it. I don't think you really are prepared to put your reputation on the line.
Astrologer: I just don't believe in the experiment, Richard, it's that simple.
Dawkins: Well you're in a kind of no-lose situation then, aren't you.
Astrologer: I hope so.
--

I dispute that this is a non-explanation. Besides referring to concepts whose existence has already been confirmed by other means, it makes a testable prediction about the degree to which abilities should run in genetic families as opposed to student lineages.

Okay, rephrase: Suppose I pull a crazy idea out of my hat and scream "I am 100% confident that every human being on earth will grow a tail in the next five minutes!" Then I am making a very forceful claim, which is not well-supported by the evidence.
The idea is that the outside view generally makes less forceful claims than the inside view - allowing for a wider range of possible outcomes, not being very detailed or precise or claiming a great deal of confidence. If we were to take both outside view and inside view perfectly at face value, giving them equal credence, the sum of the outside view and the inside view would be mostly the inside view. So saying that the sum of the outside view and the inside view equals mostly the outside view must imply that we think the inside view is not to be trusted in the strength it says its claims should have, which is indeed the argument being made.

Weakness as in the force of the claim, not how well-supported the claim may be.

I can't imagine what evidence you think there is for your claim "in most cases, the outside view gives a much weaker prediction."

And the Earth is slowly curving in its orbit, generating an apparent centrifugal force that decreases your weight at midnight, and increases your weight at noon. Except for a very tiny tidal correction, these two forces exactly cancel which is why the Earth stays in orbit in the first place. This argument would only be valid if the Earth were suspended motionless on two giant poles running through the axis or something.

So rot13?

Quick look didn't find it, but I don't see why this follows (and at a wild guess, I'm guessing it doesn't). Can you link?

Ah, but... what are the odds that A HIJACKER WOULD FLY IN BUSINESS CLASS??!?

I hope that's not a spoiler, because I haven't read that story. If it is, please delete it or ROT13 it right now and don't do it again.


I haven't taken this position just to be difficult. To look around, the world does appear to be flat, so I think it is incumbent on others to prove decisively that it isn't. And I don't think that burden of proof has been met yet.

-- Daniel Shenton, President of the Flat Earth Society as of 2010

...I don't suppose you can tell us what? I expect that if you could, you would have said, but thought I'd ask. It's difficult to work with this little.
I could toss around advices like "A lot of Major Life Decisions consist of deciding which of two high standards you should hold yourself to" but it's just a shot in the dark at this point.

Where do you get that I thought I was wrong about CR? I'd like to lose weight but I had been aware for a while that the state of evidence on caloric restriction doing the purported job of extending lifespan in mammals was bad.

It would have been good to check this suggested post topic on an Open Thread, first - in fact I should get around to editing the FAQ to suggest this for first posts.

Perhaps the AI rules wisely and well, and can give us anything we want, "save relevance".

In addition to the retreads that others have pointed out on the upload safety issue, this is a retread of the Fun Theory Sequence:
http://lesswrong.com/lw/xy/the_fun_theory_sequence/
Also the way you phrased the above suggests that we build some kind of AI and then discover what we've built. The space of mind designs is very large. If we know what we're doing, we reach in and get whatever we specify, including an AI that need not steal our relevance (see Fun Theory above). If whoever first reaches in and pulls out a self-improving AI doesn't know what they're doing, we all die. That is why SIAI and FHI agree on at least wistfully wishing that uploads would come first, not the relevance thing. This part hasn't really been organized into proper sequences on Less Wrong, but see Fake Fake Utility Functions, the Metaethics sequence, and the ai / fai tags.

Someone believes that the Singularity is a sack of absurdities and is sad about the fact that we're all deluded into believing it. To help pry us loose from our little cult, they post an argument that's really persuasive on a gut level: A photoshopped picture of me killing Meredith Kercher. (Shortly after, I am arrested by the Italian police.)
Is there a moral difference between that and what you're proposing besides, "Oh, but they're wrong and we're right?" If so, what do you think it is?

I'd actually say, start with the prior and with the strongest piece of evidence you think you have. This of itself should reveal something interesting and disputable.

Probably a wash if everyone does it, but might give a selective advantage to rationalists if practiced by rationalists only and the practice didn't spread beyond that... which seems unlikely in the long run, but not too impossible in the short run.

This would make the IRS sad if they found out. You wouldn't like them when they're sad.

I suspect that this reasoning is not only reasonably defensible, but also much more palatable; that the underlying biases are tested much less strongly by the policy conclusion "give the bulk of your money to one charity" than "give nothing to other charities". I will try to remember to use this version henceforth.

Well, the main thing that'd cause me to mistrust your judgment there, as phrased, is A8. Pre-9/11, airlines had an explicit policy of not resisting hijackers, even ones armed only with boxcutters, because they thought they could minimize casualties that way. So taking over an airplane using boxcutters pre-9/11 is perfectly normal and expected and non-anomalous; and if someone takes exception to that event, it probably implies that in general their anomaly-detectors are tuned too high.
I also suspect that some of these questions are phrased a bit promptingly, and I would ask others, like, "Do you think that malice is a more likely explanation than stupidity for the level of incompetence displayed during Hurricane Katrina? What was to be gained politically from that? Was that level of incompetence more or less than the level of hypothesized government incompetence that you think is anomalous with respect to 9/11?" and so on.


if X is such a great option, why is it not more popular?

I begin to suspect that rationalists should simply delete this question from their mental vocabularies. Most popular things are optimized to be popular with an audience that doesn't know how to resist manipulation (but thinks itself invincible, in accordance with the bias blind-spot bias); this gives rise to a case of the majority is always wrong.

Wish I could upvote you twice!

PayPal not working for SIAI specifically? If so, please specify (private email if you like).

I've done a certain amount of thinking on this particular topic, so if anyone is really genuinely truly going to do a startup around it, please call me first.

I usually try not to push people on this particular point unless I think they're already very high-level; my default assumption is that people are very akrasic and fragile when it comes to charity.
However, I'm raising my estimate of Landsburg's level based on this - I guess one mostly hears about the disputable points he got wrong, not the indisputable points he got right, of which this is one (and a rarely appreciated one at that).

Closest I know is "tu quoque".


Incidentally, there's already quite a karma-market in discussing karma. You could easily generate most of your karma discussing karma.

Upvoted.

In which case there's some specific amount of distinguishing evidence that promotes the hypothesis over the less complicated one, in which case, I suppose, the other would acquire this "burden of proof" of which you speak?

Eliezer and SteveLandsburg agree: don't diversify your (altruistic) giving.


I think I'm missing something here...

Er... a sense of humor? I regret only that I didn't get to see the look on her face in person, but I was kind of hoping for an AAAAAAAHHH in reply.

Well, the blatant version would be to take 5 possible control variables and try all 32 possible omissions and inclusions to see if any of the combinations turns up "statistically significant". This might look a little suspicious if you collected the data and then threw some of it away. If you were running regressions on an existing database with lots of potential control variables, why, they'll just have to trust that you never secretly picked and chose.
Someone who did that might not be able to convince themselves they weren't cheating... but someone who, somehow or other, got an idea of which variables would be most convenient to control for, might well find themselves influenced just a bit in that direction.

Hey, Alicorn, look what I found!
http://scienceblogs.com/casaubonsbook/2010/02/why_bunnies_are_cuter_than_bab.php

This is an excellently put objection - putting it this way makes it clear just how strong the objection is. The likelihood ratio to me sounds like it should be more or less T/F, where for the sake of conservatism T might equal .99 and F might equal .01. If we knew for a fact that there were aliens in our radio range, wouldn't this item of evidence wash out any priors we had about them eating stars? We don't see the stars being eaten!

Why isn't the obvious outside view to draw a line showing the increased peacefulness of contacts with the increasing technological development of the parties involved, and extrapolate to super-peaceful aliens? Isn't this more or less exactly why you argue that AIs will inevitably trade with us? Why extrapolate for AIs but not for aliens?
To be clear on this, I don't simply distrust the advice of an obvious outside view, I think that in cases like these, people perform a selective search for a reference class that supports a foregone conclusion (and then cry "Outside View!"). This foregone conclusion is based on inside viewing in the best case; in the worst case it is based entirely on motivated cognition or wishful thinking. Thus, to cry "Outside View!" is just to conceal the potentially very flawed thinking that went into the choice of reference class.

Forbidding "reliance" on pain of loss of prestige isn't all that much better than forbidding "exploration" on pain of loss of prestige. People are allowed to talk about my arguments but of course not take them seriously? Whereas it's perfectly okay to rely on your "outside view" estimate? I don't think the quoted paragraph is one I can let stand no matter how you reframe it...

It says nothing about my opinion of men (I think) - it just signifies to me that the person so profoundly does not even care. I don't want to be talked about without being considered. This is probably more of a pet peeve for me than for others. It would still be annoying even if it never happened to anyone else.

What did the person who mistook me for a woman not care about with respect to me? What were they not considering about me that constitutes disrespect to me? If it's not an annoying social background assumption then I genuinely don't understand what's so terrible about this.

Um... no they're not? Refuting a specific point is not the same as trying to halt a debate and stop at the current answer.

Posting 2/3 of a sequence and stopping is fine if people turn out not to be interested. I recommend fast posting and fast feedback.

"Alicorn" sounds much more feminine than either "Unicorn" or "Aliborn".

Agreed that this is the conflict of two inside views, not an inside view versus an outside view. You could as easily argue that most stars don't seem to have been eaten, therefore, the outside view suggests that any aliens within radio range are environmentalists. And certainly Robin is judging one view right and the other wrong using an inside view, not an outside view.
I simply don't see the justification for claiming the power and glory of the Outside View at all in cases like this, let alone claiming that there exists a unique obvious reference class and you have it.

http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate

I've got no problem with it phrased that way. To be clear, the part that struck me as unfair was this:

Excess inside viewing usually continues even after folks are warned that outside viewing works better; after all, inside viewing better show offs inside knowledge and abilities. People usually justify this via reasons why the current case is exceptional. (Remember how all the old rules didn't apply to the new dotcom economy?) So expect to hear excuses why the next singularity is also an exception where outside view estimates are misleading. Let's keep an open mind, but a wary open mind.


No clue hath I, though your suggestion seems plausible enough.

I've gotten 'she' from 'Eliezer Yudkowsky' no less.
Interestingly, over the course of some time monitoring blog trackbacks for Overcoming Bias, I never saw Robin Hanson mistaken for a female Robin.
So... um... I realize that this isn't really what the whole point is about at all, but I didn't feel particularly insulted to be called a girl; what does it say about your opinion of men that you're insulted to be mistaken for male? :)
(And yes, I know, it probably wouldn't be annoying if it was only happening to you personally and no one else, it's the background social assumptions that are annoying.)


I don't see how I can be described as trying to halt the conversation.

Allow me to disclaim that you usually don't. But that particular referenced post did, and taw tried it even more blatantly - to label further conversation as suspect, ill-advised, and evidence of morally nonvirtuous (giving in to pride and the temptation to show off) "inside viewing". I was frustrated with this at the time, but had too many higher-priority things to say before I could get around to describing exactly what frustrated me about it.
It's also not clear to me how you think someone should be allowed to proceed from the point where you say "My abstractions are closer to the surface than yours, so my reference class is better", or if you think you just win outright at that point. I tend to think that it's still a pretty good idea to list out the underlying events being used as alleged evidence, stripped of labels and presented as naked facts, and see how much they seem to tell us about the future event at hand, once the covering labels are gone. I think that under these circumstances the force of implication from agriculture to self-improving AI tends to sound pretty weak.

Many-worlds has made steady progress since it was invented. Especially early on, trying to bring in diversity would get you some many-worlds proponents rather than none, and their views would tend to spread.

Think of how much more progress could have been made if the early many-worlds proponents had gotten together and formed a private colloquium of the sane, providing only that they had access to the same amount of per capita grant funding (this latter point being not about a need for diversity but a need to pander to gatekeepers).

I don't mean "downvoted negative" just "downvoted relative to other posters".

I'd say damn the torpedoes, full speed ahead. If people are annoyed, let them downvote. If posts start getting downvoted, slow down.
Your posts have generally been voted up. If now is the golden moment of time where you can get everything said, then for the love of Cthulhu, say it now!

http://www.guardian.co.uk/global/2010/feb/23/flat-earth-society
Yeah, so... I'm betting if we could hook this guy up to a perfect lie detector, it would turn out to be a conscious scam. Or am I still underestimating human insanity by that much?

(It's foxes who know many things and do better; the hedgehog knows one big thing.)
I haven't read Tetlock's book yet. I'm certainly not surprised to hear that foreign affairs "experts" are full of crap on average; their incentives are dreadful. I'm much more surprised to hear that situations like the instability of the Soviet Union could be described and successfully predicted by simple linear models, and I'm extremely suspicious if the linear models were constructed in retrospect. Wasn't this more like the kind of model-based forecasting that was actually done in advance?
Conversely if the result is just that hedgehogs did worse than foxes, I'm not surprised because hedgehogs have worse incentives - internal incentives, that is, there are no external incentives AFAICT.
I have read Dawes on medical experts being beaten by improper linear models (i.e., linear models with made-up -1-or-1 weights and normalized inputs, if I understand correctly) whose factors are the judgments of the same experts on the facets of the problem. This ought to count as the triumph or failure of something but it's not quite isomorphic to outside view versus inside view.


The assertion to stop updating based on new data (ignore the inside view!) is just plain wrong.

I'd like to be able to say that, but there actually is research showing how human beings get more optimistic about their Christmas shopping estimates as they try to visualize the details of when, where, and how.
Your statement is certainly true of an ideal rational agent, but it may not be carried in human practice.

Large communities don't constitute help or progress on the "beyond the realm of feedback" problem. In the absence of feedback, how is a community supposed to know when one of its members has made progress? Even with feedback we have cases like psychotherapy and dietary science where experimental results are simply ignored. Look at the case of physics and many-worlds. What has "diversity" done for the Singularity so far? Kurzweil has gotten more people talking about "the Singularity" - and lo, the average wit of the majority hath fallen. If anything, trying to throw a large community at the problem just guarantees that you get the average result of failure, rather than being able to notice one of the rare individuals or minority communities that can make progress using lower amounts of evidence.
I may even go so far as to call "applause light" or "unrelated charge of positive affect" on the invocation of a "diverse community" here, because of the degree to which the solution fails to address the problem.

Agreed.

I honestly don't understand your objection. "Eventually" means "sometime between tomorrow and X years from now" where my probability distribution over X peaks in the 20-40 range and then starts dropping off but with a long tail because hey, gotta widen those confidence intervals.
If I knew for an absolute fact that nothing was going to happen for the next 100 years, it would still be a pretty damned urgent problem, you wouldn't want to just let things slide until we ended up in a position as awful as the one we probably occupy in real life.
I still feel shocked when I read something like this and remember how short people's time horizons are, how they live in a world that is so much tinier than known space and time, a world without a history or a future or an intergalactic civilization that bottlenecks through it. Human civilization has been around for thousands of years. Anything within the next century constitutes the last minutes of the endgame.

But the original problem was that, in order to carry an argument about the "outside view" in case of what (arguendo) was so drastic a break with the past as to have only two possible analogues if that, data was being presented about students guessing their homework times. If instead you gather data about predictions made in cases of, say, the industrial revolution or the invention of the printing press, you're still building a certain amount of your conclusion into your choice of data.
What I would expect to find is that predictive accuracy falls off with dissimilarity and attempted jumps across causal structural gaps, that the outside view wielded with reasonable competence becomes steadily less competitive with the Weak Inside View wielded with reasonable competence. And this could perhaps be extrapolated across larger putative differences, so as to say, "if the difference is this large, this is what will happen".
But I would also worry that outside-view advocates would take the best predictions and reinterpret them as predictions the outside view "could" have made (by post-hoc choice of reference class) or because the successful predictor referenced historical cases in making their argument (which they easily could have done on the basis of having a particular inside view that caused them to argue for that conclusion), and comparing this performance to a lot of wacky prophets being viewed as "the average performance of the inside view" when a rationalist of the times would have been skeptical in advance even without benefit of hindsight. (And the corresponding wackos who tried to cite historical cases in support not being considered as average outside viewers - though it is true that if you're going crazy anyway, it's a bit easier to go crazy with new ideas than with historical precedents.)
And outside-view advocates could justly worry that by selecting famous historical cases, we are likely to be selecting anomalous breaks with the past that (arguendo) could not have been predicted in advance. Or that by filtering on "reasonably competent" predictions with benefit of hindsight, inside view advocates would simply pick one person out of a crowd who happened to get it right, while surely competent outside view advocates would all tend to give similar predictions, and so not be allowed a similar post-hoc selection bias.
Note: I predict that in historical disagreements, people appealing to historical similarities will not all give mostly similar predictions, any more than taw and Robin Hanson constructed the same reference class for the Singularity.
But then by looking for historical disagreements, you're already filtering out a very large class of cases in which the answer is too obvious for there to be famous disagreements about it.
I should like to have the methodology available for inspection, and both sides comparing what they say they expect in advance, so that it can be known what part of the evidential flow is agreed-upon divergence of predictions, and what part of the alleged evidence is based on one side saying that the other side isn't being honest about what their theory predicts.
It's not like we're going to find a large historical database of people explicitly self-identifying as outside view advocates or weak inside view advocates. Not in major historical disputes, as opposed to homework.

Ah, so you meant: No physically possible series of Bayesian updates can promote a hypothesis to prominence if its prior probability is that low. And Peter meant: It is decision-theoretically useless to include a subroutine for tracking probability increments of 1/3^^^^^3 in your algorithm.
But the non-Bayesian source of your Bayesian prior might output 1/3^^^^^3 as the prior probability of an event -- as surely for the coin flip example as for Robin Hanson's anthropic one.

To be precise, it's impossible to describe any sense event with a prior probability that low. You can describe hypotheses conditional on which a macro-event has a probability that low. For example, conditional on the hypothesis that a coin is fixed to have a 1/3^^^3 probability of coming up heads, the probability of seeing heads is 1/3^^^3. But barring the specific and single case of Hanson's hypothesized anthropic penalty being rational, I know of no way to describe, in words, any hypothesis which could justly be assigned so low a prior probability as 1/3^^^3. Including the hypothesis that purple is falling upstairs, that my socks are white and not white, or that 2 + 2 = 5 is a consistent theorem of Peano arithmetic.

Confirmed.

Coin's fixed.

I see. Well, I don't object to the labels that I see. But you're allowing anyone to edit the pitch list. What happens in case of an edit war?

You can't imagine anything that improbable. Unless we adopt Robin's anthropic penalty, in which case "I am in a unique position to affect 3^^^^^3 other people" is that improbable.

Well, but she wouldn't outright lie, would she?

Thank you for the references, upvoted. But it's not clear to me that "finding babies uncute" has actually been linked to psychopathy per se, albeit it might be something interesting to investigate because of a couple of chained correlations. In fact the term "fairly strong evidence" in the original comment does seem misplaced, unless you know of a specific experiment indicating that.
(Also, would "fairly strong evidence" in this context mean say "a likelihood factor of ten for finding babies uncute, even though the base frequency of psychopaths is low" or "a substantial fraction of people who find babies uncute are in fact psychopaths"?)

I initially commented to the above effect that it was just a random brain-bleep and I did not remember your True Name if indeed I had ever been told it, but then deleted the comment, since if I had known your name to be Allison and genuinely slipped up, I would want to be the sort of person who simply wouldn't say whether or not it was a revealing slip-up, one way or the other, so as to maintain Plausible Deniability. To put it another way, if it had been your real name, I would want to be able to truthfully say, "Whether it was her real name or just a brain-cache substitution, I would not confirm or deny it one way or the other, so you cannot take any evidence from the fact that I am being apparently evasive." This requires that I say the same thing whether your name is Allison or not, since otherwise people can take Bayesian evidence from it. However since in this case you have already commented to this effect, I suppose I might as well confirm it.
I did once know an Allison and my brain seems to repeatedly substitute that name for yours. I usually catch it before commenting, but not this time. There are other bizarre things my brain does along the same lines, for example, I simply cannot remember, even after having been told a dozen times or more, whether Peter Thiel's last name is pronounced Thee-el or Tee-el.

Also, Alicorn's image found on a Google search is the cutest image on the top of TheCutest.Info. No matter how she found the image to begin with, this seems like highly relevant data! Even a search procedure that seems fair can manage to turn up an unfair point of comparison.
Albeit some of the other images in the top 40 seemed far cuter than that to me - cuter than babies. Maybe I just don't like bunnies? How could evolutionary psychology explain that?

Very compactly put. The data simply do not contradict the theory in the first place.

It might be an awful experiment to perform, but if we can find a parent with a newborn child and sufficient self-honesty to be trustworthy, we can ask them whether or not, in all honesty, their own baby is cuter than those images, which were cute enough to make my head explode into candy.
If a trustworthy self-honest rationalist parent looks at that and says "yes, my baby is cuter"... I'd have to say that explains a lot about parents and a lot about the continued survival of the human species.

Citation needed.

The site includes the cutest images. The cuteness response can be set off strongly by a cute creature associating with human stuff or (just a few of them) seeming to do a distinctively human gesture. Any theories about what's going on there?

What sort of nurturing behavior do you feel compelled to exhibit toward paperclips? Now I'm curious.

How does the same cuckoo manage to be attractive to so many host birds?

Yes, and of all the places not to get that...


But if you were walking in a forest and just happened to find a baby. If you didn't know it was a human baby, with various obligations and long-term ties, wouldn't you want to pick it up and snuggle it? Or not?

Unless the baby is likely to be a relative, isn't this actually vastly less adaptive behavior than picking up a cute bunny rabbit that you can eat later in times of famine?

Actually, it makes perfect sense for sexual selection on sexual-attractiveness-features to be subject to far greater selection pressure and fine-tuning than baby-cuteness.
I'll make a testable prediction here: Cases of parental superstimulus (like baby ducks following a stick figure, infant monkeys getting attached to puppets, etc., if I'm remembering correctly) ought to be far more common / easier to fake than sexual superstimulus. I'll limit the key part of the prediction to complex vertebrates so that they have large enough brains to be complicated, but I wouldn't be surprised to find the rule more universal than that.

Male tentacle monsters perceive Japanese schoolgirls as a superstimulus relative to female tentacle monsters. It probably has something to do with the tie on the sailor uniforms.

Someone claims the Singularity is a religious, theistic persuasion pattern that offers its believers a happy afterlife while others are left in the cold - to give an example of a typical and common accusation that people just make up, not based on any evidence, but because their brain completes the pattern for what they expect.
Do I get to defend myself? How?

Seconded. Why should this be the case?
As a counterexample, some people find rabbits cuter than babies.

1) The baby is far cuter than the rabbit.
2) There's nothing wrong with a stimulus having a superstimulus.

Pretty darned high, because at this point we already know that the world doesn't work the way we think it did.

Perhaps you have not realized how this blows up your whole site.
At present it is a neutral record of expert opinions. Who said what. On the record.
To label something as "woo" - or even the far more innocuous "persuasion pattern" - is not a neutral act. People disagree about what is woo or not woo. They disagree about whether persuasion patterns are being used. They disagree about what constitutes a logical fallacy, both in general and in the specific.
If someone claims that the Singularity is religious woo, do I get a chance to defend myself? How?
You have just taken a giant step from recording expert opinions to trying and provide a way for your audience to accuse and counteraccuse experts of being biased. This is not a trivial step. And allowing people to label things as "woo" does not seem like the best first step.

"Fallacy" and "Rhetoric" are both more neutral and broader than "woo". I'd even put a label on that says "Alleged fallacy" rather than "Fallacy". Otherwise it's simply a matter of accusing people with no way to defend themselves.
This sort of thing broadens out very quickly into Issuepedia 2.0, if you allow replies, say. It would make more sense to think about it carefully than to do it ad-hoc (although not, of course, to try to implement all features at once).


what we do is simply calculate P(E|~H) (techniques for doing this being of course the principal concern of statistics texts),

No no no. That would be a hundred times saner than frequentism. What you actually do is take the real data e-12 and put it into a giant bin E that also contains e-1, e-3, and whatever else you can make up a plausible excuse to include or exclude, and then you calculate P(E|~H). This is one of the key points of flexibility that enables frequentists to get whatever answer they like, the other being the choice of control variables in multivariate analyses.
See e.g. this part of the article:

The authors used what's called a Mann-Whitney U test, which, in simplified terms, aims to determine if two sets of data come from different distributions. The essential thing to know about this test is that it doesn't depend on the actual data except insofar as those data determine the ranks of the data points when the two data sets are combined. That is, it throws away most of the data, in the sense that data sets that generate the same ranking are equivalent under the test.


I invented it sometime around the dawn of time, don't know if Marcello did in advance or not.
Actually, I don't know if I could have claimed to invent it, there may be science fiction priors.

Does that actually attempt to halt further conversation though?

Where does the "guidance" come from? You can't cite "guidance" as evidence against the proposition that dietary scientists were making stuff up.

I do think exponential parallelism is a good description of QC, because any adequate causal model of a quantum computation will invoke an exponential number of nodes in the explanation of the computation's output. Even if we can't always take full advantage of the exponential number of calculations being performed, because of the readout problem, it is nonetheless only possible to explain quantum readouts in general by postulating that an exponential number of parallel calculations went on behind the scenes.
Here, of course, "causal model" is to be taken in the technical Pearl sense of the term, a directed acyclic graph of nodes each of whose values can be computed from its parent nodes plus a background factor of uncertainty that is uncorrelated to any other source of uncertainty, etc. I specify this to cut off any attempt to say something like "well, but those other worlds don't exist until you measure them". Any formal causal model that explains the quantum computation's output will need an exponential number of nodes, since those nodes have real, causal effects on the final probability distribution over outputs.

You understand this is more or less exactly the problem that Less Wrong was designed to solve.

I second the question. "Elements of Statistical Learning" is Bayes-aware though not Bayesian, and quite good, but that's statistical learning which isn't the same thing at all.

Could anyone recommend an introductory or intermediate text on probability and statistics that takes a Bayesian approach from the ground up? All of the big ones I've looked at seem to take an orthodox frequentist approach, aside from being intolerably boring.

I don't think you understand the degree to which people who don't want spoilers, don't want to hear them.

Voted up, but calling them "nerds" in reply is equally ad-hominem, ya know. Let's just say that they don't seem to have the very high skill level required to distinguish good unusual beliefs from bad unusual beliefs, yet. (Nor even the realization that this is a hard problem, yet.)
Yes, they're pretty softcore by LessWrongian standards but places like this are where advanced rationalists are recruited from, so if someone is making a sincere effort in the direction of Traditional Rationality, it's worthwhile trying to avoid offending them when they make probability-theoretic errors. Even if they mock you first.
Also, one person on RationalWiki saying silly things is not a good reason to launch an aggressive counterattack on a whole wiki containing many potential recruits.

Er, who's planning this? Is Michael Vassar in on it?

Please edit both of the above to avoid having your comments deleted. It's great that you have that opinion, but some people may not share it, and also there's this incredible amazing technology called rot13 which is really useful for having your cake and eating it too in the case of this conflict. And we can all consider that official LW policy from this point forward.


You don't want to rely on studies in medical journals because their conclusion-drawing methodologies are haphazard.

I dispute none of this, but so far as I can tell or guess, the main thing powering the superior statistical strength of PatientsLikeMe is the fact that medical researchers have learned to game the system and use complicated ad-hoc frequentist statistics to get whatever answer they want or think they ought to get, and PatientsLikeMe has some standard statistical techniques that they use every time.
Also, I presume, PatientsLikeMe is Bayesian or Bayes-like in that they take all available evidence into account and update incrementally, while every medical experiment is a whole new tiny little frequentist universe.
This is not really an article about PatientsLikeMe being strong, it is an article about the standard statistical methods of academic science being weak and stupid.

Have you ever done this? Example?

Please edit above to avoid spoilers.

http://lesswrong.com/lw/p1/initiation_ceremony/

When incomplete strangers - i.e. not literal complete strangers off the street, but people who've been introduced to me by a mutual friend without having any idea who I am - ask about my cryonics necklace, I often lead with, "It's my contract of immortality with the Cult of the Severed Head".
This seems to work fine.
In fact, I strongly suspect it works better than most other cryonics explainers, because I don't sound the tiniest bit nervous. It helps to understand that most people have no independent grasp on reality. I may post about this at some point.
In any case, this is successful countersignalling performed on very loose acquaintances.

She is here.
Who is absent?

Hikaru no Go, of course.

You've been in the transhumanist community for, what, at least 10 years?
I honestly have no clue what could possibly be going wrong in your mind at this moment. I do not understand what general category of malfunction corresponds to this particular mistake.

Well, okay, chanting, but let's leave the robes in the closet - so unflattering! And it's just such a pain to remember to get peroxide on the rabbit blood stains in time for them to come out clean.

Nowadays, no one does basic research.

"How much will it cost?" "How long will it take?" Who the hell is supposed to be able to answer that on a basic research problem?

BTW, I agree with this.

There are counterbalancing negative possibilities, you can't sum over just the positive ones. And since a prior probability distribution sums to 1, the contribution of even solely the positive possibilities must converge to a finite sum rather than diverging.

Doesn't look very information-theoretically hard to me. Partial preservation of function probably implies near-total preservation of information.

I'd ask Omega, "Which construal of volition are you using?"
There's light in us somewhere, a better world inside us somewhere, the question is how to let it out. It's probably more closely akin to the part of us that says "Wouldn't everyone getting their wishes really turn out to be awful?" than the part of us that thinks up cool wishes. And it may even be that Islamic fundamentalists just don't have any note of grace in them at all, that there is no better future written in them anywhere, that every reasonable construal of them ends up with an atheist who still wants others to burn in hell; and if so, the test I cited in the other comment, about filtering portions of the extrapolated volition that wouldn't respect the volition of another who unconditionally respected theirs, seems like it ought to filter that.


A way to choose what subset of humanity gets included in CEV that doesn't include too many superstitious/demented/vengeful/religious nutjobs and land those who implement it in infinite perfect hell.

What you're looking for is a way to construe the extrapolated volition that washes out superstition and dementation.
To the extent that vengefulness turns out to be a simple direct value that survives under many reasonable construals, it seems to me that one simple and morally elegant solution would be to filter, not the people, but the spread of their volitions, by the test, "Would your volition take into account the volition of a human who would unconditionally take into account yours?" This filters out extrapolations that end up perfectly selfish and those which end up with frozen values irrespective of what other people think - something of a hack, but it might be that many genuine reflective equilibria are just like that, and only a values-based decision can rule them out. The "unconditional" qualifier is meant to rule out TDT-like considerations, or they could just be ruled out by fiat, i.e., we want to test for cooperation in the Prisoner's Dilemma, not in the True Prisoner's Dilemma.

An AI that can solve philosophy problems that are beyond the ability of the designers to even conceive

It's possible that having a complete mind design on hand would mean that there were no philosophy problems left, since the resources that human minds have to solve philosophy problems are finite, and knowing the exact method to use to solve a philosophy problem usually makes solving it pretty straightforward (the limiting factor on philosophy problems is never computing power). The reason why I pick on this particular cited problem as problematic is that, as stated, it involves an inherent asymmetry between the problems you want the AI to solve and your own understanding of how to meta-approach those problems, which is indeed a difficult and dangerous sort of state.

All of the above working first time, without testing the entire superintelligence. (though you can test small subcomponents)

All approaches to superintelligence, without exception, have this problem. It is not quite as automatically lethal as it sounds (though it is certainly automatically lethal to all other parties' proposals for building superintelligence). You can build in test cases and warning criteria beforehand to your heart's content. You can detect incoherence and fail safely instead of doing something incoherent. You could, though it carries with its own set of dangers, build human checking into the system at various stages and with various degrees of information exposure. But it is the fundamental problem of superintelligence, not a problem of CEV.

And, to make it worse, if major political powers are involved, you have to solve the political problem of getting them to agree on how to skew the CEV towards a geopolitical-power-weighted set of volitions to extrapolate

I will not lend my skills to any such thing.

I certainly hope that Moore's Law will end, but I don't think we could get that lucky.


Intrinsically they aren't optimization processes but they seem computationally expressive enough for an optimization process to be implemented on them

But they aren't optimization processes. It doesn't matter if they could implement one, they don't. You might as well point to any X86 chip and ask why it doesn't RSI.

What do you think you know and how do you think you know it?

Not "and". "Or". If you don't already have it, then reading the sequences will give you a basic grounding in probability theory, decision theory, metaethics, philosophy of mind, philosophy of science, computer science, cognitive bias, evolutionary psychology, the theory of natural selection, artificial intelligence, existential risk, and quantum mechanics.

Why was this voted up to +4? Y'all are way too scared of being labeled cultish if you're voting this stuff up.
I really wish there was some way to teach arrogance. It seems to be such a large factor in whether people actually make progress as rationalists or not.

Sure, but the answer is very simple. Gene regulatory networks are not RSI because they are not optimization processes.

Be glad it's happening at all.

That is a truly beautiful story. I wonder how many places there are on Earth where people would appreciate this story.

That's the difference between the Simulation Argument and the Simulation Hypothesis. The Simulation Argument is "you must deny one of these three statements" and the Simulation Hypothesis is "the statement to be denied is 'I am not in a computer simulation'".

I'd guess CI + SA > Alcor > CI.

Agreed. "No peer-reviewed publications" is not an argument that I've ever used or would use, even in advance of the CRU emails, because of course that is how academia works in general.

AGW skeptics have often been challenged on the lack of peer reviewed papers in credible climate science journals supporting their arguments. Now it is quite possible that this is the case because skeptical papers have been rejected purely due to being bad science (as is the case with the lack of papers supporting the effectiveness of homeopathy in medical journals). However, the absence of papers from the key journals cannot be treated as independent evidence of the badness of the science if there is a concerted effort by AGW believers to keep such papers out of the journals.
It is legitimate to attack the science the AGW skeptics are doing. It is not legitimate to dismiss the science purely on the basis that they have not been published in peer reviewed journals if there is a concerted effort to keep them out of peer reviewed journals based on their conclusions rather than on their methods. Now I'm sure the AGW believers feel that they are rejecting bad science rather than rejecting conclusions they don't like but emails like the above certainly make it appear that it is the conclusions as much as the methods that they are actually objecting to.
In my opinion the CRU emails mean that it no longer appears justified to ignore claims by AGW skeptics purely because they have not appeared in a peer reviewed journal. They may still be wrong but there is sufficient evidence of biased selection by the journals to not trust that journal publication is an unbiased signal of scientific quality.

The book title had already occurred to me, but it shouldn't be the first book in the series.

Pointing people to Lanier as a naysayer isn't playing fair; it just makes the opposition look crazy.

I wonder if the distinction between self-modification and recursive self-improvement is one of those things that requires a magic gear to get, and otherwise can't be explained by any amount of effort.

I hate to sound complimentary, but... I get the impression that the comments on LW are substantially higher-quality than the comments on OB.
And that the comments on LW come from a smaller group of core readers as well, which is to some extent unfortunate.
I wonder if it's the karma system or the registration requirement that does it?

Of course. But it is logically rude to demand some knowably unobtainable even-if-they're-right proof instead, and then toss all the other arguments out the window.

Um... that's a rather odd argument to make, considering steel, wheels, nuclear power, transistors, radio, lasers, books, LEDs...
Proteins are held together by van der Waals forces, which are much weaker than covalent bonds. Preliminary calculations show gargantuan opportunities for improvement (see Drexler's Nanosystems).

http://money.cnn.com/2009/11/16/news/companies/berkshire_walmart/index.htm
Buffett increased his stake in Walmart in Nov 2009.

http://www.overcomingbias.com/2009/11/its-news-on-academia-not-climate.html

Yup, this behavior has long been typical when academics form competing groups, whether the public hears about such groups or not. If you knew how academia worked, this news would not surprise you nor change your opinions on global warming.

People are crazy, the world is mad. Of course there's gross misbehavior by climate scientists, just like the rest of academia is malfunctioning. But the amount of scrutiny leveled on climate science is vastly greater than the amount of scrutiny leveled on, say, the dietary scientists who randomly made up the idea that saturated fat was bad for you; and the scrutiny really hasn't turned up anything that bad, just typical behavior by "working" scientists. So I doubt that this is one of the cases where the academic field is just grossly entirely wrong.

We don't. I'm not sure what's up with that, unless it was a deliberately bad example.

The term "believe in" is used whenever someone else assigns a probability sufficiently higher than yours for you to take offense. For example, if someone assigns a negligible probability to hard takeoff and Robin Hanson assigns it a 10% probability, they will say that Robin Hanson believes in hard takeoff.

fixed

Um... am I missing something or did no one link to, ahem:
http://lesswrong.com/lw/q1/bells_theorem_no_epr_reality/

A question, at what probability level should you go around saying "I believe in X"?

I think I did specify that no one would die who would otherwise be immortal; eternal insanity or 3^^^3 years of insanity ought to be implicitly included, I'd think.

What if the TORTURE occurs during a random time in the next 3^^^3 seconds, not right at the beginning? Also, I think we definitely require a limit on sanity damage because otherwise the scenario is being tortured for 50 years and then spending the next 3^^^3 seconds being insane which Vastly outweighs the ordinary scenario of being tortured for 50 years.

I'm not sure that merited a top-level post, but it would merit a top-level comment in this thread.

If this is your state of knowledge then... how can I put this: it seems extremely likely that you'll start playing around with very simple tools, find out just how little they can do, and, if you're lucky, start reading up and rediscovering the world of AI.

In other words, "acyclic graphs" = graphs that are acyclic after the directionality of the arrows is forgotten, which is probably what he meant.

What a fine comment to be posted under a parent voted down to -5 where no one will ever see it.

Russia seems grossly incompetent compared to China. I don't know if the conquerees would be better off.


If you think of losing as "not winning," then when you try to work out why you've lost, or (God forbid) why you're a loser, you'll tend to focus on the things you didn't do and the qualities you don't have. So it goes with any "negative" concept, one that is defined by what it isn't (think of how "background" = "everything but the foreground" or how valleys are made by the mountains around them).
I think it's worthwhile to occasionally invert the picture, to see "being a winner" as "not being a loser." That way you attend to those habits of mind that are hurting you, instead of the ones that might be helping.

-- Jsomers.net, How to be a loser (Relevance)

The one who wins or loses is the one who makes the decision. You might as well say that if someone buys a quantum lottery ticket, the one who really wins is the future self who wins the lottery a few days later; but actually, the one who buys the lottery ticket loses.

So long as you make your Newcomb's choice for what seem like good reasons rather than by flipping a quantum coin, it is likely that very many of you will pick the same good reasons, and that Omega can easily achieve 99% or higher accuracy. I would expect almost no Eliezer Yudkowskys to two-box - if Robin Hanson is right about mangled worlds and there's a cutoff for worlds of very small amplitude, possibly none of me. Remember, quantum branching does not correspond to high-level decisionmaking.

This is actually a damned good question:
http://www.scientificblogging.com/mark_changizi/why_doesn%E2%80%99t_size_matter%E2%80%A6_brain

Currently planned to be divided into three parts, "Map and Territory", "How To Actually Change Your Mind", and "Mysterious Answers to Mysterious Questions" - that should give you an idea of the intended content. No ETA, still struggling to find a writing methodology that gets up to an acceptable writing speed.

I'd have to turn this over to Michael Vassar if you want details. He's the one who convinced me that the British used to be really good at this.
One key point is that it doesn't do you much good to be conquered by conquerors who are too squeamish to keep order. Remember when the Iraqis were wishing for Saddam back because he might have been utterly evil but at least his reign of terror kept peace in the streets? That's why I mentioned China. The old-time British would've been better, but you can bet China wouldn't tolerate warfare in cities they planned to go on milking. Life in China isn't perfect but it's a whole lot better than living in a failed state.
I seriously think that Earth would be better off if we got out of the way and let China conquer everything that isn't a democracy.

I said conquered, not trashed by a bunch of Westphalians who weren't planning on owning the place afterward.

I've also noticed "liberals" making more sense, but I attribute this to smart people abandoning conservative groups and jumping ship to liberal ones. This may mean that "conservative" policies are being under-argued.

The version of this that I would put forward seriously is that the Westphalian concept of inviolable national sovereignty is a convenience to the rich and complacent inhabitants of successful nations, but a huge detriment to the inhabitants of failed states, condemning them to endless slavery at the hands of incompetent dictators who need fear no invasion as they weaken and starve their captive countries. Africa might benefit enormously from being conquered by almost anyone, including China.

Then you're much more likely to be told this by Omega in the first place, for no better reason than that you were frightened enough to hand over the cash.

You don't care about $1 billion for tuberculosis control? TB may not be an existential risk, but it's still a really big, important problem, and it's one that could easily get a lot worse if universally antibiotic resistant strains start becoming common. If the Dark Lords of the Matrix offered me the choice between "tomorrow someone invents a fusion power plant that actually works, is easily built and maintained, and generates electricity energy at a lower cost than burning fossil fuels does" and "all TB bacteria spontaneously die", it's hard to say which would actually generate more utility.

I would take the TB cure but you are not thinking on the margins. People are already worried about TB, funding is already going there.

"Shut up"
I think a better question would be, what does this theory say about mass? As opposed to volume and distance? How can an object be equidistant between two other equally-sized objects and be attracted to one of them more than the other?
It fails even as a crank theory.

I don't. Karma is a proxy for whether the community wants to hear from you. You can predictably go against that - I sometimes do - but there should generally be a strong reason behind it. Karma is a proxy sign for whether you're being helpful, not an accumulated resource that can be burned.

Apparently the full quote from Richard Feynman is:
"I think I can safely say that nobody understands quantum mechanics."
This was in 1965. Everett's first paper was in 1957 IIRC. So not only was Feynman mistaken about nobody at that time understanding quantum mechanics, but he thought this could be said safely? When there are billions of people in the world, and all ignorance and confusion is a property of the map rather than the territory?
Feynman was one of the great Traditional Rationalists, but sometimes he really does manage to get it completely wrong. Einstein was much worse in the same department: "You do not really understand something unless you can explain it to your grandmother"!?

Well, first of all, I don't see how that's an example of scope insensitivity. Second, suppose a deadly flu virus starts sweeping a country. Getting upset and outraged at the existence of flu and human suffering is unlikely to change the universe's mind. On the other hand, an inefficient response to that and other problems, making them worse, is very much our own fault. So it looks to me like that is very much a defensible position.

Do consider not starting with the metaethics sequence...

Why's this on LW?

Nope, not even close, but Michael Vassar might be able to think of something else interesting to do with a billion dollars.


To me the scenario seems to be as simple as: If Omega predicts X, X will happen. If X wouldn't have happened, Omega wouldn't predict X.

Sounds like you might be having confusion resulting from circular mental causal models. You've got an arrow from Omega to X. Wrong direction. You want to reason, "If X is likely to happen, Omega will predict X."


Probably. But would the general public find IEC (or SIAI) compelling? I'm thinking not.

Then all versions of this project that I'm interested in won't work. Still seems worth a try.
I guess I'm astounded by the degree to which people seem to value "succeeding at what we set out to do" over "trying to do something important".

If this post doesn't get voted up and promoted, then please post "the next step in the conversation" as a comment here rather than its own post.

Omega doesn't have to simulate people. It just has to know. For example, I know that if Omega says to you "Please accept a million dollars" you'll take it. I didn't have to simulate you or Omega to know that.


The question "What do you do?" implies that the answer is not locked in. If a perfect predictor has made a prediction about what I will do, than the question "What do you do?" is nonsensical.

If you don't know what the prediction is, it's not nonsensical. You still have to decide what to do.
If you do know what the prediction is, then the way in which you react to that prediction determines which prediction you'll hear. For example, if I walk up to someone and say, "I'm good at predicting people in simple problems, I'm truthful, and I predict you'll give me $5," they won't give me anything. Since I know this, I won't make that prediction. If people did decide to give me $5 in this sort of situation, I might well go around making such predictions.
You seem to be confused about free will. Keep reading the Sequences and you won't be.

Please don't use such long titles in the future.
Since the title appears in the URL, it's not possible to change the URL without causing the post to reappear in RSS feeds. I'll let you know if this ever fixes in the codebase (a simple solution would be to truncate, from the RSS feed only, the part of the URL that appears after the unique identifier; but this would require a codefix, so I'll let you know if the maintainers get around to it, or if someone sets up LW on their linux machine and submits a codefix).

Yes, wait.

If this is where the putative money is going, I'm not interested in talking about it here.
Are there any charities out there which could productively use $10M to get us closer to liquid-fluoride thorium reactors, say? The IEC fusion folks could use $10M.
A list of the 100 most interesting projects with organizational frameworks already in existence that could start scaling up with $10M might be worth compiling in and of itself.

I would like to see this bundled with a Rational Charity meme. Let's be frank here: if this ends up going to the Society for Rare Diseases in Photogenic Puppies, it wasn't worth LW's time. If we can manage to get some money to things that actually matter, it was.
Trying to get something worthwhile done, as opposed to "making a billion dollars go to charity", might make the whole project fail because of that added extra inconvenience. So what?
If you wanted to boil it down to a meme, it would be "Do something effective for a change". Supposing you actually can generate a billion dollars, that's enough for ten million dollars for one hundred charities. "Ten million dollars apiece for one hundred unusual and effective charities." Like that.

I had this sitting in my drafts folder and noticed another long discussion about two-boxing versus one-boxing and realized that the next step in the conversation was similar to the point I was trying to make here.

...I'd like to see an example?

Yes, getting hugely tangled up in meta-level arguments instead of looking at the actual arguments and evidence and object-level way-the-world-is would indeed be a classic blunder.

Wha? No. But you'd have to offer me a moral reason, as opposed to an immoral one.

Initially voted down because I was sure it was going to be stupid, but this is the first crazy idea I've ever heard for generating a billion dollars out of nothing that could actually work. I mean, ever. You win some kind of award.

Reversing entropy is insufficient. You have to interact with a past that no longer has any traces in the present. It's not enough to have a way to turn steam into ice cubes. You need a time camera.

ii is a problem, iii fits my values but may violate other sentients' rights, and as for iv, I see no difference between the concepts of "computer program" and "universe" except that a computer program has an output.

:) Sorry.
In 2006, Craigslist's CEO Jim Buckmaster said that if enough users told them to "raise revenue and plow it into charity" that they would consider doing it. (source: http://blogs.zdnet.com/BTL/?p=4082 ) They really do listen to their users and the reason there is no advertising on Craigslist is that no one is asking for it.
A single banner ad on Craigslist would raise at least one billion for charity over five years. They could put a large "X" next to the ad, allowing you to permanently close it. There seems to be little objection to this idea. The optional banner is harmless, and a billion dollars could be enough to dramatically improve the lives of millions, save very real people from lifetimes of torture or slavery, or make a serious impact in the causes we take seriously around here. As a moral calculus, the decision is a no brainer. So we just need a critical mass of Craigslist users telling Jim that we need a banner ad on Craigslist. Per a somewhat recent email to Craig, they are still receptive to this idea if the users suggest it.
The numbers involved are a little insane. Fifty thousand people should count as critical mass, which means each person could effectively cause $20,000 to be generated out of nowhere and donated to charity. My mistake last time was doing it as a Facebook group rather than a Facebook fan page, where the more useful viral functions have moved. This time I would also drop the money on advertising to get an easy initial critical mass.

I have a need to interact with real people, not to think I'm interacting with real people.

Earendil wins so hard it makes my ears bleed.

Whoops, I just sent off a "you should join LW" message to Earendil on the board without noting that Earendil was the one who posted the link here!

Why is it a sin to deny a lot of evidence, but not a little evidence?

It's a start. If it became popular and scaled up, it would provide that dataset.

I think that you are interpreting your religious fellows with too much charity. Some of them might be like you. Others won't be, unless you're hanging out with an exclusively Unitarian crowd.
If you really want to see a straight-up case, http://lesswrong.com/lw/1lf/open_thread_january_2010/1es2

I didn't think it deserved a downvote, so I upvoted, FWIW.

I've posted a link to my blog at the moment; do you think it's better that the entire article be included here?

yep

false alarm, she's not signed up

Me too. This is not just about cryonics. It is not remotely just about cryonics. It is about the general quality of published argument that you can expect to find against a true contrarian idea, as opposed to a false contrarian idea.

Agreed.


the protagonist who's confident that they're going to hell so tries to postpone eternal suffering with cryonics.

I'm not sure, but I think I heard at least one story about someone who actually did this.

Don't you mean ~b -> necessarily ~q?
Also, for c, you must specify, "Is there pleasant life after death?"

Do you ever pray? Have you ever asked God, even in the silence of your heart, for something you truly wanted?
Have you ever looked at the experimental results for intercessory prayer? What do you expect them to show?
There ya go.


It's Gould's separate magisteria. Physical materialism rejects the separate magisteria, and I'm convinced that it is self-consistent in doing so. However, dualists do believe in the separate magisteria and you cannot try to interpret their beliefs in the context of monism -- it just comes out ridiculous.

It is not possible to interpret "separate magisteria" as different kinds of stuff, one "empirical" and one "non-empirical". What they are, rather, is different rules of thinking. For example, prayer can often help and never hurt in individual cases, but have no effect in the aggregate (e.g. when surveys are performed). There's no consistent model that has this attribute, but you can have a rule for thinking about this "separate magisterium" which says, "I'll say that it works and doesn't hurt in individual cases, but when someone tries to survey the aggregate, I won't expect positive experimental results, because it's not in the magisterium of things that get positive experimental results".
Mostly, "separate magisterium" is the classical Get-Out-Of-Jail-Free card. It can't be defined consistently. Mostly it means "Stop asking me those annoying questions!"
This division, needless to say, exists in the map, not in the territory.

Average utilitarianism over the entirety of Reality looks mostly like aggregative utilitarianism locally.


Interactions between the magisteria are contradictions for you, not necessarily to a dualist who believes it all works out, somehow.

Contradictions are contradictions. If, in general, the magisteria don't interact, but in some specific case, they do interact, that's a contradiction. It's a model that doesn't meet the axioms. That is a matter of logic. You can say "The dualist asserts that no interaction is taking place", but you can't say, "for the dualist, that is not a contradiction".


Think of the relation between the magisteria as a one-way relationship. The supernatural can affect the natural but there is no way to move backwards into the supernatural.

Then the natural can perceive the supernatural but not vice versa. To perceive something is to be affected by it.
The real problem with those who go on about separate magisteria is that they are emitting words that sound impressive to them and that associate vaguely to some sort of even vaguer intuition, but they are not doing anything that would translate into thinking, let alone coherent thinking.
I'm sorry to be brutal about this, but nothing I have ever heard anyone say about "separate magisteria" has ever been conceptually coherent let alone consistent.
There's just one magisterium, it's called reality; and whatever is, is real. It's a silly concept. It cannot be salvaged. Kill it with fire.

Um... if a rock was capable of fulfilling my every need, including a need for interaction with real people, I'd probably spend a lot of time around that rock.

It's at times like this that I wish Less Wrong gave out a limited number of Mega Upvotes so I could upvote this 10 points instead of just 1.


For instance, I have heard things along the following lines: "I hope my son gets better." "Well, that's not in your hands, that's in God's hands." All this said quite matter-of-factly.

Think of the relation between the magisteria as a one-way relationship. The supernatural can affect the natural but there is no way to move backwards into the supernatural.
This is flat wrong and doesn't accurately describe the theology/cosmology of most theists, but it helps when using the concept of magisteria. Personally, I don't think the term magisteria is completely useful in this context.
There is a deep problem behind all of these things where one layer or set of beliefs trumps another. In a framework of map/territory beliefs this makes little sense. It certainly doesn't translate well when talking to someone who doesn't adhere to a map/territory framework.
An example: If you asked the person why God didn't make your son get better you will get a bazillion answers. Likewise, if you asked about taking your son to the hospital they will tell you that you should. These two beliefs aren't in conflict in their system.
I have watched an entire congregation pray for someone who had cancer. They earnestly believed that their prayer was having some effect but if you asked for particulars you will get the bazillion answers. These people are not trying to explain away a future answer. They have seen what appears to be a bazillion different endgames for the scenario they are now in. That, mixed in with the crazy amount of factions within Christian theological circles, isn't going to make sense with a map/territory framework. But they aren't using that framework.
The weak assumption in the dragon example is that the believer of the dragon hasn't already tried using a CO2 meter. Don't underestimate the amount of historical questions packed behind the confusing answers you get when you ask someone to prove their dragon exists.
That being said, the dragon example does bring up a very awesome and valid point. If I took a few of those people who were in that congregation who prayed about cancer and asked them years later about the prayee's status... what would they say? Would they expect a change in their state? Would the cancer be gone? What do they expect from the prayer? My guess is that they wouldn't make any prediction.

Okay, then we have a logical link from C-platonic to D-platonic, and causal links descending from C-platonic to C-physical, E-platonic to E-physical, and D-platonic to D-physical to F-physical = D-physical xor E-physical. The idea being that when we counterfactualize on C-platonic, we update D-platonic and its descendents, but not E-platonic or its descendents.
I suppose that as written, this requires a rule, "for purposes of computing counterfactuals, keep in the causal graph rather than the logical knowledge base, any mathematical knowledge gained by observing a fact descended from your decision-output or any logical implications of your decision-output". I could hope that this is a special case of something more elegant, but it would only be hope.

The remarkable and depressing thing to me is that most people are not able to see it at a glance. To me it just seems like a string of obvious bluffs and non-sequiturs. Do you remember what was going on in your head when you didn't see it at a glance?

Not so realistic that you become a different person who never consented to being simulated, nor so realistic that "waking up" afterward equates to killing an innocent person and substituting the old you in their place.

In my view, the chief form of "dependence" that needs to be discriminated is inferential dependence and causal dependence. If earthquakes cause burglar alarms to go off, then we can infer an earthquake from a burglar alarm or infer a burglar alarm from an earthquake. Logical reasoning doesn't have the kind of directionality that causation does - or at least, classical logical reasoning does not - there's no preferred form between ~A->B, ~B->A, and A \/ B.
The link between the Platonic decision C and the physical decision D might be different from the link between the physical decision D and the physical observation F, but I don't know of anything in the current theory that calls for treating them differently. They're just directional causal links. On the other hand, if C mathematically implies a decision C-2 somewhere else, that's a logical implication that ought to symmetrically run backward to ~C-2 -> ~C, except of course that we're presumably controlling/evaluating C rather than C-2.
Thinking out loud here, the view is that your mathematical uncertainty ought to be in one place, and your physical uncertainty should be built on top of your mathematical uncertainty. The mathematical uncertainty is a logical graph with symmetric inferences, the physical uncertainty is a directed acyclic graph. To form controlling counterfactuals, you update the mathematical uncertainty, including any logical inferences that take place in mathland, and watch it propagate downward into the physical uncertainty. When you've already observed facts that physically depend on mathematical decisions you control but you haven't yet made and hence whose values you don't know, then those observations stay in the causal, directed, acyclic world; when the counterfactual gets evaluated, they get updated in the Pearl, directional way, not the logical, symmetrical inferential way.

Er... this actually has almost no implications for cryonics. You'd just repair the old brain in situ.

I'd think UFAIs would be much more likely to run faithful generic-intelligent-species simulations than Friendly AIs would be likely to run faithful ancestor simulations.


Or would you try to build one big graph that encompasses physical and logical facts alike, and then use Pearl's decision procedure without further modification?

I definitely want one big graph if I can get it.

Wait, isn't it decision-computation C--rather than simulation D--whose "effect" (in the sense of logical consequence) on E we're concerned about here?

Sorry, yes, C.

Even with the node structure you suggest, we can still infer E from C and from the physical node that matches (D xor E)--unless the new rule prohibits relying on that physical node, which I guess is the idea. But what exactly is the prohibition? Are we forbidden to infer any mathematical fact from any physical indicator of that fact?

No, but whenever we see a physical fact F that depends on a decision C/D we're still in the process of making plus Something Else (E), then we express our uncertainty in the form of a causal graph with directed arrows from C to D, D to F, and E to F. Thus when we compute a counterfactual on C, we find that F changes, but E does not.

Relevant link: http://lesswrong.com/lw/vh/complexity_and_intelligence/

Replying out of order:
2) A quick search of Google Scholar didn't net me a Chaitin definition of K-complexity for a structure. This doesn't surprise me much, as his uses of AIT in logic are much more oriented toward proof theory than model theory. Over here you can see some of the basic definitions. If you read page 7-10 and then my explanation to Silas here you can figure out what the K-complexity of a structure means. There's also a definition of algorithmic complexity of a theory in section 3 of the Chaitin.
According to these definitions, the complexity of N is about a few hundred bits for reasonable choices of machine, and the complexity of T(N) is &infty;.
1) It actually is pretty hard to characterize N extrinsically/intensionally; to characterize it with first-order statements takes infinite information (as above). The second-order characterization. by contrast, is a little hard to interpret. It takes a finite amount of information to pin down the model[*][PA2], but the second-order theory PA<sub>2</sub> still has infinite K-complexity because of its lack of complete rules of inference.
Intrinsic/extensional characterizations, on the other hand, are simple to do, as referenced above. Really, Godel Incompleteness wouldn't be all that shocking in the first place if we couldn't specify N any other way than its first-order theory! Interesting, yes, shocking, no. The real scandal of incompleteness is that you can so simply come up with a procedure for listing all the ground (quantifier-free) truths of arithmetic and yet passing either to or from the kind of generalizations that mathematicians would like to make is fraught with literally infinite peril.
3&4) Actually I don't think that Dawkins is talking about K-complexity, exactly. If that's all you're talking about, after all, an equal-weight puddle of boiling water has more K-complexity than a squirrel does. I think there's a more involved, composite notion at work that builds on K-complexity and which has so far resisted full formalization. Something like this, I'd venture.
The complexity of the natural numbers as a subject of mathematical study, while certainly well-attested, seems to be of a different sense than either K-complexity or the above. Further, it's unclear whether we should really be placing the onus of this complexity on N, on the semantics of quantification in infinite models (which N just happens to bring out), or on the properties of computation in general. In the latter case, some would say the root of the complexity lies in physics.
Also, I very much doubt that he had in mind mathematical structures as things that "exist". Whether it turns out that the difference in the way we experience abstractions like the natural numbers and concrete physical objects like squirrels is fundamental, as many would have it, or merely a matter of our perspective from within our singular mathematical context, as you among others suspect, it's clear that there is some perceptible difference involved. It doesn't seem entirely fair to press the point this much without acknowledging the unresolved difference in ontology as the main point of conflict.
Trying to quantify which thing is more complex is really kind of a sideshow, although an interesting one. If one forces both senses of complexity into the K-complexity box then Dawkins "wins", at the expense of both of your being turned into straw men. If one goes by what you both really mean, though, I think the complexity is probably incommensurable (no common definition or scale) and the comparison is off-point.
5) Thank you. I hope the discussion here continues to grow more constructive and helpful for all involved.

Um... I'm not sure there's much I can say to that beyond "Read Probabilistic Reasoning in Intelligent Systems, or Causality".
Pearl's system is not ad-hoc. It is very not ad-hoc. It has a metric fuckload of math backing up the simple rules. But Pearl's system does not include logical uncertainty. I'm trying to put logical uncertainty into it, while obeying the rules. This is a work in progress.

Read Pearl. I think his online intros should give you a good idea of what the cocktail napkin looks like.

Frankly, you don't strike me as genuinely open to persuasion, but for the sake of any future readers I'll note the following:
1) I expect cryonics patients to actually be revived by artificial superintelligences subsequent to an intelligence explosion. My primary concern for making sure that cryonicists get revived is Friendly AI.
2) If this were not the case, I'd be concerned about the people running the cryonics companies. The cryonicists that I have met are not in it for the money. Cryonics is not an easy job or a wealthy profession! The cryonicists I have met are in it because they don't want people to die. They are concerned with choosing successors with the same attitude, first because they don't want people to die, and second because they expect their own revivals to be in their hands someday.

Logical uncertainty has always been more difficult to deal with than physical uncertainty; the problem with logical uncertainty is that if you analyze it enough, it goes away. I've never seen any really good treatment of logical uncertainty.
But if we depart from TDT for a moment, then it does seem clear that we need to have causelike nodes corresponding to logical uncertainty in a DAG which describes our probability distribution. There is no other way you can completely observe the state of a calculator sent to Mars and a calculator sent to Venus, and yet remain uncertain of their outcomes yet believe the outcomes are correlated. And if you talk about error-prone calculators, two of which say 17 and one of which says 18, and you deduce that the "Platonic answer" was probably in fact 17, you can see that logical uncertainty behaves in an even more causelike way than this.
So, going back to TDT, my hope is that there's a neat set of rules for factoring our logical uncertainty in our causal beliefs, and that these same rules also resolve the sort of situation that you describe.
If you consider the notion of the correlated error-prone calculators, two returning 17 and one returning 18, then the most intuitive way to handle this would be to see a "Platonic answer" as its own causal node, and the calculators as error-prone descendants. I'm pretty sure this is how my brain is drawing the graph, but I'm not sure it's the correct answer; it seems to me that a more principled answer would involve uncertainty about which mathematical fact affects each calculator - physically uncertain gates which determine which calculation affects each calculator.
For the (D xor E) problem, we know the behavior we want the TDT calculation to exhibit; we want (D xor E) to be a descendant node of D and E. If we view the physical observation of $1m as telling us the raw mathematical fact (D xor E), and then perform mathematical inference on D, we'll find that we can affect E, which is not what we want. Conversely if we view D as having a physical effect, and E as having a physical effect, and the node D xor E as a physical descendant of D and E, we'll get the behavior we want. So the question is whether there's any principled way of setting this up which will yield the second behavior rather than the first, and also, presumably, yield epistemically correct behavior when reasoning about calculators and so on.
That's if we go down avenue (2). If we go down avenue (1), then we give primacy to our intuition that if-counterfactually you make a different decision, this logically controls the mathematical fact (D xor E) with E held constant, but does not logically control E with (D xor E) held constant. While this does sound intuitive in a sense, it isn't quite nailed down - after all, D is ultimately just as constant as E and (D xor E), and to change any of them makes the model equally inconsistent.
These sorts of issues are something I'm still thinking through, as I think I've mentioned, so let me think out loud for a bit.
In order to observe anything that you think has already been controlled by your decision - any physical thing in which a copy of D has already played a role - then (leaving aside the question of Omega's strategy that simulated alternate versions of you to select a self-consistent problem, and whether this introduces conditional-strategy-dependence rather than just decision-dependence into the problem) there have to be other physical facts which combine with D to yield our observation.
Some of these physical facts may themselves be affected by mathematical facts, like an implemented computation of E; but the point is that in order to have observed anything controlled by D, we already had to draw a physical, causal diagram in which other nodes descended from D.
So suppose we introduce the rule that in every case like this, we will have some physical node that is affected by D, and if we can observe info that depends on D in any way, we'll view the other mathematical facts as combining with D's physical node. This is a rule that tells us not to draw the diagram with a physical node being determined by the mathematical fact D xor E, but rather to have a physical node determined by D, and then a physical descendent D xor E. (Which in this particular problem should descend from a physical node E that descends from the mathematical fact E, because the mathematical fact E is correlated with our uncertainty about other things, and a factored causal graph should have no remaining correlated sources of background uncertainty; but if E didn't correlate to anything else in particular, we could just have D descending to (D xor E) via the (xor with E) rule.)
When I evaluate this proposed solution for ad-hoc-ness, it does admittedly look a bit ad-hoc, but it solves at least one other problem than the one I started with, and which I didn't think of until now. Suppose Omega tells me that I make the same decision in the Prisoner's Dilemma as Agent X. This does not necessarily imply that I should cooperate with Agent X. X and I could have made the same decision for different (uncorrelated) reasons, and Omega could have simply found out by simulating the two of us that X and I gave the same response. In this case, presumably defecting; but if I cooperated, X wouldn't do anything differently. X is just a piece of paper with "Defect" written on it.
If I draw a causal diagram of how I came to learn this correlation from Omega, and I follow the rule of always drawing a causal boundary around the mathematical fact D as soon as it physically affects something, then, given the way Omega simulated both of us to observe the correlation, I see that D and X separately physically affected the correlation-checker node.
On the other hand, if I can analyze the two pieces of code D and X and see that they return the same output, without yet knowing the output, then this knowledge was obtained in a way that doesn't involve D producing an output, so I don't have to draw a hard causal boundary around that output.
If this works, the underlying principle that makes it work is something along the lines of "for D to control X, the correlation between our uncertainty about D and X has to emerge in a way that doesn't involve anyone already computing D". Otherwise D has no free will (said firmly tongue-in-cheek). I am not sure that this principle has any more elegant expression than the rule, "whenever, in your physical model of the universe, D finishes computing, draw a physical/causal boundary around that finished computation and have other things physically/causally descend from it".
If this principle is violated then D ends up "correlated" to all sorts of other things we observe, like the price of fish and whether it's raining outside, via the magic of xor.

Thanks for looking into this for us!
One of my ever-pending posts to write is on what sort of simple interface might prevent online arguments from retracing the same points over and over. I suspect it will not be graphical with boxes, because that makes poor use of screen real estate. I suspect it will not have lots of fancy argument types and patterns, because no one really uses that stuff. I think it does need to have a karma system, because otherwise there's no way to find the good stuff.

Reasoning by perceptual recognition. Cryonics seems weird and involves money, therefore it's perceptually recognized as a scam. The fact that it would be immensely labor-intensive to develop the suspension tech, isn't marketed well or at all really, and would have a very poor payoff on invested labor as scams go, will have little impact on this. The lightning-fast perceptual system hath spoken.
I'm surprised that you say your friends are computer programmers. Programmers need to be capable of abstract thought.

This would require cryonics companies to lie about their finances. Otherwise they have no way to extract money from their reserves without alarming customers.

Pearl's account doesn't include logical uncertainty at all so far as I know, but I made my case here
http://lesswrong.com/lw/15z/ingredients_of_timeless_decision_theory/
that Pearl's account has to be modified to include logical uncertainty on purely epistemic grounds, never mind decision theory.
If this isn't what you're asking about then please further clarify the question?

Omega can use the following algorithm:
"Simulate telling the human that they got the answer wrong. If in this case they get the answer wrong, actually tell them that they get the answer wrong. Otherwise say nothing."
This ought to make it relatively easy for Omega to truthfully put you in a "you're screwed" situation a fair amount of the time. Albeit, if you know that this is Omega's procedure, the rest of the time you should figure out what you would have done if Omega said "you're wrong" and then do that.
This kind of thinking is, I think, outside the domain of current TDT, because it involves strategies that depend on actions you would have taken in counterfactual branches. I think it may even be outside the domain of current UDT for the same reason.


why would a human, or AI, do the work necessary to verify the opponent's precommitment, when it would be better off if the opponent couldn't precommit?

Because the AI has already precommitted to go ahead and carry through the threat anyway if you refuse to inspect its code.

Surely most humans would be too dumb to understand such a proof? And even if you could understand it, how does the AI convince you that it doesn't contain a deliberate flaw that you aren't smart enough to find? Or even better, you can just refuse to look at the proof. How does the AI make its precommitment credible to you if you don't look at the proof?
EDIT: I realized that the last two sentences are not an advantage of being dumb, or human, since AIs can do the same thing. This seems like a (separate) big puzzle to me: why would a human, or AI, do the work necessary to verify the opponent's precommitment, when it would be better off if the opponent couldn't precommit?
EDIT2: Sorry, forgot to say that you have a good point about simulation not necessary for verifying precommitment.

Um... first of all, you've got a signed contract. Second, they screw over one customer and all their other customers leave. Same as for any other business. Focusing on this in particular sounds like a rationalization of a wiggy reaction.

http://lesswrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/

By "unsolvable" I mean that you're screwed over in final outcomes, not that TDT fails to have an output.
The interesting part of the problem is that, whatever you decide, you deduce facts about the background such that you know that what you are doing is the wrong thing. However, if you do anything differently, you would have to make a different deduction about the background facts, and again know that what you were doing was the wrong thing. Since we don't believe that our decision is capable of affecting the background facts, the background facts ought to be a fixed constant, and we should be able to alter our decision without affecting the background facts... however, as soon as we do so, our inference about the unalterable background facts changes. It's not 100% clear how to square this with TDT.


If the subject of the problem will two-box if he sees the big box has the million dollars, but will one-box if he sees the big box is empty. Then there is no action Omega could take to satisfy the conditions of the problem.

In this case the paradox lies within having made a false statement about Omega, not about TDT. In other words, it's not a problem with the decision theory, but a problem with what we supposedly believe about Omega.
But yes, whenever you suppose that the agent can observe an effect of its decision before making that decision, there must be given a consistent account of how Omega simulates possible versions of you that see different versions of your own decision, and on that basis selects at least one consistent version to show you. In general, I think, maximizing may require choosing among possible strategies for sets of conditional responses. And this indeed intersects with some of the open issues in TDT and UDT.
This is what I was alluding to by saying, "The exact details here will depend on how I believe the simulator chose to tell me this".

And this was my reply:
This is an unfinished part of the theory that I've also thought about,
though your example puts it very crisply (you might consider posting
it to LW?)
My current thoughts on resolution tend to see two main avenues:
1) Construct a full-blown DAG of math and Platonic facts, an account
of which mathematical facts make other mathematical facts true, so
that we can compute mathematical counterfactuals.
2) Treat differently mathematical knowledge that we learn by
genuinely mathematical reasoning and by physical observation. In this
case we know (D xor E) not by mathematical reasoning, but by
physically observing a box whose state we believe to be correlated
with D xor E. This may justify constructing a causal DAG with a node
descending from D and E, so a counterfactual setting of D won't affect
the setting of E.
Currently I'd say that (2) looks like the better avenue. Can you come
up with an improper mathematical dependency where E is inferred from
D, and shouldn't be seen as counterfactually affected, based on
mathematical reasoning only without postulating the observation of a
physical variable that descends from both E and D?
Incidentally, note that an unsolvable problem that should stay
unsolvable is as follows: I'm asked to pick red or green, and told "A
simulation of you given this information as well picked the wrong
color and got shot." Whichever choice I make, I deduce that the other
choice was better. The exact details here will depend on how I
believe the simulator chose to tell me this, but ceteris paribus it's
an unsolvable problem.

That was eloquent, but... I honestly don't understand why you couldn't just sign up for cryonics and then get on with your (first) life. I mean, I get that I'm the wrong person to ask, I've known about cryonics since age eleven and I've never really planned on dying. But most of our society is built around not thinking about death, not any sort of rational, considered adaptation to death. Add the uncertain prospect of immortality and... not a whole lot changes so far as I can tell.
There's all the people who believe in Heaven. Some of them are probably even genuinely sincere about it. They think they've got a certainty of immortality. And they still walk on two feet and go to work every day.

Fair 'nuff.

I'm not sure I understand the point of this argument... since I always push the "Reset" button in that situation too, an AI who knows me well enough to simulate me knows that there's no point in making the threat or carrying it out.

Research like this seems very hopeful to me. It breaks down into a nice component describing what people actually want and a lot of other components describing shifts of attention and noise. If anything, that seems too optimistic compared to, say, prospect theory, in which the basic units of motivation are shifts from a baseline and there's no objective baseline or obvious way to translate shift valuations into fixed-level valuations.


or retaliating spitefully

I like it. Splicing some altruistic punishment into TDT/UDT might overcome the signalling problem.

That's not a splice. It ought to be emergent in a timeless decision theory, if it's the right thing to do.

Teaching requires empathy. A small application of empathy should tell you that no one in your audience will take your word for granted on this. This lack of trust could be very easily remedied with a single example: "cannot be dismissed as easily as you believe, because of some refuting point that wasn't brought up in any of the previous discussions, as you will also see wasn't brought up in any previous discussions".
I honestly don't see any valid reason to wait for a debate with komponisto in particular to provide this sort of evidence, either; which makes it sound suspiciously like an excuse for delay.


As someone who has no interest in the case (I haven't made any comment related to the case, nor read any of the discussions), judging from just the posts,

With respect, I suggest reading the discussions, not just the posts, before making any negative judgment of komponisto.

How can we possibly know what your comparative advantage is, better than you do? In all seriousness, a certain amount of background information seems to be missing here.

I'm not sure, but I think it's impossible to construct a computable nonstandard model of the integers (one where you can implement operations like +).

People liked it.

I did not have any idea what komponisto thought of the matter until I started reading the Wikipedia article. After that it was pretty obvious why komponisto selected the case.


Why expect that we have a canonical model when we talk about sets or predicates if we're entertaining skepticism that we have a canonical model for integer-talk?

We don't. Skepticism of sets, predicates, and canonical integers are all the same position in the debate.

And then responding to everyone by saying "but I want to talk about C1 and you're not talking about C1" gave it the appearance of downright trolling. If I didn't have previous familiarity with you, that's what I'd have assumed, actually.
You might say, "But they didn't directly address C1!"
But no one cared, at that point, what you wanted them to address. You didn't have enough credit built up with them to steer the conversation - and objecting that it wasn't going where you wanted it to go just pissed them off further. If you'd engaged with them on their points, you could have built up credibility. As it is, the post was just dropped into the void.

Example problem: You stepped into a giant past discussion and didn't refer to it. If, for each point, you had either pointed to and refuted previous comments about those points, or else said, "And I read through the comments and found no reference to this point", you would have been picking up the conversation where it left off. As it is, the reaction is more like, "Oh, same points being rehashed again and ignoring the previous conversation we had about it." This reaction was sufficiently severe that no one bothered to talk about your points - so far as they were concerned, it had probably been already discussed and refuted in the past conversation, since you didn't bother to refer back to it.
My own impression was that you hadn't read the ~700 comments on the previous two posts.

Just as the wise FAI will ignore threats of torture, so too the wise paperclipper will ignore threats to destroy paperclips, and listen attentively to offers to make new ones.
Of course classical causal decision theorists get the living daylights exploited out of them, but I think everyone on this website knows better than to two-box on Newcomb by now.


This kind of extortion also seems like a general problem for FAIs dealing with UFAIs. An FAI can be extorted by threats of torture (of simulations of beings that it cares about), but a paperclip maximizer can't.

It can. Remember "true prisoner's dilemma": one paperclip may be fair trade of a billion lives. The threat to NOT make a paperclip also works fine: the only thing you need is two counterfactual-options where one of them is paperclipper-worse than then other, chosen conditionally on paperclipper's cooperation.

Sure, and that's the age-old argument for why we should not take second-order logic at face value. But in this case we cannot go around blithely talking about the integers for there is no language we could use to speak of them, or any other infinite set. We would be forbidden of saying that there is something we cannot talk about, and this is not surprising - what is it you can't refer to?


Whether you accept on TDT/UDT depends on why the AI started torturing them. If it did so to blackmail you, you should turn the offer down. If, on the other hand, it started torturing them because it enjoyed doing so, then its offer is positive sum and should be accepted.

Correct. But this reaches into the arbitrary past, including a decision a billion years ago to enjoy something in order to provide better blackmail material.

There's also the issue of mistakes - what to do with an AI that mistakenly thought you were not using TDT/UDT, and started the torture for blackmail purposes (or maybe it estimated that the likelyhood of you using TDT/UDT was not quite 1, and that it was worth trying the blackmail anyway)?

Ignoring it or retaliating spitefully are two possibilities.

It'd help if you picked a group belief that's actually demonstrably wrong to illustrate this, but you picked a hella wrong target this time around. Did you read through ~700 or so comments on the other Knox posts, or only the original posts?

A decision procedure is a finite specification of all truths of euclidean geometry; I can use that finite fact anywhere I could use any truth of geometry. I suppose there is a difference, but even so, it's the wrong thing to say in a Godelian discussion.

Considering the extraordinary rarity of good explainers in this entire civilization, I'm saddened to say that talent may have something to do with it, not just practice.

I should note that the ability to explain things isn't the same attribute as intelligence. I am lucky enough to have it. Other legitimately intelligent people do not.

So, Rolf, do you understand how this post failed? Hint: Others are not to blame.


But my dilemma is that Chris Langan is the smartest known living man, which makes it really hard for me to shrug the CTMU off as nonsense.

Eh, I'm smart too. Looks to me like you were right the first time and need to have greater confidence in yourself.

If it actually worked, I wouldn't question it afterward. I try not to argue with superintelligences on occasions when they turn out to be right.
In advance, I have to say that the risk/reward ratio seems to imply an unreasonable degree of certainty about a noisy human brain, though.

Sure, if you believe everything you see in the movies, but that seems like obvious Rebel propaganda to me.

To take advantage of professional specialization, gains from trade, capital infrastructure, comparative advantage, and economies of scale, the way grownups do it when they actually care, I'd say that the activist is the one who pays someone else to clean up the river.

I've decided to spend today abstractly worrying about sharks.

Note that Landsburg is thus also incorrect in saying "we can never know all the truths of euclidean geometry".


no set of axioms suffices to specify the standard model of arithmetic (i.e. to distinguish it from other models).

Then what do you mean when you say "integers"^H^H "natural numbers", if no set of premises suffices to talk about it as opposed to something else?
Anyway, no countable set of first-order axioms works. But a finite set of second-order axioms work. So to talk about the natural numbers, it suffices merely to think that when you say "Any predicate that is true of zero, and is true of the successor of every number it is true of, is true of all natural numbers" you made sense when you said "any predicate".
It is this sort of minor-seeming yet important technical inaccuracy that separates "The Big Questions" from "Good and Real", I'm afraid.


People say things like this here something like ten to a hundred times more frequently than they do anywhere else I have ever frequented.

That's still not much.

As I always press the "Reset" button in situations like this, I will never find myself in such a situation.
EDIT: Just to be clear, the idea is not that I quickly shut off the AI before it can torture simulated Eliezers; it could have already done so in the past, as Wei Dai points out below. Rather, because in this situation I immediately perform an action detrimental to the AI (switching it off), any AI that knows me well enough to simulate me knows that there's no point in making or carrying out such a threat.

Well, suppose the situation is arbitrarily worse - you can only prevent 3^^^3 dustspeckings by torturing millions of sentient beings.

It seems obvious that the correct answer is simply "I ignore all threats of blackmail, but respond to offers of positive-sum trades" but I am not sure how to derive this answer - it relies on parts of TDT/UDT that haven't been worked out yet.

First-order logic can't distinguish between different sizes of infinity. Any finite or countable set of first-order statements with an infinite model has models of all sizes.
However, if you take second-order logic at face value, it's actually quite easy to uniquely specify the integers up to isomorphism. The price of this is that second-order logic is not complete - the full set of semantic implications, the theorems which follow, can't be derived by any finite set of syntactic rules.
So if you can use second-order statements - and if you can't, it's not clear how we can possibly talk about the integers - then the structure of integers, the subject matter of integers, can be compactly singled out by a small set of finite axioms. However, the implications of these axioms cannot all be printed out by any finite Turing machine.
Appropriately defined, you could state this as "finitely complex premises can yield infinitely complex conclusions" provided that the finite complexity of the premises is measured by the size of the Turing machine which prints out the axioms, yielding is defined as semantic implication (that which is true in all models of which the axioms are true), and the infinite complexity of the conclusions is defined by the nonexistence of any finite Turing machine which prints them all.
However this is not at all the sort of thing that Dawkins is talking about when he talks about evolution starting simple and yielding complexity. That's a different sense of complexity and a different sense of yielding.

Now censoring replies by DWCrmcm.

I should also mention that, judging from the stories I've heard, it's a lot easier to talk about your doubts with your spouse when they're doubts. I presume you have a wife and kids and parents and siblings and local community who are all deeply religious? I don't know about the others, but the sooner you start talking to your wife about your doubts, the more likely you are to stay together as you go down whatever path you go down.

Expected utility. It's more powerful than the Force.

Yeah, let me do it.


It's amazing the things people would rather have than money.

-- Garfield


The object of opening the mind, as of opening the mouth, is to close it again on something solid.

-- G.K. Chesterton


Who am I to judge myself?

-- Karp


Karl Marx's writings glorifying communism (though Western capitalists regard it as grim and joyless) may well have reflected merely his alienation from society due to a lifelong series of excruciatingly painful boils, according to a recent British Journal of Dermatology article. In an 1867 letter, Marx wrote, "The bourgeoisie will remember my carbuncles until their dying day." [Reuters, 10-30-07]

-- News of the Weird (relevance)


Organizations don't suffer pathologies; they are intrinsically pathological constructs. Idealized organizations are not perfect. They are perfectly pathological.

-- http://www.ribbonfarm.com/2009/10/07/the-gervais-principle-or-the-office-according-to-the-office/

That was mean. In spirit, and in context.
This was not spam. I have nothing to sell.
But I will not shrink from stating my beliefs and sorting out theoretical claptrap and mythology from the truth that technology presents.
Are you declaring the lever and the pendulum to be untrue?
You talk about value as if you know what it means.
Please state your clear and precise definitions for Value and Intelligence.
Artificial intelligence is at best an oxymoron.
Do you really think you can computer program intelligence !?
http://www.facebook.com/group.php?gid=248127242832

AFAIK these things just get more difficult the longer you put them off. This is the usual rule, and it's also the usual rule that people are heavily motivated on a cognitive level to find excuses to let things slide. Someone wrote about this very eloquently - I'm not sure who, possibly Tim Ferris or Robert Greene - with the notion that "hoping" things will get better isn't really hope so much as a form of passivity, motivated more by fear of action and change than any positive hope. Any delay of this sort should have a definite deadline attached to it.

Blueberry, the human species has got to do this sometime. Please don't get in the way.


The one thing I will say now is that it would completely wreck almost every aspect of my life. I have everything invested in this.

Wow. Then it's not at all surprising you feel this way. You've left out a lot of details of your life, so I can't really comment on specifics (though please feel free to share them if you're ever ready to do so here). But given that, it's going to be almost impossible for you to change that belief.
I'm very confident that a detailed, unbiased examination of your theistic beliefs would reveal that there's no evidence for them and you hold them for social reasons. Do you agree? That being the case, you may not want to try to engage in this kind of examination right now. It sounds like you need time to think about what you really want in your life, and what kind of life you want to lead, independent of your beliefs about theism. Do you want to uproot your life right now?


If the child-rescuer and child-murderer seem to be feeling the same emotions, having the same experience of righteousness, when imagining their opposite acts, would you still conclude that it is a mistranslation/misuse to identify our word "morality" with whatever word the righteous-feeling child-murderer is using for what appears to be the same feeling?

Depends. If the child-murderer knew everything about the true state of affairs and everything about the workings of their own inner mind, would they still disagree with the child-rescuer? If so, then it's pretty futile to pretend that they're talking about the same subject matter when they talk about that-which-makes-me-experience-a-feeling-of-being-justified. It would be like if one species of aliens saw green when contemplating real numbers and another species of aliens saw green when contemplating ordinals; attempts to discuss that-which-makes-me-see-green as if it were the same mathematical subject matter are doomed to chaos. By the way, it looks to me like a strong possibility is that reasonable methods of extrapolating volitions will give you a spread of extrapolated-child-murderers some of which are perfectly selfish hedonists, some of which are child-rescuers, and some of which are Babyeaters.
And yes, this was the approximate point of the Babyeater thought experiment.

Gary Drescher's "Good and Real" is an example of this sort of Deep Book done right. Landsburg seems to make a lot more errors - like he tried to write Good and Real but failed.

http://home.netcom.com/~rogermw2/force_skeptics.html
This page persuaded me, by the way - I am now a Force Skeptic with respect to the Star Wars universe.


You could observe how it acts in its simulated world, and hope it would act in a similar way if released into our world.

Sounds like a rather drastic context change, and a rather forlorn hope if the AI figures out that it's being tested.


Possible solution: I think there are ways to write it a program such that even if it inferred our existence, it would optimize away from us, rather than over us. Loosely: A goal like "I need to organize these instructions within this block of memory to solve a problem specified at address X." needs to be implemented such that it produces a subgoal like "I need to write a subroutine to patch over the fact that an error in the VM I'm running on gives me a window of access into a universe with huge computation resources and godlike power over my memory space, so that my solution get get the right answer to it's arithmetic and sole the puzzle." It should want to do things in a way that isn't cheating.

Marcello had a crazy idea for doing this; it's the only suggestion for AI-boxing I've ever heard that doesn't have an obvious cloud of doom hanging over it. However, you still have to prove stability of the boxed AI's goal system.

If you've got the time, use a little horde of stick figures, entering into a testing machine and with test-positive results getting spit out.

Non-Born rules give us anthropic superpowers. It is plausibly the case that the laws of reality are such that no anthropic superpowers are ever possible, and that this is a quickie explanation for why the laws of reality give rise to the Born rules. One would still like to know what, exactly, these laws are.
To put it another way, the universe runs on causality, not modus tollens. Causality is rules like "and then, gravity accelerates the bowling ball downward". Saying, "Well, if the bowling ball stayed up, we could have too much fun by hanging off it, and the universe won't let us have that much fun, so modus tollens makes the ball fall downward" isn't very causal.

I don't see anything here that is not a mixture of physical facts and logical facts (that is, truths about causal events and truths about premise-conclusion links). Physical computers within our universe may be neatly described by compact axioms. Logic (in my not-uncommon view) deals with semantic implication: what is true in a model given that the axioms are true of it. If you prove P!=NP using axioms that happen to apply to the computers of this universe then P!=NP for them as well, and the axioms will have been picked out to be applicable to real physics - a mixture of physical fact and logical fact. I don't know where logical facts are stored or what they are, just as I don't yet know what makes the universe real, although I repose some confidence that the previous two questions are wrong - but so far I'm standing by my view that truths are about causal events, logical implications, or some mix of the two.
Axioms are that which mathematicians use to talk about integers instead of something else. You could also take the perspective of trying to talk about groups of two pebbles as they exist in the real world, and wanting your axioms to correspond to their behavior. But when you stop looking at the real world and close your eyes and try to do math, then in order to do math about something, like about the integers, about these abstract objects of thought that you abstracted away from the groups of pebbles, you need axioms that identify the integers in mathspace. And having thus gained a subject of discourse, you can use the axioms to prove theorems that are about integers because the theorems hold wherever the axioms hold. And if those axioms are true of physical reality from the appropriate standpoint, your conclusions will also hold of groups of pebbles.

Perhaps more to the point, do you agree that there is a coherent meta-ethical position that does deserve to be called moral realism, which asserts that moral and meta-moral computations are about something outside of individual humans or humanity as a whole (even if we're not sure how that works)?

That depends; is morality a subject matter that we need premises to identify in subjectspace, in order to talk about morality rather than something else, stored in that same mysterious place as 2 + 2 = 4 being true of the integers but needing axioms to talk about the integers in the first place? Or are we talking about transcendent ineffable compelling stuff? The first view is, I think, coherent; I should think so, it's my own. The second view is not.

This is exactly what I refer to as a "logical fact" or "which conclusions follow from which premises". Wasn't that clear?
Actually, I guess it could be a bit less clear if you're not already used to thinking of all math as being about theorems derived from axioms which are premise-conclusion links, i.e., if the axioms are true of a model then the theorem is true of that model. Which is, I think, conventional in mathematics, but I suppose it could be less obvious.
In the case of P!=NP, you'll still need some axioms to prove it, and the axioms will identify the subject matter - they will let you talk about computations and running time, just as the Peano axioms identify the subject matter of the integers. It's not that you can make 2 + 2 = 5 by believing differently about the same subject matter, but that different axioms would cause you to be talking about a different subject matter than what we name the "integers".
Is this starting to sound a little familiar?

